Loading python/3.10.12-gcc-11.3.1-n4zmj3v
  Loading requirement: bzip2/1.0.8-gcc-11.3.1-iysrubl
    libmd/1.0.4-gcc-11.3.1-h5w65w6 libbsd/0.11.7-gcc-11.3.1-cnufgdy
    expat/2.5.0-gcc-11.3.1-kxak63w ncurses/6.4-gcc-11.3.1-abkwnn7
    readline/8.2-gcc-11.3.1-akb2nsr gdbm/1.23-gcc-11.3.1-wysplva
    libiconv/1.17-gcc-11.3.1-6duptdz xz/5.4.1-gcc-11.3.1-ugsg6m4
    zlib/1.2.13-gcc-11.3.1-7nnzgv7 libxml2/2.10.3-gcc-11.3.1-teyofav
    tar/1.34-gcc-11.3.1-vf67e5f gettext/0.21.1-gcc-11.3.1-k6kxxpq
    libffi/3.4.4-gcc-11.3.1-a54yiv5 libxcrypt/4.4.33-gcc-11.3.1-cocwhr5
    openssl/1.1.1u-gcc-11.3.1-vhqbhs2 sqlite/3.42.0-gcc-11.3.1-zztfjr4
    util-linux-uuid/2.38.1-gcc-11.3.1-puhtnhk
/var/spool/slurmd/job47233190/slurm_script: line 19: nvcc: command not found
2025-02-10 17:25:22.848938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739237122.866756  949742 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739237122.872973  949742 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-10 17:25:22.896271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Arguments: {'submodel': 'signalEncoderModel', 'optimizerType': 'NAdam', 'irreversibleLearningProtocol': 'FC', 'reversibleLearningProtocol': 'rCNN', 'deviceListed': 'HPC-GPU', 'encodedDimension': 512, 'operatorType': 'wavelet', 'initialProfileAmp': 0.001, 'angularThresholdMax': 45.0, 'angularThresholdMin': 1.0, 'numSpecificEncoderLayers': 1, 'numSharedEncoderLayers': 2, 'cullingEpoch': 1, 'profileDimension': 128, 'numProfileShots': 32, 'percentParamsKeeping': 4, 'numBasicEmotions': 6, 'numActivityModelLayers': 4, 'activityLearningRate': 0.1, 'numEmotionModelLayers': 4, 'emotionLearningRate': 0.01, 'numActivityChannels': 4, 'profileLR': 0.067, 'reversibleLR': 0.0003, 'physGenLR': 0.0001, 'profileWD': 0.0, 'reversibleWD': 0.0, 'physGenWD': 0.0, 'momentum_decay': 0.001, 'beta1': 0.7, 'beta2': 0.8, 'neuralOperatorParameters': {'wavelet': {'waveletType': 'bior3.1', 'encodeHighFrequencyProtocol': 'highFreq', 'encodeLowFrequencyProtocol': 'lowFreq'}, 'fourier': {'encodeImaginaryFrequencies': True, 'encodeRealFrequencies': True}}}
2025-02-10 signalEncoder cull1 Deg4-1-0-45-0 NAdam 2-shared specific-1 WD0-0-0-0-0-0 LR0-067-0-0003-0-0001 profileParams128 numShots32 encodedDim512 bior3-1 

Reading in metadata for wesad
Reading in metadata for amigos
Reading in metadata for dapper
Reading in metadata for case
Reading in metadata for emognition
Reading in data for empatch

Splitting the data into meta-models:
	Wesad: Found 32 (out of 32) emotions across 60 experiments for 28 signals with 4.0 batches of 15 experiments
	Amigos: Found 12 (out of 12) emotions across 673 experiments for 60 signals with 15.651 batches of 43 experiments
	Dapper: Found 12 (out of 12) emotions across 364 experiments for 15 signals with 15.167 batches of 24 experiments
	Case: Found 2 (out of 2) emotions across 1442 experiments for 35 signals with 15.846 batches of 91 experiments
	Emognition: Found 12 (out of 12) emotions across 407 experiments for 39 signals with 15.654 batches of 26 experiments
Loading in previous metaTrainingModels weights and attributes

Splitting the data into models:
	Empatch: Found 30 (out of 30) emotions across 165 experiments for 28 signals with 7.5 batches of 22 experiments
Loading in previous metaTrainingModels weights and attributes

Training amigos model
Final encoder loss: 0.18076977133750916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 1.4288914203643799 0.25820159912109375

Final encoder loss: 0.18783259391784668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4988980293273926 0.07933568954467773

Final encoder loss: 0.18363294005393982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47797107696533203 0.08363080024719238

Final encoder loss: 0.16775192320346832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4848175048828125 0.07627344131469727

Final encoder loss: 0.17204800248146057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4720621109008789 0.07512378692626953

Final encoder loss: 0.16968141496181488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4600865840911865 0.07637763023376465

Final encoder loss: 0.16063810884952545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.488020658493042 0.07743597030639648

Final encoder loss: 0.1636233627796173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4943735599517822 0.07204198837280273

Final encoder loss: 0.16271454095840454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4790189266204834 0.07693243026733398

Final encoder loss: 0.1567876935005188
Final encoder loss: 0.15901179611682892
Final encoder loss: 0.1589447408914566

Training dapper model
Final encoder loss: 0.2024422287940979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11935782432556152 0.037705421447753906

Final encoder loss: 0.20820505917072296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11872053146362305 0.03436708450317383

Final encoder loss: 0.18247084319591522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.12034749984741211 0.03464770317077637

Final encoder loss: 0.1885935366153717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11967182159423828 0.0336761474609375

Final encoder loss: 0.17156347632408142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11871623992919922 0.03393363952636719

Final encoder loss: 0.17796273529529572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11913943290710449 0.03464031219482422

Final encoder loss: 0.1646665632724762
Final encoder loss: 0.17211948335170746

Training case model
Final encoder loss: 0.20296335220336914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.31256890296936035 0.05436134338378906

Final encoder loss: 0.18890757858753204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.31173062324523926 0.05124163627624512

Final encoder loss: 0.1901501566171646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.3057899475097656 0.05143857002258301

Final encoder loss: 0.19218802452087402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.3097069263458252 0.052368879318237305

Final encoder loss: 0.1808108389377594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.30913281440734863 0.05238962173461914

Final encoder loss: 0.19192324578762054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.302459716796875 0.0544431209564209

Final encoder loss: 0.19152244925498962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2984437942504883 0.052927494049072266

Final encoder loss: 0.17841032147407532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.31380367279052734 0.050766944885253906

Final encoder loss: 0.17936472594738007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.31134724617004395 0.0531156063079834

Final encoder loss: 0.18139614164829254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.3080275058746338 0.05358433723449707

Final encoder loss: 0.16962659358978271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.313037633895874 0.051603078842163086

Final encoder loss: 0.1800154745578766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2895324230194092 0.052120208740234375

Final encoder loss: 0.1875135898590088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.3104548454284668 0.0522000789642334

Final encoder loss: 0.17532624304294586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.31496119499206543 0.05315995216369629

Final encoder loss: 0.17640767991542816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.30743908882141113 0.05326986312866211

Final encoder loss: 0.17839564383029938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.30879783630371094 0.052634239196777344

Final encoder loss: 0.16641320288181305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.30504417419433594 0.05291247367858887

Final encoder loss: 0.1765592098236084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.28810834884643555 0.05022621154785156

Final encoder loss: 0.18664057552814484
Final encoder loss: 0.1739414781332016
Final encoder loss: 0.1742793470621109
Final encoder loss: 0.17581872642040253
Final encoder loss: 0.16281984746456146
Final encoder loss: 0.1722792685031891

Training emognition model
Final encoder loss: 0.19356486201286316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.27602601051330566 0.054619550704956055

Final encoder loss: 0.19496355950832367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25954723358154297 0.05171942710876465

Final encoder loss: 0.18289761245250702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.255565881729126 0.04808759689331055

Final encoder loss: 0.18464766442775726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2558481693267822 0.04994821548461914

Final encoder loss: 0.17797206342220306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2559356689453125 0.04764509201049805

Final encoder loss: 0.18006041646003723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2561335563659668 0.04877638816833496

Final encoder loss: 0.17517009377479553
Final encoder loss: 0.17762690782546997

Training empatch model
Final encoder loss: 0.17116133868694305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1821279525756836 0.046335697174072266

Final encoder loss: 0.15885302424430847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18266844749450684 0.042119741439819336

Final encoder loss: 0.1531844139099121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.19826912879943848 0.04356718063354492

Final encoder loss: 0.1494714766740799

Training wesad model
Final encoder loss: 0.2155998945236206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11616277694702148 0.03600287437438965

Final encoder loss: 0.20687177777290344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1077580451965332 0.0321497917175293

Final encoder loss: 0.20294223725795746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10811758041381836 0.033266305923461914

Final encoder loss: 0.20038942992687225

Calculating loss for amigos model
	Full Pass 0.720644474029541
numFreeParamsPath 18
Reconstruction loss values: 0.15920010209083557 0.1556355506181717

Calculating loss for dapper model
	Full Pass 0.1605546474456787
numFreeParamsPath 18
Reconstruction loss values: 0.16872186958789825 0.16781361401081085

Calculating loss for case model
	Full Pass 0.9009900093078613
numFreeParamsPath 18
Reconstruction loss values: 0.17514429986476898 0.17356431484222412

Calculating loss for emognition model
	Full Pass 0.2950260639190674
numFreeParamsPath 18
Reconstruction loss values: 0.17738959193229675 0.17159976065158844

Calculating loss for empatch model
	Full Pass 0.10986518859863281
numFreeParamsPath 18
Reconstruction loss values: 0.14921416342258453 0.15168710052967072

Calculating loss for wesad model
	Full Pass 0.08170700073242188
numFreeParamsPath 18
Reconstruction loss values: 0.19885148108005524 0.21317161619663239
Total loss calculation time: 3.987931728363037

Epoch: 1

Training emognition model
Final encoder loss: 0.174858029267454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.09272503852844238 0.3724215030670166

Final encoder loss: 0.16834035012522966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.09012532234191895 0.33022356033325195

Final encoder loss: 0.17277203853243603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.09050822257995605 0.33095788955688477

Final encoder loss: 0.17191416783624014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08985543251037598 0.3308289051055908

Final encoder loss: 0.16602615617361538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.09044551849365234 0.33092641830444336

Final encoder loss: 0.16063049968219326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.09128093719482422 0.3319578170776367

Final encoder loss: 0.16142204510545105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.09076952934265137 0.3313288688659668

Final encoder loss: 0.1575410295663711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.09034252166748047 0.332822322845459

Final encoder loss: 0.15352267578751003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.09220337867736816 0.3317432403564453

Final encoder loss: 0.14868823719021612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.09032154083251953 0.3314390182495117

Final encoder loss: 0.15346837411700565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.09078121185302734 0.33226537704467773

Final encoder loss: 0.14453382827378541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.09078049659729004 0.3301970958709717

Final encoder loss: 0.13858737967863194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.09070420265197754 0.3307609558105469

Final encoder loss: 0.13286325621979114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.09273433685302734 0.3333702087402344

Final encoder loss: 0.13699462104558857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.09069538116455078 0.3318963050842285

Final encoder loss: 0.12132778112474628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.09427523612976074 0.33442068099975586


Training dapper model
Final encoder loss: 0.1635133918625101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06667757034301758 0.17862629890441895

Final encoder loss: 0.14684866248705503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06390023231506348 0.17243170738220215

Final encoder loss: 0.16224618004134397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.0643606185913086 0.17229723930358887

Final encoder loss: 0.14967766178222047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06437373161315918 0.17201542854309082

Final encoder loss: 0.14219240487956608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06385040283203125 0.17168521881103516

Final encoder loss: 0.13604464916397152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06396746635437012 0.17135834693908691

Final encoder loss: 0.1301719103800744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06384086608886719 0.17117929458618164

Final encoder loss: 0.12637767883715578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06390643119812012 0.1717379093170166

Final encoder loss: 0.125137724519826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.0638573169708252 0.17075276374816895

Final encoder loss: 0.14645933043502238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06398367881774902 0.17174196243286133

Final encoder loss: 0.11925610724022885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06370043754577637 0.17186760902404785

Final encoder loss: 0.12128415884806784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06382632255554199 0.17159676551818848

Final encoder loss: 0.1148423164876703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06385564804077148 0.17176127433776855

Final encoder loss: 0.12504905050054352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06389665603637695 0.17063093185424805

Final encoder loss: 0.11079622918583319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06401896476745605 0.17049551010131836

Final encoder loss: 0.1190829184730309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06894969940185547 0.17480230331420898


Training case model
Final encoder loss: 0.17142564250375572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.10260748863220215 0.3217353820800781

Final encoder loss: 0.16305459878006498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09809398651123047 0.3147132396697998

Final encoder loss: 0.1579268835775458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09788131713867188 0.3140268325805664

Final encoder loss: 0.15159452625279324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09739470481872559 0.3148612976074219

Final encoder loss: 0.14459010308902484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09802699089050293 0.3145561218261719

Final encoder loss: 0.14530869557144527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09727263450622559 0.31439995765686035

Final encoder loss: 0.13571713918334463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09793567657470703 0.3155488967895508

Final encoder loss: 0.13458335274469327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09712696075439453 0.31435203552246094

Final encoder loss: 0.13182589393380498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09786725044250488 0.3143007755279541

Final encoder loss: 0.1271483126921726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0971825122833252 0.3142697811126709

Final encoder loss: 0.12391093565076702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09774017333984375 0.31447815895080566

Final encoder loss: 0.1261086545649527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09745526313781738 0.3137364387512207

Final encoder loss: 0.11960945964904778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09764242172241211 0.3147740364074707

Final encoder loss: 0.12126846390323318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09719419479370117 0.3144874572753906

Final encoder loss: 0.11585464114398328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09758853912353516 0.3143894672393799

Final encoder loss: 0.11970107901017109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.09623289108276367 0.31389880180358887


Training amigos model
Final encoder loss: 0.15071664036663462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.12081146240234375 0.48181843757629395

Final encoder loss: 0.16029281154961977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.11823892593383789 0.47377705574035645

Final encoder loss: 0.15168232311586607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.11838245391845703 0.4746222496032715

Final encoder loss: 0.1464657763695881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.11844086647033691 0.47437047958374023

Final encoder loss: 0.13773395342642902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.11863493919372559 0.47437000274658203

Final encoder loss: 0.1350788900963036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1190638542175293 0.4749290943145752

Final encoder loss: 0.13456623093176598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1187443733215332 0.4748563766479492

Final encoder loss: 0.15158782838747034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.11879563331604004 0.4746067523956299

Final encoder loss: 0.12968239373786847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.11856389045715332 0.474273681640625

Final encoder loss: 0.13249286703037014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.11867332458496094 0.4746420383453369

Final encoder loss: 0.11793857997128045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1185612678527832 0.474851131439209

Final encoder loss: 0.11661096139291117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1187293529510498 0.4745216369628906

Final encoder loss: 0.12647829339556346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.11868667602539062 0.4745018482208252

Final encoder loss: 0.13641827607870283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.11864614486694336 0.47454023361206055

Final encoder loss: 0.11466208826634049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.11891555786132812 0.4746689796447754

Final encoder loss: 0.1231172074489439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.11566758155822754 0.4711008071899414


Training amigos model
Final encoder loss: 0.10390828964969072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.11669802665710449 0.427520751953125

Final encoder loss: 0.11437543769157991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.11684203147888184 0.4277379512786865

Final encoder loss: 0.09997406060376779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.11692142486572266 0.42750048637390137

Final encoder loss: 0.09648969756658744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1166074275970459 0.427412748336792

Final encoder loss: 0.09905764003469886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1167306900024414 0.42749810218811035

Final encoder loss: 0.09971701560339061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.11693477630615234 0.42752718925476074

Final encoder loss: 0.10527621897460604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.11676859855651855 0.4285244941711426

Final encoder loss: 0.09804834398587353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1167144775390625 0.42742156982421875

Final encoder loss: 0.0965977067460148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.11656546592712402 0.42685675621032715

Final encoder loss: 0.10309458656474192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.11682367324829102 0.4277312755584717

Final encoder loss: 0.09794417483594589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.11681056022644043 0.4273676872253418

Final encoder loss: 0.09001965068169543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.11688613891601562 0.4274008274078369

Final encoder loss: 0.09103276953060008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.11662125587463379 0.4274172782897949

Final encoder loss: 0.10437776844483031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.11666512489318848 0.4277944564819336

Final encoder loss: 0.09512923362978926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.11633968353271484 0.4273796081542969

Final encoder loss: 0.08936025180516624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.11160612106323242 0.4235203266143799


Training amigos model
Final encoder loss: 0.18075136840343475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4546489715576172 0.07122230529785156

Final encoder loss: 0.1878528743982315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.49143123626708984 0.07604122161865234

Final encoder loss: 0.18362122774124146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47844409942626953 0.0752863883972168

Final encoder loss: 0.11662895977497101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47916340827941895 0.07428312301635742

Final encoder loss: 0.1266619861125946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4780449867248535 0.07703852653503418

Final encoder loss: 0.12119756639003754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4756741523742676 0.0735623836517334

Final encoder loss: 0.093861423432827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4875459671020508 0.07480549812316895

Final encoder loss: 0.09988120198249817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.48638105392456055 0.07326221466064453

Final encoder loss: 0.09291332215070724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.404099702835083 0.08002543449401855

Final encoder loss: 0.08840688318014145
Final encoder loss: 0.09302308410406113
Final encoder loss: 0.08689263463020325

Training dapper model
Final encoder loss: 0.10877553260587969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.0638124942779541 0.1290302276611328

Final encoder loss: 0.10622473706359864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06283688545227051 0.12937021255493164

Final encoder loss: 0.09953109398617564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0627589225769043 0.12900876998901367

Final encoder loss: 0.10824020370889342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06279397010803223 0.1291639804840088

Final encoder loss: 0.10288731906988585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06420731544494629 0.12987565994262695

Final encoder loss: 0.09735931629305555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06255412101745605 0.1293027400970459

Final encoder loss: 0.09865529301022269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0626516342163086 0.12893176078796387

Final encoder loss: 0.09713944510448096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06345629692077637 0.13007473945617676

Final encoder loss: 0.09791774753776046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.0639350414276123 0.1293799877166748

Final encoder loss: 0.09057611275822332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06282830238342285 0.1291191577911377

Final encoder loss: 0.10999912344722805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06261968612670898 0.12961316108703613

Final encoder loss: 0.0911303333213164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06406378746032715 0.12866616249084473

Final encoder loss: 0.09710753189990733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06221437454223633 0.12807822227478027

Final encoder loss: 0.09400505912981968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.062003374099731445 0.12769579887390137

Final encoder loss: 0.09537048094498739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.061722517013549805 0.1279158592224121

Final encoder loss: 0.12308357024133078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.06159353256225586 0.12755727767944336


Training dapper model
Final encoder loss: 0.20245753228664398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11658024787902832 0.034691572189331055

Final encoder loss: 0.2081892192363739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.12558841705322266 0.03326892852783203

Final encoder loss: 0.11222922801971436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11606264114379883 0.03361916542053223

Final encoder loss: 0.11250293254852295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11542701721191406 0.033651113510131836

Final encoder loss: 0.09115543216466904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1163167953491211 0.03292369842529297

Final encoder loss: 0.09139487892389297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11601710319519043 0.03361248970031738

Final encoder loss: 0.0823713019490242
Final encoder loss: 0.08357913792133331

Training case model
Final encoder loss: 0.11258480813346113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.09533381462097168 0.26859140396118164

Final encoder loss: 0.11244403054004505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09533309936523438 0.26897692680358887

Final encoder loss: 0.11429212196477093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0953073501586914 0.2690141201019287

Final encoder loss: 0.11058730079647193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09531784057617188 0.2696418762207031

Final encoder loss: 0.11072348885030797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.09548020362854004 0.26953983306884766

Final encoder loss: 0.11062857012336824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09623026847839355 0.27001166343688965

Final encoder loss: 0.10867249962889229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09571599960327148 0.2697274684906006

Final encoder loss: 0.10821220553993287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.09577226638793945 0.2698352336883545

Final encoder loss: 0.1096742726335192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09602928161621094 0.2695455551147461

Final encoder loss: 0.10767056392481945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09582710266113281 0.2697608470916748

Final encoder loss: 0.1058655411204634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09558987617492676 0.26908326148986816

Final encoder loss: 0.10865572151904072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09546065330505371 0.2693290710449219

Final encoder loss: 0.10676073924488498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0956106185913086 0.2693190574645996

Final encoder loss: 0.10560650256505874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09589219093322754 0.26934289932250977

Final encoder loss: 0.10253142343905775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09543108940124512 0.26923060417175293

Final encoder loss: 0.10389183807351154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.09231281280517578 0.267427921295166


Training case model
Final encoder loss: 0.20295822620391846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.30499958992004395 0.05010557174682617

Final encoder loss: 0.18890653550624847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.30817461013793945 0.05145144462585449

Final encoder loss: 0.19014693796634674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.3002314567565918 0.051462650299072266

Final encoder loss: 0.19218625128269196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.30281591415405273 0.051670074462890625

Final encoder loss: 0.18080516159534454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.3164181709289551 0.0528569221496582

Final encoder loss: 0.19192615151405334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.30702805519104004 0.05246710777282715

Final encoder loss: 0.12133188545703888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.3126959800720215 0.05341458320617676

Final encoder loss: 0.11511459201574326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.3105137348175049 0.05137515068054199

Final encoder loss: 0.11286263912916183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.29081249237060547 0.05220532417297363

Final encoder loss: 0.11683158576488495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.321199893951416 0.052573204040527344

Final encoder loss: 0.11177651584148407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.3111608028411865 0.056292057037353516

Final encoder loss: 0.11725237965583801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.30628061294555664 0.05477118492126465

Final encoder loss: 0.10359636694192886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.31228160858154297 0.05071568489074707

Final encoder loss: 0.1006782278418541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.31158924102783203 0.05206489562988281

Final encoder loss: 0.09836219996213913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.3165438175201416 0.05281376838684082

Final encoder loss: 0.10259836912155151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.31389379501342773 0.05009007453918457

Final encoder loss: 0.097653828561306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.29906773567199707 0.053412437438964844

Final encoder loss: 0.10098157823085785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.3093724250793457 0.051085472106933594

Final encoder loss: 0.10375240445137024
Final encoder loss: 0.10203861445188522
Final encoder loss: 0.09774371236562729
Final encoder loss: 0.09973026067018509
Final encoder loss: 0.09344049543142319
Final encoder loss: 0.0945158526301384

Training emognition model
Final encoder loss: 0.1324714926998651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08894753456115723 0.28702616691589355

Final encoder loss: 0.12413426321598199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08819198608398438 0.2872323989868164

Final encoder loss: 0.13148551914458814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08879733085632324 0.2869229316711426

Final encoder loss: 0.11734492276419427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08805418014526367 0.2869150638580322

Final encoder loss: 0.1285919784076919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08801913261413574 0.28692150115966797

Final encoder loss: 0.11570744563969551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08844685554504395 0.2871129512786865

Final encoder loss: 0.12220502241899052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08950662612915039 0.286757230758667

Final encoder loss: 0.12311480492452535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08907914161682129 0.2880263328552246

Final encoder loss: 0.12011829480583734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08815145492553711 0.28696727752685547

Final encoder loss: 0.12168864222920785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08780694007873535 0.286376953125

Final encoder loss: 0.11783327565763824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08784317970275879 0.2867472171783447

Final encoder loss: 0.12185433384790677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08764863014221191 0.28637242317199707

Final encoder loss: 0.11889495099079946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08773660659790039 0.28591442108154297

Final encoder loss: 0.11914612460553282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08732342720031738 0.28701305389404297

Final encoder loss: 0.1164098771862602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08757519721984863 0.28544020652770996

Final encoder loss: 0.1137568321031256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08659577369689941 0.28499770164489746


Training emognition model
Final encoder loss: 0.19356396794319153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.27292585372924805 0.04754233360290527

Final encoder loss: 0.19495803117752075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.28020739555358887 0.04802346229553223

Final encoder loss: 0.12388361245393753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.26307249069213867 0.04912710189819336

Final encoder loss: 0.12442322075366974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2891366481781006 0.04680681228637695

Final encoder loss: 0.10904934257268906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.27228569984436035 0.04948163032531738

Final encoder loss: 0.10974309593439102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25516223907470703 0.05079984664916992

Final encoder loss: 0.10241921991109848
Final encoder loss: 0.10379163175821304

Training empatch model
Final encoder loss: 0.1355198433341162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07876229286193848 0.21651959419250488

Final encoder loss: 0.14915967103503464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07782959938049316 0.21425962448120117

Final encoder loss: 0.13868972052928089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07624292373657227 0.21444010734558105

Final encoder loss: 0.1502585177335384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07685136795043945 0.21410536766052246

Final encoder loss: 0.1335515250381102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07613706588745117 0.21475982666015625

Final encoder loss: 0.14557563760679126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07670879364013672 0.21501708030700684

Final encoder loss: 0.133725003688313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0763392448425293 0.2140181064605713

Final encoder loss: 0.13016045020272082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07863807678222656 0.21552348136901855

Final encoder loss: 0.1293141761171353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.076324462890625 0.21413064002990723

Final encoder loss: 0.13703552771843044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0764155387878418 0.21475481986999512

Final encoder loss: 0.1250248310327156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07660150527954102 0.21387863159179688

Final encoder loss: 0.12290996924135032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07591128349304199 0.2130427360534668

Final encoder loss: 0.1218071315737173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07556533813476562 0.21336770057678223

Final encoder loss: 0.1247463394043317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0757598876953125 0.21302270889282227

Final encoder loss: 0.11666488355221286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0756838321685791 0.2129504680633545

Final encoder loss: 0.11435077990577751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07497286796569824 0.21230745315551758


Training empatch model
Final encoder loss: 0.17117027938365936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17655038833618164 0.04536795616149902

Final encoder loss: 0.12635304033756256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18699860572814941 0.04372596740722656

Final encoder loss: 0.11279841512441635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2033400535583496 0.043834686279296875

Final encoder loss: 0.10598709434270859

Training wesad model
Final encoder loss: 0.19533784689470096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07857728004455566 0.2153153419494629

Final encoder loss: 0.19446259452684284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07535791397094727 0.21268415451049805

Final encoder loss: 0.17693625033446134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07522940635681152 0.2125840187072754

Final encoder loss: 0.18417593105730842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07513594627380371 0.21391940116882324

Final encoder loss: 0.1737017907482031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07613396644592285 0.21422553062438965

Final encoder loss: 0.17254232323536814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0758352279663086 0.21401238441467285

Final encoder loss: 0.170634729658857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07616019248962402 0.21437764167785645

Final encoder loss: 0.16379301050484804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07710051536560059 0.21353530883789062

Final encoder loss: 0.15803863029932416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07589006423950195 0.21426033973693848

Final encoder loss: 0.15088830306527043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07578849792480469 0.21289706230163574

Final encoder loss: 0.15322881899518906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0756375789642334 0.21251177787780762

Final encoder loss: 0.1541087314428485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07475519180297852 0.21294093132019043

Final encoder loss: 0.14420569863440433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.10706162452697754 0.21317124366760254

Final encoder loss: 0.15284088520923841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07538318634033203 0.2128903865814209

Final encoder loss: 0.1448597879213178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07510495185852051 0.2125999927520752

Final encoder loss: 0.13195107129976313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07534527778625488 0.21296048164367676


Training wesad model
Final encoder loss: 0.21559472382068634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11050081253051758 0.0326385498046875

Final encoder loss: 0.15462981164455414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10851597785949707 0.033010244369506836

Final encoder loss: 0.13815666735172272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1077871322631836 0.03299832344055176

Final encoder loss: 0.1298021674156189

Calculating loss for amigos model
	Full Pass 0.6846089363098145
numFreeParamsPath 18
Reconstruction loss values: 0.16846376657485962 0.16633690893650055

Calculating loss for dapper model
	Full Pass 0.1501767635345459
numFreeParamsPath 18
Reconstruction loss values: 0.19346743822097778 0.1949956715106964

Calculating loss for case model
	Full Pass 0.8562595844268799
numFreeParamsPath 18
Reconstruction loss values: 0.1891777068376541 0.18991805613040924

Calculating loss for emognition model
	Full Pass 0.2819685935974121
numFreeParamsPath 18
Reconstruction loss values: 0.18263810873031616 0.17946600914001465

Calculating loss for empatch model
	Full Pass 0.10432720184326172
numFreeParamsPath 18
Reconstruction loss values: 0.16222842037677765 0.1671263426542282

Calculating loss for wesad model
	Full Pass 0.07666254043579102
numFreeParamsPath 18
Reconstruction loss values: 0.19607970118522644 0.21857582032680511
Total loss calculation time: 3.8406291007995605

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 2.8924484252929688
Total epoch time: 94.07816672325134

Epoch: 2

Training case model
Final encoder loss: 0.18010025906078647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09151792526245117 0.2644684314727783

Final encoder loss: 0.15947017229688362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09044694900512695 0.2644813060760498

Final encoder loss: 0.14509443594566074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09051394462585449 0.2654407024383545

Final encoder loss: 0.135688988799804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09139466285705566 0.26373863220214844

Final encoder loss: 0.1302476722962648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09141778945922852 0.2645761966705322

Final encoder loss: 0.1237606002424765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09100127220153809 0.26389455795288086

Final encoder loss: 0.11450053682183085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09064912796020508 0.2636411190032959

Final encoder loss: 0.11182622857806489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09136772155761719 0.26453375816345215

Final encoder loss: 0.10678684002791135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09152960777282715 0.26566147804260254

Final encoder loss: 0.10643272895860381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09045600891113281 0.26389408111572266

Final encoder loss: 0.10322893078124391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09048318862915039 0.2631075382232666

Final encoder loss: 0.10127335933245489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09076952934265137 0.26370787620544434

Final encoder loss: 0.09656910609854062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0907127857208252 0.2643287181854248

Final encoder loss: 0.0967822942752733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09106278419494629 0.2639896869659424

Final encoder loss: 0.0959177513633866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09114623069763184 0.26392316818237305

Final encoder loss: 0.09492066687154815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.0868382453918457 0.2601611614227295


Training dapper model
Final encoder loss: 0.19587620096418415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06088876724243164 0.14773321151733398

Final encoder loss: 0.1653837611491021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.060756683349609375 0.1479032039642334

Final encoder loss: 0.15317213151581358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062040090560913086 0.14862585067749023

Final encoder loss: 0.13423025686022155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06094717979431152 0.14817404747009277

Final encoder loss: 0.11214194388340233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06071758270263672 0.14879107475280762

Final encoder loss: 0.11474897183717768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06096935272216797 0.1483478546142578

Final encoder loss: 0.10865629160317453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06084442138671875 0.14893722534179688

Final encoder loss: 0.10722975340405987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06111550331115723 0.14840245246887207

Final encoder loss: 0.0986870789205195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061258554458618164 0.148514986038208

Final encoder loss: 0.10296392704376785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06105399131774902 0.14820194244384766

Final encoder loss: 0.0967603716020368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.060744524002075195 0.14850258827209473

Final encoder loss: 0.09619857488752949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.0609126091003418 0.14781904220581055

Final encoder loss: 0.10017869911454792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0609891414642334 0.14872050285339355

Final encoder loss: 0.0950340068004838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06074786186218262 0.1487879753112793

Final encoder loss: 0.09800304577987626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06074237823486328 0.1478431224822998

Final encoder loss: 0.08202377413305724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06062483787536621 0.14807486534118652


Training amigos model
Final encoder loss: 0.15489796918221116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10750031471252441 0.3879566192626953

Final encoder loss: 0.15478650693671483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10758376121520996 0.3877830505371094

Final encoder loss: 0.1409606942790442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10753846168518066 0.3882896900177002

Final encoder loss: 0.13019838491314628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10759544372558594 0.38866662979125977

Final encoder loss: 0.11997273581670477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10716915130615234 0.3884594440460205

Final encoder loss: 0.11763953400017708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10760664939880371 0.388932466506958

Final encoder loss: 0.1229280049527375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10802698135375977 0.3879892826080322

Final encoder loss: 0.10273775472394665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10716557502746582 0.38884615898132324

Final encoder loss: 0.1045545915915748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10772919654846191 0.38810276985168457

Final encoder loss: 0.10483418554117677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10793828964233398 0.38865184783935547

Final encoder loss: 0.1071869015331572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10776424407958984 0.387941837310791

Final encoder loss: 0.091478416206032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10742640495300293 0.3878934383392334

Final encoder loss: 0.09403141249002707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10715007781982422 0.3870267868041992

Final encoder loss: 0.10540791862800629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10808348655700684 0.3874812126159668

Final encoder loss: 0.10328312109187517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10724353790283203 0.3873279094696045

Final encoder loss: 0.08548667138821982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10286569595336914 0.3817133903503418


Training emognition model
Final encoder loss: 0.17893450317865073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08224892616271973 0.27262210845947266

Final encoder loss: 0.1507097353132218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08172821998596191 0.27271533012390137

Final encoder loss: 0.14426390958516294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08265304565429688 0.27315831184387207

Final encoder loss: 0.13538848265509648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08309340476989746 0.2731318473815918

Final encoder loss: 0.12632000443758867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08198761940002441 0.27378249168395996

Final encoder loss: 0.12243132644727904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08284997940063477 0.2726478576660156

Final encoder loss: 0.11962175338710565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08183741569519043 0.27306342124938965

Final encoder loss: 0.11226733268660358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08231639862060547 0.2724728584289551

Final encoder loss: 0.1157980120287197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08203411102294922 0.2731010913848877

Final encoder loss: 0.10600310846812248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08242154121398926 0.273831844329834

Final encoder loss: 0.10974167031974812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08214998245239258 0.27295970916748047

Final encoder loss: 0.11365190057428505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08240127563476562 0.27301812171936035

Final encoder loss: 0.10489569675859105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08206439018249512 0.2740745544433594

Final encoder loss: 0.10966855210475884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08249926567077637 0.2736382484436035

Final encoder loss: 0.10619334443458471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0824286937713623 0.2796461582183838

Final encoder loss: 0.10492610388424495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0823371410369873 0.27292370796203613


Training amigos model
Final encoder loss: 0.08753304266769728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10540342330932617 0.3408691883087158

Final encoder loss: 0.08317390053872348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10555863380432129 0.3407158851623535

Final encoder loss: 0.08286139616707953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1054840087890625 0.3409242630004883

Final encoder loss: 0.07644382088621275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10576725006103516 0.340986967086792

Final encoder loss: 0.09195453879374162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1054220199584961 0.34108400344848633

Final encoder loss: 0.07699638593666228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10568404197692871 0.3414316177368164

Final encoder loss: 0.0900623655420132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10559964179992676 0.3407588005065918

Final encoder loss: 0.08500987221915318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10554623603820801 0.34110260009765625

Final encoder loss: 0.0829021359424302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.1055760383605957 0.34124231338500977

Final encoder loss: 0.07646075606614264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10543680191040039 0.3408172130584717

Final encoder loss: 0.0811456340653804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10514187812805176 0.3406195640563965

Final encoder loss: 0.08190602023305567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10550284385681152 0.34096431732177734

Final encoder loss: 0.07470073959956212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10571408271789551 0.34078335762023926

Final encoder loss: 0.07639490977350892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10558700561523438 0.3412461280822754

Final encoder loss: 0.074612296605816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10545802116394043 0.3409707546234131

Final encoder loss: 0.07834131140050347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10056805610656738 0.33707308769226074


Training amigos model
Final encoder loss: 0.18078352510929108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.42416834831237793 0.07455706596374512

Final encoder loss: 0.1878310889005661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.42931532859802246 0.07379722595214844

Final encoder loss: 0.18365462124347687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4557676315307617 0.07077264785766602

Final encoder loss: 0.09274869412183762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4615507125854492 0.07478880882263184

Final encoder loss: 0.10073158144950867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4510526657104492 0.07577729225158691

Final encoder loss: 0.09512310475111008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654717445373535 0.07400393486022949

Final encoder loss: 0.07493851333856583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47244954109191895 0.07543706893920898

Final encoder loss: 0.08091569691896439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4691462516784668 0.08394622802734375

Final encoder loss: 0.07536889612674713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44873976707458496 0.08019661903381348

Final encoder loss: 0.07260017096996307
Final encoder loss: 0.07658001035451889
Final encoder loss: 0.07023653388023376

Training dapper model
Final encoder loss: 0.08944368753662886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06145000457763672 0.10722684860229492

Final encoder loss: 0.08881078942036955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05980277061462402 0.10726761817932129

Final encoder loss: 0.09022678788719084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06002545356750488 0.1072089672088623

Final encoder loss: 0.08291051463142038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059705257415771484 0.10646986961364746

Final encoder loss: 0.07795984296360052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05987405776977539 0.10868716239929199

Final encoder loss: 0.07908361051316609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06122326850891113 0.10804462432861328

Final encoder loss: 0.07467052196457906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059543609619140625 0.10768389701843262

Final encoder loss: 0.07746479046706582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05973482131958008 0.1071169376373291

Final encoder loss: 0.0834962350477801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05962634086608887 0.10705447196960449

Final encoder loss: 0.08466445016204517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05942082405090332 0.1073915958404541

Final encoder loss: 0.08439776047422634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.0606691837310791 0.10759401321411133

Final encoder loss: 0.08730007378933505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.059589385986328125 0.10705375671386719

Final encoder loss: 0.07996430779495473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06032443046569824 0.10767745971679688

Final encoder loss: 0.07430119804068228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059523820877075195 0.10663771629333496

Final encoder loss: 0.07509287364857074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059601783752441406 0.10776662826538086

Final encoder loss: 0.10172507844693178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.06035447120666504 0.10718226432800293


Training dapper model
Final encoder loss: 0.20243187248706818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1129903793334961 0.033856868743896484

Final encoder loss: 0.20818910002708435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11457204818725586 0.033762216567993164

Final encoder loss: 0.09317488968372345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11457943916320801 0.03477621078491211

Final encoder loss: 0.09467989951372147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11389756202697754 0.03403067588806152

Final encoder loss: 0.0739932507276535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11295318603515625 0.03429913520812988

Final encoder loss: 0.07524363696575165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1121060848236084 0.03418707847595215

Final encoder loss: 0.06733984500169754
Final encoder loss: 0.06887384504079819

Training case model
Final encoder loss: 0.0977038923714985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0886697769165039 0.21823978424072266

Final encoder loss: 0.09396569629847779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0885152816772461 0.21799731254577637

Final encoder loss: 0.09239156521085157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08853268623352051 0.2183382511138916

Final encoder loss: 0.09018403574730162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08821511268615723 0.21814465522766113

Final encoder loss: 0.088773719898328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08868718147277832 0.21823906898498535

Final encoder loss: 0.08987338979924057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0884103775024414 0.218278169631958

Final encoder loss: 0.08932222558615623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08903193473815918 0.21806669235229492

Final encoder loss: 0.08527362985722786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08828926086425781 0.21792960166931152

Final encoder loss: 0.08564967109428677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08826351165771484 0.2180778980255127

Final encoder loss: 0.08370768592717237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08840346336364746 0.21815228462219238

Final encoder loss: 0.08593424588779337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08869028091430664 0.21822571754455566

Final encoder loss: 0.08300689709484602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08919167518615723 0.21913862228393555

Final encoder loss: 0.08415996548878366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08953046798706055 0.2187809944152832

Final encoder loss: 0.08224720127295725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08894968032836914 0.2189342975616455

Final encoder loss: 0.08146703800360115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08907008171081543 0.21903514862060547

Final encoder loss: 0.08338548063082428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08562397956848145 0.2153029441833496


Training case model
Final encoder loss: 0.20296484231948853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28397607803344727 0.052826881408691406

Final encoder loss: 0.18891794979572296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.29760098457336426 0.053090572357177734

Final encoder loss: 0.19015821814537048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27002453804016113 0.050788164138793945

Final encoder loss: 0.19218841195106506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2759840488433838 0.05085873603820801

Final encoder loss: 0.18080969154834747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.29119133949279785 0.051871538162231445

Final encoder loss: 0.19193366169929504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26778244972229004 0.051252126693725586

Final encoder loss: 0.10386404395103455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2712841033935547 0.05140113830566406

Final encoder loss: 0.09638470411300659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2698700428009033 0.053107261657714844

Final encoder loss: 0.09406467527151108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2848222255706787 0.05119442939758301

Final encoder loss: 0.0962272360920906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2764706611633301 0.05013561248779297

Final encoder loss: 0.09248195588588715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27715301513671875 0.05269145965576172

Final encoder loss: 0.09471149742603302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.27448105812072754 0.05015158653259277

Final encoder loss: 0.08500871062278748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2764124870300293 0.053482770919799805

Final encoder loss: 0.08085384219884872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26602888107299805 0.05163741111755371

Final encoder loss: 0.07944577187299728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2753591537475586 0.052631378173828125

Final encoder loss: 0.08333076536655426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28401875495910645 0.05080819129943848

Final encoder loss: 0.08106601238250732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.290163516998291 0.053336143493652344

Final encoder loss: 0.08258266746997833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2702157497406006 0.05112814903259277

Final encoder loss: 0.08579321950674057
Final encoder loss: 0.08313070237636566
Final encoder loss: 0.08057324588298798
Final encoder loss: 0.08298493176698685
Final encoder loss: 0.07953540980815887
Final encoder loss: 0.07881832867860794

Training emognition model
Final encoder loss: 0.09450997542475233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08091449737548828 0.23062801361083984

Final encoder loss: 0.09546521894483796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08138155937194824 0.23030734062194824

Final encoder loss: 0.10088019489914261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08076190948486328 0.2306995391845703

Final encoder loss: 0.09485825657796929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0805978775024414 0.23054981231689453

Final encoder loss: 0.09754999861912392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08052206039428711 0.23065471649169922

Final encoder loss: 0.0937381387112075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08055257797241211 0.23035502433776855

Final encoder loss: 0.09241733746839638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08059358596801758 0.231095552444458

Final encoder loss: 0.09437631014043313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0810554027557373 0.23016977310180664

Final encoder loss: 0.08933173316511273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08144688606262207 0.2310502529144287

Final encoder loss: 0.09558523882949607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08096051216125488 0.2300710678100586

Final encoder loss: 0.09537420463029522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08194899559020996 0.23070478439331055

Final encoder loss: 0.09572784167374385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08078455924987793 0.2307596206665039

Final encoder loss: 0.09055235139904262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08214664459228516 0.230576753616333

Final encoder loss: 0.09113227262507308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08077812194824219 0.23066401481628418

Final encoder loss: 0.09307766970732946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0814523696899414 0.23021554946899414

Final encoder loss: 0.09040065975297204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07989954948425293 0.22983860969543457


Training emognition model
Final encoder loss: 0.1935809850692749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24873709678649902 0.04701089859008789

Final encoder loss: 0.1949807107448578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24669981002807617 0.04783463478088379

Final encoder loss: 0.10198890417814255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474226951599121 0.04984641075134277

Final encoder loss: 0.10320774465799332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24680757522583008 0.048636674880981445

Final encoder loss: 0.08761013299226761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477281093597412 0.048819780349731445

Final encoder loss: 0.08855920284986496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24719023704528809 0.04930734634399414

Final encoder loss: 0.08298660069704056
Final encoder loss: 0.08419960737228394

Training empatch model
Final encoder loss: 0.1568115203818763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07051682472229004 0.17249250411987305

Final encoder loss: 0.1587335213451409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07033467292785645 0.1727149486541748

Final encoder loss: 0.1520545966770156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07024526596069336 0.17278194427490234

Final encoder loss: 0.13695316835520135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07057881355285645 0.17282938957214355

Final encoder loss: 0.13347102728825913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07049798965454102 0.17271947860717773

Final encoder loss: 0.12960270926119194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07028937339782715 0.172468900680542

Final encoder loss: 0.12786699012529293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07016110420227051 0.17246127128601074

Final encoder loss: 0.12369386514613302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06976461410522461 0.17190289497375488

Final encoder loss: 0.11666859639091322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07051515579223633 0.17271661758422852

Final encoder loss: 0.11690195548442717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07050323486328125 0.17249631881713867

Final encoder loss: 0.10151278014912928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07024431228637695 0.17254114151000977

Final encoder loss: 0.10784441096450884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07047748565673828 0.1723322868347168

Final encoder loss: 0.10476343188894806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07028079032897949 0.17279887199401855

Final encoder loss: 0.0986558185596724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07013726234436035 0.17269372940063477

Final encoder loss: 0.10059570529672524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0697932243347168 0.17352867126464844

Final encoder loss: 0.10037671021996439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07072997093200684 0.17360281944274902


Training empatch model
Final encoder loss: 0.17117033898830414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1729106903076172 0.04279184341430664

Final encoder loss: 0.10755946487188339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471671104431152 0.04444384574890137

Final encoder loss: 0.09441173076629639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17046236991882324 0.0437159538269043

Final encoder loss: 0.08873037993907928

Training wesad model
Final encoder loss: 0.1938869743533292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07145214080810547 0.1738576889038086

Final encoder loss: 0.17311420672030625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07076072692871094 0.17345404624938965

Final encoder loss: 0.1730523177279296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07110023498535156 0.17381501197814941

Final encoder loss: 0.16033683008013958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07072997093200684 0.17364287376403809

Final encoder loss: 0.1433355340071592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07086968421936035 0.1736612319946289

Final encoder loss: 0.1384112974435715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07061433792114258 0.17368793487548828

Final encoder loss: 0.14037663183364818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07070207595825195 0.17391061782836914

Final encoder loss: 0.13071161927889377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07148051261901855 0.17333555221557617

Final encoder loss: 0.12415685858569693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07045125961303711 0.17353129386901855

Final encoder loss: 0.11878224817746856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07198500633239746 0.17383909225463867

Final encoder loss: 0.12356734752869802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710606575012207 0.17376995086669922

Final encoder loss: 0.11397767581131693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0706634521484375 0.17262768745422363

Final encoder loss: 0.10851325091602594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0699913501739502 0.17258882522583008

Final encoder loss: 0.10492302322205373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.06977558135986328 0.17218279838562012

Final encoder loss: 0.1128486484464353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07027363777160645 0.17398881912231445

Final encoder loss: 0.10450696995060514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07013654708862305 0.1719202995300293


Training wesad model
Final encoder loss: 0.21561314165592194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10233521461486816 0.03236675262451172

Final encoder loss: 0.12849457561969757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1024634838104248 0.03265690803527832

Final encoder loss: 0.11023616045713425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10193657875061035 0.032067060470581055

Final encoder loss: 0.10254388302564621

Calculating loss for amigos model
	Full Pass 0.6354312896728516
numFreeParamsPath 18
Reconstruction loss values: 0.13952882587909698 0.13975898921489716

Calculating loss for dapper model
	Full Pass 0.15017938613891602
numFreeParamsPath 18
Reconstruction loss values: 0.14006094634532928 0.1423671692609787

Calculating loss for case model
	Full Pass 0.8564059734344482
numFreeParamsPath 18
Reconstruction loss values: 0.15599195659160614 0.15712346136569977

Calculating loss for emognition model
	Full Pass 0.27996134757995605
numFreeParamsPath 18
Reconstruction loss values: 0.14969930052757263 0.14937449991703033

Calculating loss for empatch model
	Full Pass 0.10432720184326172
numFreeParamsPath 18
Reconstruction loss values: 0.14726433157920837 0.14827026426792145

Calculating loss for wesad model
	Full Pass 0.07673883438110352
numFreeParamsPath 18
Reconstruction loss values: 0.1831645965576172 0.2036890685558319
Total loss calculation time: 3.632059335708618

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.643674850463867
Total epoch time: 84.87099504470825

Epoch: 3

Training dapper model
Final encoder loss: 0.13898384081030313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06577420234680176 0.15847039222717285

Final encoder loss: 0.11762037735717597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.0616302490234375 0.15042376518249512

Final encoder loss: 0.10392159028477801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.0622866153717041 0.14856195449829102

Final encoder loss: 0.10027543934316094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.0605013370513916 0.1506807804107666

Final encoder loss: 0.09684528551488228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06144261360168457 0.15082907676696777

Final encoder loss: 0.0872201345341304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06316804885864258 0.15254878997802734

Final encoder loss: 0.0903850618887974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0633687973022461 0.1522049903869629

Final encoder loss: 0.09096400382693035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06164193153381348 0.14924097061157227

Final encoder loss: 0.09067077238851053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06184506416320801 0.15167808532714844

Final encoder loss: 0.08124273147582706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.07641959190368652 0.1490027904510498

Final encoder loss: 0.07644037315768681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06139969825744629 0.15030694007873535

Final encoder loss: 0.08563068283552473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06162714958190918 0.1498122215270996

Final encoder loss: 0.07607307044354324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06137871742248535 0.14992690086364746

Final encoder loss: 0.07957634818787493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06102800369262695 0.14997315406799316

Final encoder loss: 0.07471858056640295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06512331962585449 0.15093708038330078

Final encoder loss: 0.06747316599322621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06255030632019043 0.15136361122131348


Training emognition model
Final encoder loss: 0.14642178600055966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0853569507598877 0.2754068374633789

Final encoder loss: 0.12813995741106435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08280324935913086 0.274944543838501

Final encoder loss: 0.12390454628742986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08416366577148438 0.27628564834594727

Final encoder loss: 0.11231131649614604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08324050903320312 0.2752072811126709

Final encoder loss: 0.10581823600279831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0828104019165039 0.2741425037384033

Final encoder loss: 0.10624636525398984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08283519744873047 0.2743985652923584

Final encoder loss: 0.1064216288061079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0851294994354248 0.27415013313293457

Final encoder loss: 0.09942664736679738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08304309844970703 0.27429771423339844

Final encoder loss: 0.10390867091090734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08287549018859863 0.27431750297546387

Final encoder loss: 0.09821521233670126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08285880088806152 0.27439260482788086

Final encoder loss: 0.09860057260530047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08244919776916504 0.273104190826416

Final encoder loss: 0.09376008155106692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08222603797912598 0.2740187644958496

Final encoder loss: 0.0991878354618091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0822136402130127 0.2745230197906494

Final encoder loss: 0.08734977855569313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08392786979675293 0.27565670013427734

Final encoder loss: 0.09381235357647845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08229994773864746 0.27400708198547363

Final encoder loss: 0.09398333839662318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0813589096069336 0.2717099189758301


Training case model
Final encoder loss: 0.1582400753903956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09116053581237793 0.26421523094177246

Final encoder loss: 0.12811949919159427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09050989151000977 0.2636091709136963

Final encoder loss: 0.11579739407204533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09106826782226562 0.26377296447753906

Final encoder loss: 0.11223315143952313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09102034568786621 0.2639451026916504

Final encoder loss: 0.10454550875392235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09079790115356445 0.26448726654052734

Final encoder loss: 0.09912070826337326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09035944938659668 0.264585018157959

Final encoder loss: 0.09516663793180219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09096312522888184 0.2633790969848633

Final encoder loss: 0.09548604931491703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0907449722290039 0.26398777961730957

Final encoder loss: 0.09203606750481977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09064793586730957 0.26488566398620605

Final encoder loss: 0.08882989755877796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09065699577331543 0.2644803524017334

Final encoder loss: 0.08786929626739715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0910649299621582 0.26408815383911133

Final encoder loss: 0.08669522432446539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09058737754821777 0.2643868923187256

Final encoder loss: 0.08622089628592448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09060359001159668 0.2649819850921631

Final encoder loss: 0.0830070093903869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09089851379394531 0.26402950286865234

Final encoder loss: 0.08084671955477857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09035277366638184 0.26392340660095215

Final encoder loss: 0.08302021374045397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08768653869628906 0.2608802318572998


Training amigos model
Final encoder loss: 0.14445090449739442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10752010345458984 0.38817858695983887

Final encoder loss: 0.12435484342389001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10777664184570312 0.3879883289337158

Final encoder loss: 0.10372947791422628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10761451721191406 0.3876192569732666

Final encoder loss: 0.10477737166081097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10786581039428711 0.388042688369751

Final encoder loss: 0.10373882001116926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10750293731689453 0.3880009651184082

Final encoder loss: 0.0982528531595783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10909152030944824 0.38756871223449707

Final encoder loss: 0.08843083530352974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10738253593444824 0.3879735469818115

Final encoder loss: 0.08531067401249524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10761165618896484 0.38798975944519043

Final encoder loss: 0.0907338424268722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10771417617797852 0.38768911361694336

Final encoder loss: 0.0907965686698012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10765504837036133 0.3874020576477051

Final encoder loss: 0.07965683150075732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10747599601745605 0.3881256580352783

Final encoder loss: 0.08016140857682993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10770177841186523 0.388491153717041

Final encoder loss: 0.07906118248339218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10787177085876465 0.38836145401000977

Final encoder loss: 0.09751491346736896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10793304443359375 0.3878352642059326

Final encoder loss: 0.07849946637449026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10759902000427246 0.3880929946899414

Final encoder loss: 0.08233405572920428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10319375991821289 0.385129451751709


Training amigos model
Final encoder loss: 0.06720028434578491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1059725284576416 0.3408486843109131

Final encoder loss: 0.06716803951053497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1059122085571289 0.34078359603881836

Final encoder loss: 0.0632226259400723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1055753231048584 0.34125375747680664

Final encoder loss: 0.06612764835244372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10596346855163574 0.3411526679992676

Final encoder loss: 0.07136103931512246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1056370735168457 0.3412950038909912

Final encoder loss: 0.06693511639631425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1058967113494873 0.34102559089660645

Final encoder loss: 0.07073721438877814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1057431697845459 0.34114527702331543

Final encoder loss: 0.07498650672504252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10606551170349121 0.3409595489501953

Final encoder loss: 0.06760555340409778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10553693771362305 0.3415038585662842

Final encoder loss: 0.06746186637229147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10562324523925781 0.34087705612182617

Final encoder loss: 0.06966678402634624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10577607154846191 0.3408832550048828

Final encoder loss: 0.07438864666579209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10564279556274414 0.34079432487487793

Final encoder loss: 0.06628516930513847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1056663990020752 0.3410460948944092

Final encoder loss: 0.06533733530079863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10592222213745117 0.3410613536834717

Final encoder loss: 0.07579475877457084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10565781593322754 0.3406825065612793

Final encoder loss: 0.07104701122914087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10013437271118164 0.3369112014770508


Training amigos model
Final encoder loss: 0.1807458996772766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.39337611198425293 0.0730888843536377

Final encoder loss: 0.18781216442584991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4081094264984131 0.07575368881225586

Final encoder loss: 0.183648020029068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40567636489868164 0.07322335243225098

Final encoder loss: 0.08213189989328384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.40815114974975586 0.07666468620300293

Final encoder loss: 0.087369404733181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.39293599128723145 0.07502269744873047

Final encoder loss: 0.0817674770951271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4306488037109375 0.07641792297363281

Final encoder loss: 0.06662221997976303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.445542573928833 0.07142353057861328

Final encoder loss: 0.07054801285266876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44386792182922363 0.07408332824707031

Final encoder loss: 0.06606901437044144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40441370010375977 0.0738372802734375

Final encoder loss: 0.0644475668668747
Final encoder loss: 0.06774616986513138
Final encoder loss: 0.062455322593450546

Training dapper model
Final encoder loss: 0.0685408205220923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059537410736083984 0.10652828216552734

Final encoder loss: 0.0652661435528746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05916333198547363 0.10573339462280273

Final encoder loss: 0.0648434675253578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05894923210144043 0.1061396598815918

Final encoder loss: 0.0651551241916468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059014320373535156 0.10790705680847168

Final encoder loss: 0.07332429601542735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05990195274353027 0.1077888011932373

Final encoder loss: 0.063845022415175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05998873710632324 0.10728979110717773

Final encoder loss: 0.06844929849281038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05952596664428711 0.10813021659851074

Final encoder loss: 0.0736411779131824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.061301231384277344 0.10779452323913574

Final encoder loss: 0.06466832953178601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05967116355895996 0.10708832740783691

Final encoder loss: 0.06847199864377237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.060001373291015625 0.10748457908630371

Final encoder loss: 0.0664294983157863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05971693992614746 0.10735249519348145

Final encoder loss: 0.06285793600160014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05981922149658203 0.10743570327758789

Final encoder loss: 0.06667745413707248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06137824058532715 0.10803747177124023

Final encoder loss: 0.06423679902424823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06186985969543457 0.10651683807373047

Final encoder loss: 0.06460704712458466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05969595909118652 0.10736799240112305

Final encoder loss: 0.07430735500680291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05942964553833008 0.10685133934020996


Training dapper model
Final encoder loss: 0.20247438549995422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1116173267364502 0.034163475036621094

Final encoder loss: 0.2081882804632187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11279153823852539 0.03457927703857422

Final encoder loss: 0.0820726677775383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11243534088134766 0.033064842224121094

Final encoder loss: 0.0848919153213501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11149930953979492 0.03389883041381836

Final encoder loss: 0.06295445561408997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11069035530090332 0.033989667892456055

Final encoder loss: 0.06474926322698593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11021161079406738 0.03400373458862305

Final encoder loss: 0.05681738629937172
Final encoder loss: 0.05889417231082916

Training case model
Final encoder loss: 0.07986413096407016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08902144432067871 0.21899867057800293

Final encoder loss: 0.08032656459899569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08921265602111816 0.21916627883911133

Final encoder loss: 0.07988577251777948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09052205085754395 0.21927475929260254

Final encoder loss: 0.08036546434525699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08923578262329102 0.21889948844909668

Final encoder loss: 0.07946813631077854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08913803100585938 0.21917200088500977

Final encoder loss: 0.07860528761717789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09073114395141602 0.2193925380706787

Final encoder loss: 0.07714788534112693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08974385261535645 0.2191460132598877

Final encoder loss: 0.0769599496570498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08934187889099121 0.21920371055603027

Final encoder loss: 0.07669125626230082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09183931350708008 0.21939826011657715

Final encoder loss: 0.0749394526874801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08971881866455078 0.2192535400390625

Final encoder loss: 0.07670841106465733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.0892786979675293 0.21967291831970215

Final encoder loss: 0.07560327505049554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08994936943054199 0.2191147804260254

Final encoder loss: 0.07506910327723078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08917856216430664 0.2192094326019287

Final encoder loss: 0.07502889815380394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08998847007751465 0.21925115585327148

Final encoder loss: 0.07274806462997532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08862113952636719 0.219130277633667

Final encoder loss: 0.0757297427709824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08649897575378418 0.21610069274902344


Training case model
Final encoder loss: 0.20296603441238403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26064634323120117 0.053121328353881836

Final encoder loss: 0.18890975415706635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25836181640625 0.055104970932006836

Final encoder loss: 0.19016383588314056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25623345375061035 0.05315804481506348

Final encoder loss: 0.19218279421329498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2675600051879883 0.051969051361083984

Final encoder loss: 0.18081524968147278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26637911796569824 0.05191206932067871

Final encoder loss: 0.19194307923316956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25206542015075684 0.051267385482788086

Final encoder loss: 0.0987396389245987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25605154037475586 0.05206179618835449

Final encoder loss: 0.08950430899858475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26720714569091797 0.051790475845336914

Final encoder loss: 0.08766886591911316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26583027839660645 0.05131220817565918

Final encoder loss: 0.08959096670150757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25710344314575195 0.05176973342895508

Final encoder loss: 0.0857018381357193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25629591941833496 0.0510716438293457

Final encoder loss: 0.08733955025672913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2528057098388672 0.050701141357421875

Final encoder loss: 0.07638537883758545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25684380531311035 0.05251431465148926

Final encoder loss: 0.07175065577030182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25833797454833984 0.051276445388793945

Final encoder loss: 0.07069333642721176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25670957565307617 0.053113698959350586

Final encoder loss: 0.07449212670326233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573060989379883 0.0537569522857666

Final encoder loss: 0.07290628552436829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25848937034606934 0.05263066291809082

Final encoder loss: 0.07415001839399338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25362396240234375 0.05149555206298828

Final encoder loss: 0.07499542087316513
Final encoder loss: 0.07190744578838348
Final encoder loss: 0.07005281746387482
Final encoder loss: 0.07299688458442688
Final encoder loss: 0.07127857208251953
Final encoder loss: 0.0702769011259079

Training emognition model
Final encoder loss: 0.08807850706171926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08073210716247559 0.22978639602661133

Final encoder loss: 0.09303116539573904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08091616630554199 0.2298142910003662

Final encoder loss: 0.08399910125691955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08065199851989746 0.22990894317626953

Final encoder loss: 0.0833439237327134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08073568344116211 0.22984528541564941

Final encoder loss: 0.08614224574143065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08071517944335938 0.2295541763305664

Final encoder loss: 0.08286359069027813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08023524284362793 0.2310028076171875

Final encoder loss: 0.08790569210842962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08098888397216797 0.23089098930358887

Final encoder loss: 0.08415739704058364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0807499885559082 0.2305300235748291

Final encoder loss: 0.07996803449715821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08099603652954102 0.2311687469482422

Final encoder loss: 0.08172049617760109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08160710334777832 0.23058032989501953

Final encoder loss: 0.07977713290291376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08203458786010742 0.23117685317993164

Final encoder loss: 0.07883445930379383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0819087028503418 0.2306060791015625

Final encoder loss: 0.07942872836805771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08323884010314941 0.23137688636779785

Final encoder loss: 0.07741220949015103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08167576789855957 0.23130416870117188

Final encoder loss: 0.0782244310268039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08295512199401855 0.23049497604370117

Final encoder loss: 0.07885233993674438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0808403491973877 0.23006319999694824


Training emognition model
Final encoder loss: 0.19357798993587494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472541332244873 0.04983377456665039

Final encoder loss: 0.194978266954422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24714946746826172 0.04816460609436035

Final encoder loss: 0.09371962398290634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24788451194763184 0.04974031448364258

Final encoder loss: 0.09517311304807663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2465050220489502 0.049802303314208984

Final encoder loss: 0.07865098118782043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24869656562805176 0.04856157302856445

Final encoder loss: 0.07915046811103821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24628496170043945 0.049567461013793945

Final encoder loss: 0.07457970082759857
Final encoder loss: 0.07523602992296219

Training empatch model
Final encoder loss: 0.1406352451009299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07102513313293457 0.1736140251159668

Final encoder loss: 0.14085135128997062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07134127616882324 0.17408990859985352

Final encoder loss: 0.1347069530971575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07071805000305176 0.17341184616088867

Final encoder loss: 0.12613257892010676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07119131088256836 0.17410612106323242

Final encoder loss: 0.11634815754184819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07188725471496582 0.17382478713989258

Final encoder loss: 0.1185148110942443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07125401496887207 0.17330598831176758

Final encoder loss: 0.10659215829310191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0717172622680664 0.1745738983154297

Final encoder loss: 0.09325345002302045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07093524932861328 0.1734294891357422

Final encoder loss: 0.09471788186014454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07086372375488281 0.17438173294067383

Final encoder loss: 0.09114887378694192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07137370109558105 0.1733684539794922

Final encoder loss: 0.08752937300415131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07120633125305176 0.17393898963928223

Final encoder loss: 0.08725948184981663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07241415977478027 0.17444610595703125

Final encoder loss: 0.08966145581250282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07120895385742188 0.1731865406036377

Final encoder loss: 0.09004166497436407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07167625427246094 0.17418932914733887

Final encoder loss: 0.09215270486567564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07106137275695801 0.1740555763244629

Final encoder loss: 0.08640300040736838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07099604606628418 0.17389369010925293


Training empatch model
Final encoder loss: 0.171168252825737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17583131790161133 0.043337106704711914

Final encoder loss: 0.09739651530981064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17470693588256836 0.04354047775268555

Final encoder loss: 0.08341629058122635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17566323280334473 0.0436863899230957

Final encoder loss: 0.07813521474599838

Training wesad model
Final encoder loss: 0.19424729441454475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07087326049804688 0.17393136024475098

Final encoder loss: 0.1562338663329093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07196283340454102 0.1741328239440918

Final encoder loss: 0.15098966734290561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07076597213745117 0.17378616333007812

Final encoder loss: 0.14723257413053645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07137775421142578 0.17379117012023926

Final encoder loss: 0.12427086830185878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07088255882263184 0.1737806797027588

Final encoder loss: 0.11950991461015367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07113146781921387 0.17390823364257812

Final encoder loss: 0.12324494897414517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07187414169311523 0.17372941970825195

Final encoder loss: 0.11475389469785975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0707399845123291 0.17322373390197754

Final encoder loss: 0.10560142976538818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.06990885734558105 0.17209243774414062

Final encoder loss: 0.10526023808894701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07002806663513184 0.1725444793701172

Final encoder loss: 0.10451944990652762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.06984806060791016 0.1721179485321045

Final encoder loss: 0.09922000382125515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07012701034545898 0.17238092422485352

Final encoder loss: 0.09333311730068375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0702517032623291 0.17267203330993652

Final encoder loss: 0.09239682877613876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.06999802589416504 0.17220139503479004

Final encoder loss: 0.09473669747286888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0699307918548584 0.17270469665527344

Final encoder loss: 0.09486068808147641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07032203674316406 0.17270183563232422


Training wesad model
Final encoder loss: 0.21561291813850403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10298538208007812 0.032759904861450195

Final encoder loss: 0.11710066348314285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10243105888366699 0.032891273498535156

Final encoder loss: 0.09732405841350555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10224676132202148 0.03229570388793945

Final encoder loss: 0.08993399888277054

Calculating loss for amigos model
	Full Pass 0.6686966419219971
numFreeParamsPath 18
Reconstruction loss values: 0.12302407622337341 0.12511879205703735

Calculating loss for dapper model
	Full Pass 0.1518537998199463
numFreeParamsPath 18
Reconstruction loss values: 0.12329386174678802 0.12536373734474182

Calculating loss for case model
	Full Pass 0.9118571281433105
numFreeParamsPath 18
Reconstruction loss values: 0.13567370176315308 0.13673684000968933

Calculating loss for emognition model
	Full Pass 0.28016209602355957
numFreeParamsPath 18
Reconstruction loss values: 0.13996456563472748 0.13956278562545776

Calculating loss for empatch model
	Full Pass 0.10539722442626953
numFreeParamsPath 18
Reconstruction loss values: 0.13989673554897308 0.1398255079984665

Calculating loss for wesad model
	Full Pass 0.07725214958190918
numFreeParamsPath 18
Reconstruction loss values: 0.17189988493919373 0.1934066116809845
Total loss calculation time: 3.917926788330078

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.73633337020874
Total epoch time: 84.52933669090271

Epoch: 4

Training emognition model
Final encoder loss: 0.14169851225286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08846211433410645 0.2830991744995117

Final encoder loss: 0.1154553687554498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08293652534484863 0.2746548652648926

Final encoder loss: 0.1081817665639409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08364009857177734 0.275165319442749

Final encoder loss: 0.10061575311931845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08334469795227051 0.27567267417907715

Final encoder loss: 0.10606290157421606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08353638648986816 0.2753760814666748

Final encoder loss: 0.1004123677542488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08349800109863281 0.27373814582824707

Final encoder loss: 0.09048975195312045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08222126960754395 0.2735733985900879

Final encoder loss: 0.09415131640412187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08217310905456543 0.2738218307495117

Final encoder loss: 0.09303251582327998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08241868019104004 0.2728583812713623

Final encoder loss: 0.09061146870696844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08160114288330078 0.273301362991333

Final encoder loss: 0.08841774494450833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08235526084899902 0.2731766700744629

Final encoder loss: 0.08952299002848321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08236026763916016 0.2735898494720459

Final encoder loss: 0.08735429040925845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08257198333740234 0.2729053497314453

Final encoder loss: 0.0865889799240513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08246397972106934 0.2731339931488037

Final encoder loss: 0.08330920178437662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08215856552124023 0.2735011577606201

Final encoder loss: 0.08207854893823543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0814657211303711 0.27424144744873047


Training amigos model
Final encoder loss: 0.12158105551249256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10819530487060547 0.38877415657043457

Final encoder loss: 0.10620932695820487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10864448547363281 0.3886559009552002

Final encoder loss: 0.09393442256327075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1078634262084961 0.38913679122924805

Final encoder loss: 0.10033476771406832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10795426368713379 0.3883204460144043

Final encoder loss: 0.09640238133377897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.11013436317443848 0.38979482650756836

Final encoder loss: 0.08858382941858028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10835099220275879 0.39003753662109375

Final encoder loss: 0.07843272424426882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1077573299407959 0.3897206783294678

Final encoder loss: 0.08501819819535514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10811758041381836 0.38953471183776855

Final encoder loss: 0.07935257136684516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10946059226989746 0.39049506187438965

Final encoder loss: 0.07698907942671758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10874104499816895 0.3891103267669678

Final encoder loss: 0.08328566890098714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10925102233886719 0.3892502784729004

Final encoder loss: 0.07829055789297415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1083071231842041 0.3907175064086914

Final encoder loss: 0.07938562918324751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1074221134185791 0.3886575698852539

Final encoder loss: 0.0857994083118777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10788321495056152 0.38881468772888184

Final encoder loss: 0.07398519567058051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10791492462158203 0.38862156867980957

Final encoder loss: 0.0780443624788378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.1028904914855957 0.38248753547668457


Training dapper model
Final encoder loss: 0.12164066755700086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06142687797546387 0.1478886604309082

Final encoder loss: 0.10952812405965648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06110119819641113 0.14920592308044434

Final encoder loss: 0.09385977653569695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06167030334472656 0.14940714836120605

Final encoder loss: 0.08909863157169286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06124091148376465 0.15045881271362305

Final encoder loss: 0.07660307721638462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06269454956054688 0.1520094871520996

Final encoder loss: 0.08318069547093415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0616152286529541 0.1492176055908203

Final encoder loss: 0.07416507187804787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06116294860839844 0.1496260166168213

Final encoder loss: 0.07410834607529594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061182498931884766 0.14914488792419434

Final encoder loss: 0.07890004316724049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06106853485107422 0.14910078048706055

Final encoder loss: 0.0665576530090126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06096816062927246 0.14883756637573242

Final encoder loss: 0.0701639019611476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06086087226867676 0.14868521690368652

Final encoder loss: 0.06564074624394639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06133723258972168 0.14882969856262207

Final encoder loss: 0.0716918973439547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06065034866333008 0.14908409118652344

Final encoder loss: 0.06410759381917906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.061679840087890625 0.14964675903320312

Final encoder loss: 0.07347747120982323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06154441833496094 0.14913368225097656

Final encoder loss: 0.07184782240619354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06119728088378906 0.14796853065490723


Training case model
Final encoder loss: 0.14117939817921146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09120583534240723 0.26394176483154297

Final encoder loss: 0.11713919964639878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09081721305847168 0.2636587619781494

Final encoder loss: 0.10572738178401406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09106898307800293 0.2644479274749756

Final encoder loss: 0.09874471174664168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09127306938171387 0.2638556957244873

Final encoder loss: 0.0946636242545282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09101557731628418 0.2637600898742676

Final encoder loss: 0.08862560517061478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09084725379943848 0.26455116271972656

Final encoder loss: 0.08659125239256901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09087753295898438 0.2643284797668457

Final encoder loss: 0.08386171553949394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09064364433288574 0.2644343376159668

Final encoder loss: 0.08385343543202321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09062981605529785 0.26448798179626465

Final encoder loss: 0.07986159439609467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09079122543334961 0.26435351371765137

Final encoder loss: 0.08207385506181719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09074997901916504 0.2645728588104248

Final encoder loss: 0.07865514368953819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0910634994506836 0.26369142532348633

Final encoder loss: 0.07737065677610348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09078550338745117 0.26383328437805176

Final encoder loss: 0.07712938128085756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09079718589782715 0.2641744613647461

Final encoder loss: 0.07706094384744182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.0908050537109375 0.26439404487609863

Final encoder loss: 0.07542603859871738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.09076595306396484 0.2607901096343994


Training amigos model
Final encoder loss: 0.06861177087885564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10604023933410645 0.3410804271697998

Final encoder loss: 0.0635265693267136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10589313507080078 0.3412938117980957

Final encoder loss: 0.05925464652110815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10586285591125488 0.3413522243499756

Final encoder loss: 0.06744823267850528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1060638427734375 0.341353178024292

Final encoder loss: 0.059013906325572615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10601258277893066 0.34138941764831543

Final encoder loss: 0.058296389482271005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1061251163482666 0.34133148193359375

Final encoder loss: 0.06766778207400168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10609292984008789 0.34143638610839844

Final encoder loss: 0.06917292948730479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10612154006958008 0.34126973152160645

Final encoder loss: 0.06322540542153318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10598397254943848 0.3412308692932129

Final encoder loss: 0.06468360040884066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10547614097595215 0.3412010669708252

Final encoder loss: 0.05969858870375682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1059565544128418 0.34140777587890625

Final encoder loss: 0.06655505440613399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10581684112548828 0.3409106731414795

Final encoder loss: 0.059084270261035576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10584187507629395 0.34095072746276855

Final encoder loss: 0.06554582938905465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.105743408203125 0.3408658504486084

Final encoder loss: 0.06805910270773245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10551810264587402 0.3408374786376953

Final encoder loss: 0.0657300700547389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10082578659057617 0.3369913101196289


Training amigos model
Final encoder loss: 0.18079274892807007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44957637786865234 0.07401299476623535

Final encoder loss: 0.18782061338424683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4646146297454834 0.07618880271911621

Final encoder loss: 0.18363836407661438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4543442726135254 0.07397031784057617

Final encoder loss: 0.0750570297241211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44590020179748535 0.07223105430603027

Final encoder loss: 0.07941670715808868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4580216407775879 0.07230472564697266

Final encoder loss: 0.07473833858966827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45587992668151855 0.07193517684936523

Final encoder loss: 0.05980316922068596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4603874683380127 0.07441020011901855

Final encoder loss: 0.06302769482135773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46111059188842773 0.07606101036071777

Final encoder loss: 0.06044347584247589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4545314311981201 0.07442593574523926

Final encoder loss: 0.05733412504196167
Final encoder loss: 0.05948309600353241
Final encoder loss: 0.05692088603973389

Training dapper model
Final encoder loss: 0.06676740249095942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059325218200683594 0.10637450218200684

Final encoder loss: 0.06035761785796144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.0595090389251709 0.10609698295593262

Final encoder loss: 0.06553513163043674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0589902400970459 0.10647964477539062

Final encoder loss: 0.06377875155394556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05913496017456055 0.1069343090057373

Final encoder loss: 0.059709284361314854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05956006050109863 0.1065361499786377

Final encoder loss: 0.05617742687208467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05871438980102539 0.10653400421142578

Final encoder loss: 0.05669927942137593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05932354927062988 0.10633730888366699

Final encoder loss: 0.056639766697914755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05919146537780762 0.10648703575134277

Final encoder loss: 0.05793069788616533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05939292907714844 0.10658621788024902

Final encoder loss: 0.056607511658652805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.059355974197387695 0.10644364356994629

Final encoder loss: 0.058234623108476304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05928349494934082 0.10661840438842773

Final encoder loss: 0.05597001801004837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05909109115600586 0.10624575614929199

Final encoder loss: 0.05857709905163486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.0592195987701416 0.10651898384094238

Final encoder loss: 0.05068468586013111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05931973457336426 0.10569453239440918

Final encoder loss: 0.05969068376436081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0591127872467041 0.10632038116455078

Final encoder loss: 0.04789203790837082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05904102325439453 0.10637593269348145


Training dapper model
Final encoder loss: 0.2024621218442917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11246109008789062 0.033538103103637695

Final encoder loss: 0.20820365846157074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11179852485656738 0.03313255310058594

Final encoder loss: 0.0764222964644432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11185359954833984 0.033805131912231445

Final encoder loss: 0.07811857759952545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1114187240600586 0.03385162353515625

Final encoder loss: 0.056884076446294785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11211514472961426 0.03341245651245117

Final encoder loss: 0.057682108134031296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11191892623901367 0.03389596939086914

Final encoder loss: 0.051296964287757874
Final encoder loss: 0.05220776051282883

Training case model
Final encoder loss: 0.07230137427232146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0888972282409668 0.21886754035949707

Final encoder loss: 0.07207918536545897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08875131607055664 0.2188267707824707

Final encoder loss: 0.06971713937173098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08893299102783203 0.21869587898254395

Final encoder loss: 0.06987483274272557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08907771110534668 0.2186279296875

Final encoder loss: 0.06890217590183507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08896780014038086 0.21852827072143555

Final encoder loss: 0.07039231501839176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08897614479064941 0.21863579750061035

Final encoder loss: 0.06932078998892975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08924221992492676 0.21837878227233887

Final encoder loss: 0.07094303972700279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08884763717651367 0.21854162216186523

Final encoder loss: 0.06895635978043338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08902168273925781 0.21842455863952637

Final encoder loss: 0.06997917318379607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09039855003356934 0.21928143501281738

Final encoder loss: 0.06888765374417753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08934926986694336 0.21928000450134277

Final encoder loss: 0.0678966707695674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0912473201751709 0.21896100044250488

Final encoder loss: 0.0667854895832308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08913612365722656 0.21914339065551758

Final encoder loss: 0.0662400634519099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08940839767456055 0.2201085090637207

Final encoder loss: 0.06880225615897922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0890192985534668 0.21895313262939453

Final encoder loss: 0.06797111122416297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.0863792896270752 0.2156693935394287


Training case model
Final encoder loss: 0.20296703279018402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580528259277344 0.05336451530456543

Final encoder loss: 0.18891002237796783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25657200813293457 0.0530855655670166

Final encoder loss: 0.19015638530254364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26885437965393066 0.05246305465698242

Final encoder loss: 0.1921764761209488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26093006134033203 0.052521467208862305

Final encoder loss: 0.1808183342218399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2612757682800293 0.05116701126098633

Final encoder loss: 0.19192859530448914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.253894567489624 0.05074286460876465

Final encoder loss: 0.0947810485959053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587430477142334 0.05286717414855957

Final encoder loss: 0.08613166213035583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2565298080444336 0.053072452545166016

Final encoder loss: 0.08365867286920547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2598729133605957 0.05198168754577637

Final encoder loss: 0.08558200299739838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2562432289123535 0.05395650863647461

Final encoder loss: 0.08095932006835938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2589254379272461 0.05276918411254883

Final encoder loss: 0.0827619731426239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569131851196289 0.05298495292663574

Final encoder loss: 0.07096116989850998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25941896438598633 0.05664181709289551

Final encoder loss: 0.06693586707115173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25567030906677246 0.0522153377532959

Final encoder loss: 0.06563345342874527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25835585594177246 0.05263876914978027

Final encoder loss: 0.06957343965768814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567901611328125 0.05149102210998535

Final encoder loss: 0.06765138357877731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25876951217651367 0.05180549621582031

Final encoder loss: 0.06845240294933319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25585317611694336 0.05260157585144043

Final encoder loss: 0.06948240101337433
Final encoder loss: 0.06715795397758484
Final encoder loss: 0.06472668796777725
Final encoder loss: 0.06779592484235764
Final encoder loss: 0.06568512320518494
Final encoder loss: 0.06457480043172836

Training emognition model
Final encoder loss: 0.08012516289683755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.0810861587524414 0.22939062118530273

Final encoder loss: 0.08016024423114614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08019781112670898 0.2291867733001709

Final encoder loss: 0.07684522857559266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08005261421203613 0.22971057891845703

Final encoder loss: 0.07683641843116527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08026432991027832 0.22905969619750977

Final encoder loss: 0.07639863381292003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08033013343811035 0.23041343688964844

Final encoder loss: 0.07728519765549113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08080792427062988 0.23062419891357422

Final encoder loss: 0.07621577177544271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08089804649353027 0.23070502281188965

Final encoder loss: 0.07650868643331415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08079767227172852 0.23086881637573242

Final encoder loss: 0.07726788144193066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08130073547363281 0.2304997444152832

Final encoder loss: 0.07121231059434417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08121132850646973 0.23026132583618164

Final encoder loss: 0.0759173337373118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08164262771606445 0.2309095859527588

Final encoder loss: 0.07559854409754962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08173203468322754 0.23065447807312012

Final encoder loss: 0.07156176650848445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08052587509155273 0.22964239120483398

Final encoder loss: 0.07361961881969781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08050680160522461 0.2312939167022705

Final encoder loss: 0.07523684884803203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0806431770324707 0.22983908653259277

Final encoder loss: 0.07472532980875453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07998776435852051 0.22909998893737793


Training emognition model
Final encoder loss: 0.19356602430343628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2476520538330078 0.04832029342651367

Final encoder loss: 0.19496935606002808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24559640884399414 0.04834556579589844

Final encoder loss: 0.08935626596212387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24787497520446777 0.04786515235900879

Final encoder loss: 0.0907520279288292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24509215354919434 0.047913312911987305

Final encoder loss: 0.07338306307792664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478194236755371 0.048734188079833984

Final encoder loss: 0.07405611127614975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24524641036987305 0.04932236671447754

Final encoder loss: 0.06924779713153839
Final encoder loss: 0.07011878490447998

Training empatch model
Final encoder loss: 0.1346701671373181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07112383842468262 0.17408108711242676

Final encoder loss: 0.13832011579421047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07121634483337402 0.17368054389953613

Final encoder loss: 0.12180809947999992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0711662769317627 0.17415761947631836

Final encoder loss: 0.12195624359526476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07160329818725586 0.17371749877929688

Final encoder loss: 0.10397498282550985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07123613357543945 0.17440414428710938

Final encoder loss: 0.10659502561141398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07275032997131348 0.17409133911132812

Final encoder loss: 0.10210051402667605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07147645950317383 0.1738293170928955

Final encoder loss: 0.10634810374577575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.070709228515625 0.17389559745788574

Final encoder loss: 0.08365965249168529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07134151458740234 0.1739058494567871

Final encoder loss: 0.08405749243363268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07126426696777344 0.1741807460784912

Final encoder loss: 0.0807049391819234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07170629501342773 0.1740725040435791

Final encoder loss: 0.08680290432058803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07107901573181152 0.1737961769104004

Final encoder loss: 0.08654768987056016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.072540283203125 0.17405176162719727

Final encoder loss: 0.07935082714659887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07150793075561523 0.17413592338562012

Final encoder loss: 0.08598976564305819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07041668891906738 0.17374873161315918

Final encoder loss: 0.07803409142010914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07098078727722168 0.1735842227935791


Training empatch model
Final encoder loss: 0.1711539477109909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748208999633789 0.043625593185424805

Final encoder loss: 0.09188490360975266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17171478271484375 0.04367947578430176

Final encoder loss: 0.0774204209446907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17250609397888184 0.043909311294555664

Final encoder loss: 0.07218711823225021

Training wesad model
Final encoder loss: 0.16995847518765844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07089424133300781 0.17342162132263184

Final encoder loss: 0.15635503473772833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0724172592163086 0.17395329475402832

Final encoder loss: 0.13298688073694237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07097530364990234 0.17411470413208008

Final encoder loss: 0.136821785673325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07067561149597168 0.1741015911102295

Final encoder loss: 0.11938146023608466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0708315372467041 0.1737842559814453

Final encoder loss: 0.11689804161620547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07124972343444824 0.17427706718444824

Final encoder loss: 0.10722618921152058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0718851089477539 0.17363476753234863

Final encoder loss: 0.10736798662024924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0710744857788086 0.173417329788208

Final encoder loss: 0.09513057306269634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07227826118469238 0.1741328239440918

Final encoder loss: 0.0904953870403469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07130026817321777 0.1738889217376709

Final encoder loss: 0.09979823744485047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0889897346496582 0.1741771697998047

Final encoder loss: 0.09739387393717167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07100129127502441 0.17337346076965332

Final encoder loss: 0.08663112492726337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07130265235900879 0.17416787147521973

Final encoder loss: 0.08667555179269135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0716094970703125 0.17394471168518066

Final encoder loss: 0.08405359490489059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07126855850219727 0.17400169372558594

Final encoder loss: 0.08567653289328156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07218027114868164 0.17423248291015625


Training wesad model
Final encoder loss: 0.215606227517128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10476374626159668 0.03275561332702637

Final encoder loss: 0.11069879680871964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1021265983581543 0.033190250396728516

Final encoder loss: 0.08982900530099869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385942459106445 0.033939361572265625

Final encoder loss: 0.08242183923721313

Calculating loss for amigos model
	Full Pass 0.6717524528503418
numFreeParamsPath 18
Reconstruction loss values: 0.11468033492565155 0.11777074635028839

Calculating loss for dapper model
	Full Pass 0.15240907669067383
numFreeParamsPath 18
Reconstruction loss values: 0.1107146367430687 0.11225248128175735

Calculating loss for case model
	Full Pass 0.8588957786560059
numFreeParamsPath 18
Reconstruction loss values: 0.12149669229984283 0.12278524041175842

Calculating loss for emognition model
	Full Pass 0.2803843021392822
numFreeParamsPath 18
Reconstruction loss values: 0.1288715898990631 0.12974995374679565

Calculating loss for empatch model
	Full Pass 0.10407567024230957
numFreeParamsPath 18
Reconstruction loss values: 0.13277384638786316 0.1330868899822235

Calculating loss for wesad model
	Full Pass 0.07677698135375977
numFreeParamsPath 18
Reconstruction loss values: 0.1627539098262787 0.18274332582950592
Total loss calculation time: 3.8072688579559326

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.220559120178223
Total epoch time: 84.27518153190613

Epoch: 5

Training case model
Final encoder loss: 0.12075500309887534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09920287132263184 0.2711598873138428

Final encoder loss: 0.10110474701093465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09080791473388672 0.26529455184936523

Final encoder loss: 0.09571958772766648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09118103981018066 0.26538586616516113

Final encoder loss: 0.08868375208524581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09194564819335938 0.26549267768859863

Final encoder loss: 0.08511634993586056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09085440635681152 0.264585018157959

Final encoder loss: 0.08295675529745172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09120607376098633 0.2656135559082031

Final encoder loss: 0.08066221807252716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09242725372314453 0.2648453712463379

Final encoder loss: 0.07941234410141669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09119224548339844 0.26490306854248047

Final encoder loss: 0.0793861870603912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09140181541442871 0.26515865325927734

Final encoder loss: 0.07621911775886353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0909421443939209 0.2643153667449951

Final encoder loss: 0.07725221828467649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09060931205749512 0.26372838020324707

Final encoder loss: 0.07270575705329382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0904686450958252 0.2640421390533447

Final encoder loss: 0.0736681281720214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09069681167602539 0.26407527923583984

Final encoder loss: 0.07404606675615875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09119701385498047 0.26320791244506836

Final encoder loss: 0.06953266829746808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09038400650024414 0.26400113105773926

Final encoder loss: 0.07321541710840061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08706927299499512 0.26079845428466797


Training emognition model
Final encoder loss: 0.1301371918364563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08231496810913086 0.27295947074890137

Final encoder loss: 0.10665385052934968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08259010314941406 0.27344202995300293

Final encoder loss: 0.10190353313452655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08185720443725586 0.2731311321258545

Final encoder loss: 0.10063019648358441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08296918869018555 0.27527856826782227

Final encoder loss: 0.09704962784580184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08344149589538574 0.2754230499267578

Final encoder loss: 0.09240591383390215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0844731330871582 0.27567505836486816

Final encoder loss: 0.09470316772418386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08469748497009277 0.2752089500427246

Final encoder loss: 0.08897432494167148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08449888229370117 0.27449893951416016

Final encoder loss: 0.0874260143254087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08261275291442871 0.27370619773864746

Final encoder loss: 0.08131718601114003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0824742317199707 0.27320384979248047

Final encoder loss: 0.08157212207954413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08231282234191895 0.27309322357177734

Final encoder loss: 0.08016622855364071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08206868171691895 0.2728579044342041

Final encoder loss: 0.08036128234274095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08223390579223633 0.2733004093170166

Final encoder loss: 0.08219643536883199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08241772651672363 0.2735016345977783

Final encoder loss: 0.08114614984292083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08245444297790527 0.2737269401550293

Final encoder loss: 0.07794023108823285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0817258358001709 0.27242136001586914


Training amigos model
Final encoder loss: 0.11370655752542652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1075143814086914 0.38776350021362305

Final encoder loss: 0.09638147801056476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10773706436157227 0.3890268802642822

Final encoder loss: 0.09091518811783283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10866355895996094 0.3891866207122803

Final encoder loss: 0.09352018253912413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10817551612854004 0.39074110984802246

Final encoder loss: 0.07439737516127991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.1073911190032959 0.38939762115478516

Final encoder loss: 0.07724768322908217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1100010871887207 0.3907334804534912

Final encoder loss: 0.07213849698772698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10831451416015625 0.3894484043121338

Final encoder loss: 0.0858590609418016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10744953155517578 0.38907361030578613

Final encoder loss: 0.07691688512265198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10798239707946777 0.38913416862487793

Final encoder loss: 0.0713624674129656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1079554557800293 0.389479398727417

Final encoder loss: 0.07375286232414885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1094057559967041 0.390977144241333

Final encoder loss: 0.07371113989014978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10769009590148926 0.3891255855560303

Final encoder loss: 0.0772437549709933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10892844200134277 0.38928651809692383

Final encoder loss: 0.07611029317271606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10741066932678223 0.38840532302856445

Final encoder loss: 0.0661185448521227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10755205154418945 0.3901059627532959

Final encoder loss: 0.06521675403805838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10325002670288086 0.38286614418029785


Training dapper model
Final encoder loss: 0.10735197814900266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061089277267456055 0.14847874641418457

Final encoder loss: 0.09319414336306095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06108975410461426 0.14747047424316406

Final encoder loss: 0.08399398447696872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06086277961730957 0.14939641952514648

Final encoder loss: 0.08337778606928128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061159610748291016 0.14899706840515137

Final encoder loss: 0.07544428481735235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.060912132263183594 0.14842963218688965

Final encoder loss: 0.07386921223232684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06145191192626953 0.1506812572479248

Final encoder loss: 0.07037464632639442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06269264221191406 0.15173006057739258

Final encoder loss: 0.07171111162731439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06161665916442871 0.14892125129699707

Final encoder loss: 0.06761634701715417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061188697814941406 0.14885735511779785

Final encoder loss: 0.06404919248177267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06124281883239746 0.14924311637878418

Final encoder loss: 0.06794772410901741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06123042106628418 0.14949750900268555

Final encoder loss: 0.06407825224993167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06084108352661133 0.14789557456970215

Final encoder loss: 0.05782716562237209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06081700325012207 0.14911842346191406

Final encoder loss: 0.06355285752974367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06105327606201172 0.14858222007751465

Final encoder loss: 0.061548393824880566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.061173439025878906 0.14789962768554688

Final encoder loss: 0.06781888732602957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06055951118469238 0.14885640144348145


Training amigos model
Final encoder loss: 0.061489465530140584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10601043701171875 0.3408639430999756

Final encoder loss: 0.05966688134754015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10579252243041992 0.34093165397644043

Final encoder loss: 0.05920908864579113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1055612564086914 0.34084320068359375

Final encoder loss: 0.06837375601250079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10586380958557129 0.3408498764038086

Final encoder loss: 0.058343188390606336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.105560302734375 0.3409895896911621

Final encoder loss: 0.05826057303140186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10588335990905762 0.3408544063568115

Final encoder loss: 0.06185764807975987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10538315773010254 0.34061717987060547

Final encoder loss: 0.0589343232090112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10551762580871582 0.34085965156555176

Final encoder loss: 0.058662056200368136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10562705993652344 0.3410515785217285

Final encoder loss: 0.05589790523444714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.1056058406829834 0.3408238887786865

Final encoder loss: 0.06251930864118387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10569334030151367 0.3409748077392578

Final encoder loss: 0.05754482757772619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10558867454528809 0.34078383445739746

Final encoder loss: 0.057484855029681536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10539889335632324 0.3409261703491211

Final encoder loss: 0.06057876942573233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10573458671569824 0.340836763381958

Final encoder loss: 0.06489123861517482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10514497756958008 0.3405604362487793

Final encoder loss: 0.058285266092965145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10015487670898438 0.33710384368896484


Training amigos model
Final encoder loss: 0.18075720965862274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43065428733825684 0.07392430305480957

Final encoder loss: 0.1878274828195572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4316713809967041 0.07418107986450195

Final encoder loss: 0.18362516164779663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40549516677856445 0.07333517074584961

Final encoder loss: 0.07234873622655869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4324076175689697 0.07559514045715332

Final encoder loss: 0.07577172666788101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.42290520668029785 0.07457494735717773

Final encoder loss: 0.0720144510269165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40565061569213867 0.07381534576416016

Final encoder loss: 0.05720354989171028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.40876078605651855 0.07404494285583496

Final encoder loss: 0.059120383113622665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.40830254554748535 0.07670855522155762

Final encoder loss: 0.05774146318435669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4065663814544678 0.07342958450317383

Final encoder loss: 0.05550037696957588
Final encoder loss: 0.055827926844358444
Final encoder loss: 0.05427725613117218

Training dapper model
Final encoder loss: 0.050512560234230966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05950736999511719 0.10610508918762207

Final encoder loss: 0.05601025932580302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05907320976257324 0.10632085800170898

Final encoder loss: 0.054091301950891055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059563636779785156 0.10620403289794922

Final encoder loss: 0.05523946044941318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0590512752532959 0.10666036605834961

Final encoder loss: 0.05307499012909013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059050798416137695 0.10661911964416504

Final encoder loss: 0.05115460317286495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059282779693603516 0.10566377639770508

Final encoder loss: 0.04778997541971857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05895638465881348 0.10666584968566895

Final encoder loss: 0.05065260606533451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05936169624328613 0.10638618469238281

Final encoder loss: 0.06049833917716088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059172868728637695 0.10636782646179199

Final encoder loss: 0.05110138344874212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05933976173400879 0.1068425178527832

Final encoder loss: 0.048756649138313346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05915713310241699 0.10616064071655273

Final encoder loss: 0.05204903021580784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.059072256088256836 0.10666894912719727

Final encoder loss: 0.052774355754827605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05938005447387695 0.10660910606384277

Final encoder loss: 0.05087165054639698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05911111831665039 0.10639524459838867

Final encoder loss: 0.04908643535539148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05916762351989746 0.10648107528686523

Final encoder loss: 0.05843217444563205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05869412422180176 0.10568714141845703


Training dapper model
Final encoder loss: 0.20244912803173065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11326885223388672 0.03327345848083496

Final encoder loss: 0.20821867883205414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11165952682495117 0.03389120101928711

Final encoder loss: 0.0725703090429306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11142873764038086 0.03401350975036621

Final encoder loss: 0.07369181513786316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11191153526306152 0.03319358825683594

Final encoder loss: 0.053188715130090714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11146855354309082 0.03397178649902344

Final encoder loss: 0.05328112840652466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11169791221618652 0.0340266227722168

Final encoder loss: 0.04781420901417732
Final encoder loss: 0.04782550036907196

Training case model
Final encoder loss: 0.07617755353077875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08888053894042969 0.21834421157836914

Final encoder loss: 0.07151124165063628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08906888961791992 0.2186293601989746

Final encoder loss: 0.07037717901668902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08845710754394531 0.21870827674865723

Final encoder loss: 0.07014042732407597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08889627456665039 0.21854162216186523

Final encoder loss: 0.06748286406924313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08865952491760254 0.2185375690460205

Final encoder loss: 0.06814407093838712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08890271186828613 0.21860814094543457

Final encoder loss: 0.067488656521684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08893513679504395 0.21839642524719238

Final encoder loss: 0.06757534422812538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08863019943237305 0.21864104270935059

Final encoder loss: 0.06586293280996217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.5298972129821777 0.21887731552124023

Final encoder loss: 0.06821002858784803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08911609649658203 0.21875953674316406

Final encoder loss: 0.06952261426051741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08916163444519043 0.21872329711914062

Final encoder loss: 0.06499169666855627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08913612365722656 0.21857523918151855

Final encoder loss: 0.06608717043201665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08905696868896484 0.2189028263092041

Final encoder loss: 0.06655273344910446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08876800537109375 0.21875929832458496

Final encoder loss: 0.06401367961847274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08846688270568848 0.21839237213134766

Final encoder loss: 0.06948193695670103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08513593673706055 0.21524286270141602


Training case model
Final encoder loss: 0.2029578685760498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2584829330444336 0.05144619941711426

Final encoder loss: 0.18890394270420074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25570058822631836 0.05228376388549805

Final encoder loss: 0.19015853106975555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.254777193069458 0.05262899398803711

Final encoder loss: 0.1921808272600174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2557356357574463 0.05225205421447754

Final encoder loss: 0.18079911172389984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25414061546325684 0.05232095718383789

Final encoder loss: 0.19193001091480255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25182247161865234 0.051465749740600586

Final encoder loss: 0.09196244180202484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587103843688965 0.053232431411743164

Final encoder loss: 0.08372224122285843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25555419921875 0.05145525932312012

Final encoder loss: 0.08103512972593307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26630449295043945 0.05256390571594238

Final encoder loss: 0.08312123268842697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2541356086730957 0.05238676071166992

Final encoder loss: 0.07828715443611145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25724124908447266 0.05422520637512207

Final encoder loss: 0.0801396295428276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2573249340057373 0.055351972579956055

Final encoder loss: 0.06889453530311584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25659894943237305 0.05368757247924805

Final encoder loss: 0.0654425397515297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25876474380493164 0.05581402778625488

Final encoder loss: 0.0639089047908783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582433223724365 0.051889896392822266

Final encoder loss: 0.06787130981683731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25871729850769043 0.05436134338378906

Final encoder loss: 0.06562387943267822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25892162322998047 0.052388668060302734

Final encoder loss: 0.06625117361545563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25629115104675293 0.05382370948791504

Final encoder loss: 0.06792876124382019
Final encoder loss: 0.06588603556156158
Final encoder loss: 0.06349221616983414
Final encoder loss: 0.06627016514539719
Final encoder loss: 0.06424622982740402
Final encoder loss: 0.062417056411504745

Training emognition model
Final encoder loss: 0.07467593653604576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08156228065490723 0.23050642013549805

Final encoder loss: 0.0742288861303036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0812833309173584 0.23158764839172363

Final encoder loss: 0.07131639370329762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08068323135375977 0.23067617416381836

Final encoder loss: 0.07125762147454512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.09096717834472656 0.2306983470916748

Final encoder loss: 0.07097724410744405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08298444747924805 0.23090195655822754

Final encoder loss: 0.07419902994948024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0812687873840332 0.23085808753967285

Final encoder loss: 0.07202900453772249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08099055290222168 0.23108220100402832

Final encoder loss: 0.07465041826419877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0817255973815918 0.23127508163452148

Final encoder loss: 0.07224357369698828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08171749114990234 0.2309436798095703

Final encoder loss: 0.0718032028217192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08101630210876465 0.23085713386535645

Final encoder loss: 0.07062598650925274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08263325691223145 0.230987548828125

Final encoder loss: 0.07062245489678533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08173394203186035 0.23086094856262207

Final encoder loss: 0.07116301974511097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08260440826416016 0.23084712028503418

Final encoder loss: 0.07108110062119624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08113908767700195 0.2312154769897461

Final encoder loss: 0.07287449459853977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08155131340026855 0.23017358779907227

Final encoder loss: 0.07406071245616604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07982182502746582 0.22874069213867188


Training emognition model
Final encoder loss: 0.1935737580060959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2463974952697754 0.048330068588256836

Final encoder loss: 0.19493944942951202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2460010051727295 0.048230886459350586

Final encoder loss: 0.08450980484485626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24727153778076172 0.04945707321166992

Final encoder loss: 0.08578650653362274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24601340293884277 0.048200368881225586

Final encoder loss: 0.06920025497674942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24672889709472656 0.0489659309387207

Final encoder loss: 0.06971221417188644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24640226364135742 0.04924821853637695

Final encoder loss: 0.06577841937541962
Final encoder loss: 0.06624878197908401

Training empatch model
Final encoder loss: 0.1326661707791733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07106590270996094 0.17339301109313965

Final encoder loss: 0.11913161062789193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07145094871520996 0.1739943027496338

Final encoder loss: 0.11555711013268818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07094216346740723 0.17417478561401367

Final encoder loss: 0.10700834759097926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07161402702331543 0.17405366897583008

Final encoder loss: 0.10165211850651457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07150530815124512 0.17382073402404785

Final encoder loss: 0.0948878930325841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07122254371643066 0.17381906509399414

Final encoder loss: 0.10007874788422814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07088732719421387 0.17361712455749512

Final encoder loss: 0.09193909797261209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07048606872558594 0.17354035377502441

Final encoder loss: 0.08567188215979764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07110834121704102 0.17363691329956055

Final encoder loss: 0.07719401936333566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07092976570129395 0.17307829856872559

Final encoder loss: 0.08194708505003638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07068300247192383 0.17308616638183594

Final encoder loss: 0.07909675285472874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07066774368286133 0.1732006072998047

Final encoder loss: 0.07888187840240647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07109904289245605 0.17312192916870117

Final encoder loss: 0.07138153957840176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07083272933959961 0.17303180694580078

Final encoder loss: 0.0765355045276158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07106423377990723 0.17301011085510254

Final encoder loss: 0.09194964112562634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07073760032653809 0.172760009765625


Training empatch model
Final encoder loss: 0.17115457355976105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17100882530212402 0.04297351837158203

Final encoder loss: 0.08791464567184448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.172929048538208 0.043961524963378906

Final encoder loss: 0.07315182685852051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16870665550231934 0.04308819770812988

Final encoder loss: 0.06827989965677261

Training wesad model
Final encoder loss: 0.14996807154132785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0704946517944336 0.17258739471435547

Final encoder loss: 0.14465863508282734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07057023048400879 0.1732161045074463

Final encoder loss: 0.13856504960583407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07050204277038574 0.1730046272277832

Final encoder loss: 0.12391564906948223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07073259353637695 0.1724081039428711

Final encoder loss: 0.10276549889304597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0700216293334961 0.17370915412902832

Final encoder loss: 0.10769164901884236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07108855247497559 0.1737501621246338

Final encoder loss: 0.1009034238310074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0713496208190918 0.17405462265014648

Final encoder loss: 0.10429676530909744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0708010196685791 0.17373013496398926

Final encoder loss: 0.09509212176719246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07111716270446777 0.17414236068725586

Final encoder loss: 0.08777718543881381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07162189483642578 0.17370080947875977

Final encoder loss: 0.08225633591920833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0709378719329834 0.1735527515411377

Final encoder loss: 0.08663715995146701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07228469848632812 0.1733553409576416

Final encoder loss: 0.07547449231957468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07159566879272461 0.17388176918029785

Final encoder loss: 0.078861518236867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07099366188049316 0.1740584373474121

Final encoder loss: 0.07853584167949057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710000991821289 0.1736764907836914

Final encoder loss: 0.07774310891664256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0711214542388916 0.17394042015075684


Training wesad model
Final encoder loss: 0.21558381617069244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10419082641601562 0.03296709060668945

Final encoder loss: 0.10341165214776993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10320186614990234 0.033124685287475586

Final encoder loss: 0.08258561789989471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10268497467041016 0.03280520439147949

Final encoder loss: 0.07583875209093094

Calculating loss for amigos model
	Full Pass 0.6368005275726318
numFreeParamsPath 18
Reconstruction loss values: 0.10382728278636932 0.10855785757303238

Calculating loss for dapper model
	Full Pass 0.15075397491455078
numFreeParamsPath 18
Reconstruction loss values: 0.09257256239652634 0.09678458422422409

Calculating loss for case model
	Full Pass 0.8598175048828125
numFreeParamsPath 18
Reconstruction loss values: 0.11427309364080429 0.11627450585365295

Calculating loss for emognition model
	Full Pass 0.2804386615753174
numFreeParamsPath 18
Reconstruction loss values: 0.1180490255355835 0.1201467514038086

Calculating loss for empatch model
	Full Pass 0.10460519790649414
numFreeParamsPath 18
Reconstruction loss values: 0.12572769820690155 0.1261032521724701

Calculating loss for wesad model
	Full Pass 0.0777442455291748
numFreeParamsPath 18
Reconstruction loss values: 0.15396234393119812 0.1742030829191208
Total loss calculation time: 3.651933431625366

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.263753414154053
Total epoch time: 84.19939804077148

Epoch: 6

Training case model
Final encoder loss: 0.11290006483805101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09827065467834473 0.2692241668701172

Final encoder loss: 0.0958946324279047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09096384048461914 0.26456594467163086

Final encoder loss: 0.08845142358093644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09154939651489258 0.2649259567260742

Final encoder loss: 0.08635378076655402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09159374237060547 0.264850378036499

Final encoder loss: 0.08292970822666892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09174871444702148 0.26501893997192383

Final encoder loss: 0.07904563678336154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09194827079772949 0.26427769660949707

Final encoder loss: 0.07988984981709084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09069538116455078 0.26353025436401367

Final encoder loss: 0.07705234322405091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09068489074707031 0.26367688179016113

Final encoder loss: 0.07630731163762842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09048628807067871 0.2642018795013428

Final encoder loss: 0.07465073092488918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09087276458740234 0.2642083168029785

Final encoder loss: 0.0716859293592595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09050607681274414 0.2641141414642334

Final encoder loss: 0.07446747394731515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09236383438110352 0.2640976905822754

Final encoder loss: 0.07358518502568842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09069252014160156 0.2635180950164795

Final encoder loss: 0.07025068331467113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09067869186401367 0.26419568061828613

Final encoder loss: 0.0685853508945077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09062004089355469 0.26340198516845703

Final encoder loss: 0.0693069020363946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08756279945373535 0.2614274024963379


Training dapper model
Final encoder loss: 0.09853695509589594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06161904335021973 0.1488814353942871

Final encoder loss: 0.08830066454580497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.0621485710144043 0.15160894393920898

Final encoder loss: 0.07882912169599242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06177854537963867 0.15100955963134766

Final encoder loss: 0.07152280495224822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06187129020690918 0.15071654319763184

Final encoder loss: 0.06959534393697216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0629429817199707 0.15227603912353516

Final encoder loss: 0.06340168149777069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06232929229736328 0.15006470680236816

Final encoder loss: 0.06087593518646812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.061834096908569336 0.15213680267333984

Final encoder loss: 0.06263663395714049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06199145317077637 0.15083575248718262

Final encoder loss: 0.06154378391270824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06201004981994629 0.15080595016479492

Final encoder loss: 0.06535734723086886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06331062316894531 0.15118646621704102

Final encoder loss: 0.05856290100961544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06219887733459473 0.14966678619384766

Final encoder loss: 0.0622300315426368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06142759323120117 0.15126729011535645

Final encoder loss: 0.056269547139483574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.3882317543029785 0.15198564529418945

Final encoder loss: 0.059364431846111405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06284451484680176 0.15111446380615234

Final encoder loss: 0.0548703356026666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06179690361022949 0.15024662017822266

Final encoder loss: 0.06325963951592298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06305241584777832 0.15117263793945312


Training emognition model
Final encoder loss: 0.11353123453551471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08278346061706543 0.27399158477783203

Final encoder loss: 0.09820963230845359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0828561782836914 0.27310729026794434

Final encoder loss: 0.0910530529163946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08264279365539551 0.2733883857727051

Final encoder loss: 0.09344791297215747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08235621452331543 0.27329158782958984

Final encoder loss: 0.08835341048661245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08239245414733887 0.2734382152557373

Final encoder loss: 0.0864002796072508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08236289024353027 0.27384305000305176

Final encoder loss: 0.09102357866466776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08311581611633301 0.27344202995300293

Final encoder loss: 0.08289051562296446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08265066146850586 0.27331972122192383

Final encoder loss: 0.08180661285120114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08266282081604004 0.27391886711120605

Final encoder loss: 0.0886747381458559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08252382278442383 0.2735581398010254

Final encoder loss: 0.0776627187685952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08260583877563477 0.27292943000793457

Final encoder loss: 0.08409745518628965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08263921737670898 0.2761650085449219

Final encoder loss: 0.0780205620903796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08433699607849121 0.2756357192993164

Final encoder loss: 0.07623130631910023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08487582206726074 0.2754971981048584

Final encoder loss: 0.07357744225423826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08405494689941406 0.2763497829437256

Final encoder loss: 0.0763394444893082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08285760879516602 0.27425050735473633


Training amigos model
Final encoder loss: 0.09686627933717078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11784100532531738 0.38770389556884766

Final encoder loss: 0.08416911787684994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10747122764587402 0.3871471881866455

Final encoder loss: 0.0907925810547153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10750150680541992 0.3876380920410156

Final encoder loss: 0.07799815404857767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10753893852233887 0.38747262954711914

Final encoder loss: 0.07905114016345355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10756397247314453 0.387753963470459

Final encoder loss: 0.07696520187388921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10776424407958984 0.3877294063568115

Final encoder loss: 0.08262973715859319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10754847526550293 0.38872671127319336

Final encoder loss: 0.08133183804899262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10812091827392578 0.3887002468109131

Final encoder loss: 0.06685255514947452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10845160484313965 0.38933682441711426

Final encoder loss: 0.07921856031174382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10851335525512695 0.38825297355651855

Final encoder loss: 0.07290582441937055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1074686050415039 0.3894524574279785

Final encoder loss: 0.06573349699046376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.11127257347106934 0.39038562774658203

Final encoder loss: 0.06866956848330999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10733747482299805 0.3891739845275879

Final encoder loss: 0.07272747391886024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1078646183013916 0.38890671730041504

Final encoder loss: 0.07084887592672705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10760307312011719 0.38800621032714844

Final encoder loss: 0.06810573829776159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10355663299560547 0.3861100673675537


Training amigos model
Final encoder loss: 0.05539536025538633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10805559158325195 0.34195590019226074

Final encoder loss: 0.06080868641484818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1056215763092041 0.34134411811828613

Final encoder loss: 0.055531106731151275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10640811920166016 0.34179091453552246

Final encoder loss: 0.05912952201594962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10559868812561035 0.34122729301452637

Final encoder loss: 0.05779372047939181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10582304000854492 0.34147071838378906

Final encoder loss: 0.05557387205584085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1190497875213623 0.34259867668151855

Final encoder loss: 0.05540675951168977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10559201240539551 0.341327428817749

Final encoder loss: 0.05467776840341685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1059579849243164 0.34129929542541504

Final encoder loss: 0.05324549528347035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10567665100097656 0.34146571159362793

Final encoder loss: 0.053111807176912953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10649895668029785 0.3420889377593994

Final encoder loss: 0.053470218101779564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1059718132019043 0.3411743640899658

Final encoder loss: 0.05948523606495191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10572671890258789 0.34081220626831055

Final encoder loss: 0.054588704875919936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10588264465332031 0.34077024459838867

Final encoder loss: 0.05434048971550397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10570001602172852 0.3411223888397217

Final encoder loss: 0.05319609851028604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10593533515930176 0.34113407135009766

Final encoder loss: 0.06066064085945163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1008753776550293 0.33725738525390625


Training amigos model
Final encoder loss: 0.1807606816291809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4086432456970215 0.07630658149719238

Final encoder loss: 0.1878332793712616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.39228367805480957 0.07550907135009766

Final encoder loss: 0.1836363524198532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40520453453063965 0.07468914985656738

Final encoder loss: 0.06850361824035645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.41938042640686035 0.07533907890319824

Final encoder loss: 0.07230377197265625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.40893983840942383 0.07455873489379883

Final encoder loss: 0.06750821322202682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4156346321105957 0.07598352432250977

Final encoder loss: 0.0531221479177475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.39272022247314453 0.07362842559814453

Final encoder loss: 0.05600734427571297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.40715861320495605 0.07547760009765625

Final encoder loss: 0.05377763882279396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.3901076316833496 0.07429385185241699

Final encoder loss: 0.05134614184498787
Final encoder loss: 0.05289285257458687
Final encoder loss: 0.05067459121346474

Training dapper model
Final encoder loss: 0.05373252928011723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05961251258850098 0.10620307922363281

Final encoder loss: 0.048508688325784366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059137582778930664 0.1058504581451416

Final encoder loss: 0.04830570204411047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.058858394622802734 0.10568404197692871

Final encoder loss: 0.052993146354212976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05840563774108887 0.10620236396789551

Final encoder loss: 0.047141155612189165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.058949947357177734 0.10626745223999023

Final encoder loss: 0.04944860183876403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.058858394622802734 0.10586309432983398

Final encoder loss: 0.047850384175403764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0590672492980957 0.10574555397033691

Final encoder loss: 0.050588141704700024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05900001525878906 0.1064920425415039

Final encoder loss: 0.04857531288735414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05911993980407715 0.10614132881164551

Final encoder loss: 0.04973293624088098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05899190902709961 0.10639667510986328

Final encoder loss: 0.051441878852438526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059593915939331055 0.10651040077209473

Final encoder loss: 0.0516260501054436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05889105796813965 0.10646677017211914

Final encoder loss: 0.0493160321773429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05898261070251465 0.10644102096557617

Final encoder loss: 0.048863935265922066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05910444259643555 0.10640335083007812

Final encoder loss: 0.05311105313555257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05906820297241211 0.10638070106506348

Final encoder loss: 0.05841498564467194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05870652198791504 0.10628604888916016


Training dapper model
Final encoder loss: 0.2024507075548172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11327195167541504 0.03374505043029785

Final encoder loss: 0.20818041265010834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11106061935424805 0.033667564392089844

Final encoder loss: 0.06996738165616989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1120004653930664 0.0338284969329834

Final encoder loss: 0.07129795104265213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11133241653442383 0.033308982849121094

Final encoder loss: 0.049614883959293365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11158347129821777 0.03393983840942383

Final encoder loss: 0.05037098377943039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11195015907287598 0.03379988670349121

Final encoder loss: 0.04411671310663223
Final encoder loss: 0.04498407989740372

Training case model
Final encoder loss: 0.07119798509242066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08894109725952148 0.21832823753356934

Final encoder loss: 0.0664046271298376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08860397338867188 0.21841907501220703

Final encoder loss: 0.06766098242281196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08860921859741211 0.2186117172241211

Final encoder loss: 0.06731281498551775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08927726745605469 0.2185065746307373

Final encoder loss: 0.06650975540649012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08954954147338867 0.21842360496520996

Final encoder loss: 0.06474871148373595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08897161483764648 0.2184433937072754

Final encoder loss: 0.06709977173861967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08890318870544434 0.21809029579162598

Final encoder loss: 0.0672321547386574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.0890493392944336 0.2185366153717041

Final encoder loss: 0.06451666627609247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08846616744995117 0.2186286449432373

Final encoder loss: 0.06427967270917248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08888554573059082 0.21829771995544434

Final encoder loss: 0.06484231042864771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08875775337219238 0.2184004783630371

Final encoder loss: 0.06356540593469666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08899903297424316 0.21857404708862305

Final encoder loss: 0.06436624992720155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08913660049438477 0.2185070514678955

Final encoder loss: 0.06504726584876266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08878707885742188 0.2184925079345703

Final encoder loss: 0.06371539737209242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.088653564453125 0.21829938888549805

Final encoder loss: 0.0631930962859946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08536839485168457 0.21474742889404297


Training case model
Final encoder loss: 0.2029666155576706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25698161125183105 0.05186963081359863

Final encoder loss: 0.18891195952892303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2650301456451416 0.05232381820678711

Final encoder loss: 0.19013884663581848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25557613372802734 0.051621437072753906

Final encoder loss: 0.192182719707489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25663280487060547 0.05247378349304199

Final encoder loss: 0.1808091104030609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25568509101867676 0.05217719078063965

Final encoder loss: 0.1919243186712265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2529759407043457 0.0509183406829834

Final encoder loss: 0.09035420417785645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2557651996612549 0.052073001861572266

Final encoder loss: 0.08201563358306885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25556373596191406 0.05289053916931152

Final encoder loss: 0.07825452834367752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25536179542541504 0.053026437759399414

Final encoder loss: 0.08116760104894638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25580430030822754 0.05197739601135254

Final encoder loss: 0.07641902565956116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2570338249206543 0.05198311805725098

Final encoder loss: 0.0781221091747284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541184425354004 0.051399946212768555

Final encoder loss: 0.06668189913034439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.255967378616333 0.053462982177734375

Final encoder loss: 0.06299745291471481
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2554199695587158 0.052700042724609375

Final encoder loss: 0.06090036779642105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2651553153991699 0.05181407928466797

Final encoder loss: 0.06516794115304947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2551565170288086 0.05186748504638672

Final encoder loss: 0.06306017190217972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26569557189941406 0.05317044258117676

Final encoder loss: 0.0634022131562233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25116753578186035 0.05058908462524414

Final encoder loss: 0.06527300924062729
Final encoder loss: 0.06335389614105225
Final encoder loss: 0.060759536921978
Final encoder loss: 0.06365112215280533
Final encoder loss: 0.06106557324528694
Final encoder loss: 0.05997602641582489

Training emognition model
Final encoder loss: 0.07294741470478742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08103132247924805 0.22951102256774902

Final encoder loss: 0.06621109310824481
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0808100700378418 0.22959399223327637

Final encoder loss: 0.0665158472419805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08064746856689453 0.22945475578308105

Final encoder loss: 0.06442652309774986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08043575286865234 0.2294907569885254

Final encoder loss: 0.0692826069253888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08036136627197266 0.2295827865600586

Final encoder loss: 0.0673151878147252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08009505271911621 0.23068690299987793

Final encoder loss: 0.0695772562376749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08121299743652344 0.23085594177246094

Final encoder loss: 0.06970836247325969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08106279373168945 0.23131561279296875

Final encoder loss: 0.06871572419946677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08227968215942383 0.23045086860656738

Final encoder loss: 0.07227856321007839
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08101105690002441 0.23052644729614258

Final encoder loss: 0.06716772788914717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08187031745910645 0.2310326099395752

Final encoder loss: 0.06949474867786286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08092880249023438 0.23092055320739746

Final encoder loss: 0.06417958150266462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0811314582824707 0.230881929397583

Final encoder loss: 0.06962016680207765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08198738098144531 0.23110151290893555

Final encoder loss: 0.06803773715235015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08140850067138672 0.23112201690673828

Final encoder loss: 0.0680295377609234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0800468921661377 0.23052740097045898


Training emognition model
Final encoder loss: 0.193562313914299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2504563331604004 0.0492093563079834

Final encoder loss: 0.19496212899684906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.246537446975708 0.04854989051818848

Final encoder loss: 0.08162087947130203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482302188873291 0.04966998100280762

Final encoder loss: 0.08210735023021698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24802231788635254 0.04940223693847656

Final encoder loss: 0.06592825055122375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25176429748535156 0.04843735694885254

Final encoder loss: 0.0659232959151268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24666905403137207 0.048551321029663086

Final encoder loss: 0.06278423219919205
Final encoder loss: 0.06260818988084793

Training empatch model
Final encoder loss: 0.13158613088920804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07108592987060547 0.1735377311706543

Final encoder loss: 0.11715789073139339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07074451446533203 0.1739206314086914

Final encoder loss: 0.10817657163321044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0732579231262207 0.1742715835571289

Final encoder loss: 0.10253479948315422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0717921257019043 0.17398762702941895

Final encoder loss: 0.09979706920198263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07135462760925293 0.1741783618927002

Final encoder loss: 0.09153793422166132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0726780891418457 0.1738905906677246

Final encoder loss: 0.09609652808897629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07089853286743164 0.17345118522644043

Final encoder loss: 0.0956063157000162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07036137580871582 0.17331504821777344

Final encoder loss: 0.0789385572289872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07113218307495117 0.1742243766784668

Final encoder loss: 0.07273850198248857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07297730445861816 0.17394804954528809

Final encoder loss: 0.07831288330353872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07105255126953125 0.17417311668395996

Final encoder loss: 0.07075426512563875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07315373420715332 0.17455744743347168

Final encoder loss: 0.0778355478698926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07146358489990234 0.17380619049072266

Final encoder loss: 0.07869500555978949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07132768630981445 0.1742231845855713

Final encoder loss: 0.0687021068604064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07237577438354492 0.17345976829528809

Final encoder loss: 0.0755343609610175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0699927806854248 0.17250609397888184


Training empatch model
Final encoder loss: 0.17114906013011932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17191338539123535 0.043066978454589844

Final encoder loss: 0.08487813174724579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17167019844055176 0.042910099029541016

Final encoder loss: 0.07023550570011139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17155933380126953 0.04354500770568848

Final encoder loss: 0.06542175263166428

Training wesad model
Final encoder loss: 0.15296676234621187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07085418701171875 0.17224478721618652

Final encoder loss: 0.1358971558400007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07026386260986328 0.1724851131439209

Final encoder loss: 0.12318575414360004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07025837898254395 0.1723794937133789

Final encoder loss: 0.12163718739763331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07016515731811523 0.17223787307739258

Final encoder loss: 0.10013691677732821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.06972837448120117 0.17255210876464844

Final encoder loss: 0.10167768571199902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07014250755310059 0.17271065711975098

Final encoder loss: 0.09874553457307624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07037687301635742 0.17268776893615723

Final encoder loss: 0.09457582951715929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07039475440979004 0.17272520065307617

Final encoder loss: 0.07922549770435242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.06995820999145508 0.1736142635345459

Final encoder loss: 0.08257598851686236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0714576244354248 0.17349958419799805

Final encoder loss: 0.08272278312210177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07050156593322754 0.1739637851715088

Final encoder loss: 0.08714748349775427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07105565071105957 0.17410778999328613

Final encoder loss: 0.07303207022889342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07142210006713867 0.17383718490600586

Final encoder loss: 0.07437359083995908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07114076614379883 0.17409706115722656

Final encoder loss: 0.07319547788706711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07099509239196777 0.17397189140319824

Final encoder loss: 0.075085916080183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07132840156555176 0.1735544204711914


Training wesad model
Final encoder loss: 0.21561381220817566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10476016998291016 0.0327610969543457

Final encoder loss: 0.09993647783994675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10282278060913086 0.03318452835083008

Final encoder loss: 0.0786365494132042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10305023193359375 0.03237462043762207

Final encoder loss: 0.07203156501054764

Calculating loss for amigos model
	Full Pass 0.6692769527435303
numFreeParamsPath 18
Reconstruction loss values: 0.09694190323352814 0.10250566899776459

Calculating loss for dapper model
	Full Pass 0.1501762866973877
numFreeParamsPath 18
Reconstruction loss values: 0.08855771273374557 0.09247667342424393

Calculating loss for case model
	Full Pass 0.8543610572814941
numFreeParamsPath 18
Reconstruction loss values: 0.10701470822095871 0.10938309878110886

Calculating loss for emognition model
	Full Pass 0.27843546867370605
numFreeParamsPath 18
Reconstruction loss values: 0.10998163372278214 0.11327563971281052

Calculating loss for empatch model
	Full Pass 0.10439181327819824
numFreeParamsPath 18
Reconstruction loss values: 0.11961178481578827 0.12064211815595627

Calculating loss for wesad model
	Full Pass 0.07674002647399902
numFreeParamsPath 18
Reconstruction loss values: 0.1458061784505844 0.16620075702667236
Total loss calculation time: 3.8301916122436523

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.7764573097229
Total epoch time: 84.61504411697388

Epoch: 7

Training dapper model
Final encoder loss: 0.08842406549646963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06662654876708984 0.1539759635925293

Final encoder loss: 0.07546897846371832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06220698356628418 0.15041446685791016

Final encoder loss: 0.06475501573520326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06305909156799316 0.1500849723815918

Final encoder loss: 0.06590397030209112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.0619206428527832 0.1502234935760498

Final encoder loss: 0.06691924829055661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06226634979248047 0.1520226001739502

Final encoder loss: 0.06103783624035391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06196880340576172 0.15037298202514648

Final encoder loss: 0.05881322213482424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.061986446380615234 0.15111422538757324

Final encoder loss: 0.059544848979088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06365799903869629 0.1516427993774414

Final encoder loss: 0.057403571435758464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061971187591552734 0.1512596607208252

Final encoder loss: 0.05854439497391625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06206631660461426 0.15082406997680664

Final encoder loss: 0.059852771242954385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.0634605884552002 0.14989924430847168

Final encoder loss: 0.05590724801527853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06196022033691406 0.15042519569396973

Final encoder loss: 0.05640630475407477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062024831771850586 0.1523149013519287

Final encoder loss: 0.05879213622164306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06180286407470703 0.1510007381439209

Final encoder loss: 0.05571479602203507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06213879585266113 0.1509692668914795

Final encoder loss: 0.05098319217326282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06242227554321289 0.1508333683013916


Training case model
Final encoder loss: 0.10534819074080636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09193253517150879 0.26497793197631836

Final encoder loss: 0.09131283195551079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09225583076477051 0.26528406143188477

Final encoder loss: 0.08616123388495932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09156203269958496 0.2658538818359375

Final encoder loss: 0.08338857063428094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09132623672485352 0.2652769088745117

Final encoder loss: 0.07777938192487358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09217619895935059 0.26570558547973633

Final encoder loss: 0.07852152488889858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09146499633789062 0.26531052589416504

Final encoder loss: 0.07661419333655428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0921180248260498 0.264357328414917

Final encoder loss: 0.07455737460335714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09146785736083984 0.2663602828979492

Final encoder loss: 0.07207983030036676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09174561500549316 0.2652266025543213

Final encoder loss: 0.06989520506071661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09131956100463867 0.26470303535461426

Final encoder loss: 0.07141994565366845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09142422676086426 0.26512932777404785

Final encoder loss: 0.07013329645184808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0920717716217041 0.26549458503723145

Final encoder loss: 0.06873647834062163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09273362159729004 0.26585912704467773

Final encoder loss: 0.06923633096027416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09207415580749512 0.2652604579925537

Final encoder loss: 0.06731037352811343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09124326705932617 0.2642526626586914

Final encoder loss: 0.06752217060641924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08771777153015137 0.2603580951690674


Training emognition model
Final encoder loss: 0.10391791122014608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0829010009765625 0.27271199226379395

Final encoder loss: 0.0927632278681485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08246898651123047 0.27394628524780273

Final encoder loss: 0.09147423225330116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08275890350341797 0.2735772132873535

Final encoder loss: 0.0878276802331912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08248019218444824 0.2732400894165039

Final encoder loss: 0.08903632505362276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08268237113952637 0.27344751358032227

Final encoder loss: 0.08635348084211156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08226513862609863 0.27336740493774414

Final encoder loss: 0.08409622907508651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08222198486328125 0.27364492416381836

Final encoder loss: 0.08330984679214948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08238840103149414 0.27312254905700684

Final encoder loss: 0.0831627349508501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0824120044708252 0.2759377956390381

Final encoder loss: 0.07907899198031448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08414363861083984 0.2758207321166992

Final encoder loss: 0.07535514819396202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08347010612487793 0.2748234272003174

Final encoder loss: 0.07371779929870471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08347821235656738 0.27562427520751953

Final encoder loss: 0.0757531672018927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08375310897827148 0.2757561206817627

Final encoder loss: 0.07112626373298003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08303451538085938 0.2759866714477539

Final encoder loss: 0.07341815476109019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08353734016418457 0.27510619163513184

Final encoder loss: 0.0753683270238619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08300042152404785 0.2752249240875244


Training amigos model
Final encoder loss: 0.09217531417873846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10835576057434082 0.39031267166137695

Final encoder loss: 0.09454175793020346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.1080930233001709 0.38878917694091797

Final encoder loss: 0.08104161110247732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10811734199523926 0.3892221450805664

Final encoder loss: 0.07400083148643012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10779190063476562 0.3872044086456299

Final encoder loss: 0.07270931341571193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10766172409057617 0.38794660568237305

Final encoder loss: 0.07860254687501186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10723376274108887 0.38744568824768066

Final encoder loss: 0.06749665433258638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1077263355255127 0.3878788948059082

Final encoder loss: 0.0629838193391756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10768508911132812 0.3875758647918701

Final encoder loss: 0.07504321869390869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10734152793884277 0.3881523609161377

Final encoder loss: 0.07009383471626499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10772967338562012 0.38918638229370117

Final encoder loss: 0.06752985529800579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10822725296020508 0.38831448554992676

Final encoder loss: 0.06662969149550203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10828042030334473 0.388836145401001

Final encoder loss: 0.0679274431491075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10899806022644043 0.39027881622314453

Final encoder loss: 0.0718188516446186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10846829414367676 0.3879201412200928

Final encoder loss: 0.0675051909167546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10725188255310059 0.38775062561035156

Final encoder loss: 0.06639904112704656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10696864128112793 0.38292765617370605


Training amigos model
Final encoder loss: 0.05477085492577656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10600638389587402 0.34075260162353516

Final encoder loss: 0.06098198818027506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10587215423583984 0.3414433002471924

Final encoder loss: 0.053713891893675335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10592436790466309 0.3407173156738281

Final encoder loss: 0.05584862531843822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10543155670166016 0.34053659439086914

Final encoder loss: 0.048719798677270824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10542011260986328 0.3412806987762451

Final encoder loss: 0.05332676201889102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10590410232543945 0.34134340286254883

Final encoder loss: 0.05517079077940344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10613036155700684 0.3416452407836914

Final encoder loss: 0.05088411951447454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10604023933410645 0.3413877487182617

Final encoder loss: 0.0541251506129754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10618734359741211 0.34160423278808594

Final encoder loss: 0.053924895556635044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10569930076599121 0.34192705154418945

Final encoder loss: 0.052122843750322986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10762548446655273 0.34164905548095703

Final encoder loss: 0.05334031506804133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10645413398742676 0.3415961265563965

Final encoder loss: 0.05299985687004401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10562539100646973 0.34108877182006836

Final encoder loss: 0.05457923652582239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10579252243041992 0.3414628505706787

Final encoder loss: 0.05055476391717886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1064612865447998 0.34218859672546387

Final encoder loss: 0.05328460200859479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1019737720489502 0.3379049301147461


Training amigos model
Final encoder loss: 0.1807844042778015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4374656677246094 0.07708096504211426

Final encoder loss: 0.18783098459243774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4482128620147705 0.07660913467407227

Final encoder loss: 0.18361517786979675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44680047035217285 0.07439851760864258

Final encoder loss: 0.06658250838518143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44832324981689453 0.07469725608825684

Final encoder loss: 0.07000536471605301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.41918301582336426 0.07587528228759766

Final encoder loss: 0.06665120273828506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4290444850921631 0.07470417022705078

Final encoder loss: 0.0505831204354763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44715213775634766 0.07657241821289062

Final encoder loss: 0.0532795749604702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4315214157104492 0.07555103302001953

Final encoder loss: 0.0520913228392601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4176943302154541 0.07385468482971191

Final encoder loss: 0.04911664500832558
Final encoder loss: 0.050476375967264175
Final encoder loss: 0.048475466668605804

Training dapper model
Final encoder loss: 0.04570340933668039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.0594632625579834 0.10612177848815918

Final encoder loss: 0.05069334540081423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05888533592224121 0.10640192031860352

Final encoder loss: 0.04925155154127207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05872917175292969 0.10660219192504883

Final encoder loss: 0.0520008283793297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0589900016784668 0.10619997978210449

Final encoder loss: 0.04849006015560347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.058969736099243164 0.10641670227050781

Final encoder loss: 0.04428059472061964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05880451202392578 0.10605120658874512

Final encoder loss: 0.04591121655892367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05898332595825195 0.1062006950378418

Final encoder loss: 0.04939379639665809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05858469009399414 0.10566210746765137

Final encoder loss: 0.048216904849586045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05892753601074219 0.10629081726074219

Final encoder loss: 0.046796147981157596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0592801570892334 0.1055917739868164

Final encoder loss: 0.049140755335280686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059151411056518555 0.10607528686523438

Final encoder loss: 0.050981953911833584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05898261070251465 0.10636472702026367

Final encoder loss: 0.04561545308953845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05887484550476074 0.10624504089355469

Final encoder loss: 0.047116477366764255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05917978286743164 0.10644102096557617

Final encoder loss: 0.048955815396200225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05904650688171387 0.10648703575134277

Final encoder loss: 0.04614084758921867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05879473686218262 0.1062462329864502


Training dapper model
Final encoder loss: 0.20239856839179993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1142420768737793 0.03361678123474121

Final encoder loss: 0.2082034796476364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11245226860046387 0.033033132553100586

Final encoder loss: 0.06831183284521103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11247801780700684 0.03383231163024902

Final encoder loss: 0.07008954882621765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11255836486816406 0.03356456756591797

Final encoder loss: 0.04819644242525101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11311864852905273 0.033799171447753906

Final encoder loss: 0.048701170831918716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11232542991638184 0.033838748931884766

Final encoder loss: 0.04288097098469734
Final encoder loss: 0.04349028319120407

Training case model
Final encoder loss: 0.0660402601876307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0890955924987793 0.21823501586914062

Final encoder loss: 0.06596467662353281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0890662670135498 0.2183992862701416

Final encoder loss: 0.06449940672890127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08859515190124512 0.2185056209564209

Final encoder loss: 0.06343776529173031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08931922912597656 0.21851849555969238

Final encoder loss: 0.06456993501913132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08936071395874023 0.2183837890625

Final encoder loss: 0.06312102287563698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08904528617858887 0.21802234649658203

Final encoder loss: 0.06181899225200297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08882474899291992 0.21821331977844238

Final encoder loss: 0.061190003471232275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08879685401916504 0.21850275993347168

Final encoder loss: 0.06349545619236166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.0886833667755127 0.21834135055541992

Final encoder loss: 0.06245903617340139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08868122100830078 0.21864938735961914

Final encoder loss: 0.06144557916982208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08919215202331543 0.21858954429626465

Final encoder loss: 0.06111910859982955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08942556381225586 0.21803665161132812

Final encoder loss: 0.06281578955452116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08874917030334473 0.21847152709960938

Final encoder loss: 0.06049743205986223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08951425552368164 0.21849942207336426

Final encoder loss: 0.062088653339550236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08886981010437012 0.2185678482055664

Final encoder loss: 0.061679694186703674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08582901954650879 0.21503233909606934


Training case model
Final encoder loss: 0.20296727120876312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2734684944152832 0.05149674415588379

Final encoder loss: 0.18890470266342163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25580787658691406 0.05326390266418457

Final encoder loss: 0.19014623761177063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2659015655517578 0.0516057014465332

Final encoder loss: 0.1922016441822052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2557826042175293 0.05178642272949219

Final encoder loss: 0.1808099001646042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2656998634338379 0.0524141788482666

Final encoder loss: 0.19191868603229523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2532386779785156 0.051329612731933594

Final encoder loss: 0.08741379529237747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2664175033569336 0.05221843719482422

Final encoder loss: 0.07973591238260269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26624083518981934 0.05169224739074707

Final encoder loss: 0.07704170048236847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2797353267669678 0.05189037322998047

Final encoder loss: 0.07868687063455582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27152156829833984 0.050968170166015625

Final encoder loss: 0.07464104890823364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2795436382293701 0.052460670471191406

Final encoder loss: 0.07614852488040924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2625894546508789 0.050704002380371094

Final encoder loss: 0.06376324594020844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2653677463531494 0.05244946479797363

Final encoder loss: 0.06075966730713844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26517391204833984 0.052869558334350586

Final encoder loss: 0.05921217426657677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2726731300354004 0.05278825759887695

Final encoder loss: 0.06280950456857681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2651786804199219 0.05320286750793457

Final encoder loss: 0.06132578104734421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26647210121154785 0.05233597755432129

Final encoder loss: 0.06183693930506706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2624640464782715 0.053473472595214844

Final encoder loss: 0.06314609944820404
Final encoder loss: 0.06130477413535118
Final encoder loss: 0.058785974979400635
Final encoder loss: 0.0612807422876358
Final encoder loss: 0.05952310562133789
Final encoder loss: 0.05780654028058052

Training emognition model
Final encoder loss: 0.06760572622941895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08095026016235352 0.23005938529968262

Final encoder loss: 0.06253946500641674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08066487312316895 0.22943377494812012

Final encoder loss: 0.06759989612592059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08058691024780273 0.23004889488220215

Final encoder loss: 0.06450714305688847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08089089393615723 0.22941184043884277

Final encoder loss: 0.06477870463622337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08019399642944336 0.2299494743347168

Final encoder loss: 0.06630593287997136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08031940460205078 0.23006963729858398

Final encoder loss: 0.06527289888417882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08040046691894531 0.22994470596313477

Final encoder loss: 0.0657837994153454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0807194709777832 0.22954916954040527

Final encoder loss: 0.06153315779766037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08008813858032227 0.2296006679534912

Final encoder loss: 0.06612079345575646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.0803072452545166 0.22984886169433594

Final encoder loss: 0.06800208630465547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08015060424804688 0.2294611930847168

Final encoder loss: 0.06684102233454099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08045029640197754 0.22930550575256348

Final encoder loss: 0.06644701783895286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08052730560302734 0.23008060455322266

Final encoder loss: 0.06533550805555959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08040976524353027 0.22946906089782715

Final encoder loss: 0.06582016868462184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08019447326660156 0.22965764999389648

Final encoder loss: 0.06611936292548991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07979941368103027 0.22880005836486816


Training emognition model
Final encoder loss: 0.19356822967529297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494058609008789 0.048102617263793945

Final encoder loss: 0.19495177268981934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24611496925354004 0.04845905303955078

Final encoder loss: 0.07932397723197937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24674296379089355 0.049582719802856445

Final encoder loss: 0.07982101291418076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24657154083251953 0.04787325859069824

Final encoder loss: 0.06356481462717056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24680042266845703 0.048364877700805664

Final encoder loss: 0.06350505352020264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463243007659912 0.048815250396728516

Final encoder loss: 0.06035418435931206
Final encoder loss: 0.06001521274447441

Training empatch model
Final encoder loss: 0.12157196923939775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07071709632873535 0.17347383499145508

Final encoder loss: 0.10867631195423837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07135844230651855 0.17376494407653809

Final encoder loss: 0.10293087294866317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07114052772521973 0.1743144989013672

Final encoder loss: 0.10395473127859294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0731515884399414 0.17381572723388672

Final encoder loss: 0.09453481308567159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07140159606933594 0.17430877685546875

Final encoder loss: 0.09142698718461795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07257461547851562 0.17440462112426758

Final encoder loss: 0.08497804298784536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07245135307312012 0.17381572723388672

Final encoder loss: 0.08459953431018434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0704188346862793 0.17392206192016602

Final encoder loss: 0.07347776486132691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.08920001983642578 0.17352008819580078

Final encoder loss: 0.07580216082203699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0711526870727539 0.17479729652404785

Final encoder loss: 0.07477264553185849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0727376937866211 0.17362570762634277

Final encoder loss: 0.07075549652332652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07163310050964355 0.17359495162963867

Final encoder loss: 0.06588178827552857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07114815711975098 0.17409110069274902

Final encoder loss: 0.07263407798563616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07230949401855469 0.1743180751800537

Final encoder loss: 0.0726414630451244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0709238052368164 0.1739940643310547

Final encoder loss: 0.07333682414286151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0709683895111084 0.17387628555297852


Training empatch model
Final encoder loss: 0.17116303741931915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1738748550415039 0.044069766998291016

Final encoder loss: 0.08222730457782745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758875846862793 0.0428011417388916

Final encoder loss: 0.06743676215410233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1691422462463379 0.04443979263305664

Final encoder loss: 0.06269621849060059

Training wesad model
Final encoder loss: 0.14451292621254203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07212257385253906 0.17443037033081055

Final encoder loss: 0.13662132266477112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07120203971862793 0.17383742332458496

Final encoder loss: 0.1141660145829091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0707249641418457 0.173569917678833

Final encoder loss: 0.11462271132357826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07199907302856445 0.17459726333618164

Final encoder loss: 0.09457507941319197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07091212272644043 0.1729888916015625

Final encoder loss: 0.09381621732577647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07118701934814453 0.17369842529296875

Final encoder loss: 0.0918668886445873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07078146934509277 0.1742994785308838

Final encoder loss: 0.08983095572479145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07247066497802734 0.1743764877319336

Final encoder loss: 0.07936948116371226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0710761547088623 0.17391228675842285

Final encoder loss: 0.07660121731421496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07087278366088867 0.17357802391052246

Final encoder loss: 0.08384629692357935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07169628143310547 0.17427325248718262

Final encoder loss: 0.07883953770349407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07061982154846191 0.17359590530395508

Final encoder loss: 0.06893164581260744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07040143013000488 0.17371630668640137

Final encoder loss: 0.06824636127051562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07084918022155762 0.17418646812438965

Final encoder loss: 0.06995430130331044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07215213775634766 0.17394018173217773

Final encoder loss: 0.07191825301482221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07120919227600098 0.17381691932678223


Training wesad model
Final encoder loss: 0.2155759632587433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10601043701171875 0.03334236145019531

Final encoder loss: 0.09627073258161545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10396361351013184 0.033515214920043945

Final encoder loss: 0.07488204538822174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10236048698425293 0.03319430351257324

Final encoder loss: 0.06833150237798691

Calculating loss for amigos model
	Full Pass 0.636803388595581
numFreeParamsPath 18
Reconstruction loss values: 0.09102444350719452 0.09765108674764633

Calculating loss for dapper model
	Full Pass 0.14992690086364746
numFreeParamsPath 18
Reconstruction loss values: 0.08283250033855438 0.08693718910217285

Calculating loss for case model
	Full Pass 0.8544397354125977
numFreeParamsPath 18
Reconstruction loss values: 0.10136627405881882 0.10280773788690567

Calculating loss for emognition model
	Full Pass 0.2788081169128418
numFreeParamsPath 18
Reconstruction loss values: 0.10359766334295273 0.10756050050258636

Calculating loss for empatch model
	Full Pass 0.10417294502258301
numFreeParamsPath 18
Reconstruction loss values: 0.11419415473937988 0.11627525836229324

Calculating loss for wesad model
	Full Pass 0.07664108276367188
numFreeParamsPath 18
Reconstruction loss values: 0.137969508767128 0.1594545841217041
Total loss calculation time: 3.69770884513855

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.3547203540802
Total epoch time: 84.36330461502075

Epoch: 8

Training emognition model
Final encoder loss: 0.09940399740684137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0880880355834961 0.2825005054473877

Final encoder loss: 0.0878404097982261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08277559280395508 0.2734811305999756

Final encoder loss: 0.08754584310286587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08232426643371582 0.27337002754211426

Final encoder loss: 0.0829652989476208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08254218101501465 0.27405548095703125

Final encoder loss: 0.08121067155997021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08306455612182617 0.27448582649230957

Final encoder loss: 0.08155139956059622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08240103721618652 0.27500176429748535

Final encoder loss: 0.07765843037600134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08390116691589355 0.2758524417877197

Final encoder loss: 0.0780585983015276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08445525169372559 0.2756187915802002

Final encoder loss: 0.079291402808312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08349943161010742 0.27485108375549316

Final encoder loss: 0.07593498567474127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08280348777770996 0.27494382858276367

Final encoder loss: 0.07718991240011001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08341574668884277 0.27630138397216797

Final encoder loss: 0.07221496843462882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08318448066711426 0.27547669410705566

Final encoder loss: 0.0736882414250304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0841679573059082 0.2747018337249756

Final encoder loss: 0.07099091590125095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08305978775024414 0.27462267875671387

Final encoder loss: 0.07184684379447567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08330607414245605 0.27564072608947754

Final encoder loss: 0.06885908865271283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0836191177368164 0.27524852752685547


Training amigos model
Final encoder loss: 0.09421045780185129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10825109481811523 0.38846492767333984

Final encoder loss: 0.07871805565762217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.1077582836151123 0.38886475563049316

Final encoder loss: 0.07194946420352372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1082456111907959 0.3892478942871094

Final encoder loss: 0.07930981347704054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10852241516113281 0.3892648220062256

Final encoder loss: 0.06599055857474984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10925173759460449 0.38861513137817383

Final encoder loss: 0.06810111659533355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10887312889099121 0.39014697074890137

Final encoder loss: 0.06640178290655185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1080319881439209 0.3898005485534668

Final encoder loss: 0.06879848327414484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10807538032531738 0.3890969753265381

Final encoder loss: 0.0707484281378483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10821747779846191 0.38927340507507324

Final encoder loss: 0.0646300306986747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10820674896240234 0.3899650573730469

Final encoder loss: 0.06427378879859322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10824108123779297 0.38966822624206543

Final encoder loss: 0.0603582612672949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10910844802856445 0.38811421394348145

Final encoder loss: 0.05941508246501605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10917139053344727 0.3905808925628662

Final encoder loss: 0.06264432145322116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10808658599853516 0.3904609680175781

Final encoder loss: 0.06301207127505522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10809636116027832 0.38919901847839355

Final encoder loss: 0.06493652122401436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10304975509643555 0.3851776123046875


Training dapper model
Final encoder loss: 0.08165307992234704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06201672554016113 0.15136098861694336

Final encoder loss: 0.07319557166370452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06193733215332031 0.15070772171020508

Final encoder loss: 0.06725768875166623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06256246566772461 0.1513500213623047

Final encoder loss: 0.06030750269515243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.062162160873413086 0.15046024322509766

Final encoder loss: 0.06112331515157379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06239652633666992 0.15212535858154297

Final encoder loss: 0.05698262194928667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06188702583312988 0.15056180953979492

Final encoder loss: 0.06115407165447059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062285661697387695 0.1511373519897461

Final encoder loss: 0.05475624708723585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06154632568359375 0.14875149726867676

Final encoder loss: 0.05330145835692776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06141829490661621 0.14803552627563477

Final encoder loss: 0.058319119613645456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.060965776443481445 0.1488797664642334

Final encoder loss: 0.05369641037970538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061077117919921875 0.1484372615814209

Final encoder loss: 0.05343650591541312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06085777282714844 0.14874505996704102

Final encoder loss: 0.05205493357453118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06090235710144043 0.14878630638122559

Final encoder loss: 0.04943553172934376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06132817268371582 0.14935040473937988

Final encoder loss: 0.05822689569461847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.061094045639038086 0.1493992805480957

Final encoder loss: 0.050944721601782844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.060367584228515625 0.14794707298278809


Training case model
Final encoder loss: 0.10206223423807378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0908803939819336 0.2636687755584717

Final encoder loss: 0.08755774523957101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09065628051757812 0.26437997817993164

Final encoder loss: 0.08294915024567923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09129214286804199 0.2639434337615967

Final encoder loss: 0.07939228275379615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09089207649230957 0.264235258102417

Final encoder loss: 0.07602535756224035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09071516990661621 0.2639439105987549

Final encoder loss: 0.07597465521411396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09107327461242676 0.2657923698425293

Final encoder loss: 0.07359311098481341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09226512908935547 0.2660396099090576

Final encoder loss: 0.069615208281267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09121155738830566 0.265516996383667

Final encoder loss: 0.07083769114969041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09119224548339844 0.26497626304626465

Final encoder loss: 0.0685693009511449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09197664260864258 0.2660374641418457

Final encoder loss: 0.06941184022532994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09153008460998535 0.26494932174682617

Final encoder loss: 0.06800174878808456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09110140800476074 0.26514363288879395

Final encoder loss: 0.06554724371224291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09227466583251953 0.26640963554382324

Final encoder loss: 0.06869008803976949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09101319313049316 0.2649960517883301

Final encoder loss: 0.06546271341560798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09115195274353027 0.2650024890899658

Final encoder loss: 0.0636085665759141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08942365646362305 0.2628040313720703


Training amigos model
Final encoder loss: 0.054096426628971926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10719537734985352 0.3410604000091553

Final encoder loss: 0.05423680219007352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10559773445129395 0.34110164642333984

Final encoder loss: 0.04771985103879117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10580277442932129 0.34095311164855957

Final encoder loss: 0.05144988297145293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10597968101501465 0.34073925018310547

Final encoder loss: 0.049941302199887336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10571074485778809 0.34073925018310547

Final encoder loss: 0.05469726696314828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10565328598022461 0.3408350944519043

Final encoder loss: 0.050811315979473956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10555410385131836 0.34073591232299805

Final encoder loss: 0.05241674364786138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1057288646697998 0.3409240245819092

Final encoder loss: 0.050433855423627084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10584115982055664 0.3414161205291748

Final encoder loss: 0.05090226553862841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10698533058166504 0.34154319763183594

Final encoder loss: 0.05360486110879365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10677838325500488 0.3418009281158447

Final encoder loss: 0.04917362556700479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.1067805290222168 0.3429110050201416

Final encoder loss: 0.04835019856713248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10595703125 0.34087705612182617

Final encoder loss: 0.05769623248284915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10586285591125488 0.3407866954803467

Final encoder loss: 0.05417199326855179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10512542724609375 0.3405170440673828

Final encoder loss: 0.0539112010224148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10037541389465332 0.3366563320159912


Training amigos model
Final encoder loss: 0.18079447746276855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43817806243896484 0.07343602180480957

Final encoder loss: 0.18784627318382263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4476895332336426 0.0756690502166748

Final encoder loss: 0.1836167722940445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.40709614753723145 0.07346105575561523

Final encoder loss: 0.06538888812065125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45293354988098145 0.07492446899414062

Final encoder loss: 0.06892494857311249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4084208011627197 0.07566523551940918

Final encoder loss: 0.06380950659513474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4256441593170166 0.08074784278869629

Final encoder loss: 0.04915207624435425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4102640151977539 0.08155655860900879

Final encoder loss: 0.051350753754377365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44554948806762695 0.07195544242858887

Final encoder loss: 0.049893613904714584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.41738128662109375 0.07959294319152832

Final encoder loss: 0.04675016179680824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.436237096786499 0.07467532157897949

Final encoder loss: 0.048603154718875885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4350872039794922 0.07362651824951172

Final encoder loss: 0.048053763806819916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4204227924346924 0.07482194900512695

Final encoder loss: 0.05000082775950432
Final encoder loss: 0.05018039792776108
Final encoder loss: 0.04769515246152878

Training dapper model
Final encoder loss: 0.0497995771165671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05978512763977051 0.10746407508850098

Final encoder loss: 0.04705130361832193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06003284454345703 0.10806822776794434

Final encoder loss: 0.043313661059184425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06013321876525879 0.1068258285522461

Final encoder loss: 0.04433491506117268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05970025062561035 0.10649490356445312

Final encoder loss: 0.04337414559797763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05901384353637695 0.10693120956420898

Final encoder loss: 0.046175618207829984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05949282646179199 0.10654926300048828

Final encoder loss: 0.04435270202790214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05924654006958008 0.10664916038513184

Final encoder loss: 0.048641777327217355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05935811996459961 0.10660767555236816

Final encoder loss: 0.045077542926026275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05944705009460449 0.1064453125

Final encoder loss: 0.04531835712494425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.059499502182006836 0.10675764083862305

Final encoder loss: 0.043507293575271846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05886125564575195 0.10645508766174316

Final encoder loss: 0.043210652336313146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05928635597229004 0.10651659965515137

Final encoder loss: 0.04438432371014734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05931568145751953 0.10645198822021484

Final encoder loss: 0.04461249778456859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05898237228393555 0.10631346702575684

Final encoder loss: 0.043261305269653445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05913066864013672 0.10591650009155273

Final encoder loss: 0.04578918442060103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.0590362548828125 0.1057882308959961


Training dapper model
Final encoder loss: 0.20244893431663513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11233711242675781 0.034120798110961914

Final encoder loss: 0.2081936001777649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11233401298522949 0.033808231353759766

Final encoder loss: 0.06637415289878845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11241602897644043 0.034053802490234375

Final encoder loss: 0.06668106466531754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11097359657287598 0.03360867500305176

Final encoder loss: 0.045634619891643524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11176085472106934 0.03384971618652344

Final encoder loss: 0.04571593552827835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11143207550048828 0.03375053405761719

Final encoder loss: 0.04020592197775841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11146354675292969 0.03385162353515625

Final encoder loss: 0.04072028025984764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11261725425720215 0.03316521644592285

Final encoder loss: 0.03989348188042641
Final encoder loss: 0.03964542970061302

Training case model
Final encoder loss: 0.06020827825388467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08872079849243164 0.21846842765808105

Final encoder loss: 0.0602529236700668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08889961242675781 0.218519926071167

Final encoder loss: 0.06051268113396402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08909058570861816 0.21848630905151367

Final encoder loss: 0.06065086702450227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08879280090332031 0.2187361717224121

Final encoder loss: 0.060927487558831595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08878850936889648 0.2186145782470703

Final encoder loss: 0.060976576697831866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0887458324432373 0.2185368537902832

Final encoder loss: 0.06020860671705065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08938479423522949 0.21832561492919922

Final encoder loss: 0.058752015945160045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08857083320617676 0.2184133529663086

Final encoder loss: 0.05820356638017157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08881950378417969 0.2185986042022705

Final encoder loss: 0.06068284344530002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08905220031738281 0.21854138374328613

Final encoder loss: 0.05908022364662016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08857059478759766 0.21841025352478027

Final encoder loss: 0.0578748747683605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08960747718811035 0.21851444244384766

Final encoder loss: 0.057746351961617724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08861732482910156 0.2203214168548584

Final encoder loss: 0.05980915374946115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08922672271728516 0.21838736534118652

Final encoder loss: 0.05840926375851013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08926630020141602 0.21837925910949707

Final encoder loss: 0.059713851063562425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08520817756652832 0.21487021446228027


Training case model
Final encoder loss: 0.2029689997434616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568025588989258 0.05204629898071289

Final encoder loss: 0.1889098733663559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25531911849975586 0.05137157440185547

Final encoder loss: 0.1901545226573944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2565023899078369 0.0518801212310791

Final encoder loss: 0.19218389689922333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25682806968688965 0.05228996276855469

Final encoder loss: 0.18080326914787292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2560722827911377 0.05158567428588867

Final encoder loss: 0.19191701710224152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537076473236084 0.050779104232788086

Final encoder loss: 0.08581450581550598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25562214851379395 0.05165553092956543

Final encoder loss: 0.07823087275028229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2569122314453125 0.05269742012023926

Final encoder loss: 0.07504920661449432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25577783584594727 0.05067753791809082

Final encoder loss: 0.07681022584438324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25545835494995117 0.05217909812927246

Final encoder loss: 0.07294619083404541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2659127712249756 0.05229926109313965

Final encoder loss: 0.07432036101818085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25317978858947754 0.05144691467285156

Final encoder loss: 0.061344023793935776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2559325695037842 0.05257081985473633

Final encoder loss: 0.05852990970015526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25673985481262207 0.05106353759765625

Final encoder loss: 0.057082220911979675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2562069892883301 0.05260920524597168

Final encoder loss: 0.06052275374531746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257235050201416 0.0517728328704834

Final encoder loss: 0.05890835449099541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.267075777053833 0.05215024948120117

Final encoder loss: 0.05909692123532295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25355029106140137 0.0504913330078125

Final encoder loss: 0.06034667044878006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25534558296203613 0.05299210548400879

Final encoder loss: 0.05919196084141731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564525604248047 0.05182671546936035

Final encoder loss: 0.05851030349731445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2557389736175537 0.050933122634887695

Final encoder loss: 0.06195054575800896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568955421447754 0.0517725944519043

Final encoder loss: 0.06212307885289192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2557101249694824 0.052309513092041016

Final encoder loss: 0.060828372836112976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550935745239258 0.0510563850402832

Final encoder loss: 0.06511732935905457
Final encoder loss: 0.06284481287002563
Final encoder loss: 0.06059873476624489
Final encoder loss: 0.06084968522191048
Final encoder loss: 0.05785674974322319
Final encoder loss: 0.05526495352387428

Training emognition model
Final encoder loss: 0.06660292397214626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08037972450256348 0.22949838638305664

Final encoder loss: 0.06281778361794794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08040308952331543 0.22932910919189453

Final encoder loss: 0.060829970656233724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08014893531799316 0.22939801216125488

Final encoder loss: 0.06404023347759609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08022451400756836 0.2292485237121582

Final encoder loss: 0.06471997883373458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08054900169372559 0.2294304370880127

Final encoder loss: 0.06284731902619108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08067703247070312 0.22981929779052734

Final encoder loss: 0.06742809109149754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.0804131031036377 0.2293715476989746

Final encoder loss: 0.0655472263518264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08043503761291504 0.22915029525756836

Final encoder loss: 0.06028105658453374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.07985401153564453 0.22961139678955078

Final encoder loss: 0.06269837988905336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08032941818237305 0.22940397262573242

Final encoder loss: 0.06287161536256122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08057975769042969 0.22950458526611328

Final encoder loss: 0.06534875463492187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08047723770141602 0.22988033294677734

Final encoder loss: 0.06107404139061986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08050704002380371 0.22920823097229004

Final encoder loss: 0.06398977864812928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.0803065299987793 0.22942590713500977

Final encoder loss: 0.06114640109786874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08063864707946777 0.22908639907836914

Final encoder loss: 0.060066734955447475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07940816879272461 0.22873902320861816


Training emognition model
Final encoder loss: 0.19356577098369598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24715185165405273 0.04896664619445801

Final encoder loss: 0.19494186341762543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24610614776611328 0.048508644104003906

Final encoder loss: 0.07831792533397675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2458493709564209 0.04861187934875488

Final encoder loss: 0.07831583172082901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24575304985046387 0.04821467399597168

Final encoder loss: 0.06178585812449455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2453474998474121 0.047688961029052734

Final encoder loss: 0.06149069964885712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2453758716583252 0.04786968231201172

Final encoder loss: 0.05833309888839722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2464277744293213 0.04780745506286621

Final encoder loss: 0.058078669011592865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24462652206420898 0.048783063888549805

Final encoder loss: 0.05868803709745407
Final encoder loss: 0.0574408620595932

Training empatch model
Final encoder loss: 0.11419715485442425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07066607475280762 0.17288470268249512

Final encoder loss: 0.10284335359433852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07090544700622559 0.17265057563781738

Final encoder loss: 0.10156521321541419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07101154327392578 0.17301464080810547

Final encoder loss: 0.09729894472786924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07054543495178223 0.17247796058654785

Final encoder loss: 0.0956653134019789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07081794738769531 0.17297601699829102

Final encoder loss: 0.08772139770886436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07069087028503418 0.17275691032409668

Final encoder loss: 0.08350300953793839
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07031083106994629 0.1740117073059082

Final encoder loss: 0.08435519448953271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07051920890808105 0.1734631061553955

Final encoder loss: 0.07089810499266892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07105708122253418 0.1743621826171875

Final encoder loss: 0.06917908106909067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07320737838745117 0.1736767292022705

Final encoder loss: 0.06866612700601774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07158923149108887 0.17390012741088867

Final encoder loss: 0.07253404270885512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07129216194152832 0.17417573928833008

Final encoder loss: 0.07037887937427108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07210373878479004 0.17423534393310547

Final encoder loss: 0.07220279578302756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07074880599975586 0.17409181594848633

Final encoder loss: 0.06647119064025317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07123899459838867 0.17373037338256836

Final encoder loss: 0.07044957963691265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07074666023254395 0.17392873764038086


Training empatch model
Final encoder loss: 0.17115554213523865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1770164966583252 0.043222904205322266

Final encoder loss: 0.08067210763692856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17487621307373047 0.043210506439208984

Final encoder loss: 0.0657445564866066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17893004417419434 0.04377913475036621

Final encoder loss: 0.060765717178583145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17637395858764648 0.0429534912109375

Final encoder loss: 0.058707345277071

Training wesad model
Final encoder loss: 0.1410519142561238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07102298736572266 0.17388534545898438

Final encoder loss: 0.12119327832690832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07221865653991699 0.17406821250915527

Final encoder loss: 0.12300101268748302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07081007957458496 0.1741786003112793

Final encoder loss: 0.10361113361843279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07108592987060547 0.17376327514648438

Final encoder loss: 0.0909201043528035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07105469703674316 0.1741945743560791

Final encoder loss: 0.08757850284715488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07344889640808105 0.17392420768737793

Final encoder loss: 0.0919533238768359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07132744789123535 0.17360615730285645

Final encoder loss: 0.08822747621407645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07082700729370117 0.17382311820983887

Final encoder loss: 0.07575685619724405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07226443290710449 0.17437005043029785

Final encoder loss: 0.07544770853107906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07120871543884277 0.17395377159118652

Final encoder loss: 0.07294406649543192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07125520706176758 0.17394590377807617

Final encoder loss: 0.07547755141083934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07116222381591797 0.17413926124572754

Final encoder loss: 0.06537959448016828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.5304265022277832 0.17396116256713867

Final encoder loss: 0.06404879687337439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.08235549926757812 0.17505264282226562

Final encoder loss: 0.0712086781961092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07123780250549316 0.17399930953979492

Final encoder loss: 0.06409217511610625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.08369326591491699 0.17355751991271973


Training wesad model
Final encoder loss: 0.21558593213558197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10287976264953613 0.033698081970214844

Final encoder loss: 0.09326975792646408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10518550872802734 0.03401041030883789

Final encoder loss: 0.07179425656795502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10364842414855957 0.03240609169006348

Final encoder loss: 0.06520519405603409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10225582122802734 0.03375506401062012

Final encoder loss: 0.06272703409194946

Calculating loss for amigos model
	Full Pass 0.6728549003601074
numFreeParamsPath 18
Reconstruction loss values: 0.08855485916137695 0.0940960943698883

Calculating loss for dapper model
	Full Pass 0.1528775691986084
numFreeParamsPath 18
Reconstruction loss values: 0.07748696208000183 0.08200562745332718

Calculating loss for case model
	Full Pass 0.8584620952606201
numFreeParamsPath 18
Reconstruction loss values: 0.09722898155450821 0.10011115670204163

Calculating loss for emognition model
	Full Pass 0.2798173427581787
numFreeParamsPath 18
Reconstruction loss values: 0.09824231266975403 0.10204952210187912

Calculating loss for empatch model
	Full Pass 0.10405802726745605
numFreeParamsPath 18
Reconstruction loss values: 0.10851133614778519 0.1109519675374031

Calculating loss for wesad model
	Full Pass 0.07657814025878906
numFreeParamsPath 18
Reconstruction loss values: 0.13066746294498444 0.15289762616157532
Total loss calculation time: 3.832568645477295

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.760415554046631
Total epoch time: 91.42072916030884

Epoch: 9

Training case model
Final encoder loss: 0.0954638876734549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09858036041259766 0.27362680435180664

Final encoder loss: 0.08315632462155355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09140515327453613 0.2646949291229248

Final encoder loss: 0.07743396674169147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09190773963928223 0.2645266056060791

Final encoder loss: 0.07259153584891112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09169793128967285 0.2650134563446045

Final encoder loss: 0.07337827332118511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09104108810424805 0.2647542953491211

Final encoder loss: 0.07167855641651168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0912179946899414 0.2649855613708496

Final encoder loss: 0.07024650823240687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09135603904724121 0.26448702812194824

Final encoder loss: 0.0688946867227428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09101200103759766 0.26525235176086426

Final encoder loss: 0.06677238390042557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09104585647583008 0.26421427726745605

Final encoder loss: 0.06730352963784145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09125781059265137 0.2647089958190918

Final encoder loss: 0.06866426558740567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09105563163757324 0.26452016830444336

Final encoder loss: 0.06454538938799247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.091064453125 0.264829158782959

Final encoder loss: 0.06551293497233926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09128880500793457 0.2643563747406006

Final encoder loss: 0.0644870779101546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09091305732727051 0.2648317813873291

Final encoder loss: 0.06324653244863107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09115457534790039 0.2649955749511719

Final encoder loss: 0.06353353735980383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08827757835388184 0.2629702091217041


Training emognition model
Final encoder loss: 0.10273427596950246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08333563804626465 0.2750244140625

Final encoder loss: 0.0870905721079562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0829763412475586 0.27614521980285645

Final encoder loss: 0.08357021689905696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08439230918884277 0.2755393981933594

Final encoder loss: 0.07525835011851205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08515167236328125 0.2751789093017578

Final encoder loss: 0.07272168562653096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08357453346252441 0.2752687931060791

Final encoder loss: 0.07637587724067044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08336281776428223 0.27524900436401367

Final encoder loss: 0.07634792867020786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08437895774841309 0.27651500701904297

Final encoder loss: 0.07341650335421715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08378481864929199 0.2752218246459961

Final encoder loss: 0.07223129403760666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0844113826751709 0.2750401496887207

Final encoder loss: 0.07167203574141012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08373570442199707 0.2761342525482178

Final encoder loss: 0.07201788667058018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0837702751159668 0.2758657932281494

Final encoder loss: 0.06786618093426569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0847008228302002 0.276151180267334

Final encoder loss: 0.06973497536659211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08363080024719238 0.2749364376068115

Final encoder loss: 0.07206834605911305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08324408531188965 0.27576446533203125

Final encoder loss: 0.06758353823151986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08412766456604004 0.2767374515533447

Final encoder loss: 0.06895176904672781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08662557601928711 0.27510833740234375


Training dapper model
Final encoder loss: 0.08149373701347278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06350207328796387 0.15002679824829102

Final encoder loss: 0.0682049201118498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06231808662414551 0.15033745765686035

Final encoder loss: 0.05408204018677744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.061850547790527344 0.15193819999694824

Final encoder loss: 0.057781644634049285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061913251876831055 0.15088295936584473

Final encoder loss: 0.054679898862533466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06232857704162598 0.15079450607299805

Final encoder loss: 0.058807820639256526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06358027458190918 0.15129637718200684

Final encoder loss: 0.054338818084069936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062306880950927734 0.1512749195098877

Final encoder loss: 0.050759078959614415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.0623171329498291 0.1509246826171875

Final encoder loss: 0.05079432210461627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06366205215454102 0.15099787712097168

Final encoder loss: 0.053323041040755983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.0778646469116211 0.15090322494506836

Final encoder loss: 0.05025594621688678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.062259674072265625 0.15181493759155273

Final encoder loss: 0.04862724563110384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06170153617858887 0.1508181095123291

Final encoder loss: 0.05144320276337887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06233835220336914 0.1504654884338379

Final encoder loss: 0.0501402832095563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06325149536132812 0.15115976333618164

Final encoder loss: 0.04874639863564714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06241297721862793 0.15088510513305664

Final encoder loss: 0.06673635441902448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06192374229431152 0.15067672729492188


Training amigos model
Final encoder loss: 0.08841618616715691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10879325866699219 0.38889050483703613

Final encoder loss: 0.07716271447767685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.1097879409790039 0.3903486728668213

Final encoder loss: 0.06816787177678324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1087486743927002 0.3904585838317871

Final encoder loss: 0.06958587763787678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10845136642456055 0.38915205001831055

Final encoder loss: 0.06672079265978881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10854101181030273 0.389420747756958

Final encoder loss: 0.07197317268013916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10848712921142578 0.38927292823791504

Final encoder loss: 0.06926478110341958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10847997665405273 0.3904132843017578

Final encoder loss: 0.07078428439336316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10921692848205566 0.3895840644836426

Final encoder loss: 0.06305887289852603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10982203483581543 0.38988471031188965

Final encoder loss: 0.06288360416788538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10920906066894531 0.3906090259552002

Final encoder loss: 0.0635207675576076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10942339897155762 0.39026689529418945

Final encoder loss: 0.06136430301670805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10915064811706543 0.38944578170776367

Final encoder loss: 0.06209729490540158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10841965675354004 0.38907575607299805

Final encoder loss: 0.057686076869033626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10854768753051758 0.3891932964324951

Final encoder loss: 0.05921549647818508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10768556594848633 0.38856077194213867

Final encoder loss: 0.06484030740317905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10281872749328613 0.3823506832122803


Training amigos model
Final encoder loss: 0.04408857477792301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10640454292297363 0.3411827087402344

Final encoder loss: 0.050723520346032475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1061103343963623 0.3410205841064453

Final encoder loss: 0.048521508614943205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10608792304992676 0.3408036231994629

Final encoder loss: 0.05128636091602775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1060028076171875 0.3407459259033203

Final encoder loss: 0.051560363306934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1058800220489502 0.34104251861572266

Final encoder loss: 0.048371887358156736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10562419891357422 0.3415968418121338

Final encoder loss: 0.04630360019637751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1063847541809082 0.3415532112121582

Final encoder loss: 0.04852043243327708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10608458518981934 0.34179186820983887

Final encoder loss: 0.04759747907959881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10661792755126953 0.3415799140930176

Final encoder loss: 0.047509520930956416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10698986053466797 0.341658353805542

Final encoder loss: 0.04599988469160208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1062922477722168 0.3418142795562744

Final encoder loss: 0.04413991756736547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10661053657531738 0.3414924144744873

Final encoder loss: 0.04992633327710022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10658717155456543 0.3418247699737549

Final encoder loss: 0.05140128136220347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10642528533935547 0.3416597843170166

Final encoder loss: 0.04813650477494505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10741996765136719 0.34166860580444336

Final encoder loss: 0.05119894895999372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10173320770263672 0.33855533599853516


Training amigos model
Final encoder loss: 0.1807476431131363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.457324743270874 0.07851862907409668

Final encoder loss: 0.18781167268753052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43179869651794434 0.07647180557250977

Final encoder loss: 0.18362580239772797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4432682991027832 0.07390499114990234

Final encoder loss: 0.0664534941315651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4588196277618408 0.0739600658416748

Final encoder loss: 0.0696849673986435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4324531555175781 0.07321524620056152

Final encoder loss: 0.06500844657421112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4434473514556885 0.0738987922668457

Final encoder loss: 0.04797980561852455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4248826503753662 0.07635760307312012

Final encoder loss: 0.04992884024977684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43871498107910156 0.0751504898071289

Final encoder loss: 0.04852703586220741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44727253913879395 0.07707571983337402

Final encoder loss: 0.0444665290415287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45951414108276367 0.07498860359191895

Final encoder loss: 0.04615592211484909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43224477767944336 0.07447171211242676

Final encoder loss: 0.0460580550134182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4426310062408447 0.07525134086608887

Final encoder loss: 0.047041065990924835
Final encoder loss: 0.047440119087696075
Final encoder loss: 0.045377157628536224

Training dapper model
Final encoder loss: 0.042728075211279734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05954241752624512 0.10713505744934082

Final encoder loss: 0.0449116717626921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05988669395446777 0.10735964775085449

Final encoder loss: 0.04048588356807719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05977368354797363 0.1073155403137207

Final encoder loss: 0.04284188893735961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05978107452392578 0.10758686065673828

Final encoder loss: 0.043242450649370705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059737205505371094 0.10718822479248047

Final encoder loss: 0.040034336157301585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05951404571533203 0.10749506950378418

Final encoder loss: 0.04164297224219089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05988478660583496 0.10732197761535645

Final encoder loss: 0.043112312747404694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06124305725097656 0.10739898681640625

Final encoder loss: 0.04217051888764155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05943894386291504 0.10643672943115234

Final encoder loss: 0.04006303733966274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06011772155761719 0.10726666450500488

Final encoder loss: 0.04208896033016267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05914759635925293 0.10735535621643066

Final encoder loss: 0.04186834816909073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05942988395690918 0.10625982284545898

Final encoder loss: 0.03956957841046056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.0597536563873291 0.1089324951171875

Final encoder loss: 0.035470707357728816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06107974052429199 0.10789704322814941

Final encoder loss: 0.04452445355453737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06069374084472656 0.1081840991973877

Final encoder loss: 0.051372110662322626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05957508087158203 0.10693693161010742


Training dapper model
Final encoder loss: 0.2024337649345398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11264681816101074 0.03399324417114258

Final encoder loss: 0.2081659883260727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1130213737487793 0.03502058982849121

Final encoder loss: 0.0662696436047554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11171817779541016 0.03350949287414551

Final encoder loss: 0.06752388179302216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11176681518554688 0.03394150733947754

Final encoder loss: 0.04453176259994507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11208128929138184 0.033814430236816406

Final encoder loss: 0.04506796970963478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11164617538452148 0.03327798843383789

Final encoder loss: 0.0381447933614254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11191534996032715 0.03448796272277832

Final encoder loss: 0.038925375789403915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11169195175170898 0.033431291580200195

Final encoder loss: 0.036938827484846115
Final encoder loss: 0.03704110532999039

Training case model
Final encoder loss: 0.06243201580400317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08986806869506836 0.21952199935913086

Final encoder loss: 0.060049946470044066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09032535552978516 0.21928858757019043

Final encoder loss: 0.05982442972609749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0887300968170166 0.21869635581970215

Final encoder loss: 0.060026095447106464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09116911888122559 0.219132661819458

Final encoder loss: 0.05967774509604364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08986473083496094 0.21906447410583496

Final encoder loss: 0.05933936995653658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08952450752258301 0.21932291984558105

Final encoder loss: 0.0609148656538871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08896446228027344 0.2188882827758789

Final encoder loss: 0.058489899388291404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08889436721801758 0.21872448921203613

Final encoder loss: 0.05961566053581912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08969855308532715 0.2197859287261963

Final encoder loss: 0.05904444735644382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08981513977050781 0.21866893768310547

Final encoder loss: 0.05855198735980979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08902144432067871 0.21905875205993652

Final encoder loss: 0.05972321702719896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08903765678405762 0.21875429153442383

Final encoder loss: 0.059333117976010846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08867621421813965 0.21889877319335938

Final encoder loss: 0.05838868448466419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08884978294372559 0.21873879432678223

Final encoder loss: 0.05916295477667016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0887153148651123 0.21858644485473633

Final encoder loss: 0.05962018823673486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08580303192138672 0.21518707275390625


Training case model
Final encoder loss: 0.2029707282781601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2734348773956299 0.05166339874267578

Final encoder loss: 0.18890637159347534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2790384292602539 0.0519256591796875

Final encoder loss: 0.19013641774654388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27137327194213867 0.051863908767700195

Final encoder loss: 0.19216875731945038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27831196784973145 0.05311465263366699

Final encoder loss: 0.18080662190914154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2721376419067383 0.05211949348449707

Final encoder loss: 0.1919167935848236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26316308975219727 0.05028843879699707

Final encoder loss: 0.08707967400550842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27979111671447754 0.051268815994262695

Final encoder loss: 0.0795586034655571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27257680892944336 0.05270051956176758

Final encoder loss: 0.07636474817991257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27841806411743164 0.052013397216796875

Final encoder loss: 0.07858661562204361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.271714448928833 0.05173945426940918

Final encoder loss: 0.07574672996997833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28942275047302246 0.05124640464782715

Final encoder loss: 0.07670308649539948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2690553665161133 0.05107736587524414

Final encoder loss: 0.06102980300784111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2791469097137451 0.05216360092163086

Final encoder loss: 0.05824778601527214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2737865447998047 0.05343484878540039

Final encoder loss: 0.0559537336230278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27833056449890137 0.05211615562438965

Final encoder loss: 0.059642963111400604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2664470672607422 0.05275917053222656

Final encoder loss: 0.05885247513651848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2784740924835205 0.05233049392700195

Final encoder loss: 0.05827935039997101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2636678218841553 0.050826072692871094

Final encoder loss: 0.05833667889237404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27886271476745605 0.05155587196350098

Final encoder loss: 0.05777667090296745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2722909450531006 0.052114009857177734

Final encoder loss: 0.05584841966629028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2725191116333008 0.052097320556640625

Final encoder loss: 0.059592608362436295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27843189239501953 0.05182909965515137

Final encoder loss: 0.059557896107435226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27217698097229004 0.05234265327453613

Final encoder loss: 0.058558084070682526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26916956901550293 0.05128645896911621

Final encoder loss: 0.06311532855033875
Final encoder loss: 0.06185274198651314
Final encoder loss: 0.059096746146678925
Final encoder loss: 0.05900048464536667
Final encoder loss: 0.056845977902412415
Final encoder loss: 0.05369691923260689

Training emognition model
Final encoder loss: 0.05859732812922808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08124613761901855 0.22986674308776855

Final encoder loss: 0.061659029401657094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08074474334716797 0.2300126552581787

Final encoder loss: 0.061879009222519196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0807960033416748 0.2298293113708496

Final encoder loss: 0.057779284346057204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08083343505859375 0.2295987606048584

Final encoder loss: 0.05996802661627213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08052897453308105 0.22956514358520508

Final encoder loss: 0.05722781919098949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08055567741394043 0.2306077480316162

Final encoder loss: 0.060963233293703775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08031439781188965 0.22950077056884766

Final encoder loss: 0.05825528896528376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08064675331115723 0.22986102104187012

Final encoder loss: 0.061239022169705844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08098387718200684 0.2297043800354004

Final encoder loss: 0.06288853208705869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.0808560848236084 0.23010802268981934

Final encoder loss: 0.05914838190595622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08048295974731445 0.23007822036743164

Final encoder loss: 0.06226410444703025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0806889533996582 0.22989559173583984

Final encoder loss: 0.06223027925316831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08051848411560059 0.22960972785949707

Final encoder loss: 0.05841115949975547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08078718185424805 0.2309436798095703

Final encoder loss: 0.060593873879222986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08160614967346191 0.2305145263671875

Final encoder loss: 0.057544377567534416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07954955101013184 0.22920751571655273


Training emognition model
Final encoder loss: 0.1935892105102539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25049853324890137 0.04779815673828125

Final encoder loss: 0.19497328996658325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24789977073669434 0.04842352867126465

Final encoder loss: 0.07785087078809738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24739480018615723 0.04867124557495117

Final encoder loss: 0.07814731448888779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24757742881774902 0.04839611053466797

Final encoder loss: 0.06031126156449318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24645018577575684 0.04901766777038574

Final encoder loss: 0.0600791834294796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24675297737121582 0.0476078987121582

Final encoder loss: 0.05586707219481468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24553179740905762 0.048282623291015625

Final encoder loss: 0.05581838637590408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470698356628418 0.048403024673461914

Final encoder loss: 0.0552455335855484
Final encoder loss: 0.055148474872112274

Training empatch model
Final encoder loss: 0.09941966602707329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07070255279541016 0.1726222038269043

Final encoder loss: 0.10000867160105863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07061982154846191 0.17254400253295898

Final encoder loss: 0.09306963980362482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07051658630371094 0.1729114055633545

Final encoder loss: 0.0883957876885418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07052755355834961 0.17282390594482422

Final encoder loss: 0.08808969655857789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07047438621520996 0.17311596870422363

Final encoder loss: 0.08139020375357495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07082486152648926 0.17297840118408203

Final encoder loss: 0.08134619256995845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07068371772766113 0.17280364036560059

Final encoder loss: 0.08407455018728474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06984424591064453 0.17242169380187988

Final encoder loss: 0.06229064917494016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0703585147857666 0.17253828048706055

Final encoder loss: 0.06736858224386855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0706932544708252 0.17259979248046875

Final encoder loss: 0.06276372945895337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07049202919006348 0.17299556732177734

Final encoder loss: 0.06345104790419323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07025671005249023 0.1729259490966797

Final encoder loss: 0.06597932683937187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07049798965454102 0.1727433204650879

Final encoder loss: 0.060906214085018874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07020783424377441 0.17290639877319336

Final encoder loss: 0.06393791065165012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07064485549926758 0.17286276817321777

Final encoder loss: 0.06440135128640112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07016730308532715 0.17250490188598633


Training empatch model
Final encoder loss: 0.1711721569299698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17493867874145508 0.04308009147644043

Final encoder loss: 0.08023713529109955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17398524284362793 0.04337644577026367

Final encoder loss: 0.06393126398324966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17346882820129395 0.04222893714904785

Final encoder loss: 0.058382537215948105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1734929084777832 0.043506622314453125

Final encoder loss: 0.05603579804301262

Training wesad model
Final encoder loss: 0.12561582223279405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07077169418334961 0.17300987243652344

Final encoder loss: 0.11462393150103593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07036972045898438 0.1727919578552246

Final encoder loss: 0.11140273757619894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710911750793457 0.17257070541381836

Final encoder loss: 0.10038842465581599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07051277160644531 0.17272567749023438

Final encoder loss: 0.08083430614111932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07019519805908203 0.1728198528289795

Final encoder loss: 0.07999875310867607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07047843933105469 0.17278099060058594

Final encoder loss: 0.08089239911818716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07044219970703125 0.1722562313079834

Final encoder loss: 0.08034262864481348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07035946846008301 0.17264366149902344

Final encoder loss: 0.06555026946282118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07039642333984375 0.17322039604187012

Final encoder loss: 0.06834427147525314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07017064094543457 0.17281818389892578

Final encoder loss: 0.06696210990378988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07011032104492188 0.1727128028869629

Final encoder loss: 0.07066339648649958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07065486907958984 0.1729884147644043

Final encoder loss: 0.0604376485857182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07057332992553711 0.17293643951416016

Final encoder loss: 0.057578746913622254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07028079032897949 0.17287802696228027

Final encoder loss: 0.06038953494761512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07043910026550293 0.17266225814819336

Final encoder loss: 0.05958264250963703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07050347328186035 0.17266106605529785


Training wesad model
Final encoder loss: 0.215615376830101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10420012474060059 0.03258347511291504

Final encoder loss: 0.09243539720773697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10322785377502441 0.032503366470336914

Final encoder loss: 0.06890932470560074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10208559036254883 0.032869815826416016

Final encoder loss: 0.061311881989240646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10487103462219238 0.032549381256103516

Final encoder loss: 0.058447159826755524

Calculating loss for amigos model
	Full Pass 0.6750001907348633
numFreeParamsPath 18
Reconstruction loss values: 0.08417783677577972 0.09148592501878738

Calculating loss for dapper model
	Full Pass 0.1528151035308838
numFreeParamsPath 18
Reconstruction loss values: 0.06947705149650574 0.07450295239686966

Calculating loss for case model
	Full Pass 0.861027717590332
numFreeParamsPath 18
Reconstruction loss values: 0.09551199525594711 0.09825591742992401

Calculating loss for emognition model
	Full Pass 0.2799265384674072
numFreeParamsPath 18
Reconstruction loss values: 0.0919845923781395 0.09637024253606796

Calculating loss for empatch model
	Full Pass 0.10525918006896973
numFreeParamsPath 18
Reconstruction loss values: 0.10287535935640335 0.10663209110498428

Calculating loss for wesad model
	Full Pass 0.08978033065795898
numFreeParamsPath 18
Reconstruction loss values: 0.1235797256231308 0.1467551440000534
Total loss calculation time: 3.8651885986328125

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.285474538803101
Total epoch time: 91.22313976287842

Epoch: 10

Training amigos model
Final encoder loss: 0.08032115167719728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11751031875610352 0.3913841247558594

Final encoder loss: 0.07582829931961968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10787796974182129 0.3882758617401123

Final encoder loss: 0.06825352754422614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10974526405334473 0.3902900218963623

Final encoder loss: 0.06943196671427909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.1078653335571289 0.39003920555114746

Final encoder loss: 0.06789849234094956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10845756530761719 0.38979363441467285

Final encoder loss: 0.0693177920212716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10832929611206055 0.38948869705200195

Final encoder loss: 0.05877373885333046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10837602615356445 0.38904285430908203

Final encoder loss: 0.06538494543231853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10809707641601562 0.38912034034729004

Final encoder loss: 0.06682648857647074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10926151275634766 0.3879365921020508

Final encoder loss: 0.06322585260826491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1078801155090332 0.387296199798584

Final encoder loss: 0.06791903607423753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10725998878479004 0.38791370391845703

Final encoder loss: 0.0622085754800398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10763049125671387 0.38714075088500977

Final encoder loss: 0.05722428897467614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10754656791687012 0.3882443904876709

Final encoder loss: 0.05973138755801627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10772871971130371 0.3880934715270996

Final encoder loss: 0.05987365680439816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10731863975524902 0.38761329650878906

Final encoder loss: 0.06481251343375106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10216975212097168 0.3843233585357666


Training case model
Final encoder loss: 0.09559759040263113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0908210277557373 0.2644336223602295

Final encoder loss: 0.0829815001196442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0908973217010498 0.26452064514160156

Final encoder loss: 0.07802641051499304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09140992164611816 0.2652261257171631

Final encoder loss: 0.07687836026254415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09099650382995605 0.265122652053833

Final encoder loss: 0.07412373689689711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0907285213470459 0.2650923728942871

Final encoder loss: 0.07035193153553922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09180974960327148 0.26505184173583984

Final encoder loss: 0.06723648716116541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09192347526550293 0.26478052139282227

Final encoder loss: 0.06878189816983099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09128236770629883 0.2641160488128662

Final encoder loss: 0.06635014363408787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0913400650024414 0.2646775245666504

Final encoder loss: 0.06755076712776266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0906987190246582 0.26430511474609375

Final encoder loss: 0.06622908392947591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09130072593688965 0.2646167278289795

Final encoder loss: 0.0665418332589114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0908803939819336 0.26419997215270996

Final encoder loss: 0.06567109494679428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09110736846923828 0.26464104652404785

Final encoder loss: 0.06444825818384213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09047389030456543 0.26392507553100586

Final encoder loss: 0.06238675738535785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09072041511535645 0.2644691467285156

Final encoder loss: 0.06497631705411118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08756875991821289 0.26047277450561523


Training dapper model
Final encoder loss: 0.07512955846398331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061161041259765625 0.1485452651977539

Final encoder loss: 0.06546416209012196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061295509338378906 0.14906930923461914

Final encoder loss: 0.0590622829429265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06132984161376953 0.1504817008972168

Final encoder loss: 0.05399788053630747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06266283988952637 0.14998292922973633

Final encoder loss: 0.05183363167147211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06191134452819824 0.1516251564025879

Final encoder loss: 0.05245523384177205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06286287307739258 0.15048837661743164

Final encoder loss: 0.046923248884436684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06179356575012207 0.1499185562133789

Final encoder loss: 0.04964577708447224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06248068809509277 0.15097379684448242

Final encoder loss: 0.047309405009969295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06206655502319336 0.15042376518249512

Final encoder loss: 0.0503499650635385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.061731576919555664 0.15090441703796387

Final encoder loss: 0.04474011786831451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06248927116394043 0.15199589729309082

Final encoder loss: 0.04774902071630898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06207847595214844 0.15084528923034668

Final encoder loss: 0.04413405152256835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06188249588012695 0.15106534957885742

Final encoder loss: 0.04830514586755961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06337952613830566 0.15018177032470703

Final encoder loss: 0.046936385757777546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06208300590515137 0.15021276473999023

Final encoder loss: 0.051584115884715676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.061348915100097656 0.15149879455566406


Training emognition model
Final encoder loss: 0.08970044193510238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0831756591796875 0.27471041679382324

Final encoder loss: 0.08329278740675831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08307123184204102 0.2764558792114258

Final encoder loss: 0.07777785852434184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08330178260803223 0.27517175674438477

Final encoder loss: 0.07616356220759068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08480119705200195 0.27425336837768555

Final encoder loss: 0.07473158864330934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08371710777282715 0.2756948471069336

Final encoder loss: 0.0743919738491592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08316445350646973 0.2747788429260254

Final encoder loss: 0.07155540247693083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08406424522399902 0.27597999572753906

Final encoder loss: 0.0732339110089362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08324575424194336 0.27565622329711914

Final encoder loss: 0.06950068064952525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08358907699584961 0.27524280548095703

Final encoder loss: 0.07274333878774654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08298921585083008 0.27518391609191895

Final encoder loss: 0.07449979367299092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08347129821777344 0.2752063274383545

Final encoder loss: 0.0691847745876753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08458399772644043 0.27513647079467773

Final encoder loss: 0.06957680332281861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08305835723876953 0.2743101119995117

Final encoder loss: 0.06894248568190822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08300447463989258 0.2756228446960449

Final encoder loss: 0.06552703855634714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08387923240661621 0.27741241455078125

Final encoder loss: 0.06903992035246707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08313441276550293 0.2743663787841797


Training amigos model
Final encoder loss: 0.0493744539494499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10594487190246582 0.34143614768981934

Final encoder loss: 0.0468545124989939
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10707259178161621 0.3415653705596924

Final encoder loss: 0.04862081423465311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1056065559387207 0.3413090705871582

Final encoder loss: 0.05074565352412256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10598874092102051 0.3415567874908447

Final encoder loss: 0.04832276345995144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.12387490272521973 0.34169554710388184

Final encoder loss: 0.048874449031583535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10656571388244629 0.34139013290405273

Final encoder loss: 0.0510703575279174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10636377334594727 0.3415098190307617

Final encoder loss: 0.05058564401643071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10586094856262207 0.3419661521911621

Final encoder loss: 0.04800341780533498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10642313957214355 0.34178972244262695

Final encoder loss: 0.04580801393479987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10630989074707031 0.341245174407959

Final encoder loss: 0.051821550491861004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10737442970275879 0.34147167205810547

Final encoder loss: 0.04883910176873126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10583853721618652 0.3414499759674072

Final encoder loss: 0.04522390099518692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1060328483581543 0.3416588306427002

Final encoder loss: 0.04977206625391559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10581612586975098 0.3413560390472412

Final encoder loss: 0.05112489725097091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10589098930358887 0.34159207344055176

Final encoder loss: 0.0497950833174938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10260295867919922 0.338350772857666


Training amigos model
Final encoder loss: 0.18075405061244965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4553508758544922 0.07917237281799316

Final encoder loss: 0.18785537779331207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45853137969970703 0.07512807846069336

Final encoder loss: 0.18362940847873688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4427347183227539 0.07486438751220703

Final encoder loss: 0.06692009419202805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44487953186035156 0.07501792907714844

Final encoder loss: 0.07050816714763641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4326658248901367 0.07476305961608887

Final encoder loss: 0.06693325191736221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.443619966506958 0.07261252403259277

Final encoder loss: 0.04816142097115517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46409106254577637 0.07643651962280273

Final encoder loss: 0.04972534254193306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4387211799621582 0.08036541938781738

Final encoder loss: 0.04821174219250679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45490312576293945 0.07410931587219238

Final encoder loss: 0.04412161931395531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46483850479125977 0.07630681991577148

Final encoder loss: 0.045480988919734955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45469141006469727 0.08117413520812988

Final encoder loss: 0.04456009715795517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4547896385192871 0.07553291320800781

Final encoder loss: 0.04627487435936928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4570021629333496 0.07487344741821289

Final encoder loss: 0.047808680683374405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4715094566345215 0.07551002502441406

Final encoder loss: 0.04593205079436302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4432079792022705 0.0759878158569336

Final encoder loss: 0.05431007221341133
Final encoder loss: 0.05245872214436531
Final encoder loss: 0.04784538596868515

Training dapper model
Final encoder loss: 0.037532666825328416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05943489074707031 0.10634541511535645

Final encoder loss: 0.039371731438494116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059190988540649414 0.10622429847717285

Final encoder loss: 0.03927376634577268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05911898612976074 0.10629534721374512

Final encoder loss: 0.04043021796465811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059096336364746094 0.10629558563232422

Final encoder loss: 0.03730898804577547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.0590054988861084 0.10646772384643555

Final encoder loss: 0.03828941976668007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059223175048828125 0.10634613037109375

Final encoder loss: 0.04181171949907836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05924487113952637 0.10715436935424805

Final encoder loss: 0.040026757883373294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059491634368896484 0.10675048828125

Final encoder loss: 0.03700268788733782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06058049201965332 0.10759973526000977

Final encoder loss: 0.036010417501798196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06014227867126465 0.10707759857177734

Final encoder loss: 0.04222237115958752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.0596768856048584 0.10785341262817383

Final encoder loss: 0.03897524584101708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05968141555786133 0.1075904369354248

Final encoder loss: 0.04083604262327011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05956721305847168 0.10788393020629883

Final encoder loss: 0.04091602210717208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06014418601989746 0.10746264457702637

Final encoder loss: 0.036383976885302906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0596461296081543 0.10711383819580078

Final encoder loss: 0.03685368893410813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.060268402099609375 0.10762405395507812


Training dapper model
Final encoder loss: 0.2024119645357132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11533808708190918 0.03404664993286133

Final encoder loss: 0.2082257717847824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11372494697570801 0.03393721580505371

Final encoder loss: 0.06697606295347214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11121010780334473 0.0335392951965332

Final encoder loss: 0.0672898143529892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1113591194152832 0.03401637077331543

Final encoder loss: 0.044072553515434265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11175036430358887 0.03374290466308594

Final encoder loss: 0.04351072758436203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11152768135070801 0.03371715545654297

Final encoder loss: 0.03709520772099495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11268758773803711 0.03379631042480469

Final encoder loss: 0.03718961775302887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11106085777282715 0.033263444900512695

Final encoder loss: 0.03562606871128082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11200308799743652 0.03396487236022949

Final encoder loss: 0.03587416186928749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11172366142272949 0.03311276435852051

Final encoder loss: 0.03704380244016647
Final encoder loss: 0.03546889126300812

Training case model
Final encoder loss: 0.05927036306514052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08867669105529785 0.218184232711792

Final encoder loss: 0.06137385101204143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08856773376464844 0.21827483177185059

Final encoder loss: 0.05794713482954802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08843183517456055 0.21839475631713867

Final encoder loss: 0.0589709974346733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08858203887939453 0.21834421157836914

Final encoder loss: 0.05860712652459075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08839607238769531 0.21814656257629395

Final encoder loss: 0.0586896305055338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08885526657104492 0.21833062171936035

Final encoder loss: 0.05887949723371906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08913922309875488 0.21881914138793945

Final encoder loss: 0.058809260548170754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08907747268676758 0.21891546249389648

Final encoder loss: 0.05883503429566538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.0891408920288086 0.218919038772583

Final encoder loss: 0.05811460044183459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0891427993774414 0.21879005432128906

Final encoder loss: 0.05728361607801269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08881783485412598 0.2194077968597412

Final encoder loss: 0.05864936507726064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08958172798156738 0.21883726119995117

Final encoder loss: 0.0585903692565012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08863186836242676 0.2188880443572998

Final encoder loss: 0.05966641849702275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08915543556213379 0.21927523612976074

Final encoder loss: 0.05781202789743668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09092974662780762 0.21961379051208496

Final encoder loss: 0.057493160428714725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08680939674377441 0.21553587913513184


Training case model
Final encoder loss: 0.20297066867351532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2751600742340088 0.051471710205078125

Final encoder loss: 0.18890546262264252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.267042875289917 0.05169081687927246

Final encoder loss: 0.19015219807624817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2705879211425781 0.051869869232177734

Final encoder loss: 0.1921922266483307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2925841808319092 0.0539398193359375

Final encoder loss: 0.1807919293642044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2692904472351074 0.05160331726074219

Final encoder loss: 0.1919126957654953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2635612487792969 0.05202007293701172

Final encoder loss: 0.08694934099912643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2732686996459961 0.05212116241455078

Final encoder loss: 0.08039247989654541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2897770404815674 0.05147862434387207

Final encoder loss: 0.07782948762178421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27124452590942383 0.052678585052490234

Final encoder loss: 0.08065090328454971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27686476707458496 0.05230379104614258

Final encoder loss: 0.07858449965715408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27025556564331055 0.05149054527282715

Final encoder loss: 0.07926838845014572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2632453441619873 0.05221867561340332

Final encoder loss: 0.06049509719014168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27736711502075195 0.052582740783691406

Final encoder loss: 0.05807863548398018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27742958068847656 0.05178356170654297

Final encoder loss: 0.056091614067554474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.280933141708374 0.05247616767883301

Final encoder loss: 0.059209566563367844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28545236587524414 0.05146360397338867

Final encoder loss: 0.05840473249554634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.284970760345459 0.05205273628234863

Final encoder loss: 0.058260317891836166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2682836055755615 0.05108952522277832

Final encoder loss: 0.05795256793498993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2852795124053955 0.05196738243103027

Final encoder loss: 0.05792158842086792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27668285369873047 0.05203676223754883

Final encoder loss: 0.055889278650283813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27785730361938477 0.05159759521484375

Final encoder loss: 0.059328705072402954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2742159366607666 0.051544904708862305

Final encoder loss: 0.05970405042171478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28343868255615234 0.0523836612701416

Final encoder loss: 0.05808299779891968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539958953857422 0.05193471908569336

Final encoder loss: 0.06506019830703735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2644188404083252 0.05252552032470703

Final encoder loss: 0.06465253978967667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27727794647216797 0.051229238510131836

Final encoder loss: 0.06410577148199081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2786753177642822 0.05100607872009277

Final encoder loss: 0.06507010012865067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2558135986328125 0.052133798599243164

Final encoder loss: 0.06781341135501862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27623701095581055 0.051619768142700195

Final encoder loss: 0.06422480195760727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25441718101501465 0.050357818603515625

Final encoder loss: 0.06669240444898605
Final encoder loss: 0.0632401704788208
Final encoder loss: 0.060020387172698975
Final encoder loss: 0.059481799602508545
Final encoder loss: 0.056179940700531006
Final encoder loss: 0.05256769061088562

Training emognition model
Final encoder loss: 0.057491088560254915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08068418502807617 0.2299516201019287

Final encoder loss: 0.057293131201310674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08050990104675293 0.22987747192382812

Final encoder loss: 0.05542964772485169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0810089111328125 0.2298440933227539

Final encoder loss: 0.0552513368407357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08076000213623047 0.22990965843200684

Final encoder loss: 0.058564592380628236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08021903038024902 0.22998809814453125

Final encoder loss: 0.05761774077895686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08089017868041992 0.230515718460083

Final encoder loss: 0.055865471006087616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08080434799194336 0.2301492691040039

Final encoder loss: 0.057758044756467446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08090019226074219 0.22969794273376465

Final encoder loss: 0.05600130045292522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08037471771240234 0.23001813888549805

Final encoder loss: 0.055765833062754025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08063411712646484 0.22988104820251465

Final encoder loss: 0.05763547353613727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0804741382598877 0.23007726669311523

Final encoder loss: 0.06041505997620366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08029890060424805 0.22975683212280273

Final encoder loss: 0.05479325099589489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08044886589050293 0.22957944869995117

Final encoder loss: 0.058317621458230634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.080474853515625 0.22981619834899902

Final encoder loss: 0.058276396261360705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08011317253112793 0.2296583652496338

Final encoder loss: 0.058883219148368415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07981705665588379 0.22918248176574707


Training emognition model
Final encoder loss: 0.19357998669147491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477099895477295 0.04869723320007324

Final encoder loss: 0.19494350254535675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2460331916809082 0.047863006591796875

Final encoder loss: 0.0765920877456665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24741005897521973 0.048831939697265625

Final encoder loss: 0.07687608152627945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24528884887695312 0.04862618446350098

Final encoder loss: 0.05841274932026863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.26407504081726074 0.04896378517150879

Final encoder loss: 0.05797399580478668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24577927589416504 0.048192501068115234

Final encoder loss: 0.053457267582416534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24652886390686035 0.04878640174865723

Final encoder loss: 0.053252048790454865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24693799018859863 0.0485692024230957

Final encoder loss: 0.05266830325126648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24682068824768066 0.048711538314819336

Final encoder loss: 0.05281403288245201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2465348243713379 0.0481104850769043

Final encoder loss: 0.05457384139299393
Final encoder loss: 0.05340332165360451

Training empatch model
Final encoder loss: 0.09894629083157709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07062578201293945 0.1729261875152588

Final encoder loss: 0.09865621472834879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07013392448425293 0.1735854148864746

Final encoder loss: 0.09549384451373737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07054352760314941 0.1728684902191162

Final encoder loss: 0.08114293646656608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07027626037597656 0.17291522026062012

Final encoder loss: 0.08186555136503867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0703737735748291 0.17277193069458008

Final encoder loss: 0.0792167443192091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07047367095947266 0.1728811264038086

Final encoder loss: 0.07614055031244503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0703125 0.17220759391784668

Final encoder loss: 0.08211655028644115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07003664970397949 0.17265653610229492

Final encoder loss: 0.05955117195473008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07078337669372559 0.17308974266052246

Final encoder loss: 0.06145691279664905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07059431076049805 0.1728987693786621

Final encoder loss: 0.05769541938594976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07070732116699219 0.1731576919555664

Final encoder loss: 0.060918243038908616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07076430320739746 0.1723499298095703

Final encoder loss: 0.06780024822420677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07108306884765625 0.17290472984313965

Final encoder loss: 0.059263221007867795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07065773010253906 0.17296767234802246

Final encoder loss: 0.06369176337975271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07058143615722656 0.17293190956115723

Final encoder loss: 0.0628786634225642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06999921798706055 0.172804594039917


Training empatch model
Final encoder loss: 0.1711416393518448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761021614074707 0.04369020462036133

Final encoder loss: 0.07984434813261032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17366576194763184 0.04386186599731445

Final encoder loss: 0.06241645663976669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1740272045135498 0.044422149658203125

Final encoder loss: 0.05643508955836296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1737203598022461 0.043540000915527344

Final encoder loss: 0.05378733202815056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17366552352905273 0.043615102767944336

Final encoder loss: 0.05287771299481392

Training wesad model
Final encoder loss: 0.12431897001141941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07050657272338867 0.1730210781097412

Final encoder loss: 0.10394389962093149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07032918930053711 0.17276883125305176

Final encoder loss: 0.10111310237257896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07027840614318848 0.17290306091308594

Final encoder loss: 0.09894016346255353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07066798210144043 0.1726844310760498

Final encoder loss: 0.07780498814818149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07027506828308105 0.17315435409545898

Final encoder loss: 0.07886365651667616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0703270435333252 0.17301154136657715

Final encoder loss: 0.07902781055845495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07061100006103516 0.17287206649780273

Final encoder loss: 0.07390137448651275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07016515731811523 0.17272448539733887

Final encoder loss: 0.06443550407262373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07048296928405762 0.17270278930664062

Final encoder loss: 0.06510863067537163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0701289176940918 0.17281031608581543

Final encoder loss: 0.0635300107097901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0701911449432373 0.1719825267791748

Final encoder loss: 0.06493347712640887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07117652893066406 0.17273950576782227

Final encoder loss: 0.055254286433622915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07019805908203125 0.1723935604095459

Final encoder loss: 0.05617360011052857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07016277313232422 0.17291522026062012

Final encoder loss: 0.05853625603483224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0699918270111084 0.17308521270751953

Final encoder loss: 0.058718685903495105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07044410705566406 0.1726360321044922


Training wesad model
Final encoder loss: 0.21557272970676422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10303568840026855 0.03293156623840332

Final encoder loss: 0.09134089201688766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10201406478881836 0.03311872482299805

Final encoder loss: 0.0670202448964119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10216307640075684 0.03348827362060547

Final encoder loss: 0.058971796184778214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10296010971069336 0.03260397911071777

Final encoder loss: 0.05591914430260658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1019284725189209 0.03313922882080078

Final encoder loss: 0.05471352860331535

Calculating loss for amigos model
	Full Pass 0.6354987621307373
numFreeParamsPath 18
Reconstruction loss values: 0.07644874602556229 0.08370228111743927

Calculating loss for dapper model
	Full Pass 0.1500711441040039
numFreeParamsPath 18
Reconstruction loss values: 0.06445857137441635 0.07037869095802307

Calculating loss for case model
	Full Pass 0.8558862209320068
numFreeParamsPath 18
Reconstruction loss values: 0.08357903361320496 0.0858893170952797

Calculating loss for emognition model
	Full Pass 0.28149843215942383
numFreeParamsPath 18
Reconstruction loss values: 0.08646339923143387 0.0918913334608078

Calculating loss for empatch model
	Full Pass 0.10495209693908691
numFreeParamsPath 18
Reconstruction loss values: 0.0964169055223465 0.0994265154004097

Calculating loss for wesad model
	Full Pass 0.0772707462310791
numFreeParamsPath 18
Reconstruction loss values: 0.1142139658331871 0.13573487102985382
Total loss calculation time: 4.130699872970581

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.279061317443848
Total epoch time: 98.03277206420898

Epoch: 11

Training case model
Final encoder loss: 0.08347121787184049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0992426872253418 0.27175235748291016

Final encoder loss: 0.07363423552218246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09133791923522949 0.2662074565887451

Final encoder loss: 0.07124401606750198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09108972549438477 0.2638578414916992

Final encoder loss: 0.06787034741192428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09134626388549805 0.2652416229248047

Final encoder loss: 0.06602038055634186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09295868873596191 0.26610708236694336

Final encoder loss: 0.0646401118110795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09178352355957031 0.26466870307922363

Final encoder loss: 0.06329458565338442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09238076210021973 0.266071081161499

Final encoder loss: 0.061741677034522144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09187912940979004 0.26593852043151855

Final encoder loss: 0.061143639129728984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09159326553344727 0.26578569412231445

Final encoder loss: 0.05856925087698554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09162068367004395 0.2649245262145996

Final encoder loss: 0.060804131891067294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09161901473999023 0.2654445171356201

Final encoder loss: 0.0580930406245804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0924978256225586 0.2670457363128662

Final encoder loss: 0.05803544587655186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09181785583496094 0.2650926113128662

Final encoder loss: 0.060394028726338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09223651885986328 0.2666153907775879

Final encoder loss: 0.06114442838600305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09159731864929199 0.26549410820007324

Final encoder loss: 0.05699974382996725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08815193176269531 0.2622489929199219


Training emognition model
Final encoder loss: 0.08639122289496873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08451437950134277 0.27375316619873047

Final encoder loss: 0.07457929223537661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08314132690429688 0.2756013870239258

Final encoder loss: 0.07559056430446169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08466553688049316 0.2762148380279541

Final encoder loss: 0.07609766509271437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08359718322753906 0.2752196788787842

Final encoder loss: 0.0726895113625221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08329391479492188 0.2765340805053711

Final encoder loss: 0.07183355517237047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08368444442749023 0.2751483917236328

Final encoder loss: 0.0721877832972729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0850372314453125 0.2770075798034668

Final encoder loss: 0.06774468526183404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08357405662536621 0.2750120162963867

Final encoder loss: 0.070654555368135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.084991455078125 0.27486753463745117

Final encoder loss: 0.06575673055984275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08248329162597656 0.2735295295715332

Final encoder loss: 0.06421711503617737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08246588706970215 0.27300262451171875

Final encoder loss: 0.06390946189746435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08241510391235352 0.273104190826416

Final encoder loss: 0.06358045294955131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08248114585876465 0.27362060546875

Final encoder loss: 0.06577174947551355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08241105079650879 0.27422356605529785

Final encoder loss: 0.05975305140261781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08236956596374512 0.27379798889160156

Final encoder loss: 0.06801883730896356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08188915252685547 0.2731492519378662


Training dapper model
Final encoder loss: 0.06873043261698619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06104755401611328 0.14851999282836914

Final encoder loss: 0.06222234244501233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06114602088928223 0.14871644973754883

Final encoder loss: 0.0526576459098124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.061222076416015625 0.14896416664123535

Final encoder loss: 0.05350911703439999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061161041259765625 0.14980459213256836

Final encoder loss: 0.04761275719391796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06166863441467285 0.1500694751739502

Final encoder loss: 0.05163484379136241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062313079833984375 0.15052485466003418

Final encoder loss: 0.04982191716356889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06256318092346191 0.15030431747436523

Final encoder loss: 0.05151548052402487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06179475784301758 0.1515178680419922

Final encoder loss: 0.04699940544611122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061747074127197266 0.1502211093902588

Final encoder loss: 0.04475283472361925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.062177419662475586 0.14980792999267578

Final encoder loss: 0.043944208376541956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06227827072143555 0.15046191215515137

Final encoder loss: 0.04559419749061068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06181931495666504 0.15050697326660156

Final encoder loss: 0.04388399618379277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.061876535415649414 0.15008759498596191

Final encoder loss: 0.04577692636677909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.062018394470214844 0.14958548545837402

Final encoder loss: 0.046623024569577405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06226181983947754 0.15087389945983887

Final encoder loss: 0.04914279000684272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06175088882446289 0.14940905570983887


Training amigos model
Final encoder loss: 0.07452441585201633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10857629776000977 0.3882629871368408

Final encoder loss: 0.06307276879850818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10830235481262207 0.38834238052368164

Final encoder loss: 0.06896184429298335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10812807083129883 0.38742685317993164

Final encoder loss: 0.06516414779191242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10846781730651855 0.3886599540710449

Final encoder loss: 0.06433782933063008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10799646377563477 0.38843774795532227

Final encoder loss: 0.06274338281328638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10769343376159668 0.3885929584503174

Final encoder loss: 0.06255542439389271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1081385612487793 0.3882567882537842

Final encoder loss: 0.06403666186226217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10807538032531738 0.38840198516845703

Final encoder loss: 0.06682215823936685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10803389549255371 0.38897180557250977

Final encoder loss: 0.06398146379837859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1080164909362793 0.38898587226867676

Final encoder loss: 0.06249836103927268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10823225975036621 0.38919782638549805

Final encoder loss: 0.05838053687761031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10815954208374023 0.3886117935180664

Final encoder loss: 0.05805988248525062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10841989517211914 0.3892698287963867

Final encoder loss: 0.06476963834067045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10825300216674805 0.389892578125

Final encoder loss: 0.05934856416924928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10872459411621094 0.3894534111022949

Final encoder loss: 0.05953359270004277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10780811309814453 0.3839297294616699


Training amigos model
Final encoder loss: 0.044620392438141095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1071937084197998 0.34189558029174805

Final encoder loss: 0.04347920165482528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10614633560180664 0.341447114944458

Final encoder loss: 0.04761504935026894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10630297660827637 0.3419191837310791

Final encoder loss: 0.04620455068062721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10630512237548828 0.34134578704833984

Final encoder loss: 0.04567616859781264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10720396041870117 0.34162020683288574

Final encoder loss: 0.04441209317361738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10639023780822754 0.34168505668640137

Final encoder loss: 0.04697942960482987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10634040832519531 0.3415524959564209

Final encoder loss: 0.04242507571008881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1062781810760498 0.3417484760284424

Final encoder loss: 0.04837967849364193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10688066482543945 0.34140634536743164

Final encoder loss: 0.04867071128100378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10652351379394531 0.3418138027191162

Final encoder loss: 0.04776714287654844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10629630088806152 0.3414120674133301

Final encoder loss: 0.05086564721843573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10640907287597656 0.3418130874633789

Final encoder loss: 0.050245967174255605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.4222588539123535 0.3415515422821045

Final encoder loss: 0.05008786579255739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10602164268493652 0.3415396213531494

Final encoder loss: 0.04584227955089854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1057124137878418 0.34185361862182617

Final encoder loss: 0.04506413690076355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10273432731628418 0.3383018970489502


Training amigos model
Final encoder loss: 0.180741548538208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43772053718566895 0.07633423805236816

Final encoder loss: 0.18783947825431824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43784379959106445 0.07941317558288574

Final encoder loss: 0.1836242973804474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4340684413909912 0.07308435440063477

Final encoder loss: 0.06460785120725632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4415009021759033 0.07616376876831055

Final encoder loss: 0.06758705526590347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45044755935668945 0.07272601127624512

Final encoder loss: 0.06263855844736099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42066335678100586 0.07481193542480469

Final encoder loss: 0.04625571519136429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.437589168548584 0.07980585098266602

Final encoder loss: 0.04812521114945412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4542109966278076 0.07435441017150879

Final encoder loss: 0.04630378261208534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43551111221313477 0.07393956184387207

Final encoder loss: 0.042245812714099884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43248462677001953 0.07633304595947266

Final encoder loss: 0.04385168105363846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4532175064086914 0.07671284675598145

Final encoder loss: 0.04309342801570892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4352889060974121 0.07961177825927734

Final encoder loss: 0.04425550997257233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45586371421813965 0.07579946517944336

Final encoder loss: 0.04525113105773926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44481396675109863 0.07658767700195312

Final encoder loss: 0.04465411975979805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.426037073135376 0.0718073844909668

Final encoder loss: 0.04923724755644798
Final encoder loss: 0.0473751574754715
Final encoder loss: 0.04371212422847748

Training dapper model
Final encoder loss: 0.03710173683157515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05973386764526367 0.10674357414245605

Final encoder loss: 0.040007950913127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05877399444580078 0.10637116432189941

Final encoder loss: 0.03746185206240089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.058897972106933594 0.10777044296264648

Final encoder loss: 0.03611633941925209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05885958671569824 0.10618925094604492

Final encoder loss: 0.041456101740300476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05900907516479492 0.10615038871765137

Final encoder loss: 0.03970670060029539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059322357177734375 0.1075131893157959

Final encoder loss: 0.03725331036148891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05989336967468262 0.107208251953125

Final encoder loss: 0.040300540919328956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05933427810668945 0.10793828964233398

Final encoder loss: 0.040533697346473954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06106829643249512 0.10706973075866699

Final encoder loss: 0.03511331460861949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06029105186462402 0.1075887680053711

Final encoder loss: 0.03600139852615411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060053348541259766 0.1071329116821289

Final encoder loss: 0.037574756497634094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06082558631896973 0.10760116577148438

Final encoder loss: 0.03740328643245857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05990004539489746 0.1079716682434082

Final encoder loss: 0.03556196452852952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059775352478027344 0.10713458061218262

Final encoder loss: 0.03263835759115492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06020474433898926 0.10787343978881836

Final encoder loss: 0.039600765168674834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05952167510986328 0.10692334175109863


Training dapper model
Final encoder loss: 0.20243093371391296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11542153358459473 0.03429412841796875

Final encoder loss: 0.2081923931837082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11388397216796875 0.034918785095214844

Final encoder loss: 0.06615117192268372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1122596263885498 0.03434872627258301

Final encoder loss: 0.06638292223215103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11197805404663086 0.03449296951293945

Final encoder loss: 0.04324411228299141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11224508285522461 0.034651994705200195

Final encoder loss: 0.04286907613277435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11279296875 0.033653974533081055

Final encoder loss: 0.03570203110575676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11215949058532715 0.03367757797241211

Final encoder loss: 0.036070406436920166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11428046226501465 0.03488492965698242

Final encoder loss: 0.03374604508280754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11338520050048828 0.03407478332519531

Final encoder loss: 0.03432851657271385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11237072944641113 0.0340273380279541

Final encoder loss: 0.03438764065504074
Final encoder loss: 0.03351407125592232

Training case model
Final encoder loss: 0.05619322377284852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08885025978088379 0.21909284591674805

Final encoder loss: 0.05713201024469398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08905744552612305 0.21922993659973145

Final encoder loss: 0.05219858863227424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08918595314025879 0.2190990447998047

Final encoder loss: 0.05532979630841367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09051895141601562 0.21922588348388672

Final encoder loss: 0.05398703717080471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08933401107788086 0.2190113067626953

Final encoder loss: 0.05419874630349177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09030413627624512 0.21895289421081543

Final encoder loss: 0.053542266501837556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08980345726013184 0.21888971328735352

Final encoder loss: 0.052604212497777546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08940386772155762 0.21913456916809082

Final encoder loss: 0.05472539082494673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08962225914001465 0.21962618827819824

Final encoder loss: 0.055038890365978296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08903908729553223 0.21845364570617676

Final encoder loss: 0.05182999297298688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08894801139831543 0.21806740760803223

Final encoder loss: 0.05173495422667998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08913040161132812 0.2183551788330078

Final encoder loss: 0.053287749328206256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08887100219726562 0.21817803382873535

Final encoder loss: 0.0518398654278918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08869671821594238 0.21835660934448242

Final encoder loss: 0.05257313332275731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08857989311218262 0.21805620193481445

Final encoder loss: 0.05309591525166115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08540177345275879 0.21475863456726074


Training case model
Final encoder loss: 0.2029702216386795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25815463066101074 0.05191516876220703

Final encoder loss: 0.1888982504606247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25514841079711914 0.05240225791931152

Final encoder loss: 0.1901540607213974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2557399272918701 0.051239967346191406

Final encoder loss: 0.19219352304935455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574653625488281 0.052734375

Final encoder loss: 0.18080449104309082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579927444458008 0.05146217346191406

Final encoder loss: 0.19192613661289215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25461506843566895 0.0522761344909668

Final encoder loss: 0.08878489583730698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26013755798339844 0.05308938026428223

Final encoder loss: 0.08102729916572571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25867223739624023 0.053359270095825195

Final encoder loss: 0.07712768018245697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572288513183594 0.05171394348144531

Final encoder loss: 0.07814713567495346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2560248374938965 0.05291032791137695

Final encoder loss: 0.07359369844198227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25522732734680176 0.05244302749633789

Final encoder loss: 0.07481113821268082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2519845962524414 0.05049443244934082

Final encoder loss: 0.05980957671999931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571077346801758 0.052510738372802734

Final encoder loss: 0.056398551911115646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25493383407592773 0.052323341369628906

Final encoder loss: 0.054301559925079346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25507092475891113 0.05202150344848633

Final encoder loss: 0.057077694684267044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25528454780578613 0.05299067497253418

Final encoder loss: 0.05579153448343277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.254591703414917 0.052202701568603516

Final encoder loss: 0.05568580701947212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2531416416168213 0.05164384841918945

Final encoder loss: 0.05474858731031418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26043152809143066 0.05194282531738281

Final encoder loss: 0.05327918007969856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25632691383361816 0.05288243293762207

Final encoder loss: 0.05223281309008598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2560231685638428 0.05138874053955078

Final encoder loss: 0.05509335920214653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571697235107422 0.05154061317443848

Final encoder loss: 0.054943185299634933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25512194633483887 0.0521693229675293

Final encoder loss: 0.05409521237015724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25507283210754395 0.05372881889343262

Final encoder loss: 0.05733304098248482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25925111770629883 0.05312538146972656

Final encoder loss: 0.05631755292415619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2592618465423584 0.05193662643432617

Final encoder loss: 0.0558127798140049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25626111030578613 0.05251121520996094

Final encoder loss: 0.05760616436600685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2660064697265625 0.051789283752441406

Final encoder loss: 0.05841155722737312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2565281391143799 0.05258321762084961

Final encoder loss: 0.05763187259435654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25701117515563965 0.053041696548461914

Final encoder loss: 0.057576172053813934
Final encoder loss: 0.05502555891871452
Final encoder loss: 0.052684392780065536
Final encoder loss: 0.05284619331359863
Final encoder loss: 0.05065440014004707
Final encoder loss: 0.048114143311977386

Training emognition model
Final encoder loss: 0.05894181366095022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08146047592163086 0.22978734970092773

Final encoder loss: 0.0548476685695705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08108234405517578 0.2300739288330078

Final encoder loss: 0.056772639513706806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08043670654296875 0.23012423515319824

Final encoder loss: 0.055103713254898996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08077645301818848 0.23023676872253418

Final encoder loss: 0.054826364973214615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08053946495056152 0.22945761680603027

Final encoder loss: 0.056135730356848955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08074688911437988 0.2298574447631836

Final encoder loss: 0.05659266064013591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08079409599304199 0.2303147315979004

Final encoder loss: 0.05760763534730109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08098793029785156 0.23038053512573242

Final encoder loss: 0.05575138507047004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08094501495361328 0.23018527030944824

Final encoder loss: 0.05524990094053192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08065176010131836 0.22948050498962402

Final encoder loss: 0.0565842682180459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08097624778747559 0.2312936782836914

Final encoder loss: 0.055438184555304096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08179688453674316 0.23065733909606934

Final encoder loss: 0.05593252395002809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0810232162475586 0.22957420349121094

Final encoder loss: 0.05932597414078238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08027219772338867 0.2306210994720459

Final encoder loss: 0.05480558635759453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08079743385314941 0.23026466369628906

Final encoder loss: 0.054306521400770705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07956910133361816 0.22908806800842285


Training emognition model
Final encoder loss: 0.19355392456054688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24772953987121582 0.0479583740234375

Final encoder loss: 0.19496901333332062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24663257598876953 0.047843217849731445

Final encoder loss: 0.07680483162403107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24649548530578613 0.048615217208862305

Final encoder loss: 0.07673221081495285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463998794555664 0.04803109169006348

Final encoder loss: 0.05779103562235832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24629783630371094 0.04874157905578613

Final encoder loss: 0.057170189917087555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24639630317687988 0.04861330986022949

Final encoder loss: 0.05227809026837349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24670767784118652 0.04881429672241211

Final encoder loss: 0.05198133364319801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2465505599975586 0.04918313026428223

Final encoder loss: 0.05135539919137955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24671006202697754 0.047895193099975586

Final encoder loss: 0.050933901220560074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2461087703704834 0.048918962478637695

Final encoder loss: 0.052640579640865326
Final encoder loss: 0.051124680787324905

Training empatch model
Final encoder loss: 0.10192733169937572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07067394256591797 0.17267131805419922

Final encoder loss: 0.08774931428623294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07054829597473145 0.17259883880615234

Final encoder loss: 0.0862149266942526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07102584838867188 0.17269229888916016

Final encoder loss: 0.07953136080782458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07042193412780762 0.17287588119506836

Final encoder loss: 0.07881612210623157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07053303718566895 0.17302179336547852

Final encoder loss: 0.08085933695359343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07086491584777832 0.17268633842468262

Final encoder loss: 0.072685660217415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07067346572875977 0.1729423999786377

Final encoder loss: 0.06849683312191633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06992578506469727 0.172438383102417

Final encoder loss: 0.056100917526991444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07054686546325684 0.1725599765777588

Final encoder loss: 0.0573085093638248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07035017013549805 0.172896146774292

Final encoder loss: 0.05646002339896797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07074379920959473 0.17304253578186035

Final encoder loss: 0.06089686454243623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07088756561279297 0.17278218269348145

Final encoder loss: 0.05785225597055968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0703427791595459 0.17289066314697266

Final encoder loss: 0.06106161533945879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07043862342834473 0.1721820831298828

Final encoder loss: 0.05845771528871399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07059812545776367 0.17287707328796387

Final encoder loss: 0.05743555178913836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07013225555419922 0.17255878448486328


Training empatch model
Final encoder loss: 0.17115898430347443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16946959495544434 0.04323101043701172

Final encoder loss: 0.07838770002126694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471933364868164 0.044149160385131836

Final encoder loss: 0.061408787965774536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16878223419189453 0.04385089874267578

Final encoder loss: 0.05497819185256958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17421245574951172 0.043639421463012695

Final encoder loss: 0.05210104584693909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16948795318603516 0.04321146011352539

Final encoder loss: 0.05099880322813988

Training wesad model
Final encoder loss: 0.11469184555710714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07069253921508789 0.1725294589996338

Final encoder loss: 0.10307964869874753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07047033309936523 0.172715425491333

Final encoder loss: 0.0938160621007762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07011938095092773 0.17277312278747559

Final encoder loss: 0.09067110525419607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07047247886657715 0.1725921630859375

Final encoder loss: 0.07119988263471581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07009291648864746 0.17297673225402832

Final encoder loss: 0.07100548840140308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07082986831665039 0.17236948013305664

Final encoder loss: 0.07557062984100152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07020378112792969 0.1721935272216797

Final encoder loss: 0.07076132902194288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07010269165039062 0.17225408554077148

Final encoder loss: 0.05942122518648236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07036375999450684 0.17259454727172852

Final encoder loss: 0.060155917811882294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07019853591918945 0.1728212833404541

Final encoder loss: 0.06116083316257083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07043957710266113 0.1726093292236328

Final encoder loss: 0.060247342141553424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07016777992248535 0.17278289794921875

Final encoder loss: 0.05387273944516594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07046985626220703 0.17250299453735352

Final encoder loss: 0.050687041031015276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07061028480529785 0.17278122901916504

Final encoder loss: 0.05322557727261996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0705711841583252 0.17274713516235352

Final encoder loss: 0.05175405668723684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07043623924255371 0.17262554168701172


Training wesad model
Final encoder loss: 0.2156093865633011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1034078598022461 0.03309059143066406

Final encoder loss: 0.09014624357223511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037454605102539 0.032872676849365234

Final encoder loss: 0.06533188372850418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10242176055908203 0.03300213813781738

Final encoder loss: 0.05662408843636513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10231828689575195 0.03309273719787598

Final encoder loss: 0.053117576986551285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1022799015045166 0.03242778778076172

Final encoder loss: 0.051701776683330536

Calculating loss for amigos model
	Full Pass 0.6355588436126709
numFreeParamsPath 18
Reconstruction loss values: 0.07235298305749893 0.08032266795635223

Calculating loss for dapper model
	Full Pass 0.15037870407104492
numFreeParamsPath 18
Reconstruction loss values: 0.059474650770425797 0.0651264488697052

Calculating loss for case model
	Full Pass 0.8549261093139648
numFreeParamsPath 18
Reconstruction loss values: 0.08094176650047302 0.08387937396764755

Calculating loss for emognition model
	Full Pass 0.2785167694091797
numFreeParamsPath 18
Reconstruction loss values: 0.0831289142370224 0.08831547945737839

Calculating loss for empatch model
	Full Pass 0.10454821586608887
numFreeParamsPath 18
Reconstruction loss values: 0.09290055185556412 0.09707053750753403

Calculating loss for wesad model
	Full Pass 0.07660961151123047
numFreeParamsPath 18
Reconstruction loss values: 0.10855303704738617 0.13204684853553772
Total loss calculation time: 3.616137981414795

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.317405700683594
Total epoch time: 97.04542684555054

Epoch: 12

Training emognition model
Final encoder loss: 0.08225437625203008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08857297897338867 0.2738614082336426

Final encoder loss: 0.07528921705381567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08241558074951172 0.2732353210449219

Final encoder loss: 0.07499116983574547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08246159553527832 0.2735409736633301

Final encoder loss: 0.07410811847284486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08245158195495605 0.27445507049560547

Final encoder loss: 0.07196360059627699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.083038330078125 0.2742807865142822

Final encoder loss: 0.0693981419778916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0827631950378418 0.27370309829711914

Final encoder loss: 0.06597621252776865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08279061317443848 0.27454590797424316

Final encoder loss: 0.06443608298957468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08258199691772461 0.2735023498535156

Final encoder loss: 0.06729127632116721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08257651329040527 0.2740342617034912

Final encoder loss: 0.0663373071959013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0823829174041748 0.27327489852905273

Final encoder loss: 0.06482386356133651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08287501335144043 0.2736191749572754

Final encoder loss: 0.06272434177353195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08327841758728027 0.27461838722229004

Final encoder loss: 0.06137122547595881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08269548416137695 0.27429699897766113

Final encoder loss: 0.06398822983716698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08264684677124023 0.2736780643463135

Final encoder loss: 0.06207484478880346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08269405364990234 0.27251386642456055

Final encoder loss: 0.05935336786560686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0818476676940918 0.2752265930175781


Training dapper model
Final encoder loss: 0.06225928935613454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06138896942138672 0.1494612693786621

Final encoder loss: 0.05516559094252501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06250691413879395 0.1522974967956543

Final encoder loss: 0.05027485741646092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06293487548828125 0.1498730182647705

Final encoder loss: 0.04920515767321878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06225395202636719 0.15118741989135742

Final encoder loss: 0.05094164353851893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06181836128234863 0.15057659149169922

Final encoder loss: 0.04453079389911618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06316685676574707 0.15321111679077148

Final encoder loss: 0.04373586098275898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06313562393188477 0.1504344940185547

Final encoder loss: 0.048506706526905474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06212472915649414 0.15117454528808594

Final encoder loss: 0.040609156647335345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06192612648010254 0.1508622169494629

Final encoder loss: 0.04045842871749335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.0627295970916748 0.15190935134887695

Final encoder loss: 0.043181114528898494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06336116790771484 0.14994430541992188

Final encoder loss: 0.04550862467614248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06243491172790527 0.1511242389678955

Final encoder loss: 0.04981886599296314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062116384506225586 0.15006041526794434

Final encoder loss: 0.045494922800690514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06272482872009277 0.15152359008789062

Final encoder loss: 0.042470125006967736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06363654136657715 0.15038084983825684

Final encoder loss: 0.03973303346686515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06175947189331055 0.149641752243042


Training amigos model
Final encoder loss: 0.07167231378754707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10802984237670898 0.3886849880218506

Final encoder loss: 0.06861508113965703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10763263702392578 0.389493465423584

Final encoder loss: 0.06312040053508651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10792422294616699 0.3900332450866699

Final encoder loss: 0.06316148048346024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10797119140625 0.3898036479949951

Final encoder loss: 0.06611492154288041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10892581939697266 0.390277624130249

Final encoder loss: 0.06914540406745591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10842275619506836 0.38950681686401367

Final encoder loss: 0.06183861661409306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.11041808128356934 0.38881993293762207

Final encoder loss: 0.0654435225566998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1077883243560791 0.3890230655670166

Final encoder loss: 0.061156092464190515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10827755928039551 0.38970303535461426

Final encoder loss: 0.058908125553720894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10872817039489746 0.3900766372680664

Final encoder loss: 0.0631729886311124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1092371940612793 0.3891563415527344

Final encoder loss: 0.05439357469032586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10978937149047852 0.390547513961792

Final encoder loss: 0.0609295218591006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1081697940826416 0.39029383659362793

Final encoder loss: 0.059218144132587566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10874605178833008 0.38968658447265625

Final encoder loss: 0.06268402846866343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10824942588806152 0.3881213665008545

Final encoder loss: 0.05898590587913035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10304427146911621 0.3837132453918457


Training case model
Final encoder loss: 0.08108299443011262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0920553207397461 0.26578569412231445

Final encoder loss: 0.07468949386898757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09065890312194824 0.2652723789215088

Final encoder loss: 0.06960978256728181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09114837646484375 0.26505398750305176

Final encoder loss: 0.06845238155862943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09205842018127441 0.264636754989624

Final encoder loss: 0.06456808596390073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0909738540649414 0.2653348445892334

Final encoder loss: 0.06374331871566691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09272909164428711 0.26674461364746094

Final encoder loss: 0.0639859125507774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09106016159057617 0.26577281951904297

Final encoder loss: 0.062260198107036376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09176802635192871 0.2647671699523926

Final encoder loss: 0.06165086702392296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09179091453552246 0.26584911346435547

Final encoder loss: 0.061071236376810084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0907447338104248 0.26416611671447754

Final encoder loss: 0.05843592270416669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09038281440734863 0.26329874992370605

Final encoder loss: 0.05927376203568714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09047961235046387 0.2638590335845947

Final encoder loss: 0.05650250319322555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09057855606079102 0.2634918689727783

Final encoder loss: 0.057276875056478306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09029603004455566 0.2633540630340576

Final encoder loss: 0.05745006751235141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09060382843017578 0.26378583908081055

Final encoder loss: 0.05711145514737226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.0873115062713623 0.2603425979614258


Training amigos model
Final encoder loss: 0.04721368111707664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10567903518676758 0.34152913093566895

Final encoder loss: 0.047117767560101824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10580611228942871 0.3411142826080322

Final encoder loss: 0.043694918503519385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10562348365783691 0.3412449359893799

Final encoder loss: 0.04772819935158274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10620641708374023 0.3414919376373291

Final encoder loss: 0.04538510636056251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10555839538574219 0.3414630889892578

Final encoder loss: 0.04442907044507426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10644197463989258 0.34154319763183594

Final encoder loss: 0.04825621405702428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10574531555175781 0.3415343761444092

Final encoder loss: 0.04666504012773342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10593962669372559 0.34160304069519043

Final encoder loss: 0.04572160058041382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10546374320983887 0.3414437770843506

Final encoder loss: 0.04728392438958675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10594415664672852 0.3412630558013916

Final encoder loss: 0.04863156268367869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1061556339263916 0.34125375747680664

Final encoder loss: 0.04486376499265717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10586810111999512 0.34086036682128906

Final encoder loss: 0.044254707776675464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10571813583374023 0.34095239639282227

Final encoder loss: 0.046831289308243804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10560894012451172 0.34125518798828125

Final encoder loss: 0.047203226833519824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1054372787475586 0.3411526679992676

Final encoder loss: 0.052035677862913784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1006925106048584 0.3371157646179199


Training amigos model
Final encoder loss: 0.18073810636997223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43311071395874023 0.07623791694641113

Final encoder loss: 0.1878376454114914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4508655071258545 0.0768442153930664

Final encoder loss: 0.18362616002559662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4378821849822998 0.07304573059082031

Final encoder loss: 0.06560519337654114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46889758110046387 0.080657958984375

Final encoder loss: 0.06767243891954422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4444763660430908 0.07679533958435059

Final encoder loss: 0.06278412789106369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43679213523864746 0.07518339157104492

Final encoder loss: 0.045810017734766006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45663952827453613 0.07719230651855469

Final encoder loss: 0.047288019210100174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4706707000732422 0.08273720741271973

Final encoder loss: 0.045170385390520096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4245612621307373 0.07534503936767578

Final encoder loss: 0.040507134050130844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4693033695220947 0.0764918327331543

Final encoder loss: 0.042008042335510254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.41327548027038574 0.07463550567626953

Final encoder loss: 0.041064027696847916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4366159439086914 0.07750415802001953

Final encoder loss: 0.04048560559749603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.39627814292907715 0.08262181282043457

Final encoder loss: 0.04215022176504135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4602484703063965 0.07647824287414551

Final encoder loss: 0.0413721427321434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43503499031066895 0.07395339012145996

Final encoder loss: 0.04350150376558304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46903324127197266 0.07752680778503418

Final encoder loss: 0.04568145424127579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4400827884674072 0.07954692840576172

Final encoder loss: 0.044370729476213455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4414968490600586 0.07335686683654785

Final encoder loss: 0.046498123556375504
Final encoder loss: 0.044814206659793854
Final encoder loss: 0.04165400564670563

Training dapper model
Final encoder loss: 0.03933281500323728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05965375900268555 0.10732173919677734

Final encoder loss: 0.03647927878718106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05933380126953125 0.10774993896484375

Final encoder loss: 0.037968567219632175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.060907602310180664 0.1075890064239502

Final encoder loss: 0.037625441202785256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05974888801574707 0.10733389854431152

Final encoder loss: 0.034873111756918616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05965828895568848 0.10742855072021484

Final encoder loss: 0.03717186662558859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060417890548706055 0.10808920860290527

Final encoder loss: 0.03739403940917121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.5093076229095459 0.10757279396057129

Final encoder loss: 0.03675988118638858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06025362014770508 0.1075448989868164

Final encoder loss: 0.03540806744868793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059782981872558594 0.10723400115966797

Final encoder loss: 0.03627840511693104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05968475341796875 0.10827922821044922

Final encoder loss: 0.03581799069351977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06041216850280762 0.10763311386108398

Final encoder loss: 0.03521960626841836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06019902229309082 0.1073155403137207

Final encoder loss: 0.0350100359850025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05991029739379883 0.10801959037780762

Final encoder loss: 0.034046190342102246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06068754196166992 0.10809874534606934

Final encoder loss: 0.03532006529201352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059998512268066406 0.10758042335510254

Final encoder loss: 0.030837371562841488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059403419494628906 0.10697603225708008


Training dapper model
Final encoder loss: 0.20246461033821106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11522674560546875 0.03458118438720703

Final encoder loss: 0.2082001119852066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11280608177185059 0.034070730209350586

Final encoder loss: 0.06643734872341156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11211395263671875 0.03403425216674805

Final encoder loss: 0.06625672429800034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11319637298583984 0.03438282012939453

Final encoder loss: 0.04297111555933952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1132500171661377 0.03435873985290527

Final encoder loss: 0.04235689714550972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11208415031433105 0.03431272506713867

Final encoder loss: 0.03491007164120674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11259913444519043 0.03521561622619629

Final encoder loss: 0.03495844081044197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11388254165649414 0.03439736366271973

Final encoder loss: 0.032683368772268295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11257433891296387 0.034127235412597656

Final encoder loss: 0.03274080902338028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11364102363586426 0.0340418815612793

Final encoder loss: 0.03314007818698883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11253762245178223 0.03441047668457031

Final encoder loss: 0.032760582864284515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11280608177185059 0.035089731216430664

Final encoder loss: 0.03434262052178383
Final encoder loss: 0.0324697382748127

Training case model
Final encoder loss: 0.05136490339572565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08919262886047363 0.2191469669342041

Final encoder loss: 0.052066030845893736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08927130699157715 0.21942687034606934

Final encoder loss: 0.052253166260231056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09014701843261719 0.21897149085998535

Final encoder loss: 0.0499289558958957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09049654006958008 0.21932554244995117

Final encoder loss: 0.051871732292446564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08907723426818848 0.21846866607666016

Final encoder loss: 0.05232076812832544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0889139175415039 0.21826934814453125

Final encoder loss: 0.05132529282376252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.0894017219543457 0.21854019165039062

Final encoder loss: 0.05036478301210501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08939528465270996 0.21828484535217285

Final encoder loss: 0.05149451872889171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08910202980041504 0.21866393089294434

Final encoder loss: 0.051033783202992226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08930802345275879 0.21855425834655762

Final encoder loss: 0.051153128595290955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08895254135131836 0.21867060661315918

Final encoder loss: 0.05192853868925289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08944559097290039 0.21842432022094727

Final encoder loss: 0.050783585652741846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08958172798156738 0.2183995246887207

Final encoder loss: 0.051297466826155894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08913373947143555 0.2182326316833496

Final encoder loss: 0.05109371488155926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0884866714477539 0.2181391716003418

Final encoder loss: 0.05098738419999597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08541178703308105 0.21565461158752441


Training case model
Final encoder loss: 0.20295940339565277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26141977310180664 0.051689863204956055

Final encoder loss: 0.18889810144901276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582082748413086 0.05487990379333496

Final encoder loss: 0.19014893472194672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2585480213165283 0.05269312858581543

Final encoder loss: 0.19218169152736664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25765538215637207 0.052001953125

Final encoder loss: 0.18081167340278625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585611343383789 0.052323102951049805

Final encoder loss: 0.19193093478679657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548046112060547 0.05386638641357422

Final encoder loss: 0.0903838574886322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582230567932129 0.05210614204406738

Final encoder loss: 0.08187853544950485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588369846343994 0.05345654487609863

Final encoder loss: 0.07824662327766418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25933361053466797 0.052497148513793945

Final encoder loss: 0.07871409505605698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577965259552002 0.05442214012145996

Final encoder loss: 0.07376495748758316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25687193870544434 0.05256009101867676

Final encoder loss: 0.07541675865650177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564382553100586 0.05238175392150879

Final encoder loss: 0.05901273339986801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583649158477783 0.051088571548461914

Final encoder loss: 0.05499434471130371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2602524757385254 0.052680015563964844

Final encoder loss: 0.05330735072493553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25990867614746094 0.05125570297241211

Final encoder loss: 0.05540110915899277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2564723491668701 0.05156207084655762

Final encoder loss: 0.054360222071409225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25554466247558594 0.05236697196960449

Final encoder loss: 0.0541873537003994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544841766357422 0.0519709587097168

Final encoder loss: 0.05212700366973877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25667858123779297 0.05151557922363281

Final encoder loss: 0.05010738596320152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26635122299194336 0.05383038520812988

Final encoder loss: 0.0488889254629612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2561838626861572 0.05261707305908203

Final encoder loss: 0.05167277902364731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25517892837524414 0.05223584175109863

Final encoder loss: 0.05140581354498863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26656413078308105 0.050992727279663086

Final encoder loss: 0.050749097019433975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2529895305633545 0.051552772521972656

Final encoder loss: 0.053520701825618744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.256807804107666 0.05197024345397949

Final encoder loss: 0.052107155323028564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2578098773956299 0.05226016044616699

Final encoder loss: 0.05147530511021614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25775909423828125 0.052184343338012695

Final encoder loss: 0.0537017285823822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25839924812316895 0.052815914154052734

Final encoder loss: 0.05437600612640381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25832533836364746 0.05236363410949707

Final encoder loss: 0.05306512117385864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25341367721557617 0.05121278762817383

Final encoder loss: 0.053951527923345566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25713253021240234 0.052269697189331055

Final encoder loss: 0.05278684198856354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25605058670043945 0.052677154541015625

Final encoder loss: 0.05230478197336197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2596728801727295 0.052332162857055664

Final encoder loss: 0.053983304649591446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2555351257324219 0.05116987228393555

Final encoder loss: 0.05401457101106644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2554929256439209 0.05179405212402344

Final encoder loss: 0.05386253073811531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2532320022583008 0.05091238021850586

Final encoder loss: 0.052548933774232864
Final encoder loss: 0.05012277513742447
Final encoder loss: 0.047866471111774445
Final encoder loss: 0.04808162525296211
Final encoder loss: 0.046510305255651474
Final encoder loss: 0.04422556981444359

Training emognition model
Final encoder loss: 0.05426993298541493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08155941963195801 0.23224711418151855

Final encoder loss: 0.05502746545951952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08083558082580566 0.23064875602722168

Final encoder loss: 0.053829334330689545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08074712753295898 0.23054218292236328

Final encoder loss: 0.05471740735270933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0806570053100586 0.23052144050598145

Final encoder loss: 0.05428099860115877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08066511154174805 0.23102140426635742

Final encoder loss: 0.05338746995887153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0821983814239502 0.23090648651123047

Final encoder loss: 0.05472053109005283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08198070526123047 0.2300870418548584

Final encoder loss: 0.05425053665770916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08058023452758789 0.23075509071350098

Final encoder loss: 0.05628552336704451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08059859275817871 0.23036432266235352

Final encoder loss: 0.05248927447618298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.0809633731842041 0.23046183586120605

Final encoder loss: 0.056640638817144644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08074426651000977 0.2315962314605713

Final encoder loss: 0.053603788742637035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08142805099487305 0.23072314262390137

Final encoder loss: 0.05372195716696985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08203887939453125 0.2312009334564209

Final encoder loss: 0.05483222001121531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08195257186889648 0.23084020614624023

Final encoder loss: 0.05577440984934027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08042263984680176 0.2303299903869629

Final encoder loss: 0.052603510027345333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0814671516418457 0.23020124435424805


Training emognition model
Final encoder loss: 0.19355729222297668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24826574325561523 0.04871392250061035

Final encoder loss: 0.19496114552021027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24646449089050293 0.047818660736083984

Final encoder loss: 0.07706835865974426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2470688819885254 0.050385475158691406

Final encoder loss: 0.07669907063245773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24691081047058105 0.04922747611999512

Final encoder loss: 0.05719010531902313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24635934829711914 0.04836440086364746

Final encoder loss: 0.05613074451684952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24656224250793457 0.04754805564880371

Final encoder loss: 0.05098851025104523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2469017505645752 0.048027753829956055

Final encoder loss: 0.05018126592040062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2458634376525879 0.048111677169799805

Final encoder loss: 0.04913487657904625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24657559394836426 0.04868340492248535

Final encoder loss: 0.048698026686906815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464313507080078 0.04883217811584473

Final encoder loss: 0.04975062608718872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24778985977172852 0.04914569854736328

Final encoder loss: 0.04959592595696449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24672341346740723 0.048308610916137695

Final encoder loss: 0.05174130201339722
Final encoder loss: 0.04986768960952759

Training empatch model
Final encoder loss: 0.10070571561695148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0707254409790039 0.17300963401794434

Final encoder loss: 0.09324517820453791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07053995132446289 0.17373132705688477

Final encoder loss: 0.08075940437150875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07207512855529785 0.17418980598449707

Final encoder loss: 0.08320549519919586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0709228515625 0.17302155494689941

Final encoder loss: 0.07570487855909308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07050943374633789 0.17295241355895996

Final encoder loss: 0.07364022324018239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07020068168640137 0.1728520393371582

Final encoder loss: 0.06679022223943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07054448127746582 0.1724410057067871

Final encoder loss: 0.06545705002096625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07007455825805664 0.17231988906860352

Final encoder loss: 0.0590067888226609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07063627243041992 0.1729133129119873

Final encoder loss: 0.05977010805649851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07073068618774414 0.17284655570983887

Final encoder loss: 0.057380229093904736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07027173042297363 0.17254376411437988

Final encoder loss: 0.0519454413644004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07036519050598145 0.17281675338745117

Final encoder loss: 0.055351176305828566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07047724723815918 0.17273235321044922

Final encoder loss: 0.05904847314752217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07059335708618164 0.173004150390625

Final encoder loss: 0.0600732747042232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07053232192993164 0.17264819145202637

Final encoder loss: 0.051895294622090686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07018041610717773 0.17259669303894043


Training empatch model
Final encoder loss: 0.1711677610874176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17543458938598633 0.04283857345581055

Final encoder loss: 0.07871850579977036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17396306991577148 0.0425875186920166

Final encoder loss: 0.061185937374830246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17387151718139648 0.0432586669921875

Final encoder loss: 0.05417313426733017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17432618141174316 0.04205060005187988

Final encoder loss: 0.05084093287587166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17456674575805664 0.04392552375793457

Final encoder loss: 0.049403995275497437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1747426986694336 0.04341292381286621

Final encoder loss: 0.04863771051168442

Training wesad model
Final encoder loss: 0.11268739968063299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07073426246643066 0.17276549339294434

Final encoder loss: 0.09848377740140934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07039523124694824 0.173109769821167

Final encoder loss: 0.09665075187538241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07081174850463867 0.17277932167053223

Final encoder loss: 0.08550483761596975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0701298713684082 0.17304491996765137

Final encoder loss: 0.07209497276956464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07058954238891602 0.17235040664672852

Final encoder loss: 0.07491020795973838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07086038589477539 0.1723642349243164

Final encoder loss: 0.06765878188870231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07032895088195801 0.17303061485290527

Final encoder loss: 0.07105546395474191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07114887237548828 0.17249011993408203

Final encoder loss: 0.059744509712328774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07037210464477539 0.1732957363128662

Final encoder loss: 0.062428076250829946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07093119621276855 0.17259740829467773

Final encoder loss: 0.056718149047938864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07039618492126465 0.17243123054504395

Final encoder loss: 0.05562583656292036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07033991813659668 0.1730201244354248

Final encoder loss: 0.04866330549707314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07065773010253906 0.17315459251403809

Final encoder loss: 0.05062843931787723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07122397422790527 0.17281413078308105

Final encoder loss: 0.05101541566429103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07042455673217773 0.17316889762878418

Final encoder loss: 0.05248913849480168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07069969177246094 0.17239856719970703


Training wesad model
Final encoder loss: 0.21561923623085022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10484766960144043 0.03262782096862793

Final encoder loss: 0.09137605130672455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1022038459777832 0.03280949592590332

Final encoder loss: 0.06546920537948608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1022346019744873 0.03325080871582031

Final encoder loss: 0.055705077946186066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10271883010864258 0.03300738334655762

Final encoder loss: 0.05155195668339729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10177421569824219 0.03315472602844238

Final encoder loss: 0.04981045052409172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10266566276550293 0.0330965518951416

Final encoder loss: 0.04902208223938942

Calculating loss for amigos model
	Full Pass 0.6727049350738525
numFreeParamsPath 18
Reconstruction loss values: 0.0714968740940094 0.08047350496053696

Calculating loss for dapper model
	Full Pass 0.1504688262939453
numFreeParamsPath 18
Reconstruction loss values: 0.06157868728041649 0.06601624190807343

Calculating loss for case model
	Full Pass 0.8562004566192627
numFreeParamsPath 18
Reconstruction loss values: 0.07691358029842377 0.08000544458627701

Calculating loss for emognition model
	Full Pass 0.2794914245605469
numFreeParamsPath 18
Reconstruction loss values: 0.08311203867197037 0.08815531432628632

Calculating loss for empatch model
	Full Pass 0.10412096977233887
numFreeParamsPath 18
Reconstruction loss values: 0.09024582803249359 0.09380611032247543

Calculating loss for wesad model
	Full Pass 0.07685732841491699
numFreeParamsPath 18
Reconstruction loss values: 0.10538358986377716 0.1287461668252945
Total loss calculation time: 3.810967206954956

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.299546480178833
Total epoch time: 103.98552680015564

Epoch: 13

Training amigos model
Final encoder loss: 0.0709837761931882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1166081428527832 0.3894309997558594

Final encoder loss: 0.06342794646582244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10776495933532715 0.3877289295196533

Final encoder loss: 0.059355780593167554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10763978958129883 0.3882308006286621

Final encoder loss: 0.05752854240753142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10776352882385254 0.3884882926940918

Final encoder loss: 0.06666554003964266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10773682594299316 0.3879249095916748

Final encoder loss: 0.05824042381509802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10779476165771484 0.3882322311401367

Final encoder loss: 0.05498651022113026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1078193187713623 0.38858628273010254

Final encoder loss: 0.05487122587417493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10791873931884766 0.3880445957183838

Final encoder loss: 0.058590925226761764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10763287544250488 0.38744425773620605

Final encoder loss: 0.058577927310995134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10776472091674805 0.38808250427246094

Final encoder loss: 0.05400771630029602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10808372497558594 0.38847994804382324

Final encoder loss: 0.057650552889407236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10781645774841309 0.3884720802307129

Final encoder loss: 0.05651684009255302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1079719066619873 0.3885171413421631

Final encoder loss: 0.05690166678425394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1083078384399414 0.38894104957580566

Final encoder loss: 0.05421909638174054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10802197456359863 0.38846302032470703

Final encoder loss: 0.05894848720151102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10297679901123047 0.3833317756652832


Training case model
Final encoder loss: 0.07645824589493211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0909583568572998 0.26413989067077637

Final encoder loss: 0.0670628754590244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09118390083312988 0.2646472454071045

Final encoder loss: 0.0637118775576276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09130382537841797 0.2642381191253662

Final encoder loss: 0.06257398712545201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09079408645629883 0.26449036598205566

Final encoder loss: 0.06146834051566891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.10622787475585938 0.26404452323913574

Final encoder loss: 0.05887557630103815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09094047546386719 0.26416015625

Final encoder loss: 0.05866287289426123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0924832820892334 0.2654380798339844

Final encoder loss: 0.05825746679825655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09161162376403809 0.2667863368988037

Final encoder loss: 0.05764882188945761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09136724472045898 0.2656548023223877

Final encoder loss: 0.05517475307394499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09125089645385742 0.26581621170043945

Final encoder loss: 0.05465593795026443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0930330753326416 0.26641178131103516

Final encoder loss: 0.05391317597260408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09151220321655273 0.2652261257171631

Final encoder loss: 0.05446747151869713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09231019020080566 0.26654505729675293

Final encoder loss: 0.05399304474715417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09154033660888672 0.26717638969421387

Final encoder loss: 0.053182839448242675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09131121635437012 0.26598262786865234

Final encoder loss: 0.054181262538820114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08895230293273926 0.2622220516204834


Training dapper model
Final encoder loss: 0.0670846832164837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061905860900878906 0.15019750595092773

Final encoder loss: 0.05829181282841552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06189274787902832 0.15189003944396973

Final encoder loss: 0.05075411318934228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06356167793273926 0.1515028476715088

Final encoder loss: 0.05131574897677681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06247448921203613 0.15050005912780762

Final encoder loss: 0.04984081830651658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06235837936401367 0.14989233016967773

Final encoder loss: 0.045978581156733124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062116384506225586 0.15199923515319824

Final encoder loss: 0.046217597390284144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06374502182006836 0.15175938606262207

Final encoder loss: 0.044939449164954036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06209063529968262 0.15010809898376465

Final encoder loss: 0.04682865743457577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06202554702758789 0.15082859992980957

Final encoder loss: 0.043631620751374105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06229066848754883 0.15108418464660645

Final encoder loss: 0.04312964694158386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06352734565734863 0.15253305435180664

Final encoder loss: 0.04030516916054276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.062384605407714844 0.15123224258422852

Final encoder loss: 0.037493655912919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06217789649963379 0.15030527114868164

Final encoder loss: 0.04174457642363415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06160235404968262 0.1508655548095703

Final encoder loss: 0.040012690437917206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06289076805114746 0.15221786499023438

Final encoder loss: 0.04751423729169836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06175851821899414 0.15035104751586914


Training emognition model
Final encoder loss: 0.08093763023281388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08328771591186523 0.2755153179168701

Final encoder loss: 0.07198937964289397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0846712589263916 0.2765648365020752

Final encoder loss: 0.06949005943859105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08384966850280762 0.2760779857635498

Final encoder loss: 0.06744088112868053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.0835883617401123 0.27616310119628906

Final encoder loss: 0.0679832282699886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08283829689025879 0.2755155563354492

Final encoder loss: 0.06845534737481561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08353471755981445 0.27619385719299316

Final encoder loss: 0.0654609360484602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0851125717163086 0.27814388275146484

Final encoder loss: 0.06654185033418532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08314037322998047 0.2751274108886719

Final encoder loss: 0.06446209075300713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08460378646850586 0.27578115463256836

Final encoder loss: 0.06669549899144124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08319497108459473 0.2753274440765381

Final encoder loss: 0.06456524313383122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08363652229309082 0.2750709056854248

Final encoder loss: 0.0678292549691368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08492612838745117 0.27495718002319336

Final encoder loss: 0.06488332572941587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08348584175109863 0.2753162384033203

Final encoder loss: 0.06594626024567593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08511590957641602 0.2763674259185791

Final encoder loss: 0.05773154303080305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.083465576171875 0.27523374557495117

Final encoder loss: 0.06105291182367221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0829308032989502 0.2753281593322754


Training amigos model
Final encoder loss: 0.04180317979289024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10616922378540039 0.34160447120666504

Final encoder loss: 0.04093685141606472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10625028610229492 0.3414802551269531

Final encoder loss: 0.04429292724175634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10669779777526855 0.3416461944580078

Final encoder loss: 0.04242708571481654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1060938835144043 0.34212470054626465

Final encoder loss: 0.041808100982726304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10599350929260254 0.3416304588317871

Final encoder loss: 0.04478265351560444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10636472702026367 0.3417937755584717

Final encoder loss: 0.04614666293982408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10673642158508301 0.34151172637939453

Final encoder loss: 0.04640610368111203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10710382461547852 0.34165167808532715

Final encoder loss: 0.04020696202413532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10618400573730469 0.34177184104919434

Final encoder loss: 0.04629971718836196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10610675811767578 0.34174275398254395

Final encoder loss: 0.047732866437110236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10732769966125488 0.342313289642334

Final encoder loss: 0.040251031873964865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.1058192253112793 0.3407318592071533

Final encoder loss: 0.046679040678157084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.11712360382080078 0.3405892848968506

Final encoder loss: 0.04328596969110088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10525703430175781 0.3407580852508545

Final encoder loss: 0.044380041715759495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10503911972045898 0.34233784675598145

Final encoder loss: 0.041700465405347074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1003572940826416 0.3367466926574707


Training amigos model
Final encoder loss: 0.18078425526618958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44685888290405273 0.07530736923217773

Final encoder loss: 0.18783140182495117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.463916540145874 0.07500481605529785

Final encoder loss: 0.18361420929431915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44765543937683105 0.07903337478637695

Final encoder loss: 0.06626514345407486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46627378463745117 0.07880568504333496

Final encoder loss: 0.06787632405757904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4528536796569824 0.07992291450500488

Final encoder loss: 0.0636802464723587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4514133930206299 0.0767674446105957

Final encoder loss: 0.04595492035150528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4678528308868408 0.07526588439941406

Final encoder loss: 0.04632258415222168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4325895309448242 0.0722196102142334

Final encoder loss: 0.045050084590911865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42830324172973633 0.07326674461364746

Final encoder loss: 0.040069084614515305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.40885186195373535 0.07648253440856934

Final encoder loss: 0.04054604098200798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44661521911621094 0.07519769668579102

Final encoder loss: 0.040210139006376266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4296557903289795 0.07200860977172852

Final encoder loss: 0.039390698075294495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4465041160583496 0.07494950294494629

Final encoder loss: 0.04013676568865776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4687530994415283 0.07625794410705566

Final encoder loss: 0.039946187287569046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45085620880126953 0.07666397094726562

Final encoder loss: 0.04257974401116371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4724719524383545 0.07571721076965332

Final encoder loss: 0.04325144737958908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45221638679504395 0.07549428939819336

Final encoder loss: 0.04270322248339653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4528534412384033 0.07262420654296875

Final encoder loss: 0.044999197125434875
Final encoder loss: 0.04331075772643089
Final encoder loss: 0.04069989547133446

Training dapper model
Final encoder loss: 0.03651101452618291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.060749053955078125 0.10783743858337402

Final encoder loss: 0.035487477557335376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05967855453491211 0.10734200477600098

Final encoder loss: 0.03729521707375241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059864044189453125 0.1075446605682373

Final encoder loss: 0.03701741986098874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06038951873779297 0.10806751251220703

Final encoder loss: 0.03391105836431734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059662818908691406 0.10764479637145996

Final encoder loss: 0.035160303271277255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05984354019165039 0.10742902755737305

Final encoder loss: 0.032220462226261105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05947613716125488 0.10774469375610352

Final encoder loss: 0.03794654411076352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06084442138671875 0.10720372200012207

Final encoder loss: 0.03269127995334956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059784889221191406 0.10739803314208984

Final encoder loss: 0.03342104613591765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05967998504638672 0.10772156715393066

Final encoder loss: 0.03543581661338555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.0605320930480957 0.10799026489257812

Final encoder loss: 0.03397454804883907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05954289436340332 0.10732769966125488

Final encoder loss: 0.03269920319569699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.059583425521850586 0.10720467567443848

Final encoder loss: 0.03458927061445837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.0599822998046875 0.10835528373718262

Final encoder loss: 0.032517391017647425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06059622764587402 0.10762882232666016

Final encoder loss: 0.030939247305927193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05937337875366211 0.10746335983276367


Training dapper model
Final encoder loss: 0.20244380831718445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11340546607971191 0.03455924987792969

Final encoder loss: 0.20822221040725708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11633014678955078 0.033411502838134766

Final encoder loss: 0.06755946576595306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1130983829498291 0.034546852111816406

Final encoder loss: 0.06846866756677628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11221098899841309 0.03421473503112793

Final encoder loss: 0.042952582240104675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11446857452392578 0.0346064567565918

Final encoder loss: 0.04314275458455086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11195969581604004 0.03381204605102539

Final encoder loss: 0.03415174037218094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1120157241821289 0.033721923828125

Final encoder loss: 0.03484145179390907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11345934867858887 0.03510451316833496

Final encoder loss: 0.030934011563658714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11330938339233398 0.03425478935241699

Final encoder loss: 0.03184087201952934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1120445728302002 0.033817291259765625

Final encoder loss: 0.030598441138863564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11292624473571777 0.03519582748413086

Final encoder loss: 0.0318167619407177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11374402046203613 0.03380250930786133

Final encoder loss: 0.03186970204114914
Final encoder loss: 0.030933210626244545

Training case model
Final encoder loss: 0.05184753296685139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08921027183532715 0.21906375885009766

Final encoder loss: 0.048341172251254456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08924078941345215 0.21882414817810059

Final encoder loss: 0.049267386077968864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08897137641906738 0.21941757202148438

Final encoder loss: 0.050370628415962494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08958888053894043 0.21927237510681152

Final encoder loss: 0.05030679612026821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0899045467376709 0.21956086158752441

Final encoder loss: 0.04752794286168375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08977913856506348 0.21935629844665527

Final encoder loss: 0.04948612709938862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09110569953918457 0.21916580200195312

Final encoder loss: 0.04854625527077098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08904623985290527 0.21928715705871582

Final encoder loss: 0.04817664969001972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08977580070495605 0.21895956993103027

Final encoder loss: 0.04861250234578379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0890049934387207 0.2194826602935791

Final encoder loss: 0.048300805122198386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08961081504821777 0.2191166877746582

Final encoder loss: 0.0468463982857659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08959603309631348 0.21939802169799805

Final encoder loss: 0.048787525434744745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08951497077941895 0.21920275688171387

Final encoder loss: 0.04779127257557825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08946800231933594 0.2189931869506836

Final encoder loss: 0.0486144695200664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08933568000793457 0.21930456161499023

Final encoder loss: 0.048747840909522666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08686065673828125 0.21605372428894043


Training case model
Final encoder loss: 0.2029707431793213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26343798637390137 0.05409979820251465

Final encoder loss: 0.18892185389995575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27258849143981934 0.05332136154174805

Final encoder loss: 0.19015224277973175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2573399543762207 0.05211949348449707

Final encoder loss: 0.19218675792217255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26001524925231934 0.05303788185119629

Final encoder loss: 0.18082210421562195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579491138458252 0.052369117736816406

Final encoder loss: 0.19192077219486237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551610469818115 0.05176734924316406

Final encoder loss: 0.0910915732383728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2668745517730713 0.05188179016113281

Final encoder loss: 0.08210090547800064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.268601655960083 0.05201077461242676

Final encoder loss: 0.07865432649850845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570974826812744 0.053774356842041016

Final encoder loss: 0.0792519673705101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25786781311035156 0.05204892158508301

Final encoder loss: 0.07428061217069626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2570171356201172 0.052544355392456055

Final encoder loss: 0.07587926089763641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564570903778076 0.05453801155090332

Final encoder loss: 0.05891384929418564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582874298095703 0.05181264877319336

Final encoder loss: 0.05469433218240738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25742387771606445 0.053736209869384766

Final encoder loss: 0.05292931944131851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2592196464538574 0.05229806900024414

Final encoder loss: 0.055227093398571014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25798630714416504 0.051804304122924805

Final encoder loss: 0.05382348969578743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25839757919311523 0.05239582061767578

Final encoder loss: 0.05375349894165993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25574660301208496 0.05080986022949219

Final encoder loss: 0.05104798078536987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.258007287979126 0.051062822341918945

Final encoder loss: 0.04912889748811722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25797128677368164 0.05195355415344238

Final encoder loss: 0.04792531207203865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25517821311950684 0.05143570899963379

Final encoder loss: 0.05047465115785599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25540947914123535 0.05154705047607422

Final encoder loss: 0.050557978451251984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2560145854949951 0.05153226852416992

Final encoder loss: 0.04947078600525856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25339579582214355 0.05109071731567383

Final encoder loss: 0.05174648389220238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2560102939605713 0.0519258975982666

Final encoder loss: 0.05057591199874878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2556889057159424 0.05207943916320801

Final encoder loss: 0.0502629354596138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25537562370300293 0.05171608924865723

Final encoder loss: 0.051767390221357346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25693750381469727 0.0515897274017334

Final encoder loss: 0.0529533289372921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581207752227783 0.05090498924255371

Final encoder loss: 0.05217796936631203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556936740875244 0.053461313247680664

Final encoder loss: 0.0522615909576416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25854992866516113 0.05224800109863281

Final encoder loss: 0.05083388835191727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25763893127441406 0.05057501792907715

Final encoder loss: 0.050825007259845734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2595357894897461 0.05140805244445801

Final encoder loss: 0.05206281691789627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25781893730163574 0.0528719425201416

Final encoder loss: 0.05241130664944649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25812506675720215 0.05179786682128906

Final encoder loss: 0.05237575247883797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25559496879577637 0.05195188522338867

Final encoder loss: 0.05066638067364693
Final encoder loss: 0.048166416585445404
Final encoder loss: 0.046424608677625656
Final encoder loss: 0.04651736840605736
Final encoder loss: 0.044833455234766006
Final encoder loss: 0.04263068735599518

Training emognition model
Final encoder loss: 0.05283441732135475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08168578147888184 0.23044967651367188

Final encoder loss: 0.05291217514175017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08076906204223633 0.2308943271636963

Final encoder loss: 0.05231847913076306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08090782165527344 0.23068976402282715

Final encoder loss: 0.05235338581112975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08096671104431152 0.23096203804016113

Final encoder loss: 0.05128594311015062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08105754852294922 0.23071599006652832

Final encoder loss: 0.05165953881602586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08133435249328613 0.22917747497558594

Final encoder loss: 0.05205406883293113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08109211921691895 0.22956371307373047

Final encoder loss: 0.053334119105679346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08087825775146484 0.2295548915863037

Final encoder loss: 0.051762495576683276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08061099052429199 0.22937631607055664

Final encoder loss: 0.0521471945830943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08078241348266602 0.22939801216125488

Final encoder loss: 0.04938596658552977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08035159111022949 0.22942113876342773

Final encoder loss: 0.05084404447101227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08070707321166992 0.2303004264831543

Final encoder loss: 0.05140738575885932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08059048652648926 0.22907185554504395

Final encoder loss: 0.05088280462636463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08179879188537598 0.22955536842346191

Final encoder loss: 0.05565908291429322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0806114673614502 0.22939515113830566

Final encoder loss: 0.05310316371859184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0796196460723877 0.2284984588623047


Training emognition model
Final encoder loss: 0.19355563819408417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2481093406677246 0.04870319366455078

Final encoder loss: 0.1949596107006073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24742913246154785 0.048358917236328125

Final encoder loss: 0.07746432721614838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483997344970703 0.04943490028381348

Final encoder loss: 0.07758872956037521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24677371978759766 0.04887533187866211

Final encoder loss: 0.05691320449113846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24887657165527344 0.04860234260559082

Final encoder loss: 0.05579681321978569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24809646606445312 0.04912686347961426

Final encoder loss: 0.0500648207962513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24779391288757324 0.04864144325256348

Final encoder loss: 0.04909447953104973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450237274169922 0.04792332649230957

Final encoder loss: 0.04758295789361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.246474027633667 0.04768013954162598

Final encoder loss: 0.04685908928513527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24543213844299316 0.04818606376647949

Final encoder loss: 0.047603677958250046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24526572227478027 0.04881429672241211

Final encoder loss: 0.047090232372283936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24597454071044922 0.04853487014770508

Final encoder loss: 0.04936416447162628
Final encoder loss: 0.04772570729255676

Training empatch model
Final encoder loss: 0.08853211122340843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07068300247192383 0.1724228858947754

Final encoder loss: 0.08191939003072599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07023286819458008 0.17254900932312012

Final encoder loss: 0.07562242250440954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07050442695617676 0.1723029613494873

Final encoder loss: 0.075779771768948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07036876678466797 0.17277264595031738

Final encoder loss: 0.07348580714195753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0706028938293457 0.17286062240600586

Final encoder loss: 0.06932935408525996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07083296775817871 0.17430901527404785

Final encoder loss: 0.07266258875151778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07107973098754883 0.17512154579162598

Final encoder loss: 0.06141387541739531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07157754898071289 0.17317986488342285

Final encoder loss: 0.0523257700995995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07107949256896973 0.17348170280456543

Final encoder loss: 0.05265149865330893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07083415985107422 0.17364048957824707

Final encoder loss: 0.0533634194016409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07091712951660156 0.17348384857177734

Final encoder loss: 0.056492401235793695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07086181640625 0.1735246181488037

Final encoder loss: 0.05107380990614106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07083940505981445 0.1739215850830078

Final encoder loss: 0.05343597857569239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07192444801330566 0.1726677417755127

Final encoder loss: 0.051166181434439705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0706024169921875 0.17327046394348145

Final encoder loss: 0.055256047190942875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07062029838562012 0.17294621467590332


Training empatch model
Final encoder loss: 0.17115111649036407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17756104469299316 0.04439425468444824

Final encoder loss: 0.0786503329873085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17650413513183594 0.04422616958618164

Final encoder loss: 0.060025155544281006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754131317138672 0.04412221908569336

Final encoder loss: 0.052446149289608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746046543121338 0.04523801803588867

Final encoder loss: 0.048735421150922775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471051216125488 0.043424129486083984

Final encoder loss: 0.047013573348522186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17499423027038574 0.04346466064453125

Final encoder loss: 0.04609507694840431

Training wesad model
Final encoder loss: 0.10507601765766994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07111597061157227 0.1730802059173584

Final encoder loss: 0.091530873598179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0709230899810791 0.17420673370361328

Final encoder loss: 0.08654235292472459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07233166694641113 0.17421746253967285

Final encoder loss: 0.0857495308107841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07196450233459473 0.17406344413757324

Final encoder loss: 0.06451292890740702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07202816009521484 0.17407011985778809

Final encoder loss: 0.0642818412761366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07080340385437012 0.17327380180358887

Final encoder loss: 0.06582245656471408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0722193717956543 0.17429661750793457

Final encoder loss: 0.06439437608348136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.08463621139526367 0.1733543872833252

Final encoder loss: 0.05328434609426045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07102227210998535 0.1739635467529297

Final encoder loss: 0.05163559171148168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07151985168457031 0.1734607219696045

Final encoder loss: 0.0549979248553222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07088303565979004 0.1730790138244629

Final encoder loss: 0.05598802492130929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07031488418579102 0.17281389236450195

Final encoder loss: 0.045420457117898785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07260560989379883 0.17307615280151367

Final encoder loss: 0.04690959672683724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07051467895507812 0.1735851764678955

Final encoder loss: 0.04642293869288009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07071566581726074 0.17308688163757324

Final encoder loss: 0.04624910886560356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07076239585876465 0.17337822914123535


Training wesad model
Final encoder loss: 0.21559885144233704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037147045135498 0.03270602226257324

Final encoder loss: 0.09208279103040695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384511947631836 0.032360076904296875

Final encoder loss: 0.06514159590005875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10198783874511719 0.032888174057006836

Final encoder loss: 0.05428057163953781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10274147987365723 0.03333687782287598

Final encoder loss: 0.04939476400613785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1024637222290039 0.03239774703979492

Final encoder loss: 0.04726671054959297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1021888256072998 0.03283810615539551

Final encoder loss: 0.04625893011689186

Calculating loss for amigos model
	Full Pass 0.6701638698577881
numFreeParamsPath 18
Reconstruction loss values: 0.07136324793100357 0.08007872104644775

Calculating loss for dapper model
	Full Pass 0.1507737636566162
numFreeParamsPath 18
Reconstruction loss values: 0.05722265690565109 0.061267927289009094

Calculating loss for case model
	Full Pass 0.8541915416717529
numFreeParamsPath 18
Reconstruction loss values: 0.07468365877866745 0.07815713435411453

Calculating loss for emognition model
	Full Pass 0.2901315689086914
numFreeParamsPath 18
Reconstruction loss values: 0.0786004588007927 0.0846269279718399

Calculating loss for empatch model
	Full Pass 0.10410022735595703
numFreeParamsPath 18
Reconstruction loss values: 0.08626854419708252 0.09076088666915894

Calculating loss for wesad model
	Full Pass 0.07651352882385254
numFreeParamsPath 18
Reconstruction loss values: 0.09971137344837189 0.12392450869083405
Total loss calculation time: 3.831977605819702

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.370060682296753
Total epoch time: 104.315988779068

Epoch: 14

Training emognition model
Final encoder loss: 0.07754583456414246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08838152885437012 0.27367424964904785

Final encoder loss: 0.07132391482825476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08255386352539062 0.2733769416809082

Final encoder loss: 0.0648637086492131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08315396308898926 0.27402210235595703

Final encoder loss: 0.06829437435973752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08301830291748047 0.27407097816467285

Final encoder loss: 0.06518452087631274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08351755142211914 0.2736949920654297

Final encoder loss: 0.06249957146661679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08267879486083984 0.27383852005004883

Final encoder loss: 0.05848934994327738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08297157287597656 0.2730839252471924

Final encoder loss: 0.06542421394209189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08306121826171875 0.27317237854003906

Final encoder loss: 0.06037404301473979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08255982398986816 0.2742185592651367

Final encoder loss: 0.0583706189537901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0825662612915039 0.2745070457458496

Final encoder loss: 0.06637319959935868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08306765556335449 0.2740778923034668

Final encoder loss: 0.05824385765000528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08285355567932129 0.2737908363342285

Final encoder loss: 0.06179415636281817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08284473419189453 0.274003267288208

Final encoder loss: 0.062467614269915886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08273696899414062 0.27388858795166016

Final encoder loss: 0.058129395867881864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08266139030456543 0.2735767364501953

Final encoder loss: 0.05634162822112499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08182930946350098 0.2726116180419922


Training case model
Final encoder loss: 0.07429021684291269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09099960327148438 0.26392102241516113

Final encoder loss: 0.06570553184097758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09083819389343262 0.2643451690673828

Final encoder loss: 0.06263365190905879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09049177169799805 0.26428890228271484

Final encoder loss: 0.06252658752538329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09076452255249023 0.2639281749725342

Final encoder loss: 0.06005820944020144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09090471267700195 0.26422786712646484

Final encoder loss: 0.05843941202401078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09070754051208496 0.2641112804412842

Final encoder loss: 0.057521949888401695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09068822860717773 0.2632460594177246

Final encoder loss: 0.05683102629576254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0907130241394043 0.2641899585723877

Final encoder loss: 0.05631929177540241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09062480926513672 0.2635617256164551

Final encoder loss: 0.05320589706918729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09130311012268066 0.2642049789428711

Final encoder loss: 0.053749771537227965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09088015556335449 0.2637906074523926

Final encoder loss: 0.05245265383576308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0908350944519043 0.2640361785888672

Final encoder loss: 0.05331519368131979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09091687202453613 0.2637953758239746

Final encoder loss: 0.051222728072202635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0908958911895752 0.26346683502197266

Final encoder loss: 0.05283357879567482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09076595306396484 0.26427769660949707

Final encoder loss: 0.05196831592152297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08777427673339844 0.25998783111572266


Training dapper model
Final encoder loss: 0.06661431483357041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06116318702697754 0.1486201286315918

Final encoder loss: 0.05643772442457737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06137490272521973 0.14888548851013184

Final encoder loss: 0.04803237233926587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06139016151428223 0.14901995658874512

Final encoder loss: 0.047545633754008096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06123089790344238 0.14856767654418945

Final encoder loss: 0.045018199624590846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.061156272888183594 0.14901947975158691

Final encoder loss: 0.045411201203176425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06235909461975098 0.1495203971862793

Final encoder loss: 0.04499380274974385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06088113784790039 0.14989161491394043

Final encoder loss: 0.04630072238336671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061463117599487305 0.14931774139404297

Final encoder loss: 0.042087369720273035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06112265586853027 0.14983749389648438

Final encoder loss: 0.04257234872266091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.061758995056152344 0.14895868301391602

Final encoder loss: 0.03789931003626415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061110734939575195 0.1491100788116455

Final encoder loss: 0.042151716915097774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06165146827697754 0.14930343627929688

Final encoder loss: 0.038661760510456535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06135702133178711 0.14960432052612305

Final encoder loss: 0.04151824766679029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.061461687088012695 0.1487417221069336

Final encoder loss: 0.03905021496186826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06148195266723633 0.14995479583740234

Final encoder loss: 0.033592603431334925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06116604804992676 0.14766860008239746


Training amigos model
Final encoder loss: 0.07127215630485832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10810637474060059 0.3881189823150635

Final encoder loss: 0.06377394539463872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10783910751342773 0.3888132572174072

Final encoder loss: 0.06351947060412284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10783076286315918 0.38853979110717773

Final encoder loss: 0.06950144812636799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10803604125976562 0.38849306106567383

Final encoder loss: 0.06155327884357176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10741543769836426 0.38853955268859863

Final encoder loss: 0.059282656181774765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10796308517456055 0.3884294033050537

Final encoder loss: 0.06381200737485403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10770797729492188 0.38829469680786133

Final encoder loss: 0.056373566030317845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10784649848937988 0.3884718418121338

Final encoder loss: 0.05903818979552991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10779380798339844 0.38838696479797363

Final encoder loss: 0.05709698464277979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10799098014831543 0.3884544372558594

Final encoder loss: 0.05168630252329278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10802316665649414 0.3886687755584717

Final encoder loss: 0.05707040091150662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10816359519958496 0.3884751796722412

Final encoder loss: 0.05643675890444967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10801053047180176 0.38969969749450684

Final encoder loss: 0.05951182765064848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10831117630004883 0.38810014724731445

Final encoder loss: 0.05833097956255536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10780811309814453 0.38895702362060547

Final encoder loss: 0.05190787805040678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10299086570739746 0.3820779323577881


Training amigos model
Final encoder loss: 0.04389008255778836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10577678680419922 0.34136509895324707

Final encoder loss: 0.045122533166463766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1064600944519043 0.3417348861694336

Final encoder loss: 0.041502625263798845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.106964111328125 0.34150004386901855

Final encoder loss: 0.04100098927169355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10628890991210938 0.3421196937561035

Final encoder loss: 0.044803106412289025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10671687126159668 0.34132957458496094

Final encoder loss: 0.0396824303917018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1064455509185791 0.341691255569458

Final encoder loss: 0.045980810050334456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1055307388305664 0.3418450355529785

Final encoder loss: 0.04010240535924493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10606813430786133 0.3416740894317627

Final encoder loss: 0.04433088784338167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10613846778869629 0.3415718078613281

Final encoder loss: 0.04042869223963008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10605955123901367 0.3415677547454834

Final encoder loss: 0.04407638697590464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10598945617675781 0.34159350395202637

Final encoder loss: 0.04630126863691735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10621881484985352 0.3420133590698242

Final encoder loss: 0.04217681935410107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1065056324005127 0.34155893325805664

Final encoder loss: 0.04370919216379616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10680007934570312 0.34165406227111816

Final encoder loss: 0.04424974174015825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10616302490234375 0.34172749519348145

Final encoder loss: 0.04577559999261213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10216212272644043 0.3393397331237793


Training amigos model
Final encoder loss: 0.18076299130916595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4682271480560303 0.08274316787719727

Final encoder loss: 0.18783780932426453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46573877334594727 0.08098435401916504

Final encoder loss: 0.18362317979335785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46469593048095703 0.07481908798217773

Final encoder loss: 0.06699156016111374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46439075469970703 0.07597708702087402

Final encoder loss: 0.06883612275123596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46492528915405273 0.07600831985473633

Final encoder loss: 0.06506149470806122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4737131595611572 0.07383394241333008

Final encoder loss: 0.04607057943940163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47177672386169434 0.07614779472351074

Final encoder loss: 0.047315675765275955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46660923957824707 0.08171772956848145

Final encoder loss: 0.04544057697057724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46412181854248047 0.08215689659118652

Final encoder loss: 0.03961033374071121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46797704696655273 0.07563185691833496

Final encoder loss: 0.04110454022884369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46613168716430664 0.07591819763183594

Final encoder loss: 0.03962301090359688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46754026412963867 0.0745234489440918

Final encoder loss: 0.038822099566459656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47377562522888184 0.0770409107208252

Final encoder loss: 0.04020407423377037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4700796604156494 0.07569360733032227

Final encoder loss: 0.03911379724740982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46068334579467773 0.07453703880310059

Final encoder loss: 0.04259207099676132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45334506034851074 0.07369875907897949

Final encoder loss: 0.04396000877022743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45513010025024414 0.07694697380065918

Final encoder loss: 0.04278605058789253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4552493095397949 0.07347512245178223

Final encoder loss: 0.045375242829322815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45355796813964844 0.07315254211425781

Final encoder loss: 0.04617808014154434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4598526954650879 0.0745077133178711

Final encoder loss: 0.0460764616727829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46623945236206055 0.07428169250488281

Final encoder loss: 0.04684958979487419
Final encoder loss: 0.04545510187745094
Final encoder loss: 0.04192765802145004

Training dapper model
Final encoder loss: 0.03606141133379134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059561967849731445 0.10723543167114258

Final encoder loss: 0.03509400193691125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059850215911865234 0.10697054862976074

Final encoder loss: 0.0343555269241627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05954909324645996 0.10703682899475098

Final encoder loss: 0.03422333457948607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.060431718826293945 0.10665130615234375

Final encoder loss: 0.034712780127627324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05985879898071289 0.10745739936828613

Final encoder loss: 0.035551807167267316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05941319465637207 0.10731720924377441

Final encoder loss: 0.034872376488509375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059288978576660156 0.10653924942016602

Final encoder loss: 0.03386759498708623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05917668342590332 0.10808730125427246

Final encoder loss: 0.03228113119500751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059076547622680664 0.10674381256103516

Final encoder loss: 0.033007229296539396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05909609794616699 0.10620498657226562

Final encoder loss: 0.03191731495566764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05910134315490723 0.10651254653930664

Final encoder loss: 0.031082718001813663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05911374092102051 0.10672616958618164

Final encoder loss: 0.03426422247283471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.0592653751373291 0.10646891593933105

Final encoder loss: 0.032828035722095875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.07681846618652344 0.10672283172607422

Final encoder loss: 0.033766921322017654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05907797813415527 0.10671019554138184

Final encoder loss: 0.03666482725727773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05851387977600098 0.10611867904663086


Training dapper model
Final encoder loss: 0.20243698358535767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11340045928955078 0.03412318229675293

Final encoder loss: 0.20819859206676483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11533069610595703 0.033128976821899414

Final encoder loss: 0.07056193053722382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1138615608215332 0.03401970863342285

Final encoder loss: 0.0711127519607544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11539173126220703 0.03395485877990723

Final encoder loss: 0.04404505714774132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11179661750793457 0.03379416465759277

Final encoder loss: 0.044056475162506104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11414408683776855 0.03382372856140137

Final encoder loss: 0.03461168333888054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11255598068237305 0.03397178649902344

Final encoder loss: 0.034912705421447754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11460638046264648 0.03320574760437012

Final encoder loss: 0.030886385589838028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11120891571044922 0.03383994102478027

Final encoder loss: 0.03158733621239662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150507926940918 0.03383636474609375

Final encoder loss: 0.03053056076169014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11201071739196777 0.03401517868041992

Final encoder loss: 0.031866610050201416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11417889595031738 0.03296208381652832

Final encoder loss: 0.03234395384788513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11183524131774902 0.03367805480957031

Final encoder loss: 0.03268127515912056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11664891242980957 0.03375649452209473

Final encoder loss: 0.03675955533981323
Final encoder loss: 0.03374398872256279

Training case model
Final encoder loss: 0.05030689323923279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.09021759033203125 0.2191760540008545

Final encoder loss: 0.04927808722162448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0890340805053711 0.21937274932861328

Final encoder loss: 0.050304596230938727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08993101119995117 0.21916532516479492

Final encoder loss: 0.04781965206517729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08904027938842773 0.21940302848815918

Final encoder loss: 0.04895093188716829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08907794952392578 0.2190687656402588

Final encoder loss: 0.04848166023037541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0892953872680664 0.21912503242492676

Final encoder loss: 0.048437630941310525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08953714370727539 0.21918678283691406

Final encoder loss: 0.049194414780383396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08972644805908203 0.21920466423034668

Final encoder loss: 0.04803451660938385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08953404426574707 0.21901249885559082

Final encoder loss: 0.04793639174475735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0898580551147461 0.21921777725219727

Final encoder loss: 0.046735032617810326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08924007415771484 0.2191762924194336

Final encoder loss: 0.04686240023257617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09022974967956543 0.21918630599975586

Final encoder loss: 0.04713416420633059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08907008171081543 0.21932530403137207

Final encoder loss: 0.0461827078532185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09080266952514648 0.21928787231445312

Final encoder loss: 0.046887028046528396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08902740478515625 0.21918559074401855

Final encoder loss: 0.048366168687217916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08649492263793945 0.21588611602783203


Training case model
Final encoder loss: 0.20296989381313324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2659456729888916 0.05158495903015137

Final encoder loss: 0.1889019012451172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25859498977661133 0.053235769271850586

Final encoder loss: 0.19014723598957062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581331729888916 0.05221891403198242

Final encoder loss: 0.19217726588249207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2603609561920166 0.05303335189819336

Final encoder loss: 0.18081529438495636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26009511947631836 0.051970720291137695

Final encoder loss: 0.1919085681438446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571542263031006 0.05161142349243164

Final encoder loss: 0.09100568294525146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2605729103088379 0.05210995674133301

Final encoder loss: 0.08203105628490448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25974249839782715 0.051453590393066406

Final encoder loss: 0.0793173685669899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26024770736694336 0.05202054977416992

Final encoder loss: 0.0795038565993309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25791144371032715 0.0524752140045166

Final encoder loss: 0.07484689354896545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26162052154541016 0.05250740051269531

Final encoder loss: 0.07641439884901047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2577211856842041 0.05228829383850098

Final encoder loss: 0.05865827202796936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588925361633301 0.0545041561126709

Final encoder loss: 0.0540766566991806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25928401947021484 0.051293134689331055

Final encoder loss: 0.052680604159832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2610166072845459 0.053122520446777344

Final encoder loss: 0.05456433817744255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2593345642089844 0.05157136917114258

Final encoder loss: 0.05362677201628685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2594335079193115 0.05314016342163086

Final encoder loss: 0.05340645834803581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25566601753234863 0.05193638801574707

Final encoder loss: 0.050342995673418045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26154470443725586 0.05149984359741211

Final encoder loss: 0.04798225685954094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25794219970703125 0.05328989028930664

Final encoder loss: 0.046976614743471146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.258286714553833 0.05359077453613281

Final encoder loss: 0.04932543262839317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2592170238494873 0.05309891700744629

Final encoder loss: 0.04941309615969658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2610619068145752 0.05153036117553711

Final encoder loss: 0.04831068590283394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567915916442871 0.05305647850036621

Final encoder loss: 0.05072249099612236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25969576835632324 0.05162382125854492

Final encoder loss: 0.049167368561029434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258927583694458 0.051630258560180664

Final encoder loss: 0.048880815505981445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2615036964416504 0.0519258975982666

Final encoder loss: 0.050671372562646866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577075958251953 0.05237889289855957

Final encoder loss: 0.05210089683532715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26036787033081055 0.051419734954833984

Final encoder loss: 0.05060078948736191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25709986686706543 0.05128765106201172

Final encoder loss: 0.05133376643061638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25975990295410156 0.05461716651916504

Final encoder loss: 0.05056804046034813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2590525150299072 0.05147862434387207

Final encoder loss: 0.049910493195056915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.260143518447876 0.05198311805725098

Final encoder loss: 0.05140085518360138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26024627685546875 0.05112957954406738

Final encoder loss: 0.05213671550154686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.259157657623291 0.053348541259765625

Final encoder loss: 0.05170281231403351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25522828102111816 0.05107426643371582

Final encoder loss: 0.04963397607207298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2594573497772217 0.05285453796386719

Final encoder loss: 0.04829788953065872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.259598970413208 0.052222251892089844

Final encoder loss: 0.047275882214307785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2605550289154053 0.05194211006164551

Final encoder loss: 0.04872645065188408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.260317325592041 0.052278757095336914

Final encoder loss: 0.049015212804079056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2609274387359619 0.05223894119262695

Final encoder loss: 0.04894525185227394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560274600982666 0.05286288261413574

Final encoder loss: 0.04970322176814079
Final encoder loss: 0.0469798818230629
Final encoder loss: 0.0450439378619194
Final encoder loss: 0.04531799629330635
Final encoder loss: 0.043752070516347885
Final encoder loss: 0.04162372276186943

Training emognition model
Final encoder loss: 0.05165438964922231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08082699775695801 0.22951340675354004

Final encoder loss: 0.05097130408892181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08038711547851562 0.22941970825195312

Final encoder loss: 0.053933926934071966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08059191703796387 0.22952008247375488

Final encoder loss: 0.05218908255098537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08062338829040527 0.22978997230529785

Final encoder loss: 0.05127698670166575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08041834831237793 0.22958946228027344

Final encoder loss: 0.049441869655578126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08049535751342773 0.2295675277709961

Final encoder loss: 0.05499883111164566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.0800631046295166 0.22920799255371094

Final encoder loss: 0.050321240527619354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08019900321960449 0.23044586181640625

Final encoder loss: 0.051569495016979716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08074665069580078 0.23079442977905273

Final encoder loss: 0.04944237728034436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08101224899291992 0.23103904724121094

Final encoder loss: 0.05517958550326815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08200407028198242 0.23115921020507812

Final encoder loss: 0.052355369552067335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08158278465270996 0.23091626167297363

Final encoder loss: 0.050008466603454364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0830378532409668 0.2308652400970459

Final encoder loss: 0.04907995361833473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.0810234546661377 0.2311711311340332

Final encoder loss: 0.05129991038297844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08136916160583496 0.2307896614074707

Final encoder loss: 0.05154703070626723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08021736145019531 0.23138785362243652


Training emognition model
Final encoder loss: 0.19356784224510193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25052332878112793 0.04941964149475098

Final encoder loss: 0.19495423138141632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24717211723327637 0.04850268363952637

Final encoder loss: 0.07812229543924332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24848604202270508 0.048477888107299805

Final encoder loss: 0.07885060459375381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472977638244629 0.04948258399963379

Final encoder loss: 0.0568288117647171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24754858016967773 0.04793834686279297

Final encoder loss: 0.05597290024161339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24798369407653809 0.04944729804992676

Final encoder loss: 0.04943503811955452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478199005126953 0.04898667335510254

Final encoder loss: 0.048722878098487854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24658870697021484 0.048673391342163086

Final encoder loss: 0.0466504767537117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2480616569519043 0.04917097091674805

Final encoder loss: 0.04622090607881546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24691390991210938 0.04968762397766113

Final encoder loss: 0.046627774834632874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24799513816833496 0.04810929298400879

Final encoder loss: 0.046378083527088165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24901056289672852 0.04916214942932129

Final encoder loss: 0.04820689186453819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24561572074890137 0.04723834991455078

Final encoder loss: 0.048522528260946274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2466263771057129 0.04759359359741211

Final encoder loss: 0.05153609439730644
Final encoder loss: 0.050101641565561295

Training empatch model
Final encoder loss: 0.09160604618986855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07056069374084473 0.17232942581176758

Final encoder loss: 0.07597369772382614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07022571563720703 0.17276334762573242

Final encoder loss: 0.07897359063350255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07045531272888184 0.17278242111206055

Final encoder loss: 0.07442436893077553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07049870491027832 0.1728200912475586

Final encoder loss: 0.06620523854694212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07022953033447266 0.17254877090454102

Final encoder loss: 0.07398568570200366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07038211822509766 0.17281007766723633

Final encoder loss: 0.06807854838892093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0703282356262207 0.17256712913513184

Final encoder loss: 0.06580697628965598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06971931457519531 0.17228221893310547

Final encoder loss: 0.05659562422093929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07064175605773926 0.17267990112304688

Final encoder loss: 0.05355392710439078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0717005729675293 0.17392516136169434

Final encoder loss: 0.04966813172523978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07191681861877441 0.17411518096923828

Final encoder loss: 0.051640624293622706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07120895385742188 0.17394661903381348

Final encoder loss: 0.052870240258674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07154178619384766 0.17374038696289062

Final encoder loss: 0.052194421556109864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07083916664123535 0.1741645336151123

Final encoder loss: 0.04944948359870859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07101249694824219 0.17390084266662598

Final encoder loss: 0.05410402941595244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07162737846374512 0.17354893684387207


Training empatch model
Final encoder loss: 0.17117132246494293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17548775672912598 0.04331827163696289

Final encoder loss: 0.07918859273195267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17400598526000977 0.043935298919677734

Final encoder loss: 0.06015612557530403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17534303665161133 0.04387712478637695

Final encoder loss: 0.0523311085999012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17618465423583984 0.04288816452026367

Final encoder loss: 0.0483151413500309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17391633987426758 0.043401479721069336

Final encoder loss: 0.046371687203645706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750326156616211 0.04273533821105957

Final encoder loss: 0.045377910137176514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17342042922973633 0.04332542419433594

Final encoder loss: 0.04526372626423836

Training wesad model
Final encoder loss: 0.09858344191080569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07054567337036133 0.1724565029144287

Final encoder loss: 0.08725729600617904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07033681869506836 0.1723005771636963

Final encoder loss: 0.09111259481714136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.06992697715759277 0.17266607284545898

Final encoder loss: 0.079174425651648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07017683982849121 0.1723034381866455

Final encoder loss: 0.06441103310326392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07019209861755371 0.17267465591430664

Final encoder loss: 0.06199311194912654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07046794891357422 0.17247605323791504

Final encoder loss: 0.06316044956979829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.06998586654663086 0.17233562469482422

Final encoder loss: 0.06371715862784207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0703439712524414 0.1725633144378662

Final encoder loss: 0.04953397840528007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.06988668441772461 0.17264509201049805

Final encoder loss: 0.05379061069212808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07042813301086426 0.17267799377441406

Final encoder loss: 0.051003332946487956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07045459747314453 0.1742250919342041

Final encoder loss: 0.051854531241323484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07111024856567383 0.17369532585144043

Final encoder loss: 0.0429439930358778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07116866111755371 0.17388916015625

Final encoder loss: 0.043997173661076014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07135605812072754 0.17386174201965332

Final encoder loss: 0.044654451855514914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710148811340332 0.17378520965576172

Final encoder loss: 0.044881459920311374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0710606575012207 0.17381501197814941


Training wesad model
Final encoder loss: 0.21560336649417877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10490632057189941 0.03313612937927246

Final encoder loss: 0.09138349443674088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10519623756408691 0.03298783302307129

Final encoder loss: 0.06432446092367172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10322904586791992 0.032793521881103516

Final encoder loss: 0.05321254953742027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10532665252685547 0.03290367126464844

Final encoder loss: 0.0480794683098793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10348057746887207 0.03290963172912598

Final encoder loss: 0.04564564675092697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10348200798034668 0.03309941291809082

Final encoder loss: 0.04450320824980736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10279512405395508 0.03229355812072754

Final encoder loss: 0.044104691594839096

Calculating loss for amigos model
	Full Pass 0.6724748611450195
numFreeParamsPath 18
Reconstruction loss values: 0.06226300448179245 0.0715242326259613

Calculating loss for dapper model
	Full Pass 0.1502692699432373
numFreeParamsPath 18
Reconstruction loss values: 0.052531417459249496 0.058504197746515274

Calculating loss for case model
	Full Pass 0.8546533584594727
numFreeParamsPath 18
Reconstruction loss values: 0.07042960822582245 0.07328690588474274

Calculating loss for emognition model
	Full Pass 0.27902936935424805
numFreeParamsPath 18
Reconstruction loss values: 0.07499940693378448 0.08188576996326447

Calculating loss for empatch model
	Full Pass 0.10488367080688477
numFreeParamsPath 18
Reconstruction loss values: 0.0822988748550415 0.08725575357675552

Calculating loss for wesad model
	Full Pass 0.07712078094482422
numFreeParamsPath 18
Reconstruction loss values: 0.09487146884202957 0.1187305822968483
Total loss calculation time: 4.174009799957275

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.412715196609497
Total epoch time: 110.89022731781006

Epoch: 15

Training emognition model
Final encoder loss: 0.07558118090240493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08998298645019531 0.28495335578918457

Final encoder loss: 0.06631052817014839
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08280587196350098 0.27312278747558594

Final encoder loss: 0.06389928430753714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0824437141418457 0.27505946159362793

Final encoder loss: 0.06488881004022978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08332228660583496 0.2744557857513428

Final encoder loss: 0.0632149062686339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08304691314697266 0.27438902854919434

Final encoder loss: 0.06307071865111535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08333301544189453 0.2740778923034668

Final encoder loss: 0.06285119739037504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08341431617736816 0.27509284019470215

Final encoder loss: 0.06000623323271795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08338356018066406 0.27400684356689453

Final encoder loss: 0.06300489422711404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08275771141052246 0.2742345333099365

Final encoder loss: 0.06409644871134038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08284997940063477 0.2744455337524414

Final encoder loss: 0.058326602619910996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08319377899169922 0.27475690841674805

Final encoder loss: 0.05926130825373709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08264541625976562 0.27414941787719727

Final encoder loss: 0.0616223925574638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08274459838867188 0.27376294136047363

Final encoder loss: 0.058992253213034085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08252501487731934 0.27463269233703613

Final encoder loss: 0.058328958599979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08296561241149902 0.2735157012939453

Final encoder loss: 0.05862182670641954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08181262016296387 0.2731482982635498


Training amigos model
Final encoder loss: 0.059842960164571676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10792970657348633 0.38758182525634766

Final encoder loss: 0.05897395929276664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10779428482055664 0.38829755783081055

Final encoder loss: 0.059403519727924695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1079254150390625 0.38900089263916016

Final encoder loss: 0.05333964765099793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10834503173828125 0.38880395889282227

Final encoder loss: 0.05726171712875774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10801196098327637 0.3884310722351074

Final encoder loss: 0.05297667757024728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10789346694946289 0.38884687423706055

Final encoder loss: 0.0572666946050265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10789251327514648 0.388411283493042

Final encoder loss: 0.05430525098772063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10819101333618164 0.38821887969970703

Final encoder loss: 0.055216197571462275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10805964469909668 0.3886380195617676

Final encoder loss: 0.05098228490854379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10807228088378906 0.3888566493988037

Final encoder loss: 0.05605484095187314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10778975486755371 0.3882288932800293

Final encoder loss: 0.04986444514401977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10823202133178711 0.38953256607055664

Final encoder loss: 0.05397073259327168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1083364486694336 0.3883202075958252

Final encoder loss: 0.05887992530399672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10796761512756348 0.3881714344024658

Final encoder loss: 0.05247968388068539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10794281959533691 0.3880045413970947

Final encoder loss: 0.054456758495856884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10366153717041016 0.3822038173675537


Training dapper model
Final encoder loss: 0.056106404946669554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.0614471435546875 0.14928460121154785

Final encoder loss: 0.05028515057539376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06132078170776367 0.1482248306274414

Final encoder loss: 0.046183647286353775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06637811660766602 0.14892005920410156

Final encoder loss: 0.04247970409929024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06113719940185547 0.1487445831298828

Final encoder loss: 0.04292501150860641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06182527542114258 0.1493520736694336

Final encoder loss: 0.042456423723111335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06138896942138672 0.148482084274292

Final encoder loss: 0.04566271352740608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.061419010162353516 0.14886188507080078

Final encoder loss: 0.04304587765987271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061041831970214844 0.1492152214050293

Final encoder loss: 0.04171455568765546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.4463503360748291 0.1501619815826416

Final encoder loss: 0.04196955431154951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06196427345275879 0.1490311622619629

Final encoder loss: 0.03962161932745588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06138944625854492 0.14765691757202148

Final encoder loss: 0.03956823325416195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06122112274169922 0.14896130561828613

Final encoder loss: 0.042768650641401354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.061454057693481445 0.1517336368560791

Final encoder loss: 0.04094474623541322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.061936378479003906 0.1480426788330078

Final encoder loss: 0.04182196105089697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.060782670974731445 0.14827299118041992

Final encoder loss: 0.036063160433526045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.060745954513549805 0.1479954719543457


Training case model
Final encoder loss: 0.07080424736620924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09117650985717773 0.2632923126220703

Final encoder loss: 0.06180181220959485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09099316596984863 0.2640964984893799

Final encoder loss: 0.060829031318771015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09110522270202637 0.2644193172454834

Final encoder loss: 0.05690477109818761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09123897552490234 0.2635314464569092

Final encoder loss: 0.05551389683283698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0913546085357666 0.263378381729126

Final encoder loss: 0.056077651662057554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09087920188903809 0.2635035514831543

Final encoder loss: 0.054452751626493524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09107685089111328 0.2644047737121582

Final encoder loss: 0.05367750338502457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09105563163757324 0.26439380645751953

Final encoder loss: 0.05350481349019849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09143686294555664 0.2641298770904541

Final encoder loss: 0.05182900292932175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0908961296081543 0.26413631439208984

Final encoder loss: 0.05283096447857311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0909278392791748 0.2644059658050537

Final encoder loss: 0.05181972087981013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09116864204406738 0.2640953063964844

Final encoder loss: 0.0519988207559972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09141016006469727 0.26393628120422363

Final encoder loss: 0.05143901385592916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09109759330749512 0.26455163955688477

Final encoder loss: 0.05021808517969066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09098291397094727 0.2642228603363037

Final encoder loss: 0.05041258354075108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08747291564941406 0.26055312156677246


Training amigos model
Final encoder loss: 0.0441686508803263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10608148574829102 0.340670108795166

Final encoder loss: 0.037956490642353055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10632443428039551 0.3408939838409424

Final encoder loss: 0.03811203569569733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1212453842163086 0.34096288681030273

Final encoder loss: 0.04185952495867886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10582947731018066 0.3406655788421631

Final encoder loss: 0.03998101778863286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10581541061401367 0.3413510322570801

Final encoder loss: 0.043287919653402285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1060495376586914 0.34121060371398926

Final encoder loss: 0.04209783994813196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10570168495178223 0.34096503257751465

Final encoder loss: 0.03971885263491348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10594511032104492 0.3412468433380127

Final encoder loss: 0.03924250509167606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10598158836364746 0.341214656829834

Final encoder loss: 0.042667189340699636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10584425926208496 0.34097909927368164

Final encoder loss: 0.03856498545249131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10575485229492188 0.34128308296203613

Final encoder loss: 0.043225720806718444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.106231689453125 0.34134411811828613

Final encoder loss: 0.036485070742786604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10549211502075195 0.34090566635131836

Final encoder loss: 0.04333890147611439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10552048683166504 0.34052252769470215

Final encoder loss: 0.04104749932658775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1071770191192627 0.34160780906677246

Final encoder loss: 0.038253186633791035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10195565223693848 0.33830976486206055


Training amigos model
Final encoder loss: 0.1807716190814972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4698953628540039 0.08114504814147949

Final encoder loss: 0.1878334879875183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4728217124938965 0.0789651870727539

Final encoder loss: 0.18364450335502625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4650461673736572 0.0733788013458252

Final encoder loss: 0.06658978015184402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4651985168457031 0.07519769668579102

Final encoder loss: 0.06864963471889496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4697699546813965 0.07761645317077637

Final encoder loss: 0.06386005133390427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47347474098205566 0.07306122779846191

Final encoder loss: 0.04457344114780426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47449493408203125 0.07512211799621582

Final encoder loss: 0.04561382904648781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4687995910644531 0.08163189888000488

Final encoder loss: 0.04402513802051544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4693586826324463 0.07865166664123535

Final encoder loss: 0.038164880126714706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46891045570373535 0.07700848579406738

Final encoder loss: 0.038750920444726944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4666602611541748 0.0768897533416748

Final encoder loss: 0.038503456860780716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4696798324584961 0.07356500625610352

Final encoder loss: 0.03691724315285683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4755239486694336 0.07652425765991211

Final encoder loss: 0.03737463057041168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4746260643005371 0.08185672760009766

Final encoder loss: 0.03715464845299721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46631503105163574 0.07899808883666992

Final encoder loss: 0.038174085319042206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46927309036254883 0.07605743408203125

Final encoder loss: 0.0394243448972702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685399532318115 0.0757148265838623

Final encoder loss: 0.038662347942590714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4626026153564453 0.07435321807861328

Final encoder loss: 0.04014460742473602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47533226013183594 0.07783985137939453

Final encoder loss: 0.04130607843399048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47394871711730957 0.07479453086853027

Final encoder loss: 0.04088067635893822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46799135208129883 0.08137750625610352

Final encoder loss: 0.041173871606588364
Final encoder loss: 0.039855170994997025
Final encoder loss: 0.03780419006943703

Training dapper model
Final encoder loss: 0.03162202273679297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06014895439147949 0.10706591606140137

Final encoder loss: 0.03489021255913307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06027054786682129 0.1070704460144043

Final encoder loss: 0.02924545321315702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059890031814575195 0.10761022567749023

Final encoder loss: 0.03469790064147749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06032276153564453 0.10864663124084473

Final encoder loss: 0.033984435398393666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06136059761047363 0.10786819458007812

Final encoder loss: 0.03339947384764403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060039520263671875 0.10758066177368164

Final encoder loss: 0.03451325328895169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05996346473693848 0.10789132118225098

Final encoder loss: 0.030254493038131807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060944557189941406 0.10840821266174316

Final encoder loss: 0.03500625156052893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06056928634643555 0.10806608200073242

Final encoder loss: 0.036771237230228807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.060066938400268555 0.10729694366455078

Final encoder loss: 0.029431415683911537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06009650230407715 0.10826468467712402

Final encoder loss: 0.027533527156498157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060830116271972656 0.10744643211364746

Final encoder loss: 0.03307171695475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06041550636291504 0.10628509521484375

Final encoder loss: 0.03359185307055783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059308528900146484 0.10625505447387695

Final encoder loss: 0.032919324601525524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059169769287109375 0.10654544830322266

Final encoder loss: 0.03050594442538538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05904531478881836 0.10617542266845703


Training dapper model
Final encoder loss: 0.20245036482810974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11381793022155762 0.03397965431213379

Final encoder loss: 0.20823949575424194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11568260192871094 0.033870697021484375

Final encoder loss: 0.06727568060159683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1120460033416748 0.03365206718444824

Final encoder loss: 0.06779507547616959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11437511444091797 0.03315544128417969

Final encoder loss: 0.04249119013547897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11233663558959961 0.03409171104431152

Final encoder loss: 0.042216893285512924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11455035209655762 0.033928871154785156

Final encoder loss: 0.033071476966142654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11263751983642578 0.03394150733947754

Final encoder loss: 0.03317887708544731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11502385139465332 0.03373289108276367

Final encoder loss: 0.02934458665549755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11238574981689453 0.033522844314575195

Final encoder loss: 0.029722409322857857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11480522155761719 0.03388166427612305

Final encoder loss: 0.028203152120113373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11250972747802734 0.034082889556884766

Final encoder loss: 0.028675686568021774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11476349830627441 0.03308510780334473

Final encoder loss: 0.028792377561330795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11235523223876953 0.03411293029785156

Final encoder loss: 0.028775880113244057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11487913131713867 0.03402566909790039

Final encoder loss: 0.030334120616316795
Final encoder loss: 0.02840391732752323

Training case model
Final encoder loss: 0.04387995282721146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0890054702758789 0.21890044212341309

Final encoder loss: 0.04383236908272676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08976507186889648 0.2192211151123047

Final encoder loss: 0.04507830985217392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08946919441223145 0.2191617488861084

Final encoder loss: 0.044031906340727775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08965110778808594 0.2191469669342041

Final encoder loss: 0.0444061930483453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08927583694458008 0.21913886070251465

Final encoder loss: 0.044024394236512485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0899817943572998 0.2189016342163086

Final encoder loss: 0.0441611198782908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08965682983398438 0.2189018726348877

Final encoder loss: 0.04494246414901269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08901619911193848 0.21892881393432617

Final encoder loss: 0.043053473635160475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08912539482116699 0.21898818016052246

Final encoder loss: 0.04416516334976014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08930206298828125 0.2190709114074707

Final encoder loss: 0.04381979613625244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08965325355529785 0.21910309791564941

Final encoder loss: 0.044227505060602225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08935284614562988 0.21905899047851562

Final encoder loss: 0.04452694203336203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08954095840454102 0.2190690040588379

Final encoder loss: 0.04325364530101459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08944964408874512 0.21931743621826172

Final encoder loss: 0.043101736557102956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08951687812805176 0.21879243850708008

Final encoder loss: 0.042784965752908424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08589744567871094 0.21538352966308594


Training case model
Final encoder loss: 0.2029569149017334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2609231472015381 0.05141305923461914

Final encoder loss: 0.1889016330242157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2575345039367676 0.05164337158203125

Final encoder loss: 0.19015075266361237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25775575637817383 0.05191349983215332

Final encoder loss: 0.19219279289245605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577171325683594 0.0523374080657959

Final encoder loss: 0.18080241978168488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25678277015686035 0.05231833457946777

Final encoder loss: 0.19192829728126526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2533137798309326 0.05101656913757324

Final encoder loss: 0.09276216477155685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25701141357421875 0.05180835723876953

Final encoder loss: 0.08324945718050003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2576742172241211 0.05137753486633301

Final encoder loss: 0.08001337200403214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25600290298461914 0.05367302894592285

Final encoder loss: 0.07961595058441162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579057216644287 0.05355072021484375

Final encoder loss: 0.07437894493341446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590482234954834 0.05294299125671387

Final encoder loss: 0.0762152373790741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25597238540649414 0.051810264587402344

Final encoder loss: 0.05807977914810181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592813968658447 0.051880836486816406

Final encoder loss: 0.05343949794769287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2593977451324463 0.05435299873352051

Final encoder loss: 0.0519287995994091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25766730308532715 0.05250668525695801

Final encoder loss: 0.05331576615571976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25812435150146484 0.05315828323364258

Final encoder loss: 0.05190805718302727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2657439708709717 0.05417633056640625

Final encoder loss: 0.05209614336490631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25551772117614746 0.051428794860839844

Final encoder loss: 0.04832407832145691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580292224884033 0.05166816711425781

Final encoder loss: 0.04605447128415108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25957632064819336 0.05175971984863281

Final encoder loss: 0.04513372853398323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25782036781311035 0.054396629333496094

Final encoder loss: 0.04707552120089531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25736165046691895 0.05175137519836426

Final encoder loss: 0.047005392611026764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590303421020508 0.05318117141723633

Final encoder loss: 0.04606344923377037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563040256500244 0.050818681716918945

Final encoder loss: 0.047655023634433746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25858259201049805 0.05511903762817383

Final encoder loss: 0.046528298407793045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2758169174194336 0.05135202407836914

Final encoder loss: 0.04613872990012169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25807642936706543 0.054309844970703125

Final encoder loss: 0.04750514030456543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2596907615661621 0.0532987117767334

Final encoder loss: 0.04887060448527336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257321834564209 0.05437874794006348

Final encoder loss: 0.04710809513926506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553746700286865 0.05179238319396973

Final encoder loss: 0.047901496291160583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2598738670349121 0.05321168899536133

Final encoder loss: 0.047068629413843155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26950573921203613 0.053750038146972656

Final encoder loss: 0.046922750771045685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575709819793701 0.05339670181274414

Final encoder loss: 0.047914594411849976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26108336448669434 0.052033424377441406

Final encoder loss: 0.04853855445981026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25995922088623047 0.052468061447143555

Final encoder loss: 0.047768428921699524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554025650024414 0.052759647369384766

Final encoder loss: 0.04667701944708824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25768280029296875 0.05299854278564453

Final encoder loss: 0.04557917267084122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577059268951416 0.05136704444885254

Final encoder loss: 0.04486626759171486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593991756439209 0.05467629432678223

Final encoder loss: 0.04625452309846878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576308250427246 0.05246233940124512

Final encoder loss: 0.046446893364191055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257218599319458 0.053131103515625

Final encoder loss: 0.04608793184161186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.256319522857666 0.05150580406188965

Final encoder loss: 0.04614750295877457
Final encoder loss: 0.04414041340351105
Final encoder loss: 0.042455412447452545
Final encoder loss: 0.04256540164351463
Final encoder loss: 0.04118684306740761
Final encoder loss: 0.03907420486211777

Training emognition model
Final encoder loss: 0.05186634485322048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08288216590881348 0.23108959197998047

Final encoder loss: 0.05120328387391911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08167624473571777 0.23057126998901367

Final encoder loss: 0.04981156344064517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0823664665222168 0.2312772274017334

Final encoder loss: 0.05186531558403257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08156228065490723 0.23059797286987305

Final encoder loss: 0.04877599533891103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08174252510070801 0.2303018569946289

Final encoder loss: 0.047745654844344655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08110404014587402 0.23069167137145996

Final encoder loss: 0.05224471942036167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08104729652404785 0.23070144653320312

Final encoder loss: 0.04939830711814853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08092164993286133 0.23058509826660156

Final encoder loss: 0.04847125438736084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08143830299377441 0.2307753562927246

Final encoder loss: 0.048413775162643204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08110690116882324 0.2310774326324463

Final encoder loss: 0.04803377731553638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08123207092285156 0.23067474365234375

Final encoder loss: 0.04810127265550465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08209919929504395 0.23107481002807617

Final encoder loss: 0.04795852717492376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08127832412719727 0.23128366470336914

Final encoder loss: 0.048316777570655965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08253192901611328 0.23102021217346191

Final encoder loss: 0.04905752578275869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08132195472717285 0.2311241626739502

Final encoder loss: 0.051122312415852446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08178186416625977 0.22986555099487305


Training emognition model
Final encoder loss: 0.1935480684041977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2512185573577881 0.04965782165527344

Final encoder loss: 0.19496287405490875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24750638008117676 0.048029422760009766

Final encoder loss: 0.07797381281852722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24897146224975586 0.048941850662231445

Final encoder loss: 0.0781891718506813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2479252815246582 0.0488588809967041

Final encoder loss: 0.05629297345876694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24740862846374512 0.049507856369018555

Final encoder loss: 0.05513963848352432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24652767181396484 0.04946589469909668

Final encoder loss: 0.048390235751867294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2493603229522705 0.04868149757385254

Final encoder loss: 0.04777030646800995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24569392204284668 0.04963517189025879

Final encoder loss: 0.04526367411017418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24874234199523926 0.048406124114990234

Final encoder loss: 0.045045919716358185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24837398529052734 0.04811215400695801

Final encoder loss: 0.04471750929951668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2501192092895508 0.04895830154418945

Final encoder loss: 0.044679831713438034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24927282333374023 0.04850316047668457

Final encoder loss: 0.045967504382133484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2495100498199463 0.048879384994506836

Final encoder loss: 0.045817118138074875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.248276948928833 0.04850268363952637

Final encoder loss: 0.04745909571647644
Final encoder loss: 0.04624246805906296

Training empatch model
Final encoder loss: 0.08038818009156211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07073307037353516 0.17415428161621094

Final encoder loss: 0.08133018664565841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07100296020507812 0.17299962043762207

Final encoder loss: 0.0733267861740077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07070159912109375 0.17278027534484863

Final encoder loss: 0.0753301890944999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0705869197845459 0.1726398468017578

Final encoder loss: 0.06648602933678398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07105302810668945 0.1730027198791504

Final encoder loss: 0.06895799530412565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07058000564575195 0.1729428768157959

Final encoder loss: 0.06637205234497534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07071208953857422 0.1727156639099121

Final encoder loss: 0.0672911422524036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0701594352722168 0.17243075370788574

Final encoder loss: 0.052664540903013916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07041382789611816 0.1724073886871338

Final encoder loss: 0.049328800948752326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07054901123046875 0.1729416847229004

Final encoder loss: 0.053147966857595375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0707540512084961 0.17226171493530273

Final encoder loss: 0.04817798950786123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07073760032653809 0.17286896705627441

Final encoder loss: 0.048341235551195294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.08204078674316406 0.1725466251373291

Final encoder loss: 0.05137722920041348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07035493850708008 0.17246437072753906

Final encoder loss: 0.050820149336763897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07033753395080566 0.1737356185913086

Final encoder loss: 0.05204182504549006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07069158554077148 0.17360401153564453


Training empatch model
Final encoder loss: 0.1711377054452896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17862319946289062 0.04421877861022949

Final encoder loss: 0.07920653373003006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17498183250427246 0.0442202091217041

Final encoder loss: 0.059766970574855804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17606401443481445 0.04429745674133301

Final encoder loss: 0.05140529200434685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17511510848999023 0.0443873405456543

Final encoder loss: 0.04716688394546509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17674827575683594 0.044257402420043945

Final encoder loss: 0.04491402953863144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17595529556274414 0.04385209083557129

Final encoder loss: 0.043719351291656494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1759638786315918 0.043508291244506836

Final encoder loss: 0.04322320222854614

Training wesad model
Final encoder loss: 0.09504064170785118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07215356826782227 0.1739799976348877

Final encoder loss: 0.08965111698900567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07129287719726562 0.1738135814666748

Final encoder loss: 0.08369426452376874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0717625617980957 0.17421746253967285

Final encoder loss: 0.0773401403066946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07166051864624023 0.17381930351257324

Final encoder loss: 0.06299686569588939
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07162761688232422 0.17432093620300293

Final encoder loss: 0.061553693908367534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07132577896118164 0.17409062385559082

Final encoder loss: 0.05823509714446739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0714571475982666 0.17398691177368164

Final encoder loss: 0.05635161091846865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07123661041259766 0.1736741065979004

Final encoder loss: 0.050487404033773546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07144713401794434 0.17429661750793457

Final encoder loss: 0.0463101492694537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07180666923522949 0.17406058311462402

Final encoder loss: 0.048222773430260955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07134628295898438 0.17397570610046387

Final encoder loss: 0.049333380879291226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07198548316955566 0.17440080642700195

Final encoder loss: 0.03987991517570349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07126021385192871 0.17389345169067383

Final encoder loss: 0.04320829416039857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07162237167358398 0.17430996894836426

Final encoder loss: 0.04133513155067956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07129788398742676 0.1739656925201416

Final encoder loss: 0.04138045955587794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07121157646179199 0.17383122444152832


Training wesad model
Final encoder loss: 0.21560682356357574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10561680793762207 0.03319859504699707

Final encoder loss: 0.09169888496398926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10413479804992676 0.03351950645446777

Final encoder loss: 0.0637245923280716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10484457015991211 0.03326296806335449

Final encoder loss: 0.05190473794937134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10447001457214355 0.033040523529052734

Final encoder loss: 0.04622863978147507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442900657653809 0.032659053802490234

Final encoder loss: 0.043485376983881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10420846939086914 0.03281593322753906

Final encoder loss: 0.04207061976194382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1047365665435791 0.034110307693481445

Final encoder loss: 0.04136400297284126

Calculating loss for amigos model
	Full Pass 0.6841332912445068
numFreeParamsPath 18
Reconstruction loss values: 0.06029941886663437 0.06836912035942078

Calculating loss for dapper model
	Full Pass 0.1502094268798828
numFreeParamsPath 18
Reconstruction loss values: 0.049914367496967316 0.05549553409218788

Calculating loss for case model
	Full Pass 0.9026093482971191
numFreeParamsPath 18
Reconstruction loss values: 0.06697750091552734 0.07000602781772614

Calculating loss for emognition model
	Full Pass 0.29014110565185547
numFreeParamsPath 18
Reconstruction loss values: 0.0727095827460289 0.07803842425346375

Calculating loss for empatch model
	Full Pass 0.10434579849243164
numFreeParamsPath 18
Reconstruction loss values: 0.07999133318662643 0.08485715836286545

Calculating loss for wesad model
	Full Pass 0.07712697982788086
numFreeParamsPath 18
Reconstruction loss values: 0.0917523130774498 0.11430297791957855
Total loss calculation time: 3.9187653064727783

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.931063890457153
Total epoch time: 111.8858015537262

Epoch: 16

Training case model
Final encoder loss: 0.06453685928924693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09793376922607422 0.27121591567993164

Final encoder loss: 0.05900181738564125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0910940170288086 0.2635462284088135

Final encoder loss: 0.056891422234955094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09112262725830078 0.2644920349121094

Final encoder loss: 0.0543224913837033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09075713157653809 0.2649037837982178

Final encoder loss: 0.054643766231195996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09116983413696289 0.26396989822387695

Final encoder loss: 0.05387448872924364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09108996391296387 0.26351308822631836

Final encoder loss: 0.052681122654046786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09106326103210449 0.26416730880737305

Final encoder loss: 0.052418852830555855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09121060371398926 0.2657046318054199

Final encoder loss: 0.04973236671175105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09145092964172363 0.26437807083129883

Final encoder loss: 0.0504809133335611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09175658226013184 0.26528215408325195

Final encoder loss: 0.04925302045809056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09133529663085938 0.2649998664855957

Final encoder loss: 0.05007442579121542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09113049507141113 0.26599764823913574

Final encoder loss: 0.047892162989639345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09168267250061035 0.26581811904907227

Final encoder loss: 0.04972037276070476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0918431282043457 0.2652420997619629

Final encoder loss: 0.048274661728068234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09102392196655273 0.26457905769348145

Final encoder loss: 0.04658015563887836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08746647834777832 0.26303696632385254


Training emognition model
Final encoder loss: 0.07536706752012219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08558773994445801 0.2760932445526123

Final encoder loss: 0.0671629044490472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08361554145812988 0.27574944496154785

Final encoder loss: 0.06606157813247429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08627820014953613 0.27460813522338867

Final encoder loss: 0.062433175918015976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08343815803527832 0.27490234375

Final encoder loss: 0.06312673096759433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08275032043457031 0.27536845207214355

Final encoder loss: 0.06076345470285253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08298158645629883 0.274946928024292

Final encoder loss: 0.06222103221839813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08475208282470703 0.27718448638916016

Final encoder loss: 0.06179418323455832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08364534378051758 0.2750539779663086

Final encoder loss: 0.05744005504266584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08347320556640625 0.2758214473724365

Final encoder loss: 0.0592474990675209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08345222473144531 0.2758815288543701

Final encoder loss: 0.05830801611087411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08445286750793457 0.2756197452545166

Final encoder loss: 0.0608779978463556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0835731029510498 0.2741067409515381

Final encoder loss: 0.057042677005148035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08313488960266113 0.27604246139526367

Final encoder loss: 0.058715014238516626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08333516120910645 0.27515149116516113

Final encoder loss: 0.05597956193815904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08274650573730469 0.27467942237854004

Final encoder loss: 0.05853315186323576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08232760429382324 0.2725517749786377


Training amigos model
Final encoder loss: 0.061434761775212086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10802483558654785 0.38860058784484863

Final encoder loss: 0.05773356184737101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10785603523254395 0.3880016803741455

Final encoder loss: 0.056435064810278865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10767698287963867 0.38912487030029297

Final encoder loss: 0.05348966949730299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10831809043884277 0.3884744644165039

Final encoder loss: 0.05280123168215697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.1078944206237793 0.3893399238586426

Final encoder loss: 0.04735702278578706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10949921607971191 0.3894200325012207

Final encoder loss: 0.05252005290220206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10810494422912598 0.38878703117370605

Final encoder loss: 0.05340791765485791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10851168632507324 0.3889162540435791

Final encoder loss: 0.054324789842402396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10822153091430664 0.3880045413970947

Final encoder loss: 0.054019914812247584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10829424858093262 0.38834691047668457

Final encoder loss: 0.05761345906709143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1083219051361084 0.3886134624481201

Final encoder loss: 0.05209172890269199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10782051086425781 0.3886287212371826

Final encoder loss: 0.05202651380154655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10842561721801758 0.3883349895477295

Final encoder loss: 0.05371344238302847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10805487632751465 0.3882420063018799

Final encoder loss: 0.05339896630969948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10788965225219727 0.3876481056213379

Final encoder loss: 0.04870201093474165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.1028738021850586 0.3826789855957031


Training dapper model
Final encoder loss: 0.05184742952544537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06177330017089844 0.14850783348083496

Final encoder loss: 0.04557870952857993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061354875564575195 0.1489264965057373

Final encoder loss: 0.04293348640448523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.0769052505493164 0.14825940132141113

Final encoder loss: 0.04320342135489416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061388254165649414 0.14949631690979004

Final encoder loss: 0.04351985088159109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0620880126953125 0.14903664588928223

Final encoder loss: 0.0422115987349255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.061374664306640625 0.1494579315185547

Final encoder loss: 0.041536228564424425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06168031692504883 0.14938759803771973

Final encoder loss: 0.044337527621815684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06142830848693848 0.1499190330505371

Final encoder loss: 0.04106486462714606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061835527420043945 0.14911866188049316

Final encoder loss: 0.03737546052841374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06125807762145996 0.14925241470336914

Final encoder loss: 0.03757770475072472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06161785125732422 0.1494617462158203

Final encoder loss: 0.03758936829337534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06146836280822754 0.14973831176757812

Final encoder loss: 0.0409106060635732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06181168556213379 0.14937424659729004

Final encoder loss: 0.03595687466914791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06143784523010254 0.14926362037658691

Final encoder loss: 0.038771276454333044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06167125701904297 0.14931464195251465

Final encoder loss: 0.040423198172130394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06102561950683594 0.1501941680908203


Training amigos model
Final encoder loss: 0.037697645396063906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1058349609375 0.34104180335998535

Final encoder loss: 0.04041588208178507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10584759712219238 0.3411529064178467

Final encoder loss: 0.041001693042777364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1059870719909668 0.3412799835205078

Final encoder loss: 0.04006658491625682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10588407516479492 0.34123730659484863

Final encoder loss: 0.035986276636528036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10587358474731445 0.3410210609436035

Final encoder loss: 0.03890352980648057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10629725456237793 0.3413045406341553

Final encoder loss: 0.04013544630356793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1060633659362793 0.34230732917785645

Final encoder loss: 0.04125702850948396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1062459945678711 0.34117817878723145

Final encoder loss: 0.04076118622648296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10584592819213867 0.34138965606689453

Final encoder loss: 0.041018591455903385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10606050491333008 0.34143877029418945

Final encoder loss: 0.03852992060061376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10640192031860352 0.3419618606567383

Final encoder loss: 0.039049932061479144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10691118240356445 0.3412587642669678

Final encoder loss: 0.04093491322323746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1069800853729248 0.341141939163208

Final encoder loss: 0.03822773115085827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10586762428283691 0.34091758728027344

Final encoder loss: 0.04091102526587816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10558652877807617 0.3410952091217041

Final encoder loss: 0.042389462128234835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10068368911743164 0.33689236640930176


Training amigos model
Final encoder loss: 0.18078459799289703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4601151943206787 0.07608652114868164

Final encoder loss: 0.18781904876232147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4610590934753418 0.07428216934204102

Final encoder loss: 0.18361307680606842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4593808650970459 0.07521677017211914

Final encoder loss: 0.06885280460119247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4606330394744873 0.07331991195678711

Final encoder loss: 0.07025358825922012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.459308385848999 0.07609438896179199

Final encoder loss: 0.06455956399440765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4579126834869385 0.07278156280517578

Final encoder loss: 0.04532627388834953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46161413192749023 0.07413220405578613

Final encoder loss: 0.04580982029438019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45861196517944336 0.07378673553466797

Final encoder loss: 0.0437508299946785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4585392475128174 0.07346129417419434

Final encoder loss: 0.038059405982494354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4627106189727783 0.07391667366027832

Final encoder loss: 0.03833339735865593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.458754301071167 0.0764315128326416

Final encoder loss: 0.03770871087908745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4583737850189209 0.07281231880187988

Final encoder loss: 0.036232613027095795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46037745475769043 0.07382464408874512

Final encoder loss: 0.03636915236711502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4578108787536621 0.07394242286682129

Final encoder loss: 0.036561522632837296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4600808620452881 0.07423019409179688

Final encoder loss: 0.03746844455599785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46157383918762207 0.07571291923522949

Final encoder loss: 0.03739963099360466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45816516876220703 0.07543253898620605

Final encoder loss: 0.037843361496925354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45580530166625977 0.0749051570892334

Final encoder loss: 0.039395272731781006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4665501117706299 0.07669329643249512

Final encoder loss: 0.03899875655770302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46808433532714844 0.0775914192199707

Final encoder loss: 0.03918081894516945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47502684593200684 0.07379388809204102

Final encoder loss: 0.03981997072696686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4739644527435303 0.07420825958251953

Final encoder loss: 0.03982607647776604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685814380645752 0.08074378967285156

Final encoder loss: 0.03932339325547218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4690585136413574 0.08037042617797852

Final encoder loss: 0.03882678970694542
Final encoder loss: 0.03701057285070419
Final encoder loss: 0.03501078858971596

Training dapper model
Final encoder loss: 0.03100065835955673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.062120676040649414 0.10707902908325195

Final encoder loss: 0.029824676460941987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05992770195007324 0.10773444175720215

Final encoder loss: 0.03291439865280882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.060149192810058594 0.10746359825134277

Final encoder loss: 0.031338657313679054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05979442596435547 0.10755085945129395

Final encoder loss: 0.029187730429203917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06040692329406738 0.10830569267272949

Final encoder loss: 0.032362887601824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06096196174621582 0.10778403282165527

Final encoder loss: 0.030686191181638064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05988669395446777 0.10747361183166504

Final encoder loss: 0.03099720156604801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060245513916015625 0.10739684104919434

Final encoder loss: 0.034379371887738104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05974888801574707 0.10707211494445801

Final encoder loss: 0.03389096852668438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0600435733795166 0.10876679420471191

Final encoder loss: 0.030386669355680553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06122398376464844 0.10819339752197266

Final encoder loss: 0.031203162098324123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05961132049560547 0.1079549789428711

Final encoder loss: 0.029413221775729866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06019330024719238 0.10745716094970703

Final encoder loss: 0.030891729438421518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059998273849487305 0.1076650619506836

Final encoder loss: 0.029476736348399847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05981564521789551 0.10826849937438965

Final encoder loss: 0.029075406372955432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.060311079025268555 0.1077420711517334


Training dapper model
Final encoder loss: 0.2024243324995041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11678409576416016 0.034642696380615234

Final encoder loss: 0.20819346606731415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11585021018981934 0.03424072265625

Final encoder loss: 0.06904265284538269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11705756187438965 0.03402519226074219

Final encoder loss: 0.06921081244945526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11802339553833008 0.034503936767578125

Final encoder loss: 0.042713068425655365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11899852752685547 0.03385448455810547

Final encoder loss: 0.04267634078860283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11603689193725586 0.03493380546569824

Final encoder loss: 0.032746005803346634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11560845375061035 0.034218549728393555

Final encoder loss: 0.03286914899945259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559486389160156 0.034397125244140625

Final encoder loss: 0.028792638331651688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11840343475341797 0.03454470634460449

Final encoder loss: 0.029118889942765236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11840629577636719 0.03380918502807617

Final encoder loss: 0.027343204244971275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11583662033081055 0.03439807891845703

Final encoder loss: 0.027873089537024498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11612153053283691 0.034516096115112305

Final encoder loss: 0.02773842215538025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11579751968383789 0.034052371978759766

Final encoder loss: 0.028185158967971802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1180119514465332 0.034964561462402344

Final encoder loss: 0.02858070470392704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11862683296203613 0.03388547897338867

Final encoder loss: 0.02886679209768772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11573576927185059 0.03418993949890137

Final encoder loss: 0.030229352414608
Final encoder loss: 0.028990663588047028

Training case model
Final encoder loss: 0.04649187825588622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08927178382873535 0.21958708763122559

Final encoder loss: 0.045793445707283935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08902239799499512 0.2189493179321289

Final encoder loss: 0.0448497070169797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08929038047790527 0.21894478797912598

Final encoder loss: 0.04517766415883121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09155964851379395 0.219282865524292

Final encoder loss: 0.045458670560059464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.09000229835510254 0.21906232833862305

Final encoder loss: 0.04527754056288683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08924746513366699 0.21907591819763184

Final encoder loss: 0.044180964294755695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09044766426086426 0.21957778930664062

Final encoder loss: 0.043729656069360606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08925437927246094 0.2191319465637207

Final encoder loss: 0.04337678988926993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08948874473571777 0.21957945823669434

Final encoder loss: 0.04409802906917579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09084534645080566 0.21913647651672363

Final encoder loss: 0.043415826709795526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08973360061645508 0.21936821937561035

Final encoder loss: 0.04394962048335823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0900883674621582 0.2196366786956787

Final encoder loss: 0.043823762849200046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0889122486114502 0.2189960479736328

Final encoder loss: 0.04397026821610229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08932828903198242 0.21889042854309082

Final encoder loss: 0.043083232939592264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09045028686523438 0.2194521427154541

Final encoder loss: 0.04352619465256912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.0864253044128418 0.21570324897766113


Training case model
Final encoder loss: 0.20296840369701385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2683863639831543 0.05390048027038574

Final encoder loss: 0.18890906870365143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25745368003845215 0.05269145965576172

Final encoder loss: 0.1901542991399765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2599818706512451 0.05253458023071289

Final encoder loss: 0.1921817511320114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25763511657714844 0.052048444747924805

Final encoder loss: 0.18082033097743988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.270338773727417 0.0532383918762207

Final encoder loss: 0.19192242622375488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25693726539611816 0.05115628242492676

Final encoder loss: 0.09347474575042725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2615675926208496 0.05211162567138672

Final encoder loss: 0.08416179567575455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25972723960876465 0.0532989501953125

Final encoder loss: 0.08110947161912918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257460355758667 0.05143284797668457

Final encoder loss: 0.08039499074220657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25510740280151367 0.053521156311035156

Final encoder loss: 0.07458341121673584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25621533393859863 0.051506996154785156

Final encoder loss: 0.07710994780063629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543818950653076 0.05192756652832031

Final encoder loss: 0.05845433473587036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2562370300292969 0.05126547813415527

Final encoder loss: 0.05393977835774422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566359043121338 0.050595760345458984

Final encoder loss: 0.052491750568151474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577493190765381 0.05136847496032715

Final encoder loss: 0.05355386808514595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25620365142822266 0.051428794860839844

Final encoder loss: 0.051862526684999466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25647664070129395 0.0518038272857666

Final encoder loss: 0.05272901803255081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543752193450928 0.05141448974609375

Final encoder loss: 0.04840835928916931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25702977180480957 0.0512700080871582

Final encoder loss: 0.04606885090470314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25849437713623047 0.052593231201171875

Final encoder loss: 0.04513546824455261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25783514976501465 0.05273771286010742

Final encoder loss: 0.04682711511850357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576014995574951 0.05221748352050781

Final encoder loss: 0.046896666288375854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257366418838501 0.05240941047668457

Final encoder loss: 0.04621997848153114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25524353981018066 0.0517730712890625

Final encoder loss: 0.04702406004071236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25722360610961914 0.051148176193237305

Final encoder loss: 0.045951295644044876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.256134033203125 0.05274391174316406

Final encoder loss: 0.045451998710632324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2684016227722168 0.05227398872375488

Final encoder loss: 0.046831000596284866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25699853897094727 0.05148148536682129

Final encoder loss: 0.0480651892721653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25662970542907715 0.05179119110107422

Final encoder loss: 0.04682263731956482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2527804374694824 0.0520327091217041

Final encoder loss: 0.04697973281145096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25650477409362793 0.05240654945373535

Final encoder loss: 0.046382371336221695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2653348445892334 0.05164337158203125

Final encoder loss: 0.04610683396458626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2564356327056885 0.052022457122802734

Final encoder loss: 0.04674939438700676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568032741546631 0.05327892303466797

Final encoder loss: 0.04746939241886139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2567477226257324 0.05234837532043457

Final encoder loss: 0.046948835253715515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25513362884521484 0.05196690559387207

Final encoder loss: 0.04617859050631523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2559335231781006 0.05165886878967285

Final encoder loss: 0.04487833380699158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25788187980651855 0.05464649200439453

Final encoder loss: 0.04431040212512016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586205005645752 0.052190303802490234

Final encoder loss: 0.045496776700019836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590980529785156 0.05116009712219238

Final encoder loss: 0.045541152358055115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2577693462371826 0.05443239212036133

Final encoder loss: 0.04549591988325119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25606274604797363 0.051161766052246094

Final encoder loss: 0.04521676525473595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257260799407959 0.05288434028625488

Final encoder loss: 0.044155657291412354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2590198516845703 0.0527188777923584

Final encoder loss: 0.04360620304942131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2580862045288086 0.052195072174072266

Final encoder loss: 0.04485345259308815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25774383544921875 0.05199432373046875

Final encoder loss: 0.045470625162124634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2603421211242676 0.05144834518432617

Final encoder loss: 0.04448949173092842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546250820159912 0.051894426345825195

Final encoder loss: 0.04508855938911438
Final encoder loss: 0.043044984340667725
Final encoder loss: 0.04163011908531189
Final encoder loss: 0.04129425808787346
Final encoder loss: 0.04012792930006981
Final encoder loss: 0.03809250518679619

Training emognition model
Final encoder loss: 0.04816055236962027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08148431777954102 0.23125505447387695

Final encoder loss: 0.04939085794666737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08131814002990723 0.2306070327758789

Final encoder loss: 0.050754325217468046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08074164390563965 0.23040461540222168

Final encoder loss: 0.04666781460293297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08088254928588867 0.2307415008544922

Final encoder loss: 0.04740860652957609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08078503608703613 0.23044371604919434

Final encoder loss: 0.04961006962208753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08118677139282227 0.23040556907653809

Final encoder loss: 0.0483497226008696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08146429061889648 0.23070240020751953

Final encoder loss: 0.049374187666448244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0815129280090332 0.23076319694519043

Final encoder loss: 0.05046650965252567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08224272727966309 0.23067975044250488

Final encoder loss: 0.048555672504247104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08138680458068848 0.23092889785766602

Final encoder loss: 0.04974324509492889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08243918418884277 0.23081612586975098

Final encoder loss: 0.04818294839833921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08088874816894531 0.23062515258789062

Final encoder loss: 0.051617648607498734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08173131942749023 0.2309398651123047

Final encoder loss: 0.049167176646760936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08070778846740723 0.23087096214294434

Final encoder loss: 0.04897410188325734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08103609085083008 0.23054051399230957

Final encoder loss: 0.046814309493925475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08048462867736816 0.2298729419708252


Training emognition model
Final encoder loss: 0.1935916244983673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24872422218322754 0.049532175064086914

Final encoder loss: 0.19496247172355652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24901509284973145 0.04957079887390137

Final encoder loss: 0.0778961032629013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24962472915649414 0.048192739486694336

Final encoder loss: 0.07740820944309235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24738478660583496 0.04914093017578125

Final encoder loss: 0.05564842000603676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24695611000061035 0.049390554428100586

Final encoder loss: 0.054312579333782196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24785661697387695 0.049129486083984375

Final encoder loss: 0.047832172363996506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24681448936462402 0.0502932071685791

Final encoder loss: 0.047043848782777786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24766016006469727 0.04926943778991699

Final encoder loss: 0.04491399973630905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2500314712524414 0.049288034439086914

Final encoder loss: 0.044323407113552094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24759769439697266 0.05116677284240723

Final encoder loss: 0.044336967170238495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2473917007446289 0.049936532974243164

Final encoder loss: 0.04413275048136711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24681520462036133 0.050386667251586914

Final encoder loss: 0.04547969251871109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24787044525146484 0.05060553550720215

Final encoder loss: 0.04494073987007141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24637866020202637 0.04891538619995117

Final encoder loss: 0.046737007796764374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24749970436096191 0.05017447471618652

Final encoder loss: 0.046675801277160645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24834513664245605 0.05108165740966797

Final encoder loss: 0.04808208718895912
Final encoder loss: 0.045523446053266525

Training empatch model
Final encoder loss: 0.08429261062172637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07137918472290039 0.17363953590393066

Final encoder loss: 0.07642695925827639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0711832046508789 0.17412543296813965

Final encoder loss: 0.07452596110855231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07105350494384766 0.1735541820526123

Final encoder loss: 0.06762338469674599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07134628295898438 0.17548060417175293

Final encoder loss: 0.062076152773279665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07267069816589355 0.17366552352905273

Final encoder loss: 0.06535600536079092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07148241996765137 0.17359137535095215

Final encoder loss: 0.06261176015567156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07172608375549316 0.1742861270904541

Final encoder loss: 0.060452785168364144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07079696655273438 0.1734321117401123

Final encoder loss: 0.05078418470422327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07122564315795898 0.17413759231567383

Final encoder loss: 0.04948857706449666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07100677490234375 0.17399382591247559

Final encoder loss: 0.052573787106023936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0714566707611084 0.17449045181274414

Final encoder loss: 0.050681884672052153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07246160507202148 0.17400002479553223

Final encoder loss: 0.04835813437396009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07139897346496582 0.17395734786987305

Final encoder loss: 0.04749542749950445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07199740409851074 0.17448163032531738

Final encoder loss: 0.046532292153397416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07183122634887695 0.17438960075378418

Final encoder loss: 0.047528680411385936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0710904598236084 0.1742875576019287


Training empatch model
Final encoder loss: 0.1711498647928238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17748594284057617 0.04281449317932129

Final encoder loss: 0.07820536941289902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1777341365814209 0.04425859451293945

Final encoder loss: 0.05878693610429764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17437219619750977 0.04350876808166504

Final encoder loss: 0.050410639494657516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17720842361450195 0.04301762580871582

Final encoder loss: 0.04612439125776291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17499613761901855 0.043618202209472656

Final encoder loss: 0.04388444870710373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17458629608154297 0.044167518615722656

Final encoder loss: 0.04275437444448471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17459559440612793 0.043682098388671875

Final encoder loss: 0.04217623546719551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1759812831878662 0.044313907623291016

Final encoder loss: 0.04192289710044861

Training wesad model
Final encoder loss: 0.09029209499559655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07125377655029297 0.17400717735290527

Final encoder loss: 0.08199374806037582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07110881805419922 0.17380905151367188

Final encoder loss: 0.07888779236214481
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07165765762329102 0.17414236068725586

Final encoder loss: 0.07453598784279895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07097339630126953 0.17366647720336914

Final encoder loss: 0.05815590923983013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07217907905578613 0.1740422248840332

Final encoder loss: 0.05673914302779747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07125234603881836 0.1740579605102539

Final encoder loss: 0.057193165116617116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0720205307006836 0.1741938591003418

Final encoder loss: 0.05501484734086985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07091951370239258 0.17389750480651855

Final encoder loss: 0.045545795487071564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07107257843017578 0.1726377010345459

Final encoder loss: 0.0469239796152985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07055068016052246 0.17289161682128906

Final encoder loss: 0.04479617428916703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07046627998352051 0.17290186882019043

Final encoder loss: 0.04722465008203537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07030153274536133 0.17264485359191895

Final encoder loss: 0.04010053274801104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07029509544372559 0.17270779609680176

Final encoder loss: 0.03922979044374601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07055997848510742 0.17271900177001953

Final encoder loss: 0.03802968735368958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07012677192687988 0.17255401611328125

Final encoder loss: 0.03960597266168091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07067704200744629 0.1724228858947754


Training wesad model
Final encoder loss: 0.21561776101589203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1053464412689209 0.03260183334350586

Final encoder loss: 0.09110566228628159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10430335998535156 0.03310394287109375

Final encoder loss: 0.062477994710206985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10370779037475586 0.03256821632385254

Final encoder loss: 0.05051124840974808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1030733585357666 0.032770395278930664

Final encoder loss: 0.04471166059374809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10407829284667969 0.032491207122802734

Final encoder loss: 0.041887231171131134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357403755187988 0.03253507614135742

Final encoder loss: 0.04045604541897774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10343742370605469 0.03317594528198242

Final encoder loss: 0.03976217657327652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10317087173461914 0.03309488296508789

Final encoder loss: 0.03963131085038185

Calculating loss for amigos model
	Full Pass 0.6715197563171387
numFreeParamsPath 18
Reconstruction loss values: 0.057428374886512756 0.06642509996891022

Calculating loss for dapper model
	Full Pass 0.15172386169433594
numFreeParamsPath 18
Reconstruction loss values: 0.046499501913785934 0.052798375487327576

Calculating loss for case model
	Full Pass 0.8578734397888184
numFreeParamsPath 18
Reconstruction loss values: 0.06539228558540344 0.06840328127145767

Calculating loss for emognition model
	Full Pass 0.27985596656799316
numFreeParamsPath 18
Reconstruction loss values: 0.07019533216953278 0.0762312188744545

Calculating loss for empatch model
	Full Pass 0.10426473617553711
numFreeParamsPath 18
Reconstruction loss values: 0.07632099092006683 0.08038212358951569

Calculating loss for wesad model
	Full Pass 0.07701396942138672
numFreeParamsPath 18
Reconstruction loss values: 0.08646952360868454 0.10964161157608032
Total loss calculation time: 3.7606992721557617

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.486748695373535
Total epoch time: 117.17920422554016

Epoch: 17

Training emognition model
Final encoder loss: 0.06917172212008921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08983421325683594 0.28326892852783203

Final encoder loss: 0.06474312791605158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0834038257598877 0.27523016929626465

Final encoder loss: 0.06273862839361946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08337926864624023 0.27518630027770996

Final encoder loss: 0.06242769974087156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.0828707218170166 0.2737162113189697

Final encoder loss: 0.05961127151200276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08278012275695801 0.273820161819458

Final encoder loss: 0.05867126826142879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0826413631439209 0.27317357063293457

Final encoder loss: 0.05918325761956513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08262109756469727 0.27327942848205566

Final encoder loss: 0.06205109844333558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08237218856811523 0.2733032703399658

Final encoder loss: 0.05845665289580445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08242177963256836 0.2730734348297119

Final encoder loss: 0.05948929508294486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08260107040405273 0.2730684280395508

Final encoder loss: 0.05772393701296581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08211326599121094 0.2731139659881592

Final encoder loss: 0.05762425253234422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08256959915161133 0.27353334426879883

Final encoder loss: 0.05807139196949738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08238863945007324 0.27263331413269043

Final encoder loss: 0.05387751628400396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08284521102905273 0.27541470527648926

Final encoder loss: 0.054660583019748896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0836179256439209 0.27636003494262695

Final encoder loss: 0.05399476779109919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08344888687133789 0.2739737033843994


Training amigos model
Final encoder loss: 0.05275934575729072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10869622230529785 0.3888545036315918

Final encoder loss: 0.05692184722421914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10801482200622559 0.3892061710357666

Final encoder loss: 0.05393070529711421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10817861557006836 0.3889937400817871

Final encoder loss: 0.051065702304112434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10897970199584961 0.3899238109588623

Final encoder loss: 0.05022965493004323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10857176780700684 0.38852858543395996

Final encoder loss: 0.05223973693814763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10756731033325195 0.3883659839630127

Final encoder loss: 0.04351543300626857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10768699645996094 0.38776063919067383

Final encoder loss: 0.0520979883975382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10773468017578125 0.3882763385772705

Final encoder loss: 0.052182252637360545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10742998123168945 0.38755154609680176

Final encoder loss: 0.05213819946395557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10748600959777832 0.38779473304748535

Final encoder loss: 0.04760055863071866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1075277328491211 0.3882710933685303

Final encoder loss: 0.0445879989073831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1076822280883789 0.38894033432006836

Final encoder loss: 0.04853436076175109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10831141471862793 0.38877320289611816

Final encoder loss: 0.046094077428550304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1087954044342041 0.38901567459106445

Final encoder loss: 0.047130822339983496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10815119743347168 0.3898911476135254

Final encoder loss: 0.04876113128586688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10348987579345703 0.3835160732269287


Training case model
Final encoder loss: 0.06615693373083338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09049010276794434 0.2641477584838867

Final encoder loss: 0.05871663600172141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09028244018554688 0.26447176933288574

Final encoder loss: 0.05638630245937671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09138703346252441 0.2678828239440918

Final encoder loss: 0.05555985383085604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.0926969051361084 0.26622676849365234

Final encoder loss: 0.055092735324476255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0911104679107666 0.26508498191833496

Final encoder loss: 0.05055101326184895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09057354927062988 0.2654287815093994

Final encoder loss: 0.05148175841746869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09119725227355957 0.2645702362060547

Final encoder loss: 0.04958549391894841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09106087684631348 0.26440906524658203

Final encoder loss: 0.049161128924632154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09103870391845703 0.2674524784088135

Final encoder loss: 0.04795413233269254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09218978881835938 0.2664456367492676

Final encoder loss: 0.04901027023902137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09097719192504883 0.26452159881591797

Final encoder loss: 0.046924186343105835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09291577339172363 0.26598596572875977

Final encoder loss: 0.0461371201202651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09176015853881836 0.2644360065460205

Final encoder loss: 0.04722985676749723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09130525588989258 0.26550889015197754

Final encoder loss: 0.04638906719939712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09111928939819336 0.2637674808502197

Final encoder loss: 0.045852632869714874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08944535255432129 0.26095080375671387


Training dapper model
Final encoder loss: 0.04933900828325257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06115913391113281 0.14917659759521484

Final encoder loss: 0.049152404256193924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06146812438964844 0.14867210388183594

Final encoder loss: 0.04317686478813457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06101179122924805 0.14947009086608887

Final encoder loss: 0.03794812319493376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06123208999633789 0.1495223045349121

Final encoder loss: 0.041151331292518016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.061235666275024414 0.14830970764160156

Final encoder loss: 0.04057116388994545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06125044822692871 0.14943599700927734

Final encoder loss: 0.04076097207297495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06115579605102539 0.14869260787963867

Final encoder loss: 0.03960528585671496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06101107597351074 0.14988088607788086

Final encoder loss: 0.04283477758684984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06175971031188965 0.14832854270935059

Final encoder loss: 0.03759495539294052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.061037302017211914 0.14934039115905762

Final encoder loss: 0.035354188429467026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.0611417293548584 0.14859509468078613

Final encoder loss: 0.03632882143896673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06118917465209961 0.14850306510925293

Final encoder loss: 0.03753233681327628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.060860633850097656 0.14887499809265137

Final encoder loss: 0.0398845870624724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06105303764343262 0.14917421340942383

Final encoder loss: 0.03609151871201136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06092262268066406 0.14864134788513184

Final encoder loss: 0.03517221215086976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.0607302188873291 0.147613525390625


Training amigos model
Final encoder loss: 0.03457123213819615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10552835464477539 0.3411736488342285

Final encoder loss: 0.03756190245138871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10589098930358887 0.3410475254058838

Final encoder loss: 0.035846673566941226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10560107231140137 0.3413200378417969

Final encoder loss: 0.037336508781603654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1060476303100586 0.3419945240020752

Final encoder loss: 0.03835181473351617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1063542366027832 0.3411900997161865

Final encoder loss: 0.03659672862034373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10630393028259277 0.3414144515991211

Final encoder loss: 0.04032232186364022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10688400268554688 0.3413999080657959

Final encoder loss: 0.04115259028111045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10578250885009766 0.3411548137664795

Final encoder loss: 0.03700180734140144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10590958595275879 0.34128499031066895

Final encoder loss: 0.035873341320965965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10571455955505371 0.34114789962768555

Final encoder loss: 0.0355730265086418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10555815696716309 0.34118103981018066

Final encoder loss: 0.03832515392495132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10584068298339844 0.34122490882873535

Final encoder loss: 0.03699185207288461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10566401481628418 0.34111809730529785

Final encoder loss: 0.036790190147470855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10575199127197266 0.341188907623291

Final encoder loss: 0.03626540599829515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10545516014099121 0.34110546112060547

Final encoder loss: 0.0394612054445197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10046577453613281 0.3369929790496826


Training amigos model
Final encoder loss: 0.18077117204666138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4306502342224121 0.07602429389953613

Final encoder loss: 0.18782541155815125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4313020706176758 0.07375025749206543

Final encoder loss: 0.18364711105823517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.41759729385375977 0.07389235496520996

Final encoder loss: 0.06858135014772415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4438967704772949 0.07249712944030762

Final encoder loss: 0.06996134668588638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45517563819885254 0.07525753974914551

Final encoder loss: 0.06458751112222672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44114184379577637 0.07446622848510742

Final encoder loss: 0.04456302151083946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45562195777893066 0.07606887817382812

Final encoder loss: 0.0451737716794014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4558234214782715 0.07611536979675293

Final encoder loss: 0.04345553740859032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43938255310058594 0.07390785217285156

Final encoder loss: 0.037017058581113815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45520949363708496 0.0752859115600586

Final encoder loss: 0.03743038326501846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45784425735473633 0.07579326629638672

Final encoder loss: 0.036983001977205276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4266476631164551 0.07264232635498047

Final encoder loss: 0.03488878905773163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4430053234100342 0.07427716255187988

Final encoder loss: 0.03529424965381622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4105558395385742 0.07539868354797363

Final encoder loss: 0.035128165036439896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.439563512802124 0.07276320457458496

Final encoder loss: 0.03612803667783737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4546639919281006 0.07628989219665527

Final encoder loss: 0.0362556092441082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44254207611083984 0.07644128799438477

Final encoder loss: 0.036084335297346115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.41538548469543457 0.07248520851135254

Final encoder loss: 0.03792636841535568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4424917697906494 0.07257604598999023

Final encoder loss: 0.03766928240656853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4310445785522461 0.07310652732849121

Final encoder loss: 0.03774149715900421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.440784215927124 0.07239961624145508

Final encoder loss: 0.03833048418164253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4550132751464844 0.07528972625732422

Final encoder loss: 0.03817259520292282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44278860092163086 0.07416772842407227

Final encoder loss: 0.038447387516498566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43087196350097656 0.07396745681762695

Final encoder loss: 0.03708387538790703
Final encoder loss: 0.03509049862623215
Final encoder loss: 0.03356313705444336

Training dapper model
Final encoder loss: 0.029816461546802658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05917930603027344 0.10645174980163574

Final encoder loss: 0.029076418254174306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.0590054988861084 0.10668802261352539

Final encoder loss: 0.034548596426059085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059392452239990234 0.1065676212310791

Final encoder loss: 0.030222233519125286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0590662956237793 0.10648345947265625

Final encoder loss: 0.02685737864934504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059127092361450195 0.10651946067810059

Final encoder loss: 0.02956133918311763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05920290946960449 0.10651850700378418

Final encoder loss: 0.028832637697770648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05899167060852051 0.10683631896972656

Final encoder loss: 0.029951962558604273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059250831604003906 0.10664582252502441

Final encoder loss: 0.030750731685274698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05953168869018555 0.10647368431091309

Final encoder loss: 0.028375798896761104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05903148651123047 0.10649824142456055

Final encoder loss: 0.031027287087030644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05931210517883301 0.10652589797973633

Final encoder loss: 0.03155568669839456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05924081802368164 0.10678720474243164

Final encoder loss: 0.028471793137151532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05926251411437988 0.1064760684967041

Final encoder loss: 0.02886515139349661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.0593564510345459 0.1064445972442627

Final encoder loss: 0.028723784679941227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05901002883911133 0.10680580139160156

Final encoder loss: 0.03550560877223594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05848360061645508 0.10600829124450684


Training dapper model
Final encoder loss: 0.20245079696178436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11515021324157715 0.03346896171569824

Final encoder loss: 0.20820258557796478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11519718170166016 0.033652544021606445

Final encoder loss: 0.06854763627052307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1134638786315918 0.0336000919342041

Final encoder loss: 0.06932970881462097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11513400077819824 0.03406190872192383

Final encoder loss: 0.042669571936130524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11321663856506348 0.03390789031982422

Final encoder loss: 0.042783595621585846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11490726470947266 0.03393292427062988

Final encoder loss: 0.03254852816462517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11268043518066406 0.03391885757446289

Final encoder loss: 0.03275907039642334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11505675315856934 0.034920454025268555

Final encoder loss: 0.028247175738215446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11359262466430664 0.034339189529418945

Final encoder loss: 0.028520429506897926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11682987213134766 0.03449702262878418

Final encoder loss: 0.02670842967927456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11381411552429199 0.035025835037231445

Final encoder loss: 0.027114778757095337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11860013008117676 0.034078359603881836

Final encoder loss: 0.026672760024666786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11301326751708984 0.03404092788696289

Final encoder loss: 0.027013598009943962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11555647850036621 0.03366351127624512

Final encoder loss: 0.027758272364735603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11345028877258301 0.0343015193939209

Final encoder loss: 0.027467701584100723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11664462089538574 0.034883975982666016

Final encoder loss: 0.029555711895227432
Final encoder loss: 0.02730659395456314

Training case model
Final encoder loss: 0.0439838904189515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08971810340881348 0.2190103530883789

Final encoder loss: 0.04282614056783713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08946728706359863 0.2195284366607666

Final encoder loss: 0.043449595571086776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09187579154968262 0.21936964988708496

Final encoder loss: 0.04170977229007399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08978939056396484 0.21902155876159668

Final encoder loss: 0.04191155935921927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08916139602661133 0.2198927402496338

Final encoder loss: 0.040982870436561625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08982419967651367 0.21924090385437012

Final encoder loss: 0.041838742813485603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09003067016601562 0.21904277801513672

Final encoder loss: 0.04172688035084696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.09026169776916504 0.2194514274597168

Final encoder loss: 0.04219500091003209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08936119079589844 0.2194652557373047

Final encoder loss: 0.04158560121784338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0891714096069336 0.21944928169250488

Final encoder loss: 0.0440297840404207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09093236923217773 0.2190241813659668

Final encoder loss: 0.04171320598581379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0897972583770752 0.21909880638122559

Final encoder loss: 0.04135029191391223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08905887603759766 0.21943902969360352

Final encoder loss: 0.041082611697359446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09126448631286621 0.2194666862487793

Final encoder loss: 0.04206705400221008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08938074111938477 0.21906256675720215

Final encoder loss: 0.04228950251086516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08606433868408203 0.21630358695983887


Training case model
Final encoder loss: 0.20296703279018402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26269102096557617 0.05263948440551758

Final encoder loss: 0.18889440596103668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26120615005493164 0.05342698097229004

Final encoder loss: 0.19015222787857056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2592601776123047 0.05214214324951172

Final encoder loss: 0.19218964874744415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2899022102355957 0.054602861404418945

Final encoder loss: 0.18082775175571442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26928186416625977 0.05240941047668457

Final encoder loss: 0.19191330671310425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2576470375061035 0.05289793014526367

Final encoder loss: 0.09366704523563385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.284686803817749 0.052298784255981445

Final encoder loss: 0.08414793014526367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2696964740753174 0.054186105728149414

Final encoder loss: 0.08119238168001175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25942540168762207 0.052664995193481445

Final encoder loss: 0.0807230994105339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25962162017822266 0.05483841896057129

Final encoder loss: 0.0747518539428711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28382086753845215 0.05249667167663574

Final encoder loss: 0.07706073671579361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25870442390441895 0.053850412368774414

Final encoder loss: 0.05790162831544876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26831984519958496 0.0524449348449707

Final encoder loss: 0.05346959829330444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2887003421783447 0.05293560028076172

Final encoder loss: 0.052149105817079544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2703118324279785 0.05070829391479492

Final encoder loss: 0.05315111204981804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2896866798400879 0.05248737335205078

Final encoder loss: 0.05128287151455879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.259993314743042 0.052108049392700195

Final encoder loss: 0.05185587331652641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565300464630127 0.0534822940826416

Final encoder loss: 0.04707172140479088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578542232513428 0.05145978927612305

Final encoder loss: 0.04531514644622803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2892427444458008 0.05294084548950195

Final encoder loss: 0.044040143489837646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582085132598877 0.05076718330383301

Final encoder loss: 0.045793317258358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2707209587097168 0.05468893051147461

Final encoder loss: 0.045650579035282135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25728464126586914 0.05270957946777344

Final encoder loss: 0.04503660276532173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25678324699401855 0.05219435691833496

Final encoder loss: 0.04550844430923462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2682948112487793 0.05344510078430176

Final encoder loss: 0.04491829872131348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690720558166504 0.05441689491271973

Final encoder loss: 0.04409688338637352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2838747501373291 0.05309247970581055

Final encoder loss: 0.04552237689495087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2703371047973633 0.05204057693481445

Final encoder loss: 0.04669630900025368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25940608978271484 0.053930044174194336

Final encoder loss: 0.045404158532619476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25723934173583984 0.053386688232421875

Final encoder loss: 0.04578198119997978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2579333782196045 0.05425715446472168

Final encoder loss: 0.04524054005742073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.283876895904541 0.05334019660949707

Final encoder loss: 0.04527563974261284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26908397674560547 0.052710533142089844

Final encoder loss: 0.04558447748422623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2690165042877197 0.053743600845336914

Final encoder loss: 0.046880774199962616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26964688301086426 0.05226397514343262

Final encoder loss: 0.04597659036517143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25784993171691895 0.052703857421875

Final encoder loss: 0.044119153171777725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28899312019348145 0.05416440963745117

Final encoder loss: 0.04350777342915535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2567150592803955 0.05299973487854004

Final encoder loss: 0.04289165139198303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2673225402832031 0.05446982383728027

Final encoder loss: 0.04384320229291916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25632643699645996 0.05238032341003418

Final encoder loss: 0.044396087527275085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26685404777526855 0.05187869071960449

Final encoder loss: 0.04394960030913353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.253925085067749 0.05160713195800781

Final encoder loss: 0.04348636418581009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25466108322143555 0.051740407943725586

Final encoder loss: 0.043141186237335205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573699951171875 0.05164599418640137

Final encoder loss: 0.04261699318885803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25642919540405273 0.05057334899902344

Final encoder loss: 0.043531689792871475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2681467533111572 0.05229496955871582

Final encoder loss: 0.043990448117256165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25894880294799805 0.05179142951965332

Final encoder loss: 0.043008171021938324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551722526550293 0.052170753479003906

Final encoder loss: 0.04320921003818512
Final encoder loss: 0.04178923740983009
Final encoder loss: 0.040237002074718475
Final encoder loss: 0.039740923792123795
Final encoder loss: 0.03890557959675789
Final encoder loss: 0.03654409199953079

Training emognition model
Final encoder loss: 0.04743133935514581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08225226402282715 0.2306194305419922

Final encoder loss: 0.049315840093869655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08138084411621094 0.2305748462677002

Final encoder loss: 0.04758479068236922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08153676986694336 0.23068761825561523

Final encoder loss: 0.047975194019498964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08171677589416504 0.23064613342285156

Final encoder loss: 0.04753971754588676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0805959701538086 0.23036432266235352

Final encoder loss: 0.04656159526190541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0814964771270752 0.23068022727966309

Final encoder loss: 0.047272775492443304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08090400695800781 0.23050689697265625

Final encoder loss: 0.048042436742347976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0810079574584961 0.22991514205932617

Final encoder loss: 0.048626943148015495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08097457885742188 0.2300710678100586

Final encoder loss: 0.049613161355691694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08084869384765625 0.22966408729553223

Final encoder loss: 0.05104028417775192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08071732521057129 0.2299199104309082

Final encoder loss: 0.045643512351973815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0806884765625 0.22973418235778809

Final encoder loss: 0.046237874957194745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08061766624450684 0.23008966445922852

Final encoder loss: 0.046449619764564035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08046984672546387 0.2303009033203125

Final encoder loss: 0.045351927801914105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08080601692199707 0.23003625869750977

Final encoder loss: 0.053038882041497315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07986235618591309 0.2290198802947998


Training emognition model
Final encoder loss: 0.19355618953704834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25176095962524414 0.04903292655944824

Final encoder loss: 0.19495928287506104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24678492546081543 0.04929828643798828

Final encoder loss: 0.07892046868801117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2469182014465332 0.04987621307373047

Final encoder loss: 0.07903529703617096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247847318649292 0.04901719093322754

Final encoder loss: 0.05616996809840202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24848413467407227 0.04882192611694336

Final encoder loss: 0.05486669763922691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24730253219604492 0.04958677291870117

Final encoder loss: 0.04760165140032768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24787187576293945 0.04892921447753906

Final encoder loss: 0.046729639172554016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24720430374145508 0.048896074295043945

Final encoder loss: 0.044077318161726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24828338623046875 0.04868960380554199

Final encoder loss: 0.04341505467891693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24800324440002441 0.05044889450073242

Final encoder loss: 0.04308278113603592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24825620651245117 0.048480987548828125

Final encoder loss: 0.042570486664772034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2486739158630371 0.04881596565246582

Final encoder loss: 0.04347679764032364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24841666221618652 0.04787492752075195

Final encoder loss: 0.04318654164671898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463831901550293 0.05067729949951172

Final encoder loss: 0.044680263847112656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24906516075134277 0.04841947555541992

Final encoder loss: 0.04505710303783417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24833178520202637 0.05010795593261719

Final encoder loss: 0.04559754207730293
Final encoder loss: 0.044580306857824326

Training empatch model
Final encoder loss: 0.07682322760794139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07084298133850098 0.17369556427001953

Final encoder loss: 0.07467357572429746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0721273422241211 0.1740889549255371

Final encoder loss: 0.07327891580330853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07140326499938965 0.17402148246765137

Final encoder loss: 0.0648079242984527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07126140594482422 0.17369842529296875

Final encoder loss: 0.062324833917417576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07118916511535645 0.17561864852905273

Final encoder loss: 0.06273420117438289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07102179527282715 0.1741492748260498

Final encoder loss: 0.06842716799387119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07221555709838867 0.17386341094970703

Final encoder loss: 0.0640368745898088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0706629753112793 0.17322611808776855

Final encoder loss: 0.045219687415306305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0718374252319336 0.17396259307861328

Final encoder loss: 0.04726499047681784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07133841514587402 0.17368674278259277

Final encoder loss: 0.05029463239310198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07111573219299316 0.1741933822631836

Final encoder loss: 0.04945922648020286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07132434844970703 0.1738734245300293

Final encoder loss: 0.04662582153721539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07124161720275879 0.17412281036376953

Final encoder loss: 0.046140150424079235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07228589057922363 0.17388343811035156

Final encoder loss: 0.042703040154463315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07117128372192383 0.17391324043273926

Final encoder loss: 0.05180212752412766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07142329216003418 0.1740109920501709


Training empatch model
Final encoder loss: 0.1711568832397461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17623162269592285 0.043683528900146484

Final encoder loss: 0.07897370308637619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1766643524169922 0.043372154235839844

Final encoder loss: 0.05905945226550102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1766202449798584 0.04423046112060547

Final encoder loss: 0.05033605545759201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17612099647521973 0.043164730072021484

Final encoder loss: 0.04569179564714432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17524123191833496 0.04325246810913086

Final encoder loss: 0.043222129344940186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1759805679321289 0.04363894462585449

Final encoder loss: 0.04182849079370499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471003532409668 0.04350423812866211

Final encoder loss: 0.04115032032132149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17551589012145996 0.044307708740234375

Final encoder loss: 0.0408475399017334

Training wesad model
Final encoder loss: 0.0843440742220506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0708012580871582 0.17398738861083984

Final encoder loss: 0.08014320198115496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07187128067016602 0.17381834983825684

Final encoder loss: 0.07603986096773066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07098793983459473 0.1737232208251953

Final encoder loss: 0.07126552249906891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07150793075561523 0.17390036582946777

Final encoder loss: 0.05521688775245854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07111120223999023 0.17332959175109863

Final encoder loss: 0.053158276629626446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07101845741271973 0.17436695098876953

Final encoder loss: 0.05448778019565715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07109522819519043 0.17384767532348633

Final encoder loss: 0.054196088443787906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0705406665802002 0.17352080345153809

Final encoder loss: 0.04292799376984865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07218217849731445 0.17450237274169922

Final encoder loss: 0.0430186818570906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07119870185852051 0.17364740371704102

Final encoder loss: 0.044391693146822474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07138395309448242 0.17409658432006836

Final encoder loss: 0.04586275724178406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0706627368927002 0.17426037788391113

Final encoder loss: 0.0356999878131447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07093572616577148 0.1740407943725586

Final encoder loss: 0.03721638564533266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0715796947479248 0.17354416847229004

Final encoder loss: 0.038197036359694896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07078075408935547 0.1739499568939209

Final encoder loss: 0.038351448802957565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0718374252319336 0.1742234230041504


Training wesad model
Final encoder loss: 0.21560171246528625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1057579517364502 0.03271770477294922

Final encoder loss: 0.09189170598983765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10490655899047852 0.033379316329956055

Final encoder loss: 0.06302761286497116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10515475273132324 0.03404831886291504

Final encoder loss: 0.050282251089811325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395336151123047 0.03295254707336426

Final encoder loss: 0.043917734175920486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409832000732422 0.033252716064453125

Final encoder loss: 0.04071802645921707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10429739952087402 0.03294873237609863

Final encoder loss: 0.039093852043151855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10531306266784668 0.032627105712890625

Final encoder loss: 0.038286078721284866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384511947631836 0.03324007987976074

Final encoder loss: 0.03794323280453682

Calculating loss for amigos model
	Full Pass 0.6828908920288086
numFreeParamsPath 18
Reconstruction loss values: 0.057470645755529404 0.06537832319736481

Calculating loss for dapper model
	Full Pass 0.15224146842956543
numFreeParamsPath 18
Reconstruction loss values: 0.044654589146375656 0.049462199211120605

Calculating loss for case model
	Full Pass 0.9137489795684814
numFreeParamsPath 18
Reconstruction loss values: 0.06326081603765488 0.06700681149959564

Calculating loss for emognition model
	Full Pass 0.2929227352142334
numFreeParamsPath 18
Reconstruction loss values: 0.07137860357761383 0.07744558155536652

Calculating loss for empatch model
	Full Pass 0.10547661781311035
numFreeParamsPath 18
Reconstruction loss values: 0.07468312233686447 0.0801149383187294

Calculating loss for wesad model
	Full Pass 0.07758164405822754
numFreeParamsPath 18
Reconstruction loss values: 0.08332665264606476 0.10639198869466782
Total loss calculation time: 3.9338130950927734

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.01520562171936
Total epoch time: 117.51434016227722

Epoch: 18

Training case model
Final encoder loss: 0.06170326595386909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.10007715225219727 0.27507543563842773

Final encoder loss: 0.05580940582870299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09160852432250977 0.26564669609069824

Final encoder loss: 0.05419323145727241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09154725074768066 0.2655069828033447

Final encoder loss: 0.05302387166121484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09202051162719727 0.26615476608276367

Final encoder loss: 0.050015191454629036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09175944328308105 0.2659034729003906

Final encoder loss: 0.049758274026183276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0921931266784668 0.265211820602417

Final encoder loss: 0.04988499174550019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09222149848937988 0.2656867504119873

Final encoder loss: 0.04906710778049299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09275150299072266 0.26608777046203613

Final encoder loss: 0.04842528698522808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0921025276184082 0.2661898136138916

Final encoder loss: 0.04830125160116875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09180903434753418 0.26577234268188477

Final encoder loss: 0.047079052191335555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09160923957824707 0.26531219482421875

Final encoder loss: 0.04720407843258262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09095311164855957 0.26367807388305664

Final encoder loss: 0.04712553858924217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09083008766174316 0.2643587589263916

Final encoder loss: 0.045667134534966194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09127545356750488 0.26444244384765625

Final encoder loss: 0.045395691664469454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09088778495788574 0.26404809951782227

Final encoder loss: 0.04580540518010381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.0875554084777832 0.2605915069580078


Training emognition model
Final encoder loss: 0.07049988232736673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0827488899230957 0.27311205863952637

Final encoder loss: 0.06590714966601104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08285188674926758 0.27411770820617676

Final encoder loss: 0.05940480015667015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0831155776977539 0.27381467819213867

Final encoder loss: 0.061920415066219064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08282756805419922 0.2739071846008301

Final encoder loss: 0.060205727362694986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08227801322937012 0.27373600006103516

Final encoder loss: 0.060750096129522625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0821998119354248 0.27471232414245605

Final encoder loss: 0.05957997735660196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08375239372253418 0.2745552062988281

Final encoder loss: 0.05856100670273987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08393073081970215 0.2747972011566162

Final encoder loss: 0.05750719876705038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08451151847839355 0.2767939567565918

Final encoder loss: 0.055927375870279745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0836935043334961 0.2753293514251709

Final encoder loss: 0.05564117881330182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08336567878723145 0.27466702461242676

Final encoder loss: 0.056591653689797655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08471179008483887 0.28185367584228516

Final encoder loss: 0.05507297594628363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08337569236755371 0.27448344230651855

Final encoder loss: 0.053778434476695405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.0835263729095459 0.2755420207977295

Final encoder loss: 0.056675426607100086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08393192291259766 0.2760353088378906

Final encoder loss: 0.05129469102533799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08333373069763184 0.27481770515441895


Training dapper model
Final encoder loss: 0.04662818201294389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061675071716308594 0.14963221549987793

Final encoder loss: 0.04322203998184395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06171894073486328 0.15044808387756348

Final encoder loss: 0.041019971683533156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06352376937866211 0.1515979766845703

Final encoder loss: 0.040523266896756664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06281781196594238 0.1508464813232422

Final encoder loss: 0.03940698217027703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.061994075775146484 0.15181660652160645

Final encoder loss: 0.037792109321622705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06179308891296387 0.1497786045074463

Final encoder loss: 0.03797637278784765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06218910217285156 0.15192818641662598

Final encoder loss: 0.03946195073718827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06319618225097656 0.15129852294921875

Final encoder loss: 0.03662441216714583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.062239885330200195 0.15072250366210938

Final encoder loss: 0.03787880737018582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.062338829040527344 0.15158319473266602

Final encoder loss: 0.034494701720500086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06244087219238281 0.15053606033325195

Final encoder loss: 0.040328350344050176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06215238571166992 0.15085577964782715

Final encoder loss: 0.04184428458643214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06331348419189453 0.15105032920837402

Final encoder loss: 0.03734091181435421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06251692771911621 0.15041494369506836

Final encoder loss: 0.039730723665987254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.062174320220947266 0.15072345733642578

Final encoder loss: 0.038210033849593185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06324028968811035 0.14939236640930176


Training amigos model
Final encoder loss: 0.05797981883096215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10825467109680176 0.389263391494751

Final encoder loss: 0.05384895337843968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10825967788696289 0.3895900249481201

Final encoder loss: 0.047568429722279995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10833382606506348 0.3893716335296631

Final encoder loss: 0.04842170243705483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10852766036987305 0.3900434970855713

Final encoder loss: 0.05234360102861917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10877633094787598 0.3892984390258789

Final encoder loss: 0.051069474411002506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10785889625549316 0.3885526657104492

Final encoder loss: 0.052349188476736114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10819268226623535 0.3899550437927246

Final encoder loss: 0.05683537112277354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1082301139831543 0.3875887393951416

Final encoder loss: 0.04775050041976811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10753917694091797 0.3877108097076416

Final encoder loss: 0.045666725958033526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1076207160949707 0.3880906105041504

Final encoder loss: 0.050126398757710106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10787367820739746 0.3885231018066406

Final encoder loss: 0.04746640983541377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10778379440307617 0.3878934383392334

Final encoder loss: 0.05033210585359404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10775041580200195 0.38999032974243164

Final encoder loss: 0.04988780956289486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10854125022888184 0.3893570899963379

Final encoder loss: 0.04752291886489581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10843324661254883 0.3899095058441162

Final encoder loss: 0.05147419783520332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10945415496826172 0.3839271068572998


Training amigos model
Final encoder loss: 0.03380899218130833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10688161849975586 0.341505765914917

Final encoder loss: 0.04056752760815557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10670089721679688 0.3415243625640869

Final encoder loss: 0.034594086496579385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10602831840515137 0.3415203094482422

Final encoder loss: 0.033424577844396335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10608792304992676 0.34137964248657227

Final encoder loss: 0.03623220154582423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10605835914611816 0.3408844470977783

Final encoder loss: 0.037020002305115124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10570812225341797 0.3408694267272949

Final encoder loss: 0.03558844490709737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10616016387939453 0.3409881591796875

Final encoder loss: 0.03474853313923541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10611152648925781 0.34076690673828125

Final encoder loss: 0.03742067779755261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10544180870056152 0.34099340438842773

Final encoder loss: 0.03497514979570869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10547947883605957 0.3405647277832031

Final encoder loss: 0.03878851939677161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10569262504577637 0.3409125804901123

Final encoder loss: 0.03814682974557558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10565853118896484 0.34135937690734863

Final encoder loss: 0.03426186738124535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.5008597373962402 0.34179067611694336

Final encoder loss: 0.03325453258817458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10613608360290527 0.3413829803466797

Final encoder loss: 0.036426833719175544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10606765747070312 0.3417830467224121

Final encoder loss: 0.03758106364820794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10141825675964355 0.3379955291748047


Training amigos model
Final encoder loss: 0.18077807128429413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4610593318939209 0.0755777359008789

Final encoder loss: 0.18781691789627075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680666923522949 0.08020782470703125

Final encoder loss: 0.18361042439937592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4657115936279297 0.07347226142883301

Final encoder loss: 0.06766723096370697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45873308181762695 0.07724857330322266

Final encoder loss: 0.06974759697914124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4632103443145752 0.07925772666931152

Final encoder loss: 0.06439106166362762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4593203067779541 0.08025789260864258

Final encoder loss: 0.0439637191593647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45820116996765137 0.07536768913269043

Final encoder loss: 0.044901374727487564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45868682861328125 0.07934141159057617

Final encoder loss: 0.04293988645076752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46031880378723145 0.07427024841308594

Final encoder loss: 0.03623393550515175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45694947242736816 0.07637333869934082

Final encoder loss: 0.03704668581485748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4563863277435303 0.07457828521728516

Final encoder loss: 0.036278486251831055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45768213272094727 0.07418537139892578

Final encoder loss: 0.03415818512439728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46642565727233887 0.0732271671295166

Final encoder loss: 0.03472287580370903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4546363353729248 0.0736229419708252

Final encoder loss: 0.03434840962290764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4594266414642334 0.07239985466003418

Final encoder loss: 0.0351557694375515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45534443855285645 0.07597947120666504

Final encoder loss: 0.035410959273576736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4542198181152344 0.07436275482177734

Final encoder loss: 0.03557727113366127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4594106674194336 0.07548737525939941

Final encoder loss: 0.03715811297297478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4548966884613037 0.07440710067749023

Final encoder loss: 0.0369466170668602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45486021041870117 0.07750344276428223

Final encoder loss: 0.03710891678929329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45804810523986816 0.07366800308227539

Final encoder loss: 0.03714179992675781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4545717239379883 0.07782602310180664

Final encoder loss: 0.037625186145305634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4540064334869385 0.07552194595336914

Final encoder loss: 0.037934012711048126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45627927780151367 0.07527542114257812

Final encoder loss: 0.035485442727804184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45495128631591797 0.07555818557739258

Final encoder loss: 0.036540381610393524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45484375953674316 0.0752859115600586

Final encoder loss: 0.03653816506266594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45618677139282227 0.07462501525878906

Final encoder loss: 0.03570300713181496
Final encoder loss: 0.03447575867176056
Final encoder loss: 0.032835084944963455

Training dapper model
Final encoder loss: 0.03137202196541883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059702157974243164 0.1062781810760498

Final encoder loss: 0.028515651132015218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059246063232421875 0.10653018951416016

Final encoder loss: 0.031113469549535654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05948519706726074 0.10651302337646484

Final encoder loss: 0.031086502485133387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05932950973510742 0.10634064674377441

Final encoder loss: 0.03319081234221881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05941581726074219 0.10650777816772461

Final encoder loss: 0.02904439969675678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05943489074707031 0.10652899742126465

Final encoder loss: 0.027864443903776227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05934739112854004 0.10649347305297852

Final encoder loss: 0.02951204980349024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05912518501281738 0.10642242431640625

Final encoder loss: 0.02804194945992281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05949997901916504 0.10627436637878418

Final encoder loss: 0.028308530771795244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05914878845214844 0.10648131370544434

Final encoder loss: 0.031599439499939354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05944252014160156 0.10652399063110352

Final encoder loss: 0.02760686284334274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05921125411987305 0.10619449615478516

Final encoder loss: 0.03228680621909615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.059346914291381836 0.10648918151855469

Final encoder loss: 0.029148843138645134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06244707107543945 0.10691046714782715

Final encoder loss: 0.028548791468033595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059319257736206055 0.10629749298095703

Final encoder loss: 0.030410797077207802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.058847904205322266 0.10558700561523438


Training dapper model
Final encoder loss: 0.2024478316307068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11708807945251465 0.033849477767944336

Final encoder loss: 0.20821507275104523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11481261253356934 0.034296274185180664

Final encoder loss: 0.06880726665258408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1154170036315918 0.034243106842041016

Final encoder loss: 0.06962121278047562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1151735782623291 0.03368258476257324

Final encoder loss: 0.0428316704928875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11623358726501465 0.03435826301574707

Final encoder loss: 0.04273565486073494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11449193954467773 0.03380393981933594

Final encoder loss: 0.032684411853551865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11534690856933594 0.03422045707702637

Final encoder loss: 0.03261255472898483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11493492126464844 0.03406405448913574

Final encoder loss: 0.028174737468361855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11495852470397949 0.03326416015625

Final encoder loss: 0.028157927095890045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11451005935668945 0.03386878967285156

Final encoder loss: 0.026239484548568726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11593961715698242 0.03371095657348633

Final encoder loss: 0.026492564007639885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11586260795593262 0.03411436080932617

Final encoder loss: 0.02603929303586483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11523890495300293 0.033289432525634766

Final encoder loss: 0.026239851489663124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11520075798034668 0.034332990646362305

Final encoder loss: 0.027191510424017906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11573624610900879 0.03407144546508789

Final encoder loss: 0.02689785696566105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11422920227050781 0.0335392951965332

Final encoder loss: 0.029346255585551262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11576437950134277 0.03397178649902344

Final encoder loss: 0.028104674071073532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11466050148010254 0.034021854400634766

Final encoder loss: 0.031022705137729645
Final encoder loss: 0.028523685410618782

Training case model
Final encoder loss: 0.04398321988639893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08933734893798828 0.21825599670410156

Final encoder loss: 0.04318715819984258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08907222747802734 0.21820068359375

Final encoder loss: 0.043331932926402764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08902382850646973 0.21870160102844238

Final encoder loss: 0.040514006961470825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08970832824707031 0.2186450958251953

Final encoder loss: 0.041197040299143166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08947372436523438 0.21880459785461426

Final encoder loss: 0.041406599764793495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08904290199279785 0.21845364570617676

Final encoder loss: 0.04188373760237409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09096193313598633 0.21882224082946777

Final encoder loss: 0.04162778177885055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08911681175231934 0.2182755470275879

Final encoder loss: 0.0410364526141283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08861756324768066 0.2186877727508545

Final encoder loss: 0.04232893156991855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0887603759765625 0.21877765655517578

Final encoder loss: 0.04166358936030141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08935880661010742 0.21857619285583496

Final encoder loss: 0.041336982044899245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08950948715209961 0.21838617324829102

Final encoder loss: 0.04170934912191804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08901476860046387 0.21880364418029785

Final encoder loss: 0.04093427079446299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08916091918945312 0.21856260299682617

Final encoder loss: 0.0411452542407009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08895373344421387 0.21869373321533203

Final encoder loss: 0.04094877638075973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08645939826965332 0.21490263938903809


Training case model
Final encoder loss: 0.2029694765806198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2616438865661621 0.05181765556335449

Final encoder loss: 0.18891067802906036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573099136352539 0.05230855941772461

Final encoder loss: 0.1901467889547348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26589441299438477 0.05244040489196777

Final encoder loss: 0.19219444692134857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25647568702697754 0.05197310447692871

Final encoder loss: 0.1808164417743683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26610302925109863 0.052239418029785156

Final encoder loss: 0.1919311285018921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553400993347168 0.05064678192138672

Final encoder loss: 0.09386783093214035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26648569107055664 0.05238485336303711

Final encoder loss: 0.08441758900880814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26793551445007324 0.05314946174621582

Final encoder loss: 0.08128776401281357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27306532859802246 0.052700042724609375

Final encoder loss: 0.0808151513338089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2738652229309082 0.052231788635253906

Final encoder loss: 0.0747237429022789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26696181297302246 0.05289816856384277

Final encoder loss: 0.07753410190343857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25567173957824707 0.05224967002868652

Final encoder loss: 0.05793709680438042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25615525245666504 0.05211496353149414

Final encoder loss: 0.05331509932875633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.267941951751709 0.053030967712402344

Final encoder loss: 0.051760341972112656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27300262451171875 0.051016807556152344

Final encoder loss: 0.052982427179813385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574188709259033 0.052964210510253906

Final encoder loss: 0.051211435347795486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2584531307220459 0.053069114685058594

Final encoder loss: 0.05182749032974243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571864128112793 0.05145430564880371

Final encoder loss: 0.04689989238977432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2783477306365967 0.052938222885131836

Final encoder loss: 0.04481806233525276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28139662742614746 0.05187034606933594

Final encoder loss: 0.04345186799764633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2706589698791504 0.05273079872131348

Final encoder loss: 0.04523898661136627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28200387954711914 0.053269386291503906

Final encoder loss: 0.04533540457487106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25805068016052246 0.05315113067626953

Final encoder loss: 0.044375162571668625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2570054531097412 0.05127835273742676

Final encoder loss: 0.04528818652033806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.260181188583374 0.051805973052978516

Final encoder loss: 0.04413018375635147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2708749771118164 0.0527803897857666

Final encoder loss: 0.043150048702955246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2713661193847656 0.052384138107299805

Final encoder loss: 0.04469563066959381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27092790603637695 0.051015377044677734

Final encoder loss: 0.04601442068815231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2695751190185547 0.053739070892333984

Final encoder loss: 0.04441959038376808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2591397762298584 0.051084041595458984

Final encoder loss: 0.045108210295438766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2683076858520508 0.053605079650878906

Final encoder loss: 0.044480036944150925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2715158462524414 0.05262184143066406

Final encoder loss: 0.04400818794965744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2595400810241699 0.05341958999633789

Final encoder loss: 0.04480542615056038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2799086570739746 0.05266284942626953

Final encoder loss: 0.04610488936305046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26720428466796875 0.05450320243835449

Final encoder loss: 0.04484548419713974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25841212272644043 0.054514169692993164

Final encoder loss: 0.04349532350897789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26853442192077637 0.05345416069030762

Final encoder loss: 0.042495016008615494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2806055545806885 0.055532217025756836

Final encoder loss: 0.04198942705988884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26940441131591797 0.05409693717956543

Final encoder loss: 0.04292367398738861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2704601287841797 0.055299997329711914

Final encoder loss: 0.04336356371641159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2687950134277344 0.05342698097229004

Final encoder loss: 0.04312381520867348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2577474117279053 0.05416250228881836

Final encoder loss: 0.042800430208444595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25780320167541504 0.0525515079498291

Final encoder loss: 0.042145803570747375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2708761692047119 0.054782867431640625

Final encoder loss: 0.041275300085544586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25801706314086914 0.05379295349121094

Final encoder loss: 0.04263041540980339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26018691062927246 0.054847002029418945

Final encoder loss: 0.043132949620485306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26840853691101074 0.05300140380859375

Final encoder loss: 0.04228988662362099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25846052169799805 0.05295968055725098

Final encoder loss: 0.042304474860429764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26822972297668457 0.05292534828186035

Final encoder loss: 0.04176264628767967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2799692153930664 0.0547480583190918

Final encoder loss: 0.04117231443524361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2785217761993408 0.05328226089477539

Final encoder loss: 0.04179118201136589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2697601318359375 0.05386018753051758

Final encoder loss: 0.04250070080161095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26880955696105957 0.05367469787597656

Final encoder loss: 0.0420990064740181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25917553901672363 0.05515289306640625

Final encoder loss: 0.042177069932222366
Final encoder loss: 0.0406048446893692
Final encoder loss: 0.03919197991490364
Final encoder loss: 0.03874499723315239
Final encoder loss: 0.0379953533411026
Final encoder loss: 0.03564949333667755

Training emognition model
Final encoder loss: 0.04748733918200908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08168172836303711 0.23040461540222168

Final encoder loss: 0.04683994462955138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08215141296386719 0.23117327690124512

Final encoder loss: 0.048944695745139204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08118104934692383 0.2309715747833252

Final encoder loss: 0.0461472622137997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08147144317626953 0.2310783863067627

Final encoder loss: 0.046018641187409615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08249211311340332 0.2309119701385498

Final encoder loss: 0.0496513869748653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0814204216003418 0.2309432029724121

Final encoder loss: 0.045887706526976664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08136725425720215 0.2311086654663086

Final encoder loss: 0.047783953679999734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08276224136352539 0.2313861846923828

Final encoder loss: 0.04724654358079352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.0821070671081543 0.23054862022399902

Final encoder loss: 0.04570067073704459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08158659934997559 0.23135876655578613

Final encoder loss: 0.04699190381559925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08220267295837402 0.23070478439331055

Final encoder loss: 0.0482749792096186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0815730094909668 0.2312300205230713

Final encoder loss: 0.04731593368818007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08204817771911621 0.2307887077331543

Final encoder loss: 0.04627067961522891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08134269714355469 0.23093104362487793

Final encoder loss: 0.04726082585822026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0810081958770752 0.22974586486816406

Final encoder loss: 0.04600439800105577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07957816123962402 0.2283632755279541


Training emognition model
Final encoder loss: 0.1935746818780899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25008630752563477 0.04984855651855469

Final encoder loss: 0.1949612945318222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24702167510986328 0.04882693290710449

Final encoder loss: 0.07878672331571579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24696993827819824 0.048158884048461914

Final encoder loss: 0.0786348208785057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2462174892425537 0.048674583435058594

Final encoder loss: 0.055835578590631485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24830293655395508 0.04852581024169922

Final encoder loss: 0.05443683639168739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24547505378723145 0.04783272743225098

Final encoder loss: 0.047033343464136124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24611234664916992 0.048642873764038086

Final encoder loss: 0.046048957854509354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24661564826965332 0.04914212226867676

Final encoder loss: 0.04318893328309059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2479863166809082 0.0487513542175293

Final encoder loss: 0.04255777224898338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464902400970459 0.049358367919921875

Final encoder loss: 0.04193207249045372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24722647666931152 0.051355838775634766

Final encoder loss: 0.04154379293322563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24773502349853516 0.04958510398864746

Final encoder loss: 0.04228390008211136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477552890777588 0.04850006103515625

Final encoder loss: 0.04204736277461052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24644923210144043 0.04865384101867676

Final encoder loss: 0.043755609542131424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24759936332702637 0.04994678497314453

Final encoder loss: 0.044425372034311295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24809789657592773 0.049388885498046875

Final encoder loss: 0.045093782246112823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24774909019470215 0.049475908279418945

Final encoder loss: 0.04642052948474884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24708938598632812 0.048673152923583984

Final encoder loss: 0.04772978648543358
Final encoder loss: 0.046137142926454544

Training empatch model
Final encoder loss: 0.07809055013277574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07165288925170898 0.17373228073120117

Final encoder loss: 0.06722088158830183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07183194160461426 0.1737232208251953

Final encoder loss: 0.0692084424090266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07096219062805176 0.1734602451324463

Final encoder loss: 0.06418093881366349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0708463191986084 0.17281508445739746

Final encoder loss: 0.06043232282826789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07098269462585449 0.17339372634887695

Final encoder loss: 0.06328271178547491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07078075408935547 0.1733250617980957

Final encoder loss: 0.06135243617281368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0710594654083252 0.17258834838867188

Final encoder loss: 0.06181127753659106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07042479515075684 0.1727142333984375

Final encoder loss: 0.04502184869510664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07107067108154297 0.1730043888092041

Final encoder loss: 0.04308747689451279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07087087631225586 0.1731712818145752

Final encoder loss: 0.044392737296510755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07088661193847656 0.17291784286499023

Final encoder loss: 0.041973130457721185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07092595100402832 0.17330384254455566

Final encoder loss: 0.04675241986885885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0708003044128418 0.17270636558532715

Final encoder loss: 0.0471938250758061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07117128372192383 0.1727128028869629

Final encoder loss: 0.046298910818887366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0707399845123291 0.17297911643981934

Final encoder loss: 0.049358566812794366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07084441184997559 0.17243218421936035


Training empatch model
Final encoder loss: 0.17116203904151917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17507028579711914 0.04436850547790527

Final encoder loss: 0.07835281640291214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.175567626953125 0.044217586517333984

Final encoder loss: 0.05820818617939949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17501616477966309 0.04509854316711426

Final encoder loss: 0.04939737170934677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17526912689208984 0.0446019172668457

Final encoder loss: 0.044637154787778854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17513203620910645 0.044051170349121094

Final encoder loss: 0.042069025337696075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17576169967651367 0.04435539245605469

Final encoder loss: 0.04063156992197037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744673252105713 0.04385685920715332

Final encoder loss: 0.03982807323336601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755390167236328 0.04457521438598633

Final encoder loss: 0.03949379548430443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758129596710205 0.04458260536193848

Final encoder loss: 0.03945835307240486

Training wesad model
Final encoder loss: 0.08497518212229448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07162308692932129 0.173722505569458

Final encoder loss: 0.07230486619243208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07130765914916992 0.17431998252868652

Final encoder loss: 0.07264757656533963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07125210762023926 0.17361998558044434

Final encoder loss: 0.07159940093652721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07155156135559082 0.1741633415222168

Final encoder loss: 0.052333482903844986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07220697402954102 0.17398643493652344

Final encoder loss: 0.05203876512090845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07149863243103027 0.17384672164916992

Final encoder loss: 0.052539937750172946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07251811027526855 0.1744072437286377

Final encoder loss: 0.05293383376345214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07172942161560059 0.17391061782836914

Final encoder loss: 0.04131058007983985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07130861282348633 0.17413806915283203

Final encoder loss: 0.0411516324688484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07117700576782227 0.17367076873779297

Final encoder loss: 0.04124427385900712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07124042510986328 0.17403697967529297

Final encoder loss: 0.04341857753674823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07249236106872559 0.1741635799407959

Final encoder loss: 0.03545325422392696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07123994827270508 0.17375397682189941

Final encoder loss: 0.033812519182736867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0716860294342041 0.17376923561096191

Final encoder loss: 0.03605658839503501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07136225700378418 0.17390108108520508

Final encoder loss: 0.03821658611885118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07143688201904297 0.17432427406311035


Training wesad model
Final encoder loss: 0.2155507504940033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10614728927612305 0.03320956230163574

Final encoder loss: 0.09275959432125092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10481977462768555 0.033445119857788086

Final encoder loss: 0.06354373693466187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10441279411315918 0.03375577926635742

Final encoder loss: 0.050408486276865005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10556507110595703 0.03315544128417969

Final encoder loss: 0.04355371743440628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10378074645996094 0.032564640045166016

Final encoder loss: 0.039894990622997284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10348248481750488 0.03319287300109863

Final encoder loss: 0.03792928531765938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10501694679260254 0.033875226974487305

Final encoder loss: 0.03698808699846268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10514450073242188 0.03288388252258301

Final encoder loss: 0.03655758127570152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10363340377807617 0.03249526023864746

Final encoder loss: 0.03663597255945206

Calculating loss for amigos model
	Full Pass 0.6395881175994873
numFreeParamsPath 18
Reconstruction loss values: 0.05247107520699501 0.06174213066697121

Calculating loss for dapper model
	Full Pass 0.15255451202392578
numFreeParamsPath 18
Reconstruction loss values: 0.043534934520721436 0.0481971874833107

Calculating loss for case model
	Full Pass 0.857558012008667
numFreeParamsPath 18
Reconstruction loss values: 0.06035882234573364 0.06386119872331619

Calculating loss for emognition model
	Full Pass 0.2810697555541992
numFreeParamsPath 18
Reconstruction loss values: 0.06559209525585175 0.07283687591552734

Calculating loss for empatch model
	Full Pass 0.1042630672454834
numFreeParamsPath 18
Reconstruction loss values: 0.0717892274260521 0.07765854895114899

Calculating loss for wesad model
	Full Pass 0.07829594612121582
numFreeParamsPath 18
Reconstruction loss values: 0.0788036361336708 0.10150568932294846
Total loss calculation time: 3.7825028896331787

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.482545852661133
Total epoch time: 124.42714309692383

Epoch: 19

Training amigos model
Final encoder loss: 0.04781311347513743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11730480194091797 0.3939025402069092

Final encoder loss: 0.04582565981812067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10762763023376465 0.38889431953430176

Final encoder loss: 0.052532142142907766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10830020904541016 0.38913583755493164

Final encoder loss: 0.04840388549543997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10823202133178711 0.389080286026001

Final encoder loss: 0.047646915982735384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10824728012084961 0.39016175270080566

Final encoder loss: 0.047314212017020185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10883522033691406 0.38942956924438477

Final encoder loss: 0.04681026919566572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10930418968200684 0.3892040252685547

Final encoder loss: 0.049088193869799346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.109100341796875 0.39049220085144043

Final encoder loss: 0.042666682479487725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10837054252624512 0.3914182186126709

Final encoder loss: 0.04627156278489131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10849285125732422 0.3894035816192627

Final encoder loss: 0.04612670581940003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10831046104431152 0.3887369632720947

Final encoder loss: 0.04747996550295852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10847187042236328 0.38891124725341797

Final encoder loss: 0.04657115588265302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10851883888244629 0.3894827365875244

Final encoder loss: 0.04426914495153874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10929346084594727 0.3887796401977539

Final encoder loss: 0.041086639158502844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10912775993347168 0.3902115821838379

Final encoder loss: 0.04803918782279653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10298752784729004 0.3848114013671875


Training case model
Final encoder loss: 0.060193903880199025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09116101264953613 0.2644922733306885

Final encoder loss: 0.05528133251170377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09098958969116211 0.2655067443847656

Final encoder loss: 0.053057765794372276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09229207038879395 0.26498866081237793

Final encoder loss: 0.05182601186184922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09147930145263672 0.265636682510376

Final encoder loss: 0.04794482085617049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09123420715332031 0.26483964920043945

Final encoder loss: 0.048132987045234325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09147524833679199 0.2659647464752197

Final encoder loss: 0.04792244271845437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09159636497497559 0.26531291007995605

Final encoder loss: 0.04602578512114358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09206819534301758 0.2647252082824707

Final encoder loss: 0.04528418236507864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09126996994018555 0.2654588222503662

Final encoder loss: 0.04511422880527577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09142732620239258 0.26578688621520996

Final encoder loss: 0.04413821945859266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09249210357666016 0.26538515090942383

Final encoder loss: 0.045717283472443415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0914449691772461 0.2649729251861572

Final encoder loss: 0.0454969383224153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09124541282653809 0.2652549743652344

Final encoder loss: 0.04471933743171526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0915524959564209 0.2660481929779053

Final encoder loss: 0.043310639478269373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09204363822937012 0.26525259017944336

Final encoder loss: 0.04334938438726198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08971929550170898 0.2624826431274414


Training emognition model
Final encoder loss: 0.06298038350211738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08374929428100586 0.27600622177124023

Final encoder loss: 0.06046202501590207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08340001106262207 0.2753276824951172

Final encoder loss: 0.06265290659883403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08283519744873047 0.2731928825378418

Final encoder loss: 0.062147125593987884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08294916152954102 0.27336668968200684

Final encoder loss: 0.06280386680929406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08225655555725098 0.2738518714904785

Final encoder loss: 0.0566764552036094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08244514465332031 0.27417492866516113

Final encoder loss: 0.0580000233988726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08238649368286133 0.2732272148132324

Final encoder loss: 0.05656973357588625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08266210556030273 0.27400922775268555

Final encoder loss: 0.058577927646075625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0825655460357666 0.2736392021179199

Final encoder loss: 0.05483645862887208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08213496208190918 0.27320432662963867

Final encoder loss: 0.055748666360369376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08248639106750488 0.27329182624816895

Final encoder loss: 0.05670948608853873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08229970932006836 0.27352476119995117

Final encoder loss: 0.0543048448113251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0820322036743164 0.2746920585632324

Final encoder loss: 0.058140963788720926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08351945877075195 0.2754817008972168

Final encoder loss: 0.057351723442290524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08352017402648926 0.27535104751586914

Final encoder loss: 0.05641385958161005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08395266532897949 0.274993896484375


Training dapper model
Final encoder loss: 0.04196875686112392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.0616602897644043 0.1501622200012207

Final encoder loss: 0.04641193820012893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06332921981811523 0.15114164352416992

Final encoder loss: 0.040470568907095536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06244516372680664 0.14998579025268555

Final encoder loss: 0.04093550686114822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06178998947143555 0.15083909034729004

Final encoder loss: 0.037522497133144185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06197786331176758 0.1502985954284668

Final encoder loss: 0.039914054003715047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06180071830749512 0.15038561820983887

Final encoder loss: 0.04061727527788843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06342077255249023 0.15152740478515625

Final encoder loss: 0.04410044447031677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.0623018741607666 0.15006089210510254

Final encoder loss: 0.03947615685900166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.0618290901184082 0.15068984031677246

Final encoder loss: 0.035809922526128364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06287860870361328 0.14987444877624512

Final encoder loss: 0.04029952638152312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06198596954345703 0.14977765083312988

Final encoder loss: 0.03872921676092267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06271624565124512 0.15184807777404785

Final encoder loss: 0.035452253029045186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06259679794311523 0.15052366256713867

Final encoder loss: 0.04204141395769079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06192612648010254 0.15089130401611328

Final encoder loss: 0.031974054124762855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06304812431335449 0.15103673934936523

Final encoder loss: 0.03990690269637038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06155753135681152 0.14925169944763184


Training amigos model
Final encoder loss: 0.036186532990779015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10685491561889648 0.34152817726135254

Final encoder loss: 0.03649046524541774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10610389709472656 0.34175658226013184

Final encoder loss: 0.03460467742984914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10632753372192383 0.3414952754974365

Final encoder loss: 0.03413277665338611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10601973533630371 0.34174036979675293

Final encoder loss: 0.03693261851864774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1059117317199707 0.3416459560394287

Final encoder loss: 0.03646925155991714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10760188102722168 0.34122610092163086

Final encoder loss: 0.03624812691304984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10662555694580078 0.34166717529296875

Final encoder loss: 0.03603748043233887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10631608963012695 0.34148359298706055

Final encoder loss: 0.03495728259074596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10640072822570801 0.3417026996612549

Final encoder loss: 0.036214748632910174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10633230209350586 0.34170055389404297

Final encoder loss: 0.03487565218314178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10633587837219238 0.3415811061859131

Final encoder loss: 0.03875155664047915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10592007637023926 0.34079742431640625

Final encoder loss: 0.03304077090505501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10560059547424316 0.3408520221710205

Final encoder loss: 0.03290864910742835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1058802604675293 0.3414487838745117

Final encoder loss: 0.03617129889328253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10553526878356934 0.34070539474487305

Final encoder loss: 0.033740111548137004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.11829400062561035 0.3369638919830322


Training amigos model
Final encoder loss: 0.18077436089515686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4578831195831299 0.07499480247497559

Final encoder loss: 0.187837153673172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4564797878265381 0.07417654991149902

Final encoder loss: 0.18362917006015778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46563005447387695 0.07801198959350586

Final encoder loss: 0.06883379817008972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.466702938079834 0.07439923286437988

Final encoder loss: 0.07075542956590652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4679596424102783 0.07445740699768066

Final encoder loss: 0.06553547084331512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46834492683410645 0.07476425170898438

Final encoder loss: 0.043830376118421555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4660685062408447 0.07275247573852539

Final encoder loss: 0.04480950906872749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45447635650634766 0.07379841804504395

Final encoder loss: 0.04276727885007858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45511317253112793 0.07317185401916504

Final encoder loss: 0.03584342077374458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45542287826538086 0.07576298713684082

Final encoder loss: 0.0364985428750515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4552292823791504 0.07532143592834473

Final encoder loss: 0.035768743604421616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4562652111053467 0.07474899291992188

Final encoder loss: 0.033468060195446014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4654529094696045 0.07599139213562012

Final encoder loss: 0.034096017479896545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4651679992675781 0.07670354843139648

Final encoder loss: 0.03351229056715965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4653642177581787 0.07288742065429688

Final encoder loss: 0.03377639129757881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45897579193115234 0.07447957992553711

Final encoder loss: 0.0344027504324913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4638240337371826 0.07975172996520996

Final encoder loss: 0.03379470854997635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4620814323425293 0.07444429397583008

Final encoder loss: 0.03503062203526497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46071362495422363 0.07273364067077637

Final encoder loss: 0.035279300063848495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45812201499938965 0.07852578163146973

Final encoder loss: 0.035019684582948685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46314263343811035 0.07323813438415527

Final encoder loss: 0.035438790917396545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46480441093444824 0.07562685012817383

Final encoder loss: 0.03563586622476578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4611363410949707 0.07444357872009277

Final encoder loss: 0.03544353321194649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45812463760375977 0.07464981079101562

Final encoder loss: 0.034438107162714005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45998382568359375 0.07434606552124023

Final encoder loss: 0.03515400364995003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45587587356567383 0.07636713981628418

Final encoder loss: 0.035051632672548294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45765161514282227 0.07280731201171875

Final encoder loss: 0.03435521572828293
Final encoder loss: 0.03292722627520561
Final encoder loss: 0.03151565417647362

Training dapper model
Final encoder loss: 0.028450772632253447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059572458267211914 0.10651516914367676

Final encoder loss: 0.033061593475884214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05907249450683594 0.10600757598876953

Final encoder loss: 0.02911566158556022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05887603759765625 0.10578560829162598

Final encoder loss: 0.0307776182002608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05914044380187988 0.10643601417541504

Final encoder loss: 0.027548515784834016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.058719635009765625 0.10651755332946777

Final encoder loss: 0.02851629546977698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059085845947265625 0.10676288604736328

Final encoder loss: 0.02976189258610331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059272050857543945 0.10580658912658691

Final encoder loss: 0.027499219707414226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.058882713317871094 0.10639023780822754

Final encoder loss: 0.031738429283300264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05919957160949707 0.10655856132507324

Final encoder loss: 0.02622191534577619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05943727493286133 0.10643291473388672

Final encoder loss: 0.025842347609501944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05950021743774414 0.10620450973510742

Final encoder loss: 0.03225660635432076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05908942222595215 0.10663962364196777

Final encoder loss: 0.030020464926732826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05902862548828125 0.10648703575134277

Final encoder loss: 0.02978438530770106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05899834632873535 0.1063995361328125

Final encoder loss: 0.02656072529933693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05927896499633789 0.10644960403442383

Final encoder loss: 0.0316139486648777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05928993225097656 0.10622239112854004


Training dapper model
Final encoder loss: 0.20244553685188293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11665010452270508 0.033768653869628906

Final encoder loss: 0.20822736620903015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11545825004577637 0.03410673141479492

Final encoder loss: 0.0697612464427948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11471414566040039 0.03326892852783203

Final encoder loss: 0.0712396577000618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11496329307556152 0.03365731239318848

Final encoder loss: 0.04305935278534889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11490154266357422 0.033712148666381836

Final encoder loss: 0.04300164803862572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11471319198608398 0.033211469650268555

Final encoder loss: 0.03244738653302193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11486148834228516 0.03382444381713867

Final encoder loss: 0.032413363456726074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557817459106445 0.03378033638000488

Final encoder loss: 0.027820773422718048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11463093757629395 0.034073829650878906

Final encoder loss: 0.027804970741271973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11525726318359375 0.03371238708496094

Final encoder loss: 0.025958465412259102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11519813537597656 0.03393673896789551

Final encoder loss: 0.025923660025000572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1153111457824707 0.03322601318359375

Final encoder loss: 0.02558255009353161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11461234092712402 0.03335690498352051

Final encoder loss: 0.025511112064123154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11583971977233887 0.03351020812988281

Final encoder loss: 0.026163630187511444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11491703987121582 0.033423662185668945

Final encoder loss: 0.02575380727648735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11533188819885254 0.033799171447753906

Final encoder loss: 0.027422849088907242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11454463005065918 0.033728599548339844

Final encoder loss: 0.026619773358106613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11472439765930176 0.03417325019836426

Final encoder loss: 0.02819879911839962
Final encoder loss: 0.02586319111287594

Training case model
Final encoder loss: 0.04226469712707736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08920145034790039 0.218397855758667

Final encoder loss: 0.04048126779535801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08863234519958496 0.21868467330932617

Final encoder loss: 0.040847057129196146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08875083923339844 0.21894097328186035

Final encoder loss: 0.040042017498789446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08961248397827148 0.21864748001098633

Final encoder loss: 0.04083097944096397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08880758285522461 0.21860408782958984

Final encoder loss: 0.04035838542864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08955550193786621 0.21846914291381836

Final encoder loss: 0.04032582398667271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.0892026424407959 0.21886420249938965

Final encoder loss: 0.03990628705166588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08858084678649902 0.21877574920654297

Final encoder loss: 0.03982367501996536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08900785446166992 0.21864676475524902

Final encoder loss: 0.038754080182665115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08881902694702148 0.21866154670715332

Final encoder loss: 0.0395699277729708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08994507789611816 0.21840620040893555

Final encoder loss: 0.040173412228654864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08921480178833008 0.21866583824157715

Final encoder loss: 0.03996620625899883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08906674385070801 0.21889543533325195

Final encoder loss: 0.03963643828851146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08917832374572754 0.21851015090942383

Final encoder loss: 0.03936679675553829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08875703811645508 0.2184891700744629

Final encoder loss: 0.039407184394065244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08573150634765625 0.21519207954406738


Training case model
Final encoder loss: 0.2029644101858139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26178741455078125 0.051477670669555664

Final encoder loss: 0.18890012800693512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2578704357147217 0.05167126655578613

Final encoder loss: 0.19016402959823608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25708532333374023 0.05070209503173828

Final encoder loss: 0.1921837329864502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257110595703125 0.05214858055114746

Final encoder loss: 0.1808106154203415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26534557342529297 0.05241847038269043

Final encoder loss: 0.19192086160182953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25475144386291504 0.05142855644226074

Final encoder loss: 0.09492702782154083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2565805912017822 0.05215287208557129

Final encoder loss: 0.08556100726127625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2660834789276123 0.05271649360656738

Final encoder loss: 0.08184678107500076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25857090950012207 0.05136537551879883

Final encoder loss: 0.08106554299592972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568516731262207 0.0503392219543457

Final encoder loss: 0.07478746026754379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26708245277404785 0.05140089988708496

Final encoder loss: 0.07737838476896286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546391487121582 0.05133056640625

Final encoder loss: 0.0583576075732708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570199966430664 0.05155134201049805

Final encoder loss: 0.05359393358230591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571072578430176 0.051508188247680664

Final encoder loss: 0.05190419405698776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570617198944092 0.0524137020111084

Final encoder loss: 0.05280311405658722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2711796760559082 0.053070783615112305

Final encoder loss: 0.050748176872730255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25693798065185547 0.052288055419921875

Final encoder loss: 0.051612500101327896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25375843048095703 0.05110621452331543

Final encoder loss: 0.04666640982031822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25719738006591797 0.05121946334838867

Final encoder loss: 0.04429760202765465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27781033515930176 0.05186653137207031

Final encoder loss: 0.04306917265057564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26601719856262207 0.05314946174621582

Final encoder loss: 0.04457780346274376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26568078994750977 0.05206727981567383

Final encoder loss: 0.044325392693281174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2663111686706543 0.05161929130554199

Final encoder loss: 0.043674636632204056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553262710571289 0.051036834716796875

Final encoder loss: 0.044182613492012024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2572360038757324 0.051140546798706055

Final encoder loss: 0.0430666022002697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26616549491882324 0.051561832427978516

Final encoder loss: 0.042497772723436356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563619613647461 0.05143427848815918

Final encoder loss: 0.04342516139149666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2660350799560547 0.05220532417297363

Final encoder loss: 0.04439510405063629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26473450660705566 0.052835702896118164

Final encoder loss: 0.043414436280727386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25525665283203125 0.05126810073852539

Final encoder loss: 0.0436992347240448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25681519508361816 0.051910400390625

Final encoder loss: 0.042771875858306885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2662205696105957 0.0517115592956543

Final encoder loss: 0.04264972731471062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25680041313171387 0.053397178649902344

Final encoder loss: 0.0431758388876915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25589752197265625 0.05192065238952637

Final encoder loss: 0.04413833096623421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25730299949645996 0.052242279052734375

Final encoder loss: 0.043233875185251236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547168731689453 0.05153346061706543

Final encoder loss: 0.04230755567550659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2555093765258789 0.05173206329345703

Final encoder loss: 0.04143567383289337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26712846755981445 0.051320552825927734

Final encoder loss: 0.041201937943696976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2662198543548584 0.05114150047302246

Final encoder loss: 0.04179299250245094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2663545608520508 0.05285024642944336

Final encoder loss: 0.04235142841935158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2798757553100586 0.05120730400085449

Final encoder loss: 0.04160231724381447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559776306152344 0.0509951114654541

Final encoder loss: 0.04159724712371826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2575695514678955 0.051508188247680664

Final encoder loss: 0.04045798256993294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2598302364349365 0.05320286750793457

Final encoder loss: 0.0403536893427372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2567882537841797 0.0512089729309082

Final encoder loss: 0.04097810387611389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28372955322265625 0.05359387397766113

Final encoder loss: 0.04175305739045143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25830841064453125 0.05303168296813965

Final encoder loss: 0.04075802490115166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554004192352295 0.0535130500793457

Final encoder loss: 0.040969863533973694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587454319000244 0.05199146270751953

Final encoder loss: 0.040346987545490265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28200650215148926 0.05365300178527832

Final encoder loss: 0.039956774562597275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25855040550231934 0.05049586296081543

Final encoder loss: 0.04041635990142822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26791977882385254 0.055274009704589844

Final encoder loss: 0.04124438390135765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25844812393188477 0.05419588088989258

Final encoder loss: 0.040557097643613815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554647922515869 0.053711891174316406

Final encoder loss: 0.04080638289451599
Final encoder loss: 0.039387453347444534
Final encoder loss: 0.03792125731706619
Final encoder loss: 0.037327371537685394
Final encoder loss: 0.036584220826625824
Final encoder loss: 0.03448702022433281

Training emognition model
Final encoder loss: 0.047463603010768414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08183026313781738 0.23054933547973633

Final encoder loss: 0.046360020032940694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08112478256225586 0.23100662231445312

Final encoder loss: 0.049480899428409456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08193397521972656 0.23089122772216797

Final encoder loss: 0.04392692967533077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08154034614562988 0.23051857948303223

Final encoder loss: 0.0448559384077196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08160662651062012 0.23118066787719727

Final encoder loss: 0.043053021729135395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08069252967834473 0.23078465461730957

Final encoder loss: 0.04649507236159202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08105897903442383 0.2307431697845459

Final encoder loss: 0.04274285328663301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08286833763122559 0.23077726364135742

Final encoder loss: 0.04748252409029594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08120322227478027 0.23042917251586914

Final encoder loss: 0.04328141063674337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08111119270324707 0.231126070022583

Final encoder loss: 0.04521489288570217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08210229873657227 0.23117566108703613

Final encoder loss: 0.04754682108259291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08158159255981445 0.2306814193725586

Final encoder loss: 0.04657855339848457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08101224899291992 0.23117399215698242

Final encoder loss: 0.04755838020677872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08294939994812012 0.23065519332885742

Final encoder loss: 0.04226868019732375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08131265640258789 0.2309274673461914

Final encoder loss: 0.04423104761492043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08025145530700684 0.23063325881958008


Training emognition model
Final encoder loss: 0.19357158243656158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25219178199768066 0.04871320724487305

Final encoder loss: 0.19497889280319214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2502586841583252 0.04999732971191406

Final encoder loss: 0.08022218942642212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24822592735290527 0.04926800727844238

Final encoder loss: 0.07921741902828217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24982738494873047 0.049066781997680664

Final encoder loss: 0.0559576116502285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24907517433166504 0.0492401123046875

Final encoder loss: 0.05396070331335068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24817967414855957 0.04761505126953125

Final encoder loss: 0.046712424606084824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2487180233001709 0.049172163009643555

Final encoder loss: 0.045287199318408966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24669504165649414 0.049123287200927734

Final encoder loss: 0.042656611651182175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.251697301864624 0.04752683639526367

Final encoder loss: 0.04181920737028122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24724268913269043 0.04816031455993652

Final encoder loss: 0.041171424090862274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2491302490234375 0.050708770751953125

Final encoder loss: 0.04084230959415436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24846959114074707 0.049340009689331055

Final encoder loss: 0.041312236338853836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2479238510131836 0.04926490783691406

Final encoder loss: 0.04122079536318779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24860525131225586 0.048198699951171875

Final encoder loss: 0.04219941422343254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24753260612487793 0.050701141357421875

Final encoder loss: 0.04219629988074303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24831390380859375 0.04896855354309082

Final encoder loss: 0.043229665607213974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2465827465057373 0.05136418342590332

Final encoder loss: 0.04325401410460472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24915266036987305 0.04825568199157715

Final encoder loss: 0.04420644789934158
Final encoder loss: 0.04257435351610184

Training empatch model
Final encoder loss: 0.07447431614470887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07038593292236328 0.17345213890075684

Final encoder loss: 0.0713073039959855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07151293754577637 0.17426204681396484

Final encoder loss: 0.06337256370668652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07063698768615723 0.17381644248962402

Final encoder loss: 0.06148529121247836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0724034309387207 0.1741199493408203

Final encoder loss: 0.0613586039841566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07094097137451172 0.17419195175170898

Final encoder loss: 0.056305275347024245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07120776176452637 0.17356467247009277

Final encoder loss: 0.05739987973610139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07231664657592773 0.1745283603668213

Final encoder loss: 0.060178517638548125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0704202651977539 0.17325305938720703

Final encoder loss: 0.043393660021733296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07154440879821777 0.17353129386901855

Final encoder loss: 0.0423882168549568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07121706008911133 0.1739943027496338

Final encoder loss: 0.045801565547622915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0726311206817627 0.17453598976135254

Final encoder loss: 0.04385858434111495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07136154174804688 0.17440271377563477

Final encoder loss: 0.04253492681331143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0713191032409668 0.17373347282409668

Final encoder loss: 0.04499430625523495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07205367088317871 0.17556095123291016

Final encoder loss: 0.04678704859235802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07113957405090332 0.17385196685791016

Final encoder loss: 0.044938676848881934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07086753845214844 0.17319798469543457


Training empatch model
Final encoder loss: 0.17117241024971008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1762228012084961 0.04518556594848633

Final encoder loss: 0.07802585512399673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17638254165649414 0.04477405548095703

Final encoder loss: 0.05765202268958092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17474794387817383 0.04409933090209961

Final encoder loss: 0.04861932992935181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1775221824645996 0.04427361488342285

Final encoder loss: 0.043766774237155914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17503786087036133 0.04544496536254883

Final encoder loss: 0.04111892357468605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17662286758422852 0.043158531188964844

Final encoder loss: 0.0395546518266201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17384648323059082 0.043657779693603516

Final encoder loss: 0.0387149453163147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17440462112426758 0.04344511032104492

Final encoder loss: 0.038259249180555344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17465472221374512 0.0431370735168457

Final encoder loss: 0.0382084846496582

Training wesad model
Final encoder loss: 0.07685214703025907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0706636905670166 0.17277884483337402

Final encoder loss: 0.0708066535392188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07037568092346191 0.1720421314239502

Final encoder loss: 0.06937335865914292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0702219009399414 0.1723315715789795

Final encoder loss: 0.06539805061977759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07001566886901855 0.1726245880126953

Final encoder loss: 0.04989756028339718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07025861740112305 0.17281270027160645

Final encoder loss: 0.0497708666237154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07029247283935547 0.1723628044128418

Final encoder loss: 0.04759539536944792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0700230598449707 0.17383551597595215

Final encoder loss: 0.05054572447090861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07037067413330078 0.17209720611572266

Final encoder loss: 0.03868219740640644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07028055191040039 0.17197847366333008

Final encoder loss: 0.04015213476379942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07001614570617676 0.1735973358154297

Final encoder loss: 0.04152277492731076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07063674926757812 0.17397022247314453

Final encoder loss: 0.040508761876993815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07087111473083496 0.17388033866882324

Final encoder loss: 0.033030804278419076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07075786590576172 0.173720121383667

Final encoder loss: 0.0343774230323651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07095813751220703 0.1740124225616455

Final encoder loss: 0.03466299590075018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07120466232299805 0.17373013496398926

Final encoder loss: 0.03472753811449225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07116460800170898 0.17383503913879395


Training wesad model
Final encoder loss: 0.21559634804725647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10525107383728027 0.03272223472595215

Final encoder loss: 0.09196952730417252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10370707511901855 0.03364300727844238

Final encoder loss: 0.06245756894350052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385632514953613 0.032709598541259766

Final encoder loss: 0.04924704506993294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10332560539245605 0.03306078910827637

Final encoder loss: 0.042370278388261795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10350465774536133 0.03255748748779297

Final encoder loss: 0.038672808557748795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1032567024230957 0.03309321403503418

Final encoder loss: 0.0367206446826458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409808158874512 0.03307509422302246

Final encoder loss: 0.03571294620633125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10358166694641113 0.032999515533447266

Final encoder loss: 0.03543520346283913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10358762741088867 0.032105445861816406

Final encoder loss: 0.0353180393576622

Calculating loss for amigos model
	Full Pass 0.6721951961517334
numFreeParamsPath 18
Reconstruction loss values: 0.05114389955997467 0.05975836515426636

Calculating loss for dapper model
	Full Pass 0.15262103080749512
numFreeParamsPath 18
Reconstruction loss values: 0.04026298597455025 0.047164130955934525

Calculating loss for case model
	Full Pass 0.8566198348999023
numFreeParamsPath 18
Reconstruction loss values: 0.05873052775859833 0.061912160366773605

Calculating loss for emognition model
	Full Pass 0.2916698455810547
numFreeParamsPath 18
Reconstruction loss values: 0.06224445253610611 0.06950373202562332

Calculating loss for empatch model
	Full Pass 0.10428690910339355
numFreeParamsPath 18
Reconstruction loss values: 0.06851700693368912 0.0746702328324318

Calculating loss for wesad model
	Full Pass 0.0766913890838623
numFreeParamsPath 18
Reconstruction loss values: 0.07526696473360062 0.09859384596347809
Total loss calculation time: 4.2622644901275635

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.494500398635864
Total epoch time: 124.21797800064087

Epoch: 20

Training dapper model
Final encoder loss: 0.039084044894292125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06674885749816895 0.15626120567321777

Final encoder loss: 0.034984353815634125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06280708312988281 0.1521914005279541

Final encoder loss: 0.03765573537589842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06242799758911133 0.15134382247924805

Final encoder loss: 0.03648762891808903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.3949744701385498 0.15082454681396484

Final encoder loss: 0.039655401132114645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.062399864196777344 0.1507859230041504

Final encoder loss: 0.03439290681700536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0631406307220459 0.15232348442077637

Final encoder loss: 0.03503969008892325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06251120567321777 0.15188050270080566

Final encoder loss: 0.033691199402404816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06249094009399414 0.1512126922607422

Final encoder loss: 0.03463629102842711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.0631570816040039 0.15084075927734375

Final encoder loss: 0.03676148076280661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06297683715820312 0.15050315856933594

Final encoder loss: 0.03485999174955334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06236743927001953 0.15191435813903809

Final encoder loss: 0.039191528436048215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.0629737377166748 0.15015912055969238

Final encoder loss: 0.035344648535691345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062127113342285156 0.14997458457946777

Final encoder loss: 0.03396224764672448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06317400932312012 0.15137863159179688

Final encoder loss: 0.03702482086495113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06192755699157715 0.1508934497833252

Final encoder loss: 0.02883001014732794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06234264373779297 0.1502676010131836


Training amigos model
Final encoder loss: 0.05285036244971749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10949850082397461 0.3888723850250244

Final encoder loss: 0.046351594857476294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10876822471618652 0.3898019790649414

Final encoder loss: 0.049774586575425765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1087803840637207 0.3903377056121826

Final encoder loss: 0.0440857382038913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10868072509765625 0.3894960880279541

Final encoder loss: 0.04598756692867559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10885357856750488 0.38974666595458984

Final encoder loss: 0.047128632200310355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10868287086486816 0.3897576332092285

Final encoder loss: 0.04680829032438257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10816240310668945 0.3898489475250244

Final encoder loss: 0.04692486996156555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10931611061096191 0.3893609046936035

Final encoder loss: 0.04590045898749887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10973119735717773 0.39027833938598633

Final encoder loss: 0.04801750163208043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10939240455627441 0.3902773857116699

Final encoder loss: 0.04343985762223238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10848355293273926 0.38982677459716797

Final encoder loss: 0.04569295369452802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10883235931396484 0.3891935348510742

Final encoder loss: 0.04676080831436195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10852169990539551 0.38947629928588867

Final encoder loss: 0.049115120238284495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10841965675354004 0.3902308940887451

Final encoder loss: 0.04391103916398565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10939836502075195 0.38956284523010254

Final encoder loss: 0.04336696670022292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10483789443969727 0.3848438262939453


Training case model
Final encoder loss: 0.0585310178519456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09209871292114258 0.2655906677246094

Final encoder loss: 0.053842445955624804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09181094169616699 0.26602888107299805

Final encoder loss: 0.0510053203043975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09231185913085938 0.265653133392334

Final encoder loss: 0.049555858307641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09161829948425293 0.26579713821411133

Final encoder loss: 0.0475824903930486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09192323684692383 0.26544904708862305

Final encoder loss: 0.047409726424744406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09287405014038086 0.2648890018463135

Final encoder loss: 0.04720489375627598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09194493293762207 0.26551127433776855

Final encoder loss: 0.045779134820512345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09165000915527344 0.26513028144836426

Final encoder loss: 0.04649041968247228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09282112121582031 0.26581621170043945

Final encoder loss: 0.04405905890757193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09158039093017578 0.26514410972595215

Final encoder loss: 0.045607515734927745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09111547470092773 0.26552367210388184

Final encoder loss: 0.04442949340368048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09190869331359863 0.2668428421020508

Final encoder loss: 0.04408843644790773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09187722206115723 0.26560044288635254

Final encoder loss: 0.04406825024552844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09211039543151855 0.26537132263183594

Final encoder loss: 0.04270839710971016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09174180030822754 0.266162633895874

Final encoder loss: 0.042649401262351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08893251419067383 0.2624976634979248


Training emognition model
Final encoder loss: 0.06388493323947575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08518385887145996 0.2751047611236572

Final encoder loss: 0.06248315632170755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08340024948120117 0.2756013870239258

Final encoder loss: 0.0599199055979508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08355951309204102 0.2765233516693115

Final encoder loss: 0.05763925922450962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08448243141174316 0.27710866928100586

Final encoder loss: 0.05355745352988313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08368253707885742 0.2749924659729004

Final encoder loss: 0.056877327246219855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08438277244567871 0.27494215965270996

Final encoder loss: 0.05452586106005781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08358573913574219 0.27588868141174316

Final encoder loss: 0.05545886258269117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08377718925476074 0.2754945755004883

Final encoder loss: 0.05655003561260354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08453774452209473 0.27669405937194824

Final encoder loss: 0.05489882974977346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08379888534545898 0.2747335433959961

Final encoder loss: 0.05824055305705075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08326578140258789 0.2757713794708252

Final encoder loss: 0.05381094839894429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08402347564697266 0.2770078182220459

Final encoder loss: 0.05444098936191705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08374452590942383 0.2754833698272705

Final encoder loss: 0.05544098017506944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08390569686889648 0.27485179901123047

Final encoder loss: 0.05334735617755585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0833120346069336 0.27548646926879883

Final encoder loss: 0.05055333309024189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08320903778076172 0.27510643005371094


Training amigos model
Final encoder loss: 0.035558180038737466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10803079605102539 0.34137821197509766

Final encoder loss: 0.034515398829863556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10617899894714355 0.3423619270324707

Final encoder loss: 0.035020970643414334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10607242584228516 0.3416311740875244

Final encoder loss: 0.035246768508607564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10623478889465332 0.3417367935180664

Final encoder loss: 0.03499193766945534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10699319839477539 0.34166789054870605

Final encoder loss: 0.03425863089830968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10695385932922363 0.34183835983276367

Final encoder loss: 0.030255646219888698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10629701614379883 0.342435359954834

Final encoder loss: 0.03575484442300675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1068425178527832 0.3417539596557617

Final encoder loss: 0.031318155192221635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10609722137451172 0.3416407108306885

Final encoder loss: 0.036106329678368264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10771751403808594 0.34162211418151855

Final encoder loss: 0.035562441338095635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10636067390441895 0.34179258346557617

Final encoder loss: 0.03405375619671223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10596060752868652 0.34175848960876465

Final encoder loss: 0.0331458764698846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10666751861572266 0.3416910171508789

Final encoder loss: 0.035328397810841385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1065218448638916 0.3414015769958496

Final encoder loss: 0.03555132955622172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10671234130859375 0.3417518138885498

Final encoder loss: 0.03158369076711764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10190796852111816 0.3382844924926758


Training amigos model
Final encoder loss: 0.1807614117860794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.476287841796875 0.0760951042175293

Final encoder loss: 0.1878470480442047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4723024368286133 0.07658123970031738

Final encoder loss: 0.1836322844028473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45655226707458496 0.07458901405334473

Final encoder loss: 0.07031883299350739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46035337448120117 0.07520198822021484

Final encoder loss: 0.07177734375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4591677188873291 0.07117390632629395

Final encoder loss: 0.06633729487657547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.456585168838501 0.07502222061157227

Final encoder loss: 0.044142432510852814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45917367935180664 0.07464766502380371

Final encoder loss: 0.04451538622379303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46616411209106445 0.08152222633361816

Final encoder loss: 0.042540278285741806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47066664695739746 0.07731771469116211

Final encoder loss: 0.03552516549825668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4685683250427246 0.07419681549072266

Final encoder loss: 0.03576319292187691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685084819793701 0.07474541664123535

Final encoder loss: 0.03528298810124397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47151803970336914 0.0758974552154541

Final encoder loss: 0.03249233216047287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4716465473175049 0.07620716094970703

Final encoder loss: 0.03270868957042694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4706909656524658 0.07436609268188477

Final encoder loss: 0.032738253474235535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4680826663970947 0.07589030265808105

Final encoder loss: 0.03228030726313591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46862053871154785 0.07614636421203613

Final encoder loss: 0.03256396949291229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4715723991394043 0.07583475112915039

Final encoder loss: 0.032440684735774994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4687626361846924 0.07525634765625

Final encoder loss: 0.03316706046462059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46935105323791504 0.07823014259338379

Final encoder loss: 0.03331705182790756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.471454381942749 0.07622647285461426

Final encoder loss: 0.033677056431770325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4635627269744873 0.07665491104125977

Final encoder loss: 0.03385934606194496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47161078453063965 0.0766150951385498

Final encoder loss: 0.03349107503890991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4598245620727539 0.07476449012756348

Final encoder loss: 0.03412804380059242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4552140235900879 0.07582259178161621

Final encoder loss: 0.03336172178387642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45903730392456055 0.07303285598754883

Final encoder loss: 0.03348332270979881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4588444232940674 0.07589268684387207

Final encoder loss: 0.03325125202536583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4554171562194824 0.07393646240234375

Final encoder loss: 0.0330805703997612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4700927734375 0.08084273338317871

Final encoder loss: 0.033143993467092514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46973443031311035 0.07601714134216309

Final encoder loss: 0.03322942927479744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4685075283050537 0.07379746437072754

Final encoder loss: 0.032524917274713516
Final encoder loss: 0.031128644943237305
Final encoder loss: 0.029964573681354523

Training dapper model
Final encoder loss: 0.03082496743351038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05971574783325195 0.10619211196899414

Final encoder loss: 0.03069508253660764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05931663513183594 0.10622048377990723

Final encoder loss: 0.029421472002378293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05912423133850098 0.10628008842468262

Final encoder loss: 0.027840403142849145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059036970138549805 0.10613751411437988

Final encoder loss: 0.02921906291634289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05915260314941406 0.10641860961914062

Final encoder loss: 0.02977877504888473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05913519859313965 0.10765361785888672

Final encoder loss: 0.026998855226999078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05907034873962402 0.10649752616882324

Final encoder loss: 0.028615557131820166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059038639068603516 0.10642099380493164

Final encoder loss: 0.024748585419513157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059296607971191406 0.10614562034606934

Final encoder loss: 0.02934451918403427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05907177925109863 0.10642886161804199

Final encoder loss: 0.027013574163334525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059049367904663086 0.1060175895690918

Final encoder loss: 0.02768849372676314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.058959245681762695 0.10649919509887695

Final encoder loss: 0.02792596488270808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05912494659423828 0.10640525817871094

Final encoder loss: 0.029167967310252316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.0590970516204834 0.10646796226501465

Final encoder loss: 0.029524634145296987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0592193603515625 0.10646820068359375

Final encoder loss: 0.02607493345243018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.058794260025024414 0.10602784156799316


Training dapper model
Final encoder loss: 0.20244692265987396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1156315803527832 0.03429388999938965

Final encoder loss: 0.2081948220729828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11740422248840332 0.0344538688659668

Final encoder loss: 0.07106862962245941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11543035507202148 0.03349184989929199

Final encoder loss: 0.07155315577983856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557722091674805 0.033782243728637695

Final encoder loss: 0.04302968084812164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11495351791381836 0.03380942344665527

Final encoder loss: 0.04249719902873039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11739015579223633 0.034330129623413086

Final encoder loss: 0.03212437033653259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11603283882141113 0.03438162803649902

Final encoder loss: 0.03179596737027168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11613845825195312 0.03508925437927246

Final encoder loss: 0.027379993349313736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11637401580810547 0.033855438232421875

Final encoder loss: 0.027059338986873627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1167001724243164 0.03398537635803223

Final encoder loss: 0.025250010192394257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11578798294067383 0.03428244590759277

Final encoder loss: 0.024881217628717422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11662745475769043 0.03438973426818848

Final encoder loss: 0.024570787325501442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11591315269470215 0.03464102745056152

Final encoder loss: 0.02422836795449257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11641216278076172 0.03499746322631836

Final encoder loss: 0.02456125244498253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11670064926147461 0.034157514572143555

Final encoder loss: 0.02431725338101387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11679816246032715 0.03434872627258301

Final encoder loss: 0.0254089143127203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11557769775390625 0.034908294677734375

Final encoder loss: 0.02483438141644001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11661529541015625 0.0343782901763916

Final encoder loss: 0.02648824080824852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11599063873291016 0.03405356407165527

Final encoder loss: 0.025900840759277344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1160743236541748 0.034738779067993164

Final encoder loss: 0.027808869257569313
Final encoder loss: 0.02477843128144741

Training case model
Final encoder loss: 0.039284927726523027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08906173706054688 0.21919775009155273

Final encoder loss: 0.03814905807221543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08905196189880371 0.21887540817260742

Final encoder loss: 0.03902219585562066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09069013595581055 0.21995139122009277

Final encoder loss: 0.03879604957639386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09096002578735352 0.2195577621459961

Final encoder loss: 0.0392854869201245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0893855094909668 0.21938490867614746

Final encoder loss: 0.03818211059243028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09111309051513672 0.21905851364135742

Final encoder loss: 0.03879697513852687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08886408805847168 0.21863532066345215

Final encoder loss: 0.03940640707332878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08909940719604492 0.21887469291687012

Final encoder loss: 0.03868029521934048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08897662162780762 0.21890854835510254

Final encoder loss: 0.03912038695799626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0895991325378418 0.21970510482788086

Final encoder loss: 0.03795778912144202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09154272079467773 0.21940922737121582

Final encoder loss: 0.0389709615977899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09027361869812012 0.21891140937805176

Final encoder loss: 0.03756746796465407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0888361930847168 0.2189939022064209

Final encoder loss: 0.037850180393404535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08965158462524414 0.21959662437438965

Final encoder loss: 0.03741925217329968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0890355110168457 0.21921300888061523

Final encoder loss: 0.03784051522945824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.09806632995605469 0.21564245223999023


Training case model
Final encoder loss: 0.20296810567378998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2609975337982178 0.053856849670410156

Final encoder loss: 0.18891413509845734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2672898769378662 0.05267143249511719

Final encoder loss: 0.19015760719776154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26729488372802734 0.05300641059875488

Final encoder loss: 0.19218574464321136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2671205997467041 0.05153489112854004

Final encoder loss: 0.1808173656463623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27375173568725586 0.051143646240234375

Final encoder loss: 0.19191931188106537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25550317764282227 0.0520014762878418

Final encoder loss: 0.09571757912635803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2565579414367676 0.05422377586364746

Final encoder loss: 0.08610118925571442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579185962677002 0.05180191993713379

Final encoder loss: 0.08273637294769287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26808857917785645 0.0527646541595459

Final encoder loss: 0.08218994736671448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25986719131469727 0.052971839904785156

Final encoder loss: 0.0754661038517952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26705336570739746 0.05185389518737793

Final encoder loss: 0.07755568623542786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551231384277344 0.0515744686126709

Final encoder loss: 0.058263473212718964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25636839866638184 0.052021026611328125

Final encoder loss: 0.05333763360977173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26741909980773926 0.05296826362609863

Final encoder loss: 0.05188868194818497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.267547607421875 0.05173969268798828

Final encoder loss: 0.05282134562730789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2549898624420166 0.05103611946105957

Final encoder loss: 0.05063438415527344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2739877700805664 0.05139803886413574

Final encoder loss: 0.051276665180921555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25488901138305664 0.05127143859863281

Final encoder loss: 0.04588005319237709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26778507232666016 0.05177426338195801

Final encoder loss: 0.043389879167079926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27262258529663086 0.05136847496032715

Final encoder loss: 0.04257765784859657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2670719623565674 0.05214571952819824

Final encoder loss: 0.043938640505075455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26552557945251465 0.05161643028259277

Final encoder loss: 0.04365229234099388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2571103572845459 0.05259084701538086

Final encoder loss: 0.04257455840706825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2527649402618408 0.051009416580200195

Final encoder loss: 0.042861245572566986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2678961753845215 0.05268669128417969

Final encoder loss: 0.04171253368258476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28919243812561035 0.05146956443786621

Final encoder loss: 0.04129606485366821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2669830322265625 0.05264616012573242

Final encoder loss: 0.042450252920389175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26685476303100586 0.0514066219329834

Final encoder loss: 0.043540239334106445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2555832862854004 0.05252957344055176

Final encoder loss: 0.04174806550145149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25299787521362305 0.05077481269836426

Final encoder loss: 0.042458999902009964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2670724391937256 0.0523984432220459

Final encoder loss: 0.0413975715637207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2735733985900879 0.05184602737426758

Final encoder loss: 0.04133991897106171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2668173313140869 0.05229496955871582

Final encoder loss: 0.042111586779356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2563300132751465 0.05128312110900879

Final encoder loss: 0.042982202023267746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26720547676086426 0.052828073501586914

Final encoder loss: 0.04195209592580795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541031837463379 0.05146956443786621

Final encoder loss: 0.04102852940559387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2669181823730469 0.051923274993896484

Final encoder loss: 0.03993549197912216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26757264137268066 0.05204963684082031

Final encoder loss: 0.03977050259709358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25560903549194336 0.05130410194396973

Final encoder loss: 0.04035220295190811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26946043968200684 0.051207780838012695

Final encoder loss: 0.040824875235557556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26682019233703613 0.05101490020751953

Final encoder loss: 0.04019242152571678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538747787475586 0.05126047134399414

Final encoder loss: 0.040203068405389786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2669553756713867 0.052129507064819336

Final encoder loss: 0.03922415152192116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25676488876342773 0.052753448486328125

Final encoder loss: 0.03898888826370239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2672300338745117 0.05182766914367676

Final encoder loss: 0.0396798774600029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2664008140563965 0.05234980583190918

Final encoder loss: 0.040418561547994614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2667081356048584 0.051254987716674805

Final encoder loss: 0.03953981027007103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25306129455566406 0.052874088287353516

Final encoder loss: 0.039713915437459946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25665950775146484 0.052233219146728516

Final encoder loss: 0.03876207396388054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2670574188232422 0.05229306221008301

Final encoder loss: 0.03872637823224068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.256389856338501 0.05179715156555176

Final encoder loss: 0.03928763046860695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25647807121276855 0.05465865135192871

Final encoder loss: 0.0400896780192852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26743292808532715 0.0517115592956543

Final encoder loss: 0.03910942003130913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539191246032715 0.05225706100463867

Final encoder loss: 0.03937466815114021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2677469253540039 0.052419424057006836

Final encoder loss: 0.038647495210170746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27374792098999023 0.05197334289550781

Final encoder loss: 0.03845864161849022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2564268112182617 0.05182647705078125

Final encoder loss: 0.03888489678502083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25580549240112305 0.05172324180603027

Final encoder loss: 0.03967573493719101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2673978805541992 0.05286836624145508

Final encoder loss: 0.03885547071695328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534325122833252 0.05193448066711426

Final encoder loss: 0.039009347558021545
Final encoder loss: 0.037287697196006775
Final encoder loss: 0.03617405891418457
Final encoder loss: 0.03554345294833183
Final encoder loss: 0.034911997616291046
Final encoder loss: 0.03275948017835617

Training emognition model
Final encoder loss: 0.04631036757766412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08122515678405762 0.22989606857299805

Final encoder loss: 0.04512094588592852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08071064949035645 0.22989106178283691

Final encoder loss: 0.044600070588206475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08090066909790039 0.22989845275878906

Final encoder loss: 0.04037636326536163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08077049255371094 0.22930359840393066

Final encoder loss: 0.04357329380912739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08062362670898438 0.23005175590515137

Final encoder loss: 0.04423452066768215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08089089393615723 0.22959113121032715

Final encoder loss: 0.04103877974473843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08054018020629883 0.23009800910949707

Final encoder loss: 0.039421536116243626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.0807960033416748 0.22988653182983398

Final encoder loss: 0.04399450084607739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08113265037536621 0.22980880737304688

Final encoder loss: 0.04286707100222239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08065247535705566 0.2294788360595703

Final encoder loss: 0.0431786959342944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08101105690002441 0.2299365997314453

Final encoder loss: 0.043893902712266404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08063626289367676 0.23011183738708496

Final encoder loss: 0.04153800503988774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08088135719299316 0.22989463806152344

Final encoder loss: 0.042220140116368314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08051919937133789 0.2293562889099121

Final encoder loss: 0.042189489757715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0807960033416748 0.22982549667358398

Final encoder loss: 0.046601583805986266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07995176315307617 0.2288646697998047


Training emognition model
Final encoder loss: 0.19357524812221527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24962353706359863 0.04879450798034668

Final encoder loss: 0.19497214257717133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24629855155944824 0.04737353324890137

Final encoder loss: 0.07972627878189087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2456355094909668 0.049344778060913086

Final encoder loss: 0.07898437976837158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24450111389160156 0.04985547065734863

Final encoder loss: 0.05442173033952713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24722957611083984 0.04935884475708008

Final encoder loss: 0.05298095569014549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741196632385254 0.048645734786987305

Final encoder loss: 0.04495314508676529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2468578815460205 0.0490117073059082

Final encoder loss: 0.044152554124593735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.250424861907959 0.04904294013977051

Final encoder loss: 0.04096974432468414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24744725227355957 0.04876971244812012

Final encoder loss: 0.040508147329092026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24821686744689941 0.04939889907836914

Final encoder loss: 0.03948873281478882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482612133026123 0.049603939056396484

Final encoder loss: 0.03921763598918915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741005897521973 0.0500645637512207

Final encoder loss: 0.039374906569719315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24757957458496094 0.04880714416503906

Final encoder loss: 0.03924377262592316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471909523010254 0.04945111274719238

Final encoder loss: 0.03993493691086769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24723410606384277 0.04949831962585449

Final encoder loss: 0.040301211178302765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24593782424926758 0.05082440376281738

Final encoder loss: 0.04053021967411041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482302188873291 0.04899907112121582

Final encoder loss: 0.04098314046859741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2481234073638916 0.04956316947937012

Final encoder loss: 0.040838707238435745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24914860725402832 0.049309730529785156

Final encoder loss: 0.04139901325106621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24879908561706543 0.0485684871673584

Final encoder loss: 0.041095562279224396
Final encoder loss: 0.03936036303639412

Training empatch model
Final encoder loss: 0.07011784383132114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07123756408691406 0.17366695404052734

Final encoder loss: 0.061682873106643464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0721743106842041 0.17441868782043457

Final encoder loss: 0.06434891913159288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07253837585449219 0.17406725883483887

Final encoder loss: 0.059774311246964945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.071624755859375 0.1739349365234375

Final encoder loss: 0.06391409248022317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07156229019165039 0.1745142936706543

Final encoder loss: 0.05897442037809343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0727388858795166 0.17387843132019043

Final encoder loss: 0.05545680832132152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07129955291748047 0.17402386665344238

Final encoder loss: 0.051907568128515415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07067513465881348 0.17348146438598633

Final encoder loss: 0.04476241774691962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07225275039672852 0.17494654655456543

Final encoder loss: 0.044318237722706426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07230567932128906 0.17409348487854004

Final encoder loss: 0.03930306764253057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07195401191711426 0.1739516258239746

Final encoder loss: 0.04177628153036472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07153105735778809 0.17479443550109863

Final encoder loss: 0.04166205138965479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0727987289428711 0.17430520057678223

Final encoder loss: 0.0461428221350924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07185578346252441 0.17381715774536133

Final encoder loss: 0.041141914990170725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07124829292297363 0.17378592491149902

Final encoder loss: 0.04429312388328734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.071533203125 0.17420649528503418


Training empatch model
Final encoder loss: 0.17116223275661469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17796850204467773 0.044615983963012695

Final encoder loss: 0.07852953672409058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17688608169555664 0.04420208930969238

Final encoder loss: 0.057525333017110825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17737388610839844 0.045618295669555664

Final encoder loss: 0.04817383363842964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17692303657531738 0.04448699951171875

Final encoder loss: 0.0430896021425724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1765153408050537 0.04451560974121094

Final encoder loss: 0.04024629667401314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17740511894226074 0.04482698440551758

Final encoder loss: 0.03859027475118637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17728877067565918 0.04375505447387695

Final encoder loss: 0.03775280341506004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17582178115844727 0.04483604431152344

Final encoder loss: 0.037137020379304886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17675113677978516 0.045079708099365234

Final encoder loss: 0.03681483119726181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17688822746276855 0.04364752769470215

Final encoder loss: 0.03679163008928299

Training wesad model
Final encoder loss: 0.07508958078303936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07102108001708984 0.17379999160766602

Final encoder loss: 0.0721017278036118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0727241039276123 0.17423415184020996

Final encoder loss: 0.06507420215586664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07120037078857422 0.17400765419006348

Final encoder loss: 0.06526493549021842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07163786888122559 0.1737356185913086

Final encoder loss: 0.04701607402135472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07126688957214355 0.1750802993774414

Final encoder loss: 0.04738369495771449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07216286659240723 0.1735992431640625

Final encoder loss: 0.04940956961690657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0718989372253418 0.17397022247314453

Final encoder loss: 0.04881616477173621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07116174697875977 0.17422056198120117

Final encoder loss: 0.04045856389574858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07200360298156738 0.17412853240966797

Final encoder loss: 0.038074430187531906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07134222984313965 0.17401766777038574

Final encoder loss: 0.04025508788429607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07130885124206543 0.17378854751586914

Final encoder loss: 0.03728753560660403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07130789756774902 0.1746983528137207

Final encoder loss: 0.03145734077155354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0730736255645752 0.17366886138916016

Final encoder loss: 0.03289912246473862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07113099098205566 0.17405319213867188

Final encoder loss: 0.03234686445669129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07105159759521484 0.17414212226867676

Final encoder loss: 0.03371183280320616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07210254669189453 0.1744372844696045


Training wesad model
Final encoder loss: 0.21558429300785065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10557699203491211 0.03329658508300781

Final encoder loss: 0.09266678988933563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10430192947387695 0.03264188766479492

Final encoder loss: 0.06279121339321136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356378555297852 0.033232927322387695

Final encoder loss: 0.04904823377728462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10465455055236816 0.0338134765625

Final encoder loss: 0.04183873161673546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10512089729309082 0.03373885154724121

Final encoder loss: 0.03792789205908775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1045689582824707 0.0329737663269043

Final encoder loss: 0.03578691929578781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037302017211914 0.03311324119567871

Final encoder loss: 0.034545961767435074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388374328613281 0.033334970474243164

Final encoder loss: 0.034000616520643234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10417437553405762 0.032585859298706055

Final encoder loss: 0.03383006155490875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10459113121032715 0.03401660919189453

Final encoder loss: 0.03414248675107956

Calculating loss for amigos model
	Full Pass 0.6731066703796387
numFreeParamsPath 18
Reconstruction loss values: 0.04959176108241081 0.05822639539837837

Calculating loss for dapper model
	Full Pass 0.1517469882965088
numFreeParamsPath 18
Reconstruction loss values: 0.04154394939541817 0.04564401134848595

Calculating loss for case model
	Full Pass 0.8600931167602539
numFreeParamsPath 18
Reconstruction loss values: 0.05638899281620979 0.05907323583960533

Calculating loss for emognition model
	Full Pass 0.278179407119751
numFreeParamsPath 18
Reconstruction loss values: 0.06074138730764389 0.0680522471666336

Calculating loss for empatch model
	Full Pass 0.10555243492126465
numFreeParamsPath 18
Reconstruction loss values: 0.06690438091754913 0.07247108966112137

Calculating loss for wesad model
	Full Pass 0.07740306854248047
numFreeParamsPath 18
Reconstruction loss values: 0.07361532747745514 0.09594987332820892
Total loss calculation time: 3.8015024662017822

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.527239084243774
Total epoch time: 131.06705141067505

Epoch: 21

Training emognition model
Final encoder loss: 0.061390842683931676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08957076072692871 0.2789788246154785

Final encoder loss: 0.05566943929606811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08268499374389648 0.27265405654907227

Final encoder loss: 0.054193462780004524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08240485191345215 0.2738523483276367

Final encoder loss: 0.05427477952497059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08352422714233398 0.27487802505493164

Final encoder loss: 0.055045685246517505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08381414413452148 0.27603816986083984

Final encoder loss: 0.051227416167080604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08364987373352051 0.27434277534484863

Final encoder loss: 0.05435130319270335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08312797546386719 0.2754092216491699

Final encoder loss: 0.04823238926950453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08331990242004395 0.2757105827331543

Final encoder loss: 0.053661127812220856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0837404727935791 0.27454543113708496

Final encoder loss: 0.05413026244808018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08351349830627441 0.2748572826385498

Final encoder loss: 0.0488598279723573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08288049697875977 0.2742142677307129

Final encoder loss: 0.04936437837029707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08308196067810059 0.2757096290588379

Final encoder loss: 0.049068887862118125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08398747444152832 0.2752678394317627

Final encoder loss: 0.04801554010890091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08358287811279297 0.27520751953125

Final encoder loss: 0.04866692396973769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08367538452148438 0.2748441696166992

Final encoder loss: 0.04633955875897317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08221650123596191 0.27442169189453125


Training case model
Final encoder loss: 0.05823583272579408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09140181541442871 0.26473045349121094

Final encoder loss: 0.0504689441276277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09186840057373047 0.26433229446411133

Final encoder loss: 0.04973114014495641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09151482582092285 0.2654843330383301

Final encoder loss: 0.047232090344134836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09151458740234375 0.2650740146636963

Final encoder loss: 0.04566345896447983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09069132804870605 0.2642240524291992

Final encoder loss: 0.0452550815100462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09100651741027832 0.26393699645996094

Final encoder loss: 0.04588075114588345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0909569263458252 0.26373887062072754

Final encoder loss: 0.04463737254314545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09102010726928711 0.26474928855895996

Final encoder loss: 0.04480356301691974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09091711044311523 0.26354336738586426

Final encoder loss: 0.041052363016315534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09118270874023438 0.2642204761505127

Final encoder loss: 0.04135972233248576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09116840362548828 0.2644212245941162

Final encoder loss: 0.04285250352132015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09077215194702148 0.26481056213378906

Final encoder loss: 0.041321089113602986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0913383960723877 0.26372575759887695

Final encoder loss: 0.041154816874366534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09089159965515137 0.2658860683441162

Final encoder loss: 0.041473016668659136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09138035774230957 0.265657901763916

Final encoder loss: 0.04138479972553256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08829259872436523 0.2617514133453369


Training dapper model
Final encoder loss: 0.04154700191870228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06342887878417969 0.15078186988830566

Final encoder loss: 0.039058557859874395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.062282562255859375 0.15082979202270508

Final encoder loss: 0.04314722593118632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06215929985046387 0.15075469017028809

Final encoder loss: 0.03732500014978765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06371641159057617 0.15114712715148926

Final encoder loss: 0.03951978147729051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06256675720214844 0.15137147903442383

Final encoder loss: 0.036951108970721784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062004804611206055 0.15191030502319336

Final encoder loss: 0.03785377389256761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06245684623718262 0.1502985954284668

Final encoder loss: 0.034970062669008595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06270170211791992 0.15056824684143066

Final encoder loss: 0.0360042184820664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06313395500183105 0.1523725986480713

Final encoder loss: 0.03872479288850041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06246161460876465 0.1508946418762207

Final encoder loss: 0.03497103719156593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06187009811401367 0.15117812156677246

Final encoder loss: 0.03436639415228567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06328678131103516 0.15058374404907227

Final encoder loss: 0.030353184427610334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06251096725463867 0.15125775337219238

Final encoder loss: 0.03503609196315121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.0622868537902832 0.15163207054138184

Final encoder loss: 0.03146645885839829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06255149841308594 0.1504673957824707

Final encoder loss: 0.029859754522858363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06164860725402832 0.15014386177062988


Training amigos model
Final encoder loss: 0.05116280467583553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10930275917053223 0.39020252227783203

Final encoder loss: 0.0473401384538038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10821032524108887 0.3901703357696533

Final encoder loss: 0.04500104497524659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10820841789245605 0.3898782730102539

Final encoder loss: 0.04359014481642671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10846590995788574 0.3892698287963867

Final encoder loss: 0.04666958814799423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10860371589660645 0.3894217014312744

Final encoder loss: 0.043642446762060784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10812211036682129 0.3899693489074707

Final encoder loss: 0.043865792832942715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10933685302734375 0.3896005153656006

Final encoder loss: 0.049509839591169336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10965561866760254 0.3896186351776123

Final encoder loss: 0.04436018902896858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10864090919494629 0.39010071754455566

Final encoder loss: 0.04291358219069251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10852360725402832 0.3893141746520996

Final encoder loss: 0.041405856921134535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10840559005737305 0.3892946243286133

Final encoder loss: 0.04018858565124204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10887742042541504 0.38932156562805176

Final encoder loss: 0.042879733657444456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1084599494934082 0.3891868591308594

Final encoder loss: 0.041765139711260404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10898208618164062 0.3896763324737549

Final encoder loss: 0.041301780884508685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.11007475852966309 0.3891458511352539

Final encoder loss: 0.04087023190332233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10424947738647461 0.3845400810241699


Training amigos model
Final encoder loss: 0.03176759439264781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1066126823425293 0.34195566177368164

Final encoder loss: 0.03195238959295367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10614728927612305 0.3416156768798828

Final encoder loss: 0.030256801262587594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1062319278717041 0.34180331230163574

Final encoder loss: 0.029638083125232643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10663342475891113 0.34154772758483887

Final encoder loss: 0.03205440426636629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10680317878723145 0.34180593490600586

Final encoder loss: 0.03028669526199351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10615944862365723 0.3419654369354248

Final encoder loss: 0.02942676603641677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10674023628234863 0.3419303894042969

Final encoder loss: 0.03423392794145917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10646796226501465 0.3418145179748535

Final encoder loss: 0.030097156875156014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10702252388000488 0.3416938781738281

Final encoder loss: 0.029382829325864928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10624313354492188 0.34157490730285645

Final encoder loss: 0.03323050292825782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10604405403137207 0.34197402000427246

Final encoder loss: 0.03434459460920084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10663485527038574 0.3418593406677246

Final encoder loss: 0.032223192269761836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10664820671081543 0.3415184020996094

Final encoder loss: 0.028682460301645455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10699748992919922 0.3419227600097656

Final encoder loss: 0.03318799968277608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10604357719421387 0.3418154716491699

Final encoder loss: 0.03529269339303265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10160255432128906 0.3383777141571045


Training amigos model
Final encoder loss: 0.18077945709228516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47296595573425293 0.07613325119018555

Final encoder loss: 0.1878216564655304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47506070137023926 0.07410526275634766

Final encoder loss: 0.18363991379737854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46828746795654297 0.07640337944030762

Final encoder loss: 0.06944446265697479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4695761203765869 0.08220839500427246

Final encoder loss: 0.07100275903940201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4755561351776123 0.07578182220458984

Final encoder loss: 0.0655512809753418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46553707122802734 0.07441473007202148

Final encoder loss: 0.04344979673624039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47070860862731934 0.08113431930541992

Final encoder loss: 0.04408087581396103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.471210241317749 0.07567977905273438

Final encoder loss: 0.04208807274699211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47188472747802734 0.07708406448364258

Final encoder loss: 0.03472987934947014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4660651683807373 0.07601571083068848

Final encoder loss: 0.035085950046777725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47043681144714355 0.07962846755981445

Final encoder loss: 0.03451528027653694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47161245346069336 0.07528519630432129

Final encoder loss: 0.03178011253476143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4726090431213379 0.07660531997680664

Final encoder loss: 0.03183029964566231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4702434539794922 0.07441496849060059

Final encoder loss: 0.03178684785962105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4664754867553711 0.08152556419372559

Final encoder loss: 0.032115645706653595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47555112838745117 0.07638382911682129

Final encoder loss: 0.031715892255306244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47308993339538574 0.08161401748657227

Final encoder loss: 0.03192251920700073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47121286392211914 0.07625794410705566

Final encoder loss: 0.03305927291512489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4719686508178711 0.07587432861328125

Final encoder loss: 0.03281553462147713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4641914367675781 0.07471919059753418

Final encoder loss: 0.033377133309841156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4554159641265869 0.07444262504577637

Final encoder loss: 0.032802652567625046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4580092430114746 0.07433891296386719

Final encoder loss: 0.03322377800941467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45862579345703125 0.07569026947021484

Final encoder loss: 0.033645324409008026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45479917526245117 0.07154250144958496

Final encoder loss: 0.03186425566673279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45842981338500977 0.07682967185974121

Final encoder loss: 0.03235822170972824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46614956855773926 0.07786226272583008

Final encoder loss: 0.032185833901166916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4666097164154053 0.0776054859161377

Final encoder loss: 0.03139990195631981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.472332239151001 0.07648754119873047

Final encoder loss: 0.03185386583209038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4720916748046875 0.07814931869506836

Final encoder loss: 0.0319492444396019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4687964916229248 0.07388114929199219

Final encoder loss: 0.03158433735370636
Final encoder loss: 0.03000098094344139
Final encoder loss: 0.029231315478682518

Training dapper model
Final encoder loss: 0.027983863778041844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.060010671615600586 0.10739612579345703

Final encoder loss: 0.026755960302472258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.060915231704711914 0.10793423652648926

Final encoder loss: 0.028167200252187725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05954265594482422 0.10782837867736816

Final encoder loss: 0.02771460647624409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05969548225402832 0.10750293731689453

Final encoder loss: 0.02710407561901306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060257673263549805 0.10812711715698242

Final encoder loss: 0.02637552120522489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059793949127197266 0.10697007179260254

Final encoder loss: 0.02734571015421438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05984163284301758 0.10743403434753418

Final encoder loss: 0.028877317008698698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05968451499938965 0.1077423095703125

Final encoder loss: 0.02876280851436118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06033945083618164 0.10724425315856934

Final encoder loss: 0.02797269387160189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.059653282165527344 0.10742425918579102

Final encoder loss: 0.02744332790763995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059973955154418945 0.1078641414642334

Final encoder loss: 0.024640274926831452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06067180633544922 0.10742568969726562

Final encoder loss: 0.024076043859920234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05994987487792969 0.10761260986328125

Final encoder loss: 0.02873238383497607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05964374542236328 0.10753226280212402

Final encoder loss: 0.027694250675423547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.060601234436035156 0.1077275276184082

Final encoder loss: 0.030122645947210556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05945086479187012 0.1075294017791748


Training dapper model
Final encoder loss: 0.2024652659893036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11853671073913574 0.03407120704650879

Final encoder loss: 0.2081795036792755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11974954605102539 0.033653974533081055

Final encoder loss: 0.07294287532567978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1153254508972168 0.03421759605407715

Final encoder loss: 0.07321766018867493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11649680137634277 0.033473968505859375

Final encoder loss: 0.044120799750089645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11780405044555664 0.03479647636413574

Final encoder loss: 0.04311208054423332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11626505851745605 0.03532838821411133

Final encoder loss: 0.032517772167921066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11628985404968262 0.03399205207824707

Final encoder loss: 0.03186137229204178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.117401123046875 0.035161733627319336

Final encoder loss: 0.02730889990925789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11551403999328613 0.03398942947387695

Final encoder loss: 0.026785241439938545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1163337230682373 0.034101009368896484

Final encoder loss: 0.024956176057457924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11755704879760742 0.03473019599914551

Final encoder loss: 0.024591881781816483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11614871025085449 0.03421902656555176

Final encoder loss: 0.02395293302834034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580753326416016 0.035019874572753906

Final encoder loss: 0.02371109649538994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11658978462219238 0.03387331962585449

Final encoder loss: 0.023953711614012718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559486389160156 0.03344249725341797

Final encoder loss: 0.02357693761587143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157219409942627 0.03400230407714844

Final encoder loss: 0.024318663403391838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11563920974731445 0.0339815616607666

Final encoder loss: 0.02355237863957882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11525487899780273 0.03381657600402832

Final encoder loss: 0.024778243154287338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11573243141174316 0.03358793258666992

Final encoder loss: 0.0242408886551857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11552786827087402 0.033319711685180664

Final encoder loss: 0.02555740252137184
Final encoder loss: 0.02352983132004738

Training case model
Final encoder loss: 0.03769968613569432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0888671875 0.21817755699157715

Final encoder loss: 0.038865510131266594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.4968392848968506 0.21844172477722168

Final encoder loss: 0.038384847640700624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08921980857849121 0.2183530330657959

Final encoder loss: 0.03696323303718398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08961296081542969 0.218414306640625

Final encoder loss: 0.03740593263044335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08859539031982422 0.21839022636413574

Final encoder loss: 0.03754919797876288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08953213691711426 0.21831035614013672

Final encoder loss: 0.03709371113612483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09003448486328125 0.21924519538879395

Final encoder loss: 0.03746465791827482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08987808227539062 0.2189950942993164

Final encoder loss: 0.036829553006024376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.0900278091430664 0.21929597854614258

Final encoder loss: 0.03655184104742175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.089111328125 0.21941685676574707

Final encoder loss: 0.036334434739772305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08962106704711914 0.2190537452697754

Final encoder loss: 0.03718593977485519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0898137092590332 0.2191622257232666

Final encoder loss: 0.036590873029030656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.09974122047424316 0.21909260749816895

Final encoder loss: 0.03677391095174951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.0899660587310791 0.21904921531677246

Final encoder loss: 0.03679912977001402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0894770622253418 0.21924710273742676

Final encoder loss: 0.036584894753672485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08623838424682617 0.21578693389892578


Training case model
Final encoder loss: 0.2029493749141693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26315999031066895 0.05268216133117676

Final encoder loss: 0.18891453742980957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26914334297180176 0.05268049240112305

Final encoder loss: 0.1901511698961258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26581883430480957 0.05230259895324707

Final encoder loss: 0.19219470024108887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575347423553467 0.05089378356933594

Final encoder loss: 0.1808176040649414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27243947982788086 0.05213427543640137

Final encoder loss: 0.19191929697990417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544224262237549 0.05090928077697754

Final encoder loss: 0.09635602682828903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2557106018066406 0.05220842361450195

Final encoder loss: 0.08685735613107681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2741587162017822 0.05308985710144043

Final encoder loss: 0.08307817578315735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26624441146850586 0.05067276954650879

Final encoder loss: 0.0826679915189743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26709818840026855 0.050415992736816406

Final encoder loss: 0.07564016431570053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26744508743286133 0.05129599571228027

Final encoder loss: 0.07798616588115692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25562095642089844 0.055448293685913086

Final encoder loss: 0.058559078723192215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26816678047180176 0.053070068359375

Final encoder loss: 0.05358830839395523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568855285644531 0.05171346664428711

Final encoder loss: 0.0518641397356987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2767190933227539 0.05289769172668457

Final encoder loss: 0.05278652906417847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25764012336730957 0.051583051681518555

Final encoder loss: 0.05035300925374031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26818275451660156 0.05459284782409668

Final encoder loss: 0.05126072093844414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25475525856018066 0.05249619483947754

Final encoder loss: 0.045842573046684265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2564253807067871 0.05522346496582031

Final encoder loss: 0.04317041113972664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27038002014160156 0.05484318733215332

Final encoder loss: 0.04207867383956909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566502094268799 0.052420616149902344

Final encoder loss: 0.04355502501130104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2887392044067383 0.05199551582336426

Final encoder loss: 0.042820654809474945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25833606719970703 0.0517578125

Final encoder loss: 0.0424305759370327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25444817543029785 0.05159306526184082

Final encoder loss: 0.04223579540848732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28005123138427734 0.05340075492858887

Final encoder loss: 0.041181501001119614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.268294095993042 0.05177783966064453

Final encoder loss: 0.040439777076244354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581493854522705 0.05232691764831543

Final encoder loss: 0.04142237827181816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575225830078125 0.05215120315551758

Final encoder loss: 0.04225805029273033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2737274169921875 0.052587270736694336

Final encoder loss: 0.04109117388725281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2536313533782959 0.05315828323364258

Final encoder loss: 0.04173039644956589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26622509956359863 0.051993608474731445

Final encoder loss: 0.04100698605179787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2731609344482422 0.05203747749328613

Final encoder loss: 0.04049941897392273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2737565040588379 0.0517725944519043

Final encoder loss: 0.04103152081370354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2736368179321289 0.05214524269104004

Final encoder loss: 0.042173631489276886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26682567596435547 0.05211591720581055

Final encoder loss: 0.04094718024134636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538132667541504 0.05038714408874512

Final encoder loss: 0.04037652537226677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26720738410949707 0.05194807052612305

Final encoder loss: 0.039206258952617645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2678992748260498 0.05162501335144043

Final encoder loss: 0.038729339838027954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27391529083251953 0.05179619789123535

Final encoder loss: 0.03935305029153824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.266660213470459 0.05094408988952637

Final encoder loss: 0.04004162549972534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2736165523529053 0.052588701248168945

Final encoder loss: 0.03935619071125984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537376880645752 0.05106639862060547

Final encoder loss: 0.03934081643819809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26743483543395996 0.05209922790527344

Final encoder loss: 0.038520172238349915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2758619785308838 0.05156278610229492

Final encoder loss: 0.037849318236112595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2672855854034424 0.0522918701171875

Final encoder loss: 0.03872319310903549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25565314292907715 0.05156302452087402

Final encoder loss: 0.0391971580684185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2734060287475586 0.052289724349975586

Final encoder loss: 0.03824998438358307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550468444824219 0.05162835121154785

Final encoder loss: 0.038683075457811356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25635218620300293 0.05214428901672363

Final encoder loss: 0.03807085007429123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.256178617477417 0.051216840744018555

Final encoder loss: 0.03732559084892273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2664055824279785 0.051952362060546875

Final encoder loss: 0.038068968802690506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2671036720275879 0.052388906478881836

Final encoder loss: 0.03882806748151779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2747182846069336 0.05164623260498047

Final encoder loss: 0.03795107454061508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537822723388672 0.05095362663269043

Final encoder loss: 0.038590699434280396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2677302360534668 0.051122426986694336

Final encoder loss: 0.03767130896449089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2723255157470703 0.05165886878967285

Final encoder loss: 0.037293095141649246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2679593563079834 0.051879167556762695

Final encoder loss: 0.037963055074214935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27324724197387695 0.052347660064697266

Final encoder loss: 0.038800254464149475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2762026786804199 0.05208706855773926

Final encoder loss: 0.037856005132198334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25406813621520996 0.05151772499084473

Final encoder loss: 0.03810621052980423
Final encoder loss: 0.036447081714868546
Final encoder loss: 0.0349719375371933
Final encoder loss: 0.03446543216705322
Final encoder loss: 0.033857230097055435
Final encoder loss: 0.03190368413925171

Training emognition model
Final encoder loss: 0.040807087322987684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08119606971740723 0.22956132888793945

Final encoder loss: 0.04291823461018956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08086442947387695 0.22986412048339844

Final encoder loss: 0.04176802570235854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08042573928833008 0.23014593124389648

Final encoder loss: 0.043799068845893935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08073067665100098 0.22968435287475586

Final encoder loss: 0.04288647713037353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08092594146728516 0.22949767112731934

Final encoder loss: 0.04139796773466578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08051228523254395 0.22958803176879883

Final encoder loss: 0.041998505773635605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08087587356567383 0.23063206672668457

Final encoder loss: 0.04350055462072196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08067727088928223 0.23023295402526855

Final encoder loss: 0.04220524822254012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08073306083679199 0.23024868965148926

Final encoder loss: 0.04126078515825456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08069491386413574 0.22916460037231445

Final encoder loss: 0.04038531052265802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08087301254272461 0.23122358322143555

Final encoder loss: 0.0420303759042342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0943448543548584 0.22997522354125977

Final encoder loss: 0.04270404723345587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0807492733001709 0.23007822036743164

Final encoder loss: 0.04314239531915749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08092856407165527 0.230010986328125

Final encoder loss: 0.04138624643901792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08083391189575195 0.23005437850952148

Final encoder loss: 0.04273869470408046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07998156547546387 0.22874832153320312


Training emognition model
Final encoder loss: 0.19356921315193176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477719783782959 0.04812908172607422

Final encoder loss: 0.19497108459472656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24651503562927246 0.048845767974853516

Final encoder loss: 0.08138561993837357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24582290649414062 0.04936814308166504

Final encoder loss: 0.07994481176137924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24648785591125488 0.04981207847595215

Final encoder loss: 0.05553736910223961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24614500999450684 0.0492708683013916

Final encoder loss: 0.05347665771842003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24703288078308105 0.04932045936584473

Final encoder loss: 0.04549079015851021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24586272239685059 0.048722267150878906

Final encoder loss: 0.044063325971364975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2459418773651123 0.04874396324157715

Final encoder loss: 0.04101195186376572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24611234664916992 0.04922366142272949

Final encoder loss: 0.03996271640062332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2454831600189209 0.04886436462402344

Final encoder loss: 0.039067089557647705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24536490440368652 0.04916667938232422

Final encoder loss: 0.03833836689591408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24543404579162598 0.049613237380981445

Final encoder loss: 0.03870048373937607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24572443962097168 0.04842567443847656

Final encoder loss: 0.03817560896277428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24581050872802734 0.048928260803222656

Final encoder loss: 0.039070140570402145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.245361328125 0.04882478713989258

Final encoder loss: 0.0390116386115551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24602866172790527 0.049326181411743164

Final encoder loss: 0.03956370800733566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24570083618164062 0.04890155792236328

Final encoder loss: 0.03975061699748039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24621868133544922 0.04858517646789551

Final encoder loss: 0.040057286620140076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24518799781799316 0.049419403076171875

Final encoder loss: 0.040273409336805344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24542617797851562 0.04877519607543945

Final encoder loss: 0.040397144854068756
Final encoder loss: 0.03842700272798538

Training empatch model
Final encoder loss: 0.06776231742756937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0705416202545166 0.17267227172851562

Final encoder loss: 0.06213555982492189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07071566581726074 0.17246246337890625

Final encoder loss: 0.06012941591253855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07093310356140137 0.17258787155151367

Final encoder loss: 0.059480112187541685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07110929489135742 0.17360877990722656

Final encoder loss: 0.052496391180722746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0709984302520752 0.17305970191955566

Final encoder loss: 0.05867995670447992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07075810432434082 0.1732935905456543

Final encoder loss: 0.056465752235068944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07084393501281738 0.17321109771728516

Final encoder loss: 0.0548036932086385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07026267051696777 0.17270374298095703

Final encoder loss: 0.0427030779306416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07100725173950195 0.1732776165008545

Final encoder loss: 0.04174324136133201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0706636905670166 0.17364048957824707

Final encoder loss: 0.0405631838541356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07098984718322754 0.1731564998626709

Final encoder loss: 0.04208500935318217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07118558883666992 0.1732618808746338

Final encoder loss: 0.03868912579618154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07067131996154785 0.17304110527038574

Final encoder loss: 0.041004853436253125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07059097290039062 0.17296075820922852

Final encoder loss: 0.04163841892164263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07069921493530273 0.17328834533691406

Final encoder loss: 0.04046866442158561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07026219367980957 0.17304229736328125


Training empatch model
Final encoder loss: 0.17114506661891937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761939525604248 0.04312276840209961

Final encoder loss: 0.07858467102050781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17506766319274902 0.043131351470947266

Final encoder loss: 0.05738639831542969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17415356636047363 0.04360008239746094

Final encoder loss: 0.04783637076616287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17377686500549316 0.04337930679321289

Final encoder loss: 0.042650606483221054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17606639862060547 0.04386091232299805

Final encoder loss: 0.03962213173508644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17427563667297363 0.044620513916015625

Final encoder loss: 0.03779235854744911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17786908149719238 0.042882442474365234

Final encoder loss: 0.03680262714624405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17431116104125977 0.04350090026855469

Final encoder loss: 0.03620601072907448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758272647857666 0.04416370391845703

Final encoder loss: 0.036041807383298874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17691850662231445 0.04363727569580078

Final encoder loss: 0.03591185063123703

Training wesad model
Final encoder loss: 0.07431388280408321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0712428092956543 0.17366623878479004

Final encoder loss: 0.07242846497635415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07148122787475586 0.17388582229614258

Final encoder loss: 0.062048456856322466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07328605651855469 0.1740119457244873

Final encoder loss: 0.06188206225246316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07149434089660645 0.17377305030822754

Final encoder loss: 0.04700338434032646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0712289810180664 0.17362737655639648

Final encoder loss: 0.04532890805768982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0727078914642334 0.1740102767944336

Final encoder loss: 0.04401996361386839
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07104778289794922 0.17365193367004395

Final encoder loss: 0.0460038284053073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07129406929016113 0.17377400398254395

Final encoder loss: 0.03703921563074628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07114219665527344 0.17432737350463867

Final encoder loss: 0.038181498468840766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07276678085327148 0.17472624778747559

Final encoder loss: 0.03594500354519741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07128787040710449 0.17400884628295898

Final encoder loss: 0.036839538316947244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07140946388244629 0.1734769344329834

Final encoder loss: 0.03100531913227109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0727841854095459 0.1745290756225586

Final encoder loss: 0.029599030520362772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07075381278991699 0.1737532615661621

Final encoder loss: 0.03215696264213306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07117772102355957 0.1735830307006836

Final encoder loss: 0.0317437947494556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07142448425292969 0.17360901832580566


Training wesad model
Final encoder loss: 0.21559177339076996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10832786560058594 0.03283262252807617

Final encoder loss: 0.09300781041383743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10333037376403809 0.03260469436645508

Final encoder loss: 0.06263552606105804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10470032691955566 0.033733367919921875

Final encoder loss: 0.04867728427052498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10436868667602539 0.033489227294921875

Final encoder loss: 0.041243936866521835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037285327911377 0.034006357192993164

Final encoder loss: 0.037032026797533035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10533785820007324 0.03400015830993652

Final encoder loss: 0.034589678049087524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10497260093688965 0.0323338508605957

Final encoder loss: 0.0331755131483078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10363888740539551 0.0336146354675293

Final encoder loss: 0.032546188682317734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10444831848144531 0.032824039459228516

Final encoder loss: 0.03239400312304497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10366010665893555 0.033365488052368164

Final encoder loss: 0.03273701295256615

Calculating loss for amigos model
	Full Pass 0.6723282337188721
numFreeParamsPath 18
Reconstruction loss values: 0.047954536974430084 0.05738627910614014

Calculating loss for dapper model
	Full Pass 0.165846586227417
numFreeParamsPath 18
Reconstruction loss values: 0.038845278322696686 0.04300494119524956

Calculating loss for case model
	Full Pass 0.8603107929229736
numFreeParamsPath 18
Reconstruction loss values: 0.05545820668339729 0.058242131024599075

Calculating loss for emognition model
	Full Pass 0.2791569232940674
numFreeParamsPath 18
Reconstruction loss values: 0.05909254774451256 0.06654057651758194

Calculating loss for empatch model
	Full Pass 0.10653162002563477
numFreeParamsPath 18
Reconstruction loss values: 0.06530896574258804 0.07047773152589798

Calculating loss for wesad model
	Full Pass 0.07764077186584473
numFreeParamsPath 18
Reconstruction loss values: 0.0727686882019043 0.0945652574300766
Total loss calculation time: 3.8191518783569336

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.976688385009766
Total epoch time: 131.74725604057312

Epoch: 22

Training dapper model
Final encoder loss: 0.03586621289668915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06807780265808105 0.1575005054473877

Final encoder loss: 0.03753286264057669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06224989891052246 0.15095019340515137

Final encoder loss: 0.03446073525303082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06288981437683105 0.1513218879699707

Final encoder loss: 0.033049673189398075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06254029273986816 0.1510484218597412

Final encoder loss: 0.0363584882592408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06446480751037598 0.15349316596984863

Final encoder loss: 0.03446149605306766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0625145435333252 0.15072917938232422

Final encoder loss: 0.03296161068956002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06298136711120605 0.15074396133422852

Final encoder loss: 0.0326781911722606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06233644485473633 0.15089058876037598

Final encoder loss: 0.034039649177665894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06910133361816406 0.15271806716918945

Final encoder loss: 0.03448617831330205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06190943717956543 0.14972162246704102

Final encoder loss: 0.03216321967670433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.0623319149017334 0.15221714973449707

Final encoder loss: 0.0317691852974499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06201791763305664 0.15100312232971191

Final encoder loss: 0.03104037838101361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06997013092041016 0.1525580883026123

Final encoder loss: 0.029868873891445474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06204676628112793 0.15015101432800293

Final encoder loss: 0.03697687284412259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06241154670715332 0.1510913372039795

Final encoder loss: 0.04085653428422946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06163525581359863 0.15004754066467285


Training emognition model
Final encoder loss: 0.059727273953106896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08497023582458496 0.27597546577453613

Final encoder loss: 0.05610968351024096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0837864875793457 0.27512454986572266

Final encoder loss: 0.0548770800378842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08358335494995117 0.27648425102233887

Final encoder loss: 0.052997980029655625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08385658264160156 0.278454065322876

Final encoder loss: 0.05494409732212976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0835268497467041 0.2754027843475342

Final encoder loss: 0.05311982871821625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08497977256774902 0.27726078033447266

Final encoder loss: 0.05297414209245524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08421039581298828 0.2751004695892334

Final encoder loss: 0.05380346128009786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08370280265808105 0.2766611576080322

Final encoder loss: 0.05492696455934415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08325648307800293 0.27459263801574707

Final encoder loss: 0.05253582223401195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08362603187561035 0.2751784324645996

Final encoder loss: 0.051049942280647076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08592844009399414 0.27596259117126465

Final encoder loss: 0.04926560735036854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08346962928771973 0.2754063606262207

Final encoder loss: 0.053535607849094335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08515048027038574 0.2767674922943115

Final encoder loss: 0.04879921775008688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.0835721492767334 0.27611875534057617

Final encoder loss: 0.05033611615148767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08319306373596191 0.2762157917022705

Final encoder loss: 0.04977932169069128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08369159698486328 0.2740921974182129


Training case model
Final encoder loss: 0.05595737022136975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0913698673248291 0.2650477886199951

Final encoder loss: 0.04957596222935485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09236359596252441 0.26990437507629395

Final encoder loss: 0.0477415797423506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09179973602294922 0.26531434059143066

Final encoder loss: 0.046258631650661575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09236550331115723 0.26624035835266113

Final encoder loss: 0.045004021695645526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09120059013366699 0.2653331756591797

Final encoder loss: 0.04432226144634774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09255743026733398 0.26620960235595703

Final encoder loss: 0.04394241826263753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09143853187561035 0.265852689743042

Final encoder loss: 0.044468174384857305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0909881591796875 0.2633841037750244

Final encoder loss: 0.04330929987716865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09044885635375977 0.2633199691772461

Final encoder loss: 0.04215356181676907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09070324897766113 0.26396965980529785

Final encoder loss: 0.043236935040149686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09078717231750488 0.2631504535675049

Final encoder loss: 0.04226354593778436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09053778648376465 0.26377105712890625

Final encoder loss: 0.04096358382897824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09087014198303223 0.2640814781188965

Final encoder loss: 0.040703945548245066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09086084365844727 0.2638704776763916

Final encoder loss: 0.040684438634873824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09084105491638184 0.2640068531036377

Final encoder loss: 0.04051232955840142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08745884895324707 0.2606620788574219


Training amigos model
Final encoder loss: 0.05068616238035996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10831189155578613 0.3888430595397949

Final encoder loss: 0.046933517881796644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.1083519458770752 0.38904786109924316

Final encoder loss: 0.04635800302429491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10850191116333008 0.3893725872039795

Final encoder loss: 0.044798882745359134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10801863670349121 0.38997411727905273

Final encoder loss: 0.04004325874025278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10867738723754883 0.38899946212768555

Final encoder loss: 0.04442392864230683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10905671119689941 0.3888263702392578

Final encoder loss: 0.0459830963721013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1086118221282959 0.38904809951782227

Final encoder loss: 0.04497697726249174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10804200172424316 0.38901352882385254

Final encoder loss: 0.04383280267986347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.1082160472869873 0.38882899284362793

Final encoder loss: 0.04390799697931561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10834336280822754 0.38919568061828613

Final encoder loss: 0.047547916813566654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10803532600402832 0.390059232711792

Final encoder loss: 0.04032408619970071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10814094543457031 0.38927769660949707

Final encoder loss: 0.04182389059987646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10880637168884277 0.3895397186279297

Final encoder loss: 0.04168163392946457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1085197925567627 0.3887174129486084

Final encoder loss: 0.040088455298120884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10828208923339844 0.3888130187988281

Final encoder loss: 0.04071924579977856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10301566123962402 0.3825716972351074


Training amigos model
Final encoder loss: 0.03254818219182467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10609650611877441 0.34125709533691406

Final encoder loss: 0.031818855381039894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10587120056152344 0.34121274948120117

Final encoder loss: 0.03173483933844361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10557436943054199 0.3409411907196045

Final encoder loss: 0.030459022567219565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10609984397888184 0.341036319732666

Final encoder loss: 0.02989060477253581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10579514503479004 0.3410797119140625

Final encoder loss: 0.03059802216201805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1059260368347168 0.3415844440460205

Final encoder loss: 0.035054603879726004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1061849594116211 0.34162116050720215

Final encoder loss: 0.03285789736983866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10642361640930176 0.34181690216064453

Final encoder loss: 0.03251561667153085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10647201538085938 0.3415186405181885

Final encoder loss: 0.03163230821165596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10679793357849121 0.34172487258911133

Final encoder loss: 0.03367236885136348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10644173622131348 0.3413560390472412

Final encoder loss: 0.02883355817868438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10689330101013184 0.342132568359375

Final encoder loss: 0.032899106713995416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10634660720825195 0.3416264057159424

Final encoder loss: 0.03171225103952905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1253798007965088 0.3415231704711914

Final encoder loss: 0.02929538193226858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10641193389892578 0.34186434745788574

Final encoder loss: 0.031059256742461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10109949111938477 0.3382706642150879


Training amigos model
Final encoder loss: 0.18076610565185547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.449146032333374 0.07727742195129395

Final encoder loss: 0.1878458559513092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46994757652282715 0.0721585750579834

Final encoder loss: 0.18363472819328308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4363367557525635 0.07217097282409668

Final encoder loss: 0.06971859186887741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4536123275756836 0.08144760131835938

Final encoder loss: 0.07083459943532944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45865321159362793 0.07377934455871582

Final encoder loss: 0.0656483992934227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4528524875640869 0.07431983947753906

Final encoder loss: 0.044049978256225586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46988844871520996 0.07697677612304688

Final encoder loss: 0.04446420818567276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44150328636169434 0.08051061630249023

Final encoder loss: 0.04249857738614082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45430445671081543 0.07432866096496582

Final encoder loss: 0.035132307559251785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4563162326812744 0.07580828666687012

Final encoder loss: 0.03553156182169914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.470217227935791 0.07836318016052246

Final encoder loss: 0.03465374559164047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4511587619781494 0.07845473289489746

Final encoder loss: 0.031777605414390564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4745934009552002 0.07595252990722656

Final encoder loss: 0.032456763088703156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45395779609680176 0.0748605728149414

Final encoder loss: 0.03174327686429024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4387016296386719 0.07657027244567871

Final encoder loss: 0.031922049820423126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4681236743927002 0.07900810241699219

Final encoder loss: 0.03225507587194443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4597458839416504 0.07644224166870117

Final encoder loss: 0.03174528107047081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45082664489746094 0.07303404808044434

Final encoder loss: 0.03281431272625923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4684939384460449 0.0766139030456543

Final encoder loss: 0.03290329873561859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45511651039123535 0.08146929740905762

Final encoder loss: 0.03341901674866676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4573025703430176 0.07673358917236328

Final encoder loss: 0.03328711539506912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44034719467163086 0.07539749145507812

Final encoder loss: 0.033496495336294174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4692394733428955 0.07665300369262695

Final encoder loss: 0.034101780503988266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.450772762298584 0.07983779907226562

Final encoder loss: 0.03201257064938545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4746129512786865 0.07285022735595703

Final encoder loss: 0.03253116458654404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.453096866607666 0.07721662521362305

Final encoder loss: 0.03271948918700218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4545578956604004 0.07494258880615234

Final encoder loss: 0.03155713155865669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4703328609466553 0.08179807662963867

Final encoder loss: 0.032094817608594894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.42615532875061035 0.07643628120422363

Final encoder loss: 0.032568056136369705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44005298614501953 0.07646751403808594

Final encoder loss: 0.031134052202105522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4502406120300293 0.07618975639343262

Final encoder loss: 0.03170190006494522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47034549713134766 0.08077716827392578

Final encoder loss: 0.03185858577489853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.440277099609375 0.07684469223022461

Final encoder loss: 0.03184761106967926
Final encoder loss: 0.03047499619424343
Final encoder loss: 0.02907615527510643

Training dapper model
Final encoder loss: 0.0288660019722613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05984807014465332 0.10731220245361328

Final encoder loss: 0.027811578178185258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06013202667236328 0.10767316818237305

Final encoder loss: 0.02709029880092008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.061515092849731445 0.10754060745239258

Final encoder loss: 0.026938408370188655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05993032455444336 0.10711431503295898

Final encoder loss: 0.025631144769327485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05974841117858887 0.10773134231567383

Final encoder loss: 0.02522687729982624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06076693534851074 0.1082754135131836

Final encoder loss: 0.026232976932251975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05968284606933594 0.10727119445800781

Final encoder loss: 0.025992390037358006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05965256690979004 0.10766029357910156

Final encoder loss: 0.025662179088104548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05992388725280762 0.10846638679504395

Final encoder loss: 0.028287075407754252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06069135665893555 0.10758519172668457

Final encoder loss: 0.030326637733914204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.0601499080657959 0.1075747013092041

Final encoder loss: 0.026553025485681742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05985260009765625 0.1078023910522461

Final encoder loss: 0.02633280011131598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06105995178222656 0.10763382911682129

Final encoder loss: 0.026527395281717922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06015300750732422 0.1076204776763916

Final encoder loss: 0.02614525123739882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.060011863708496094 0.10773086547851562

Final encoder loss: 0.027563763420835847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05980110168457031 0.10773611068725586


Training dapper model
Final encoder loss: 0.2024337649345398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11404848098754883 0.03426790237426758

Final encoder loss: 0.20817933976650238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11581230163574219 0.03491830825805664

Final encoder loss: 0.0739026889204979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11446094512939453 0.03510236740112305

Final encoder loss: 0.07437601685523987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11597776412963867 0.03361964225769043

Final encoder loss: 0.04463536664843559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1134190559387207 0.03428959846496582

Final encoder loss: 0.043713267892599106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11582159996032715 0.03501701354980469

Final encoder loss: 0.03270718455314636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1146540641784668 0.033856868743896484

Final encoder loss: 0.03206546977162361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11626529693603516 0.03414583206176758

Final encoder loss: 0.027288852259516716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11315083503723145 0.03428816795349121

Final encoder loss: 0.02672700397670269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11727166175842285 0.03502988815307617

Final encoder loss: 0.02474234439432621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11287307739257812 0.033600568771362305

Final encoder loss: 0.024281969293951988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11598920822143555 0.034178733825683594

Final encoder loss: 0.023755664005875587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11452102661132812 0.03492021560668945

Final encoder loss: 0.023350661620497704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11591410636901855 0.03402900695800781

Final encoder loss: 0.023509347811341286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11243295669555664 0.034377336502075195

Final encoder loss: 0.02326507680118084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11728668212890625 0.03377723693847656

Final encoder loss: 0.023696571588516235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11251950263977051 0.03339862823486328

Final encoder loss: 0.023400867357850075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11709117889404297 0.03461933135986328

Final encoder loss: 0.024185027927160263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11220073699951172 0.03413558006286621

Final encoder loss: 0.02375023625791073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11542224884033203 0.03524184226989746

Final encoder loss: 0.024681609123945236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11446404457092285 0.03410673141479492

Final encoder loss: 0.024264458566904068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11501550674438477 0.03408312797546387

Final encoder loss: 0.02479464001953602
Final encoder loss: 0.023053865879774094

Training case model
Final encoder loss: 0.037119962162409065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08911609649658203 0.2190999984741211

Final encoder loss: 0.038169727084815266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08905887603759766 0.21843695640563965

Final encoder loss: 0.03796890745183254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08867049217224121 0.21816492080688477

Final encoder loss: 0.036968308859742706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.0891876220703125 0.21834349632263184

Final encoder loss: 0.03582837293745218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08908224105834961 0.21863937377929688

Final encoder loss: 0.036407021420321165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08826303482055664 0.2183990478515625

Final encoder loss: 0.03612600559876854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08910655975341797 0.21849513053894043

Final encoder loss: 0.03586030186061669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08909845352172852 0.21838068962097168

Final encoder loss: 0.035985972628536776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08860969543457031 0.2185986042022705

Final encoder loss: 0.03655769320680103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08878731727600098 0.21857571601867676

Final encoder loss: 0.03635528391961624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08928394317626953 0.21858620643615723

Final encoder loss: 0.03650128392416949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08910465240478516 0.21828556060791016

Final encoder loss: 0.03629741564992891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08883118629455566 0.21848797798156738

Final encoder loss: 0.03549570423449787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08919429779052734 0.21898651123046875

Final encoder loss: 0.03577335714378408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08922529220581055 0.21931743621826172

Final encoder loss: 0.03676677809787705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08651494979858398 0.2160179615020752


Training case model
Final encoder loss: 0.20297355949878693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27240848541259766 0.052286624908447266

Final encoder loss: 0.18889236450195312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27774572372436523 0.052117109298706055

Final encoder loss: 0.19014543294906616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25948405265808105 0.05452322959899902

Final encoder loss: 0.19219130277633667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26770710945129395 0.051865339279174805

Final encoder loss: 0.18081265687942505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2691926956176758 0.05230593681335449

Final encoder loss: 0.19192907214164734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550625801086426 0.052556514739990234

Final encoder loss: 0.09684843569993973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2788071632385254 0.0520014762878418

Final encoder loss: 0.08723577111959457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2751452922821045 0.05328226089477539

Final encoder loss: 0.08334384858608246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2750403881072998 0.05136609077453613

Final encoder loss: 0.08288032561540604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26930713653564453 0.0528256893157959

Final encoder loss: 0.07631342113018036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26830410957336426 0.051770925521850586

Final encoder loss: 0.07875790446996689
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2655150890350342 0.05149030685424805

Final encoder loss: 0.05866716802120209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2746884822845459 0.05348038673400879

Final encoder loss: 0.053619105368852615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2684805393218994 0.05088663101196289

Final encoder loss: 0.05174501612782478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26964831352233887 0.05329132080078125

Final encoder loss: 0.05273725464940071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.29863929748535156 0.05253934860229492

Final encoder loss: 0.05035613849759102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2794315814971924 0.05296134948730469

Final encoder loss: 0.05136383697390556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26486921310424805 0.054445743560791016

Final encoder loss: 0.045463308691978455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2857081890106201 0.051961660385131836

Final encoder loss: 0.04287829250097275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2689032554626465 0.053827762603759766

Final encoder loss: 0.041515614837408066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2786269187927246 0.05182671546936035

Final encoder loss: 0.04296191409230232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2889275550842285 0.0529787540435791

Final encoder loss: 0.042412497103214264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26782941818237305 0.05363631248474121

Final encoder loss: 0.04211175814270973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2648930549621582 0.052104949951171875

Final encoder loss: 0.0415637232363224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27704501152038574 0.05291032791137695

Final encoder loss: 0.040545348078012466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2673337459564209 0.05157899856567383

Final encoder loss: 0.039491284638643265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2797377109527588 0.05212879180908203

Final encoder loss: 0.04066479951143265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567453384399414 0.05248141288757324

Final encoder loss: 0.041534680873155594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2671322822570801 0.05196714401245117

Final encoder loss: 0.04047822579741478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26283979415893555 0.05186295509338379

Final encoder loss: 0.04088229686021805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2973790168762207 0.0519709587097168

Final encoder loss: 0.04007575288414955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2725071907043457 0.05332517623901367

Final encoder loss: 0.039703693240880966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26721858978271484 0.05098414421081543

Final encoder loss: 0.040318846702575684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2722334861755371 0.05317044258117676

Final encoder loss: 0.04141126573085785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28826236724853516 0.051474809646606445

Final encoder loss: 0.04036811366677284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.265394926071167 0.05211448669433594

Final encoder loss: 0.0393013060092926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2753028869628906 0.05223655700683594

Final encoder loss: 0.03839467838406563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2760779857635498 0.05109834671020508

Final encoder loss: 0.03789268806576729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28600621223449707 0.05399966239929199

Final encoder loss: 0.0386454313993454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2743208408355713 0.05366683006286621

Final encoder loss: 0.03877381607890129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2671794891357422 0.053140878677368164

Final encoder loss: 0.03855495899915695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2640500068664551 0.05231618881225586

Final encoder loss: 0.038474228233098984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.297898530960083 0.05093193054199219

Final encoder loss: 0.037467312067747116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27809715270996094 0.051996707916259766

Final encoder loss: 0.036831084638834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2667708396911621 0.051772356033325195

Final encoder loss: 0.03789355233311653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27387499809265137 0.05304527282714844

Final encoder loss: 0.03866375982761383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2732982635498047 0.05164813995361328

Final encoder loss: 0.037614163011312485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2626912593841553 0.05169939994812012

Final encoder loss: 0.03769468888640404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2659282684326172 0.05178499221801758

Final encoder loss: 0.03708336129784584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27295899391174316 0.051886558532714844

Final encoder loss: 0.03652321174740791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27243947982788086 0.05181741714477539

Final encoder loss: 0.03710078448057175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27250242233276367 0.05212759971618652

Final encoder loss: 0.037986304610967636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2751936912536621 0.05165910720825195

Final encoder loss: 0.03727385029196739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25467538833618164 0.05242109298706055

Final encoder loss: 0.03761643171310425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27599620819091797 0.05228376388549805

Final encoder loss: 0.03665389493107796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26801228523254395 0.053009986877441406

Final encoder loss: 0.036415114998817444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2759847640991211 0.05305027961730957

Final encoder loss: 0.03719903901219368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2681291103363037 0.05262184143066406

Final encoder loss: 0.03769891336560249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.277390718460083 0.053160905838012695

Final encoder loss: 0.037054285407066345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2644176483154297 0.052069664001464844

Final encoder loss: 0.03692105785012245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27910900115966797 0.05259299278259277

Final encoder loss: 0.03631340712308884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2713353633880615 0.0550234317779541

Final encoder loss: 0.03576960042119026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2819218635559082 0.05228424072265625

Final encoder loss: 0.03618590161204338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2672121524810791 0.05243968963623047

Final encoder loss: 0.03699285537004471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2723526954650879 0.05106925964355469

Final encoder loss: 0.036439478397369385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26343226432800293 0.05259394645690918

Final encoder loss: 0.036846622824668884
Final encoder loss: 0.03535217419266701
Final encoder loss: 0.03384300693869591
Final encoder loss: 0.0337153859436512
Final encoder loss: 0.03294936567544937
Final encoder loss: 0.031116141006350517

Training emognition model
Final encoder loss: 0.0454056132138108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08134841918945312 0.22998595237731934

Final encoder loss: 0.04050405426603423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08022046089172363 0.23007559776306152

Final encoder loss: 0.04094416059760813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08069562911987305 0.23042559623718262

Final encoder loss: 0.04247459912871635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08078670501708984 0.22955751419067383

Final encoder loss: 0.04305917354471516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08066821098327637 0.22975564002990723

Final encoder loss: 0.041925115220793355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08063268661499023 0.2297360897064209

Final encoder loss: 0.042243448857101344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08015799522399902 0.22971343994140625

Final encoder loss: 0.04043683105162033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08067536354064941 0.229888916015625

Final encoder loss: 0.042495240564388616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.0810091495513916 0.23009872436523438

Final encoder loss: 0.04110204333784393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08021187782287598 0.2300887107849121

Final encoder loss: 0.040038940721492076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0809946060180664 0.22981476783752441

Final encoder loss: 0.04008797572242019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0816202163696289 0.23041200637817383

Final encoder loss: 0.038566375482112586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08233189582824707 0.23102712631225586

Final encoder loss: 0.039760850221086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08046460151672363 0.22939682006835938

Final encoder loss: 0.04223868716814231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08056426048278809 0.22930121421813965

Final encoder loss: 0.040015331079127636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0795755386352539 0.2286088466644287


Training emognition model
Final encoder loss: 0.19356730580329895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24994492530822754 0.04862856864929199

Final encoder loss: 0.1949598789215088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474963665008545 0.04841947555541992

Final encoder loss: 0.08111905306577682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24626779556274414 0.0481257438659668

Final encoder loss: 0.0803510993719101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24549555778503418 0.04994463920593262

Final encoder loss: 0.05508468672633171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2464447021484375 0.048829078674316406

Final encoder loss: 0.05358929559588432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24639201164245605 0.048471689224243164

Final encoder loss: 0.04494374990463257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2459850311279297 0.04861140251159668

Final encoder loss: 0.043913960456848145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24623966217041016 0.04820609092712402

Final encoder loss: 0.040312957018613815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24614548683166504 0.04832887649536133

Final encoder loss: 0.039560239762067795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24631237983703613 0.04775238037109375

Final encoder loss: 0.03822837024927139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24484467506408691 0.04800057411193848

Final encoder loss: 0.03766726329922676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24497079849243164 0.048065900802612305

Final encoder loss: 0.037678442895412445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24621939659118652 0.04840874671936035

Final encoder loss: 0.037205494940280914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470991611480713 0.04813838005065918

Final encoder loss: 0.03807816281914711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24611663818359375 0.04781460762023926

Final encoder loss: 0.03767029568552971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24595952033996582 0.04868602752685547

Final encoder loss: 0.03866347298026085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2455592155456543 0.04835104942321777

Final encoder loss: 0.03835221007466316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24688076972961426 0.04822587966918945

Final encoder loss: 0.03930824622511864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24594807624816895 0.049176692962646484

Final encoder loss: 0.03945974260568619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24584102630615234 0.04854917526245117

Final encoder loss: 0.03967535123229027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2456510066986084 0.04917263984680176

Final encoder loss: 0.03924671933054924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2454051971435547 0.04807567596435547

Final encoder loss: 0.03976406529545784
Final encoder loss: 0.037831008434295654

Training empatch model
Final encoder loss: 0.06796206277052887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07097125053405762 0.17289090156555176

Final encoder loss: 0.05680077101891597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07060432434082031 0.1729111671447754

Final encoder loss: 0.06182880747479236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0704336166381836 0.1727912425994873

Final encoder loss: 0.05999960223867289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0707848072052002 0.17299842834472656

Final encoder loss: 0.055127839367396146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07047677040100098 0.17303037643432617

Final encoder loss: 0.05547219316263469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0824575424194336 0.17225074768066406

Final encoder loss: 0.05473636444610802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07093429565429688 0.1727008819580078

Final encoder loss: 0.04723465938472019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07009243965148926 0.1722426414489746

Final encoder loss: 0.03932122521147573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07074403762817383 0.1729569435119629

Final encoder loss: 0.039474951567000885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07043933868408203 0.17305445671081543

Final encoder loss: 0.041441446691516316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0708017349243164 0.17280077934265137

Final encoder loss: 0.04042663020638187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07033443450927734 0.17238712310791016

Final encoder loss: 0.041606420728160155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0703437328338623 0.1729726791381836

Final encoder loss: 0.03941090932542624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07064223289489746 0.17257070541381836

Final encoder loss: 0.043065783726770175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07057070732116699 0.17288780212402344

Final encoder loss: 0.04118844801036503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07024145126342773 0.17261958122253418


Training empatch model
Final encoder loss: 0.17115800082683563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1764678955078125 0.043164730072021484

Final encoder loss: 0.07857285439968109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17591524124145508 0.043646812438964844

Final encoder loss: 0.05723116174340248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17516183853149414 0.043221473693847656

Final encoder loss: 0.04748225957155228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17549896240234375 0.04333686828613281

Final encoder loss: 0.04216337203979492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17461013793945312 0.04370284080505371

Final encoder loss: 0.03904273733496666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17556238174438477 0.04279375076293945

Final encoder loss: 0.037184979766607285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17520356178283691 0.043947458267211914

Final encoder loss: 0.036045338958501816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17519783973693848 0.04324913024902344

Final encoder loss: 0.035431064665317535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1751406192779541 0.043299198150634766

Final encoder loss: 0.035230882465839386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17524099349975586 0.043317556381225586

Final encoder loss: 0.03508159518241882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17516040802001953 0.0432438850402832

Final encoder loss: 0.03509128838777542

Training wesad model
Final encoder loss: 0.07269732435191859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07098174095153809 0.17283034324645996

Final encoder loss: 0.06254531900613759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07037568092346191 0.17281460762023926

Final encoder loss: 0.0648528119322246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07029962539672852 0.1728513240814209

Final encoder loss: 0.060919981573082045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07050228118896484 0.1726827621459961

Final encoder loss: 0.043418161899056895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07023143768310547 0.17281508445739746

Final encoder loss: 0.0466738103545994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07054948806762695 0.17265558242797852

Final encoder loss: 0.0444528086454044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0703270435333252 0.17280030250549316

Final encoder loss: 0.0433804599522334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0706627368927002 0.17336368560791016

Final encoder loss: 0.03579191700404321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0709528923034668 0.1727592945098877

Final encoder loss: 0.03616697022047978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07097864151000977 0.17328810691833496

Final encoder loss: 0.03559873667030053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07033205032348633 0.17309165000915527

Final encoder loss: 0.03602450307387907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07076787948608398 0.17280244827270508

Final encoder loss: 0.030298974549560515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07064294815063477 0.1733565330505371

Final encoder loss: 0.03118961502935528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07079887390136719 0.17301177978515625

Final encoder loss: 0.02959481525864265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0708005428314209 0.17324328422546387

Final encoder loss: 0.031132530080599328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07071161270141602 0.17325687408447266


Training wesad model
Final encoder loss: 0.21561118960380554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10529708862304688 0.03265738487243652

Final encoder loss: 0.09475736320018768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10466647148132324 0.03270149230957031

Final encoder loss: 0.06375297158956528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10355186462402344 0.03288865089416504

Final encoder loss: 0.04931000992655754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10302591323852539 0.033138275146484375

Final encoder loss: 0.041409362107515335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10367608070373535 0.03308701515197754

Final encoder loss: 0.036925751715898514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357928276062012 0.032814979553222656

Final encoder loss: 0.0342000387609005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10351300239562988 0.0328974723815918

Final encoder loss: 0.03264657035470009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10380339622497559 0.03271603584289551

Final encoder loss: 0.031767260283231735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10406827926635742 0.03308892250061035

Final encoder loss: 0.03154069185256958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10352468490600586 0.033013105392456055

Final encoder loss: 0.031966593116521835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365772247314453 0.032806396484375

Final encoder loss: 0.03291711211204529

Calculating loss for amigos model
	Full Pass 0.6364860534667969
numFreeParamsPath 18
Reconstruction loss values: 0.045118026435375214 0.055210668593645096

Calculating loss for dapper model
	Full Pass 0.15059375762939453
numFreeParamsPath 18
Reconstruction loss values: 0.038562215864658356 0.04169945791363716

Calculating loss for case model
	Full Pass 0.8551247119903564
numFreeParamsPath 18
Reconstruction loss values: 0.05260435491800308 0.05577696114778519

Calculating loss for emognition model
	Full Pass 0.28985166549682617
numFreeParamsPath 18
Reconstruction loss values: 0.05637727677822113 0.06361063569784164

Calculating loss for empatch model
	Full Pass 0.10416007041931152
numFreeParamsPath 18
Reconstruction loss values: 0.062116242945194244 0.06777232885360718

Calculating loss for wesad model
	Full Pass 0.07670140266418457
numFreeParamsPath 18
Reconstruction loss values: 0.06677255034446716 0.08926580846309662
Total loss calculation time: 3.71720552444458

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.4030985832214355
Total epoch time: 137.23197484016418

Epoch: 23

Training case model
Final encoder loss: 0.05082952627952378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09867024421691895 0.27185916900634766

Final encoder loss: 0.04617377686548886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0911250114440918 0.2651689052581787

Final encoder loss: 0.04639264607187799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09142374992370605 0.265488862991333

Final encoder loss: 0.04469003979545727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.0935063362121582 0.26535868644714355

Final encoder loss: 0.04323660801130815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09157538414001465 0.26520824432373047

Final encoder loss: 0.042122676070203945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0928797721862793 0.2666285037994385

Final encoder loss: 0.04122444454297752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09166431427001953 0.2654998302459717

Final encoder loss: 0.04142672724150687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09130215644836426 0.2664375305175781

Final encoder loss: 0.041207396959923925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0912935733795166 0.26545095443725586

Final encoder loss: 0.040856781551454016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09154701232910156 0.26576948165893555

Final encoder loss: 0.039325848826548195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09390044212341309 0.265791654586792

Final encoder loss: 0.039551338702231036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09163284301757812 0.26552820205688477

Final encoder loss: 0.039870018010991534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09240198135375977 0.2662320137023926

Final encoder loss: 0.03930076647693403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09156084060668945 0.2657201290130615

Final encoder loss: 0.03894107038161064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09108471870422363 0.26584744453430176

Final encoder loss: 0.03760506620493561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08904337882995605 0.2610299587249756


Training dapper model
Final encoder loss: 0.038867608292346194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.0625150203704834 0.15027475357055664

Final encoder loss: 0.034029584456984646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06158185005187988 0.15260910987854004

Final encoder loss: 0.03387896596687194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.0638129711151123 0.1519787311553955

Final encoder loss: 0.033186537838388826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06220412254333496 0.1512916088104248

Final encoder loss: 0.03795931386992115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06201028823852539 0.15077900886535645

Final encoder loss: 0.03244905148221136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062265634536743164 0.15255069732666016

Final encoder loss: 0.033152174425865016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06368470191955566 0.15187358856201172

Final encoder loss: 0.03661469984217093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06251668930053711 0.1509256362915039

Final encoder loss: 0.03768154914647555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06210637092590332 0.15052390098571777

Final encoder loss: 0.03508345747964566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06232738494873047 0.1515812873840332

Final encoder loss: 0.029800291158880356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06360769271850586 0.15278267860412598

Final encoder loss: 0.0330263776791169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.062401533126831055 0.15125322341918945

Final encoder loss: 0.0334460210957891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06229066848754883 0.15050172805786133

Final encoder loss: 0.03336872635512075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06234335899353027 0.15158629417419434

Final encoder loss: 0.030221137575300986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06322908401489258 0.15216612815856934

Final encoder loss: 0.03585944368071876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.0619196891784668 0.15005064010620117


Training emognition model
Final encoder loss: 0.05596459359311661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08370828628540039 0.27567434310913086

Final encoder loss: 0.05791296599859914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08478188514709473 0.2763974666595459

Final encoder loss: 0.053720981108742405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08370208740234375 0.2754392623901367

Final encoder loss: 0.051899591348940295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08331513404846191 0.27658510208129883

Final encoder loss: 0.05175584443798377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08290362358093262 0.2756619453430176

Final encoder loss: 0.05221959896963518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08308839797973633 0.2754549980163574

Final encoder loss: 0.04619518397900642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08474183082580566 0.27760791778564453

Final encoder loss: 0.054591153984204985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08373856544494629 0.2753114700317383

Final encoder loss: 0.05048699792419922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0838615894317627 0.2785189151763916

Final encoder loss: 0.04872805159323169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08303594589233398 0.27380990982055664

Final encoder loss: 0.050722286824772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08305621147155762 0.2757699489593506

Final encoder loss: 0.049726826014385196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0859537124633789 0.2750861644744873

Final encoder loss: 0.04976802286807978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08356809616088867 0.2748985290527344

Final encoder loss: 0.05044097517536477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08464527130126953 0.2762892246246338

Final encoder loss: 0.04883966859472579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08335709571838379 0.2766880989074707

Final encoder loss: 0.046465283519047065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08288693428039551 0.27527832984924316


Training amigos model
Final encoder loss: 0.04358241743261238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10839629173278809 0.3890223503112793

Final encoder loss: 0.043366899548444686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10797739028930664 0.38876843452453613

Final encoder loss: 0.04225909927495503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10794448852539062 0.38985538482666016

Final encoder loss: 0.0404909238244389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.1088871955871582 0.3899970054626465

Final encoder loss: 0.04095674934939971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.1080484390258789 0.3893604278564453

Final encoder loss: 0.04523251196522968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.11011075973510742 0.3901326656341553

Final encoder loss: 0.04105344190755551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10815286636352539 0.3898508548736572

Final encoder loss: 0.04143658829221639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.11008501052856445 0.38961029052734375

Final encoder loss: 0.0423426125721375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10832643508911133 0.38910865783691406

Final encoder loss: 0.04220465587254092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10882687568664551 0.38978004455566406

Final encoder loss: 0.04357558847392935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10946273803710938 0.3906850814819336

Final encoder loss: 0.04542696059350496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10876059532165527 0.38936829566955566

Final encoder loss: 0.03871352858264066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.11091375350952148 0.38904500007629395

Final encoder loss: 0.041666807301169465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10799145698547363 0.38942885398864746

Final encoder loss: 0.03648997872322293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10778617858886719 0.3898453712463379

Final encoder loss: 0.041050439276963745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10319733619689941 0.38448262214660645


Training amigos model
Final encoder loss: 0.029824805963245986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1063375473022461 0.3416464328765869

Final encoder loss: 0.02883968733505837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1070408821105957 0.3419170379638672

Final encoder loss: 0.03148211033178619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10648202896118164 0.34177589416503906

Final encoder loss: 0.03149855146800663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10804080963134766 0.34184980392456055

Final encoder loss: 0.032226109714102225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10594320297241211 0.3417203426361084

Final encoder loss: 0.030047818880346813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10750460624694824 0.34165167808532715

Final encoder loss: 0.029688702083058096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10561680793762207 0.3414478302001953

Final encoder loss: 0.029409371487004274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10615253448486328 0.3418915271759033

Final encoder loss: 0.0317806364610305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10637187957763672 0.3412454128265381

Final encoder loss: 0.030413392000197978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10528922080993652 0.34075021743774414

Final encoder loss: 0.030103119181036284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10531425476074219 0.3407621383666992

Final encoder loss: 0.029333910659746422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10571455955505371 0.34073948860168457

Final encoder loss: 0.030175608593432327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10570669174194336 0.3407289981842041

Final encoder loss: 0.031327982993402705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10550236701965332 0.34085750579833984

Final encoder loss: 0.030470592972176965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10518836975097656 0.3404805660247803

Final encoder loss: 0.03262570290748546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10027003288269043 0.33698558807373047


Training amigos model
Final encoder loss: 0.1807621866464615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4649364948272705 0.07510781288146973

Final encoder loss: 0.1878351867198944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4667949676513672 0.0794210433959961

Final encoder loss: 0.18363071978092194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46628403663635254 0.07584953308105469

Final encoder loss: 0.07083956152200699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4615941047668457 0.08065676689147949

Final encoder loss: 0.07215940207242966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46445775032043457 0.07682371139526367

Final encoder loss: 0.06621764600276947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46434760093688965 0.0751197338104248

Final encoder loss: 0.043927449733018875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4642040729522705 0.07932472229003906

Final encoder loss: 0.04440627992153168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4626579284667969 0.08048295974731445

Final encoder loss: 0.04196900501847267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46417927742004395 0.07285428047180176

Final encoder loss: 0.03464947268366814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4547305107116699 0.07527732849121094

Final encoder loss: 0.03515546768903732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45475316047668457 0.07653164863586426

Final encoder loss: 0.03387744352221489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4581942558288574 0.07230973243713379

Final encoder loss: 0.03113216906785965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4564332962036133 0.07503080368041992

Final encoder loss: 0.03158513084053993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45635199546813965 0.07558774948120117

Final encoder loss: 0.030762366950511932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46570348739624023 0.07623791694641113

Final encoder loss: 0.030733121559023857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46735548973083496 0.0754387378692627

Final encoder loss: 0.031313687562942505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4643971920013428 0.07562518119812012

Final encoder loss: 0.030791785567998886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4650402069091797 0.07344460487365723

Final encoder loss: 0.031800344586372375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46453046798706055 0.08066987991333008

Final encoder loss: 0.03166933357715607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47086071968078613 0.07868051528930664

Final encoder loss: 0.03194408491253853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4660682678222656 0.07353782653808594

Final encoder loss: 0.03202361613512039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4669196605682373 0.07717251777648926

Final encoder loss: 0.03220166265964508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4682765007019043 0.07718420028686523

Final encoder loss: 0.032225146889686584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47122669219970703 0.07465744018554688

Final encoder loss: 0.03095151297748089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4638826847076416 0.07700228691101074

Final encoder loss: 0.03113846853375435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46561169624328613 0.08082032203674316

Final encoder loss: 0.030992448329925537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46714282035827637 0.07402992248535156

Final encoder loss: 0.030663728713989258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46864914894104004 0.07561039924621582

Final encoder loss: 0.03045196644961834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4639551639556885 0.07912683486938477

Final encoder loss: 0.03072456084191799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46694016456604004 0.08027005195617676

Final encoder loss: 0.03034975193440914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46797657012939453 0.07587623596191406

Final encoder loss: 0.030189814046025276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4677267074584961 0.07644319534301758

Final encoder loss: 0.030710529536008835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46282505989074707 0.07604598999023438

Final encoder loss: 0.03043559193611145
Final encoder loss: 0.02889682538807392
Final encoder loss: 0.02802124246954918

Training dapper model
Final encoder loss: 0.02534791919460896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06104421615600586 0.1080160140991211

Final encoder loss: 0.026669986685952683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05982255935668945 0.10769033432006836

Final encoder loss: 0.028525155778199897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059801340103149414 0.10724425315856934

Final encoder loss: 0.02461748463336464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.060243844985961914 0.10815882682800293

Final encoder loss: 0.026811401510653126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06006264686584473 0.10696530342102051

Final encoder loss: 0.02407198429385869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060157060623168945 0.10717296600341797

Final encoder loss: 0.025835019624470452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05936574935913086 0.1075143814086914

Final encoder loss: 0.0247363182381306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06061983108520508 0.10782432556152344

Final encoder loss: 0.025855700016582443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05996894836425781 0.10750627517700195

Final encoder loss: 0.025834862763581254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05977940559387207 0.10719442367553711

Final encoder loss: 0.025353976562982935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060132503509521484 0.1082618236541748

Final encoder loss: 0.02559850562103228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05959153175354004 0.10753870010375977

Final encoder loss: 0.024230859237155896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05982017517089844 0.10729455947875977

Final encoder loss: 0.025411978264940852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05917525291442871 0.10784626007080078

Final encoder loss: 0.024790949442903443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.060920000076293945 0.10698699951171875

Final encoder loss: 0.026134926934346582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059435367584228516 0.10739850997924805


Training dapper model
Final encoder loss: 0.202444925904274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1188349723815918 0.034485578536987305

Final encoder loss: 0.20819763839244843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11931943893432617 0.03452277183532715

Final encoder loss: 0.07355856895446777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11660075187683105 0.03475809097290039

Final encoder loss: 0.07421790808439255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11616277694702148 0.033527374267578125

Final encoder loss: 0.04452820122241974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11779260635375977 0.03497457504272461

Final encoder loss: 0.043831415474414825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11596035957336426 0.03385806083679199

Final encoder loss: 0.03281155973672867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11635994911193848 0.03426957130432129

Final encoder loss: 0.03245808929204941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11654543876647949 0.03493833541870117

Final encoder loss: 0.027304118499159813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11713027954101562 0.03368854522705078

Final encoder loss: 0.02710365504026413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11647915840148926 0.03380179405212402

Final encoder loss: 0.02470218576490879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11574983596801758 0.034842491149902344

Final encoder loss: 0.024534735828638077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11830568313598633 0.033820152282714844

Final encoder loss: 0.023504842072725296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1165010929107666 0.03404855728149414

Final encoder loss: 0.023530296981334686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11545014381408691 0.03401303291320801

Final encoder loss: 0.02335279807448387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11804866790771484 0.03490471839904785

Final encoder loss: 0.02322034165263176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11594057083129883 0.033974409103393555

Final encoder loss: 0.023632757365703583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11564254760742188 0.03387260437011719

Final encoder loss: 0.0229344442486763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1168816089630127 0.03424572944641113

Final encoder loss: 0.02450413629412651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11689305305480957 0.034285783767700195

Final encoder loss: 0.023106927052140236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152501106262207 0.03437471389770508

Final encoder loss: 0.024200106039643288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11629557609558105 0.03542375564575195

Final encoder loss: 0.023804716765880585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11785292625427246 0.03388547897338867

Final encoder loss: 0.024317195639014244
Final encoder loss: 0.022681059315800667

Training case model
Final encoder loss: 0.03732025413388535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08915996551513672 0.21925020217895508

Final encoder loss: 0.03583599870211651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08910036087036133 0.21898603439331055

Final encoder loss: 0.03586499526385143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08943367004394531 0.21946048736572266

Final encoder loss: 0.035765520561776044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08935284614562988 0.218888521194458

Final encoder loss: 0.0348735596472285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08995509147644043 0.2191755771636963

Final encoder loss: 0.0353740079110711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08896899223327637 0.21927165985107422

Final encoder loss: 0.0351733131735971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09021449089050293 0.21941542625427246

Final encoder loss: 0.03524936404432863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08901238441467285 0.21917343139648438

Final encoder loss: 0.03486271638285109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08996891975402832 0.21942472457885742

Final encoder loss: 0.035536530586460155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08919644355773926 0.21935796737670898

Final encoder loss: 0.035698727250631626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08900833129882812 0.2192068099975586

Final encoder loss: 0.03534128767573021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08913111686706543 0.21917343139648438

Final encoder loss: 0.03420195703456058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08909726142883301 0.21932482719421387

Final encoder loss: 0.03524241876442867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08915591239929199 0.21931242942810059

Final encoder loss: 0.03463848866925097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08901047706604004 0.21914219856262207

Final encoder loss: 0.03504263042607633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08630084991455078 0.2162032127380371


Training case model
Final encoder loss: 0.20295248925685883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26140761375427246 0.05330467224121094

Final encoder loss: 0.18891079723834991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26157045364379883 0.05199885368347168

Final encoder loss: 0.19015488028526306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594718933105469 0.05189371109008789

Final encoder loss: 0.19219796359539032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26884913444519043 0.0507354736328125

Final encoder loss: 0.180813729763031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26787328720092773 0.05229616165161133

Final encoder loss: 0.1919317990541458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255906343460083 0.05085492134094238

Final encoder loss: 0.09804466366767883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576122283935547 0.052431583404541016

Final encoder loss: 0.08800657838582993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26018238067626953 0.05490708351135254

Final encoder loss: 0.0842851996421814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2675604820251465 0.05215287208557129

Final encoder loss: 0.08366264402866364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25946855545043945 0.05289149284362793

Final encoder loss: 0.07611233741044998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25905728340148926 0.05192303657531738

Final encoder loss: 0.0793018713593483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25467681884765625 0.05303239822387695

Final encoder loss: 0.05907457321882248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26760387420654297 0.052019357681274414

Final encoder loss: 0.053938109427690506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27054429054260254 0.0519099235534668

Final encoder loss: 0.052122753113508224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26964306831359863 0.05199837684631348

Final encoder loss: 0.05307038128376007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.267153263092041 0.05372452735900879

Final encoder loss: 0.04993709921836853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25859546661376953 0.051845550537109375

Final encoder loss: 0.05134589970111847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556600570678711 0.05265474319458008

Final encoder loss: 0.045310866087675095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25908613204956055 0.05251646041870117

Final encoder loss: 0.04278266802430153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25955867767333984 0.05175471305847168

Final encoder loss: 0.041502080857753754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25925111770629883 0.052747488021850586

Final encoder loss: 0.04277341440320015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2595205307006836 0.0526273250579834

Final encoder loss: 0.04192187637090683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25824546813964844 0.053398847579956055

Final encoder loss: 0.04167266935110092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545044422149658 0.05313277244567871

Final encoder loss: 0.04112211614847183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571249008178711 0.0515751838684082

Final encoder loss: 0.03977461904287338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25716304779052734 0.05204486846923828

Final encoder loss: 0.03900628164410591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572634220123291 0.05142045021057129

Final encoder loss: 0.040222931653261185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25589704513549805 0.05205678939819336

Final encoder loss: 0.04069368541240692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26738429069519043 0.05184364318847656

Final encoder loss: 0.03987725079059601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25450968742370605 0.050806522369384766

Final encoder loss: 0.03981677442789078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25695013999938965 0.05208992958068848

Final encoder loss: 0.038753289729356766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25710606575012207 0.051209211349487305

Final encoder loss: 0.03861202672123909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26618504524230957 0.051764726638793945

Final encoder loss: 0.03950128331780434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583327293395996 0.052730560302734375

Final encoder loss: 0.03991391137242317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581779956817627 0.05212759971618652

Final encoder loss: 0.039458077400922775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25715112686157227 0.05103921890258789

Final encoder loss: 0.038389429450035095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2689168453216553 0.054646968841552734

Final encoder loss: 0.0375324971973896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26823902130126953 0.05178213119506836

Final encoder loss: 0.03730766475200653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2684788703918457 0.05135774612426758

Final encoder loss: 0.037636835128068924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575345039367676 0.053461313247680664

Final encoder loss: 0.03805708885192871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26923084259033203 0.05191469192504883

Final encoder loss: 0.03744523599743843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550022602081299 0.05207228660583496

Final encoder loss: 0.03748972713947296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25777244567871094 0.051950931549072266

Final encoder loss: 0.03638115152716637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25912976264953613 0.05239105224609375

Final encoder loss: 0.03604437783360481
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25870800018310547 0.05238938331604004

Final encoder loss: 0.03700464218854904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25765061378479004 0.0539853572845459

Final encoder loss: 0.037490397691726685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25644874572753906 0.052274465560913086

Final encoder loss: 0.036760661751031876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559499740600586 0.05155658721923828

Final encoder loss: 0.03700640797615051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587890625 0.05135202407836914

Final encoder loss: 0.035996656864881516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582383155822754 0.053690195083618164

Final encoder loss: 0.035783618688583374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26766467094421387 0.05199313163757324

Final encoder loss: 0.03636620566248894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579832077026367 0.05184006690979004

Final encoder loss: 0.037004489451646805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575359344482422 0.0529637336730957

Final encoder loss: 0.0361693799495697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2575490474700928 0.051401376724243164

Final encoder loss: 0.03660399466753006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25824451446533203 0.05368351936340332

Final encoder loss: 0.035779256373643875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579624652862549 0.052587032318115234

Final encoder loss: 0.035436566919088364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26799464225769043 0.05235123634338379

Final encoder loss: 0.03611176088452339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570202350616455 0.05085420608520508

Final encoder loss: 0.03650933504104614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2694425582885742 0.05485367774963379

Final encoder loss: 0.036049794405698776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25448012351989746 0.050710201263427734

Final encoder loss: 0.03603142499923706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25871896743774414 0.05219531059265137

Final encoder loss: 0.03518873080611229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25751614570617676 0.05202150344848633

Final encoder loss: 0.03494461253285408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566385269165039 0.05129528045654297

Final encoder loss: 0.035407036542892456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257066011428833 0.051558494567871094

Final encoder loss: 0.03596058860421181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2566969394683838 0.0524594783782959

Final encoder loss: 0.03522839397192001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2531743049621582 0.05218768119812012

Final encoder loss: 0.03573211655020714
Final encoder loss: 0.03428115323185921
Final encoder loss: 0.03314409404993057
Final encoder loss: 0.03278931975364685
Final encoder loss: 0.03193225711584091
Final encoder loss: 0.030344296246767044

Training emognition model
Final encoder loss: 0.038587089236532944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08215188980102539 0.2310774326324463

Final encoder loss: 0.04080943274901769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08076691627502441 0.23076248168945312

Final encoder loss: 0.037762209471473206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08124494552612305 0.23076772689819336

Final encoder loss: 0.038478187441078705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08143305778503418 0.23058676719665527

Final encoder loss: 0.040459388961777275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08134770393371582 0.2306821346282959

Final encoder loss: 0.04065857943920854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08173203468322754 0.23119449615478516

Final encoder loss: 0.040043537402181764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.0810399055480957 0.23085665702819824

Final encoder loss: 0.041808416879069714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08105278015136719 0.2307446002960205

Final encoder loss: 0.03848161953272241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08116936683654785 0.23104405403137207

Final encoder loss: 0.041130501128930484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08121323585510254 0.23041892051696777

Final encoder loss: 0.038837583565103716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08162498474121094 0.23096895217895508

Final encoder loss: 0.038980034606026025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08093929290771484 0.23075532913208008

Final encoder loss: 0.0414388664431026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08141779899597168 0.23064160346984863

Final encoder loss: 0.04026373531891023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08095145225524902 0.22914624214172363

Final encoder loss: 0.04003133749015482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08038496971130371 0.22907519340515137

Final encoder loss: 0.0434007430110568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07934427261352539 0.22870159149169922


Training emognition model
Final encoder loss: 0.1935724914073944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24911999702453613 0.04886817932128906

Final encoder loss: 0.19496867060661316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24574589729309082 0.0486302375793457

Final encoder loss: 0.08211775869131088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24665117263793945 0.049149513244628906

Final encoder loss: 0.08151968568563461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24532103538513184 0.049542903900146484

Final encoder loss: 0.055275049060583115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24637651443481445 0.04895663261413574

Final encoder loss: 0.053831856697797775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2444150447845459 0.04850149154663086

Final encoder loss: 0.04469026252627373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24611186981201172 0.04897618293762207

Final encoder loss: 0.04373437911272049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24578547477722168 0.04921221733093262

Final encoder loss: 0.03992047533392906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496495246887207 0.04877328872680664

Final encoder loss: 0.039221879094839096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24511146545410156 0.049421072006225586

Final encoder loss: 0.0377555750310421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2470989227294922 0.04811453819274902

Final encoder loss: 0.03720584139227867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24504995346069336 0.04957771301269531

Final encoder loss: 0.03695285692811012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24797797203063965 0.04894733428955078

Final encoder loss: 0.036621034145355225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24619793891906738 0.04855012893676758

Final encoder loss: 0.037011753767728806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24780654907226562 0.048895835876464844

Final encoder loss: 0.03704427555203438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24555301666259766 0.04843926429748535

Final encoder loss: 0.037482328712940216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2504751682281494 0.05076432228088379

Final encoder loss: 0.037529993802309036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24927067756652832 0.04844021797180176

Final encoder loss: 0.03769214451313019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24685215950012207 0.04962468147277832

Final encoder loss: 0.03822731599211693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24606823921203613 0.04909801483154297

Final encoder loss: 0.03778160363435745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24735355377197266 0.04903531074523926

Final encoder loss: 0.03815191611647606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2467787265777588 0.048426151275634766

Final encoder loss: 0.038184355944395065
Final encoder loss: 0.03669067099690437

Training empatch model
Final encoder loss: 0.058597373795212244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0705559253692627 0.17329978942871094

Final encoder loss: 0.061642500379050384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07104849815368652 0.1739654541015625

Final encoder loss: 0.05692807416406901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07135152816772461 0.17410850524902344

Final encoder loss: 0.05713143050955418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07111740112304688 0.17346811294555664

Final encoder loss: 0.04895187215333813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07117724418640137 0.17449498176574707

Final encoder loss: 0.05347346422100257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07079243659973145 0.1733107566833496

Final encoder loss: 0.053019843379583216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07076096534729004 0.17258715629577637

Final encoder loss: 0.048589450256844934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07008099555969238 0.1733384132385254

Final encoder loss: 0.03935974717454908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07112360000610352 0.17331600189208984

Final encoder loss: 0.04004922731124397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07163214683532715 0.17370939254760742

Final encoder loss: 0.04037629094699519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07088541984558105 0.17348051071166992

Final encoder loss: 0.03929941903889055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07078909873962402 0.1728360652923584

Final encoder loss: 0.03825488376293742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0708615779876709 0.17236328125

Final encoder loss: 0.03627789409218569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07070398330688477 0.17303252220153809

Final encoder loss: 0.039668815353719016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07053875923156738 0.17235183715820312

Final encoder loss: 0.041264382032461716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07017946243286133 0.17314481735229492


Training empatch model
Final encoder loss: 0.1711578667163849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17711734771728516 0.04311490058898926

Final encoder loss: 0.07842876017093658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17465424537658691 0.043373823165893555

Final encoder loss: 0.05672728642821312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17440438270568848 0.04378247261047363

Final encoder loss: 0.04683220013976097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17489051818847656 0.044266462326049805

Final encoder loss: 0.041418302804231644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750965118408203 0.04350996017456055

Final encoder loss: 0.03819990158081055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17420315742492676 0.043280839920043945

Final encoder loss: 0.03623112663626671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504453659057617 0.04393649101257324

Final encoder loss: 0.03507065773010254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17520737648010254 0.04389190673828125

Final encoder loss: 0.03432415798306465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17461919784545898 0.04401397705078125

Final encoder loss: 0.034001898020505905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17554974555969238 0.04387664794921875

Final encoder loss: 0.033941954374313354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752481460571289 0.04368019104003906

Final encoder loss: 0.03391769900918007

Training wesad model
Final encoder loss: 0.06411283735290627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07068514823913574 0.17291641235351562

Final encoder loss: 0.06406022802753249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0706627368927002 0.17272686958312988

Final encoder loss: 0.059027228240646545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07036972045898438 0.1728360652923584

Final encoder loss: 0.05765196759988763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0703897476196289 0.1729426383972168

Final encoder loss: 0.04069032556579857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07062816619873047 0.17291641235351562

Final encoder loss: 0.04350876961057482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07037949562072754 0.17306995391845703

Final encoder loss: 0.03924265250362857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07059288024902344 0.1743617057800293

Final encoder loss: 0.04402425016447658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07084512710571289 0.17239665985107422

Final encoder loss: 0.03523483992296529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07040119171142578 0.17296838760375977

Final encoder loss: 0.03229528176285431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0706634521484375 0.17243504524230957

Final encoder loss: 0.033239472669271235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07061982154846191 0.17324185371398926

Final encoder loss: 0.03459606638251294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07059526443481445 0.17312026023864746

Final encoder loss: 0.03023020673789981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07060003280639648 0.172257661819458

Final encoder loss: 0.027253650003782055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07033514976501465 0.17283892631530762

Final encoder loss: 0.028410130863697997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07060790061950684 0.17276787757873535

Final encoder loss: 0.0302275888462635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07032060623168945 0.17310023307800293


Training wesad model
Final encoder loss: 0.2155759483575821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10523653030395508 0.0327301025390625

Final encoder loss: 0.09358428418636322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10490298271179199 0.033026695251464844

Final encoder loss: 0.06275446712970734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10253381729125977 0.0327143669128418

Final encoder loss: 0.04827907308936119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10394978523254395 0.03278350830078125

Final encoder loss: 0.040337156504392624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10396552085876465 0.033022165298461914

Final encoder loss: 0.035826850682497025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038975715637207 0.03311872482299805

Final encoder loss: 0.033210478723049164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10246610641479492 0.03282976150512695

Final encoder loss: 0.03167908266186714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10343170166015625 0.032616615295410156

Final encoder loss: 0.03090388886630535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10398006439208984 0.03301501274108887

Final encoder loss: 0.03059537336230278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1027517318725586 0.03304862976074219

Final encoder loss: 0.030679361894726753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10248255729675293 0.03307366371154785

Final encoder loss: 0.031043637543916702

Calculating loss for amigos model
	Full Pass 0.6813032627105713
numFreeParamsPath 18
Reconstruction loss values: 0.04360632225871086 0.05424322560429573

Calculating loss for dapper model
	Full Pass 0.15069341659545898
numFreeParamsPath 18
Reconstruction loss values: 0.03709874674677849 0.04036988690495491

Calculating loss for case model
	Full Pass 0.8560986518859863
numFreeParamsPath 18
Reconstruction loss values: 0.05177967995405197 0.054847367107868195

Calculating loss for emognition model
	Full Pass 0.29141712188720703
numFreeParamsPath 18
Reconstruction loss values: 0.05494632571935654 0.06216094270348549

Calculating loss for empatch model
	Full Pass 0.10426926612854004
numFreeParamsPath 18
Reconstruction loss values: 0.05957621708512306 0.06568153202533722

Calculating loss for wesad model
	Full Pass 0.07679224014282227
numFreeParamsPath 18
Reconstruction loss values: 0.06465262174606323 0.08745376765727997
Total loss calculation time: 3.7870709896087646

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.811466932296753
Total epoch time: 137.2685203552246

Epoch: 24

Training dapper model
Final encoder loss: 0.039077629749375994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06611204147338867 0.15418124198913574

Final encoder loss: 0.03413896655094855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06164288520812988 0.1496291160583496

Final encoder loss: 0.031640359867241855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06147623062133789 0.14915728569030762

Final encoder loss: 0.03355195004002233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.0611574649810791 0.14922165870666504

Final encoder loss: 0.0331419892047778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06145143508911133 0.1491701602935791

Final encoder loss: 0.033427292228041645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.061435699462890625 0.14923095703125

Final encoder loss: 0.029454685344861377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06158852577209473 0.14924120903015137

Final encoder loss: 0.032402347758515096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06134438514709473 0.14912986755371094

Final encoder loss: 0.03077720518614637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06140613555908203 0.14880061149597168

Final encoder loss: 0.03308707444363899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06108689308166504 0.148848295211792

Final encoder loss: 0.03127767862178945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06137990951538086 0.1493229866027832

Final encoder loss: 0.02903275316191516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061379194259643555 0.14934992790222168

Final encoder loss: 0.03386562823533975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0616145133972168 0.1490950584411621

Final encoder loss: 0.037046076793222645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06105470657348633 0.14893388748168945

Final encoder loss: 0.030274957035495806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.0613713264465332 0.14874839782714844

Final encoder loss: 0.03393899098753742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06121659278869629 0.14804863929748535


Training emognition model
Final encoder loss: 0.05551881134701669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08309555053710938 0.27388739585876465

Final encoder loss: 0.05216654700024987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08301520347595215 0.2739827632904053

Final encoder loss: 0.05351520062986543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08312606811523438 0.27324676513671875

Final encoder loss: 0.053081217172956996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08312559127807617 0.2743966579437256

Final encoder loss: 0.04762771887637818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0831899642944336 0.2737751007080078

Final encoder loss: 0.049973695009424224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08280110359191895 0.2737998962402344

Final encoder loss: 0.049405536642082526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08279728889465332 0.2745227813720703

Final encoder loss: 0.05004031181901071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08317327499389648 0.2738802433013916

Final encoder loss: 0.04910602125510232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08304381370544434 0.2731962203979492

Final encoder loss: 0.04922619568986856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0832376480102539 0.2743692398071289

Final encoder loss: 0.04722633070569731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08260464668273926 0.2740302085876465

Final encoder loss: 0.04721029919980419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08311057090759277 0.2742886543273926

Final encoder loss: 0.04924641044875391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08308935165405273 0.27413439750671387

Final encoder loss: 0.046883136172892054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08287787437438965 0.27384090423583984

Final encoder loss: 0.04657345730552759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08313274383544922 0.27347755432128906

Final encoder loss: 0.049516922112495386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08254575729370117 0.2723979949951172


Training case model
Final encoder loss: 0.05166223691375062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09161376953125 0.26398539543151855

Final encoder loss: 0.046966650999095076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09104228019714355 0.26506733894348145

Final encoder loss: 0.04415020607511627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09121131896972656 0.26467108726501465

Final encoder loss: 0.0437428664003749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09118366241455078 0.26817965507507324

Final encoder loss: 0.04325929182852164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09110641479492188 0.2645125389099121

Final encoder loss: 0.04327140428864931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09101581573486328 0.2642245292663574

Final encoder loss: 0.04109693143608608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09100770950317383 0.2640094757080078

Final encoder loss: 0.04135972843934539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09112906455993652 0.26415491104125977

Final encoder loss: 0.0419448081744743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09455537796020508 0.2644844055175781

Final encoder loss: 0.039818057599598194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09122037887573242 0.26495814323425293

Final encoder loss: 0.03909281894179085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09089541435241699 0.2636268138885498

Final encoder loss: 0.03810847093820057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09086966514587402 0.2646822929382324

Final encoder loss: 0.039147200201154524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09111356735229492 0.2647669315338135

Final encoder loss: 0.03922548373281619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09164571762084961 0.2650296688079834

Final encoder loss: 0.03944418352708138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09224653244018555 0.266859769821167

Final encoder loss: 0.03752506875813012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08852815628051758 0.2636244297027588


Training amigos model
Final encoder loss: 0.04563711174730044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10834813117980957 0.3892171382904053

Final encoder loss: 0.043060116200878684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10832905769348145 0.389556884765625

Final encoder loss: 0.039400161621734785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.11158895492553711 0.39102625846862793

Final encoder loss: 0.038015953556088236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10860776901245117 0.3927574157714844

Final encoder loss: 0.040236119746246905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.11067008972167969 0.39119672775268555

Final encoder loss: 0.04230474847931779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10808992385864258 0.388927698135376

Final encoder loss: 0.041329491897323684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10904598236083984 0.38903260231018066

Final encoder loss: 0.0442273007415879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10846805572509766 0.38993382453918457

Final encoder loss: 0.04072249096782275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10884451866149902 0.39004063606262207

Final encoder loss: 0.039686562667752716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10898709297180176 0.3907291889190674

Final encoder loss: 0.041700825749104324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1086568832397461 0.39008450508117676

Final encoder loss: 0.04062168849006163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.11011290550231934 0.3905599117279053

Final encoder loss: 0.04178220307718234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10876131057739258 0.3899869918823242

Final encoder loss: 0.04049975791846795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10805106163024902 0.3899681568145752

Final encoder loss: 0.03689695210303046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10845375061035156 0.3904707431793213

Final encoder loss: 0.04215087851978014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10374975204467773 0.3841862678527832


Training amigos model
Final encoder loss: 0.02874345096751214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10788488388061523 0.3418741226196289

Final encoder loss: 0.026708742078502084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10629034042358398 0.3417696952819824

Final encoder loss: 0.028774612954832423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10756993293762207 0.3417189121246338

Final encoder loss: 0.030643731029037405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10642290115356445 0.3417034149169922

Final encoder loss: 0.029400740254872767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10788702964782715 0.3421180248260498

Final encoder loss: 0.029799578555203658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10629606246948242 0.34354424476623535

Final encoder loss: 0.0310865983410907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10729646682739258 0.34192371368408203

Final encoder loss: 0.029737887088100403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1061861515045166 0.3417387008666992

Final encoder loss: 0.02925827759118344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10826396942138672 0.34163379669189453

Final encoder loss: 0.02992428858395031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10617828369140625 0.3419351577758789

Final encoder loss: 0.028808067773535017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10705852508544922 0.3414926528930664

Final encoder loss: 0.030632460236077616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10605454444885254 0.34198594093322754

Final encoder loss: 0.030397994004734766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10707330703735352 0.3415653705596924

Final encoder loss: 0.026980371433339184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10627961158752441 0.3422691822052002

Final encoder loss: 0.028877430303743026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10677647590637207 0.3415052890777588

Final encoder loss: 0.031237859061781597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10121369361877441 0.3387587070465088


Training amigos model
Final encoder loss: 0.1807539314031601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47594213485717773 0.08063125610351562

Final encoder loss: 0.18783684074878693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46852540969848633 0.07571530342102051

Final encoder loss: 0.18364861607551575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4628591537475586 0.07561922073364258

Final encoder loss: 0.07174821943044662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4685542583465576 0.07831430435180664

Final encoder loss: 0.07269825041294098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4778299331665039 0.07747149467468262

Final encoder loss: 0.06742091476917267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4713315963745117 0.07493233680725098

Final encoder loss: 0.04391530528664589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4707832336425781 0.0798790454864502

Final encoder loss: 0.043998003005981445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46829867362976074 0.07648873329162598

Final encoder loss: 0.042089421302080154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46541571617126465 0.07588076591491699

Final encoder loss: 0.03414702042937279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46508216857910156 0.07509589195251465

Final encoder loss: 0.034274451434612274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47330617904663086 0.07672405242919922

Final encoder loss: 0.03369000926613808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47220301628112793 0.07236790657043457

Final encoder loss: 0.03037155419588089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47319555282592773 0.08336210250854492

Final encoder loss: 0.030450807884335518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47061610221862793 0.08112931251525879

Final encoder loss: 0.030317846685647964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4647083282470703 0.07711529731750488

Final encoder loss: 0.02952381782233715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4678609371185303 0.07557487487792969

Final encoder loss: 0.029662542045116425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46718358993530273 0.0769045352935791

Final encoder loss: 0.029612280428409576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47244691848754883 0.07531046867370605

Final encoder loss: 0.030210865661501884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47463274002075195 0.07462882995605469

Final encoder loss: 0.030028607696294785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4627363681793213 0.07259440422058105

Final encoder loss: 0.030555427074432373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45620083808898926 0.07407712936401367

Final encoder loss: 0.030526410788297653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4601008892059326 0.07508349418640137

Final encoder loss: 0.030401891097426414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4587116241455078 0.07485342025756836

Final encoder loss: 0.03095695562660694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45645833015441895 0.07294249534606934

Final encoder loss: 0.02954099141061306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46811342239379883 0.07660174369812012

Final encoder loss: 0.02969420701265335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4671895503997803 0.07896971702575684

Final encoder loss: 0.029957380145788193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4666266441345215 0.07625484466552734

Final encoder loss: 0.02906428650021553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46814680099487305 0.07568836212158203

Final encoder loss: 0.029269050806760788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47066569328308105 0.07250380516052246

Final encoder loss: 0.02928900718688965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654266834259033 0.0780637264251709

Final encoder loss: 0.02891095168888569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4702742099761963 0.07573485374450684

Final encoder loss: 0.028677862137556076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680063724517822 0.07654023170471191

Final encoder loss: 0.02928169257938862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46678614616394043 0.07465577125549316

Final encoder loss: 0.029140543192625046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4698216915130615 0.07419347763061523

Final encoder loss: 0.028877178207039833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45880913734436035 0.07585668563842773

Final encoder loss: 0.029602834954857826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4571356773376465 0.07378029823303223

Final encoder loss: 0.02852414734661579
Final encoder loss: 0.0269716065376997
Final encoder loss: 0.026538021862506866

Training dapper model
Final encoder loss: 0.02645789529844165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05975604057312012 0.10674238204956055

Final encoder loss: 0.026136748972087582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05938887596130371 0.1073305606842041

Final encoder loss: 0.026830264581703536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.07698225975036621 0.10594511032104492

Final encoder loss: 0.024288851656538085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05935215950012207 0.10729813575744629

Final encoder loss: 0.027276206424237007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060068368911743164 0.10703635215759277

Final encoder loss: 0.022839864844684633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05957388877868652 0.10735845565795898

Final encoder loss: 0.02417713799403447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.06118464469909668 0.10779285430908203

Final encoder loss: 0.025166289482703944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06014537811279297 0.10777115821838379

Final encoder loss: 0.024185252367211356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06007981300354004 0.10689330101013184

Final encoder loss: 0.02247024000702238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06051325798034668 0.10815572738647461

Final encoder loss: 0.027016349280594428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06020212173461914 0.1075284481048584

Final encoder loss: 0.024998303810396784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06036496162414551 0.10710000991821289

Final encoder loss: 0.027027899534919415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05962705612182617 0.10731935501098633

Final encoder loss: 0.024842711259556758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.0607600212097168 0.10766434669494629

Final encoder loss: 0.025779922103577636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05989217758178711 0.10760736465454102

Final encoder loss: 0.029997891805719403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.0593104362487793 0.10634231567382812


Training dapper model
Final encoder loss: 0.20243415236473083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11980271339416504 0.034317731857299805

Final encoder loss: 0.2082114964723587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1161351203918457 0.03460407257080078

Final encoder loss: 0.07407789677381516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1163778305053711 0.03432154655456543

Final encoder loss: 0.07472645491361618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11708259582519531 0.03505754470825195

Final encoder loss: 0.044772710651159286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11646080017089844 0.03443789482116699

Final encoder loss: 0.04378504306077957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11530756950378418 0.03360319137573242

Final encoder loss: 0.03265522047877312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11735224723815918 0.03458809852600098

Final encoder loss: 0.03194480016827583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1166677474975586 0.033663272857666016

Final encoder loss: 0.02697434462606907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1168508529663086 0.0344545841217041

Final encoder loss: 0.02642202563583851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11596488952636719 0.03475046157836914

Final encoder loss: 0.024173643440008163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11856365203857422 0.034047603607177734

Final encoder loss: 0.02376464381814003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1154477596282959 0.034012556076049805

Final encoder loss: 0.023075314238667488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11629915237426758 0.03430461883544922

Final encoder loss: 0.022518662735819817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11812543869018555 0.03476381301879883

Final encoder loss: 0.02252308465540409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580991744995117 0.03414416313171387

Final encoder loss: 0.02215656265616417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11617660522460938 0.03380703926086426

Final encoder loss: 0.022454233840107918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11699461936950684 0.035184621810913086

Final encoder loss: 0.02229819819331169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1161949634552002 0.034293174743652344

Final encoder loss: 0.022961752489209175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11671900749206543 0.033472299575805664

Final encoder loss: 0.02272697165608406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11628961563110352 0.0343327522277832

Final encoder loss: 0.024115609005093575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11804366111755371 0.03428196907043457

Final encoder loss: 0.022869188338518143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1160426139831543 0.033968210220336914

Final encoder loss: 0.024412333965301514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11610269546508789 0.03378772735595703

Final encoder loss: 0.023565109819173813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11757397651672363 0.035382986068725586

Final encoder loss: 0.023527251556515694
Final encoder loss: 0.021884717047214508

Training case model
Final encoder loss: 0.03577820358944194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08930778503417969 0.21918129920959473

Final encoder loss: 0.03493918484602686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08923077583312988 0.21893715858459473

Final encoder loss: 0.034708923296127674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08897185325622559 0.2192082405090332

Final encoder loss: 0.03463866067670423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.0891115665435791 0.2189321517944336

Final encoder loss: 0.03367949013255361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08980488777160645 0.2192096710205078

Final encoder loss: 0.034094726208589665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08941435813903809 0.2188711166381836

Final encoder loss: 0.03415549504099944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09097814559936523 0.2193765640258789

Final encoder loss: 0.03526705195301146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08993959426879883 0.2190546989440918

Final encoder loss: 0.034339874916244954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09055781364440918 0.21917510032653809

Final encoder loss: 0.03420150065679228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08938002586364746 0.2192521095275879

Final encoder loss: 0.03376207201045109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08955597877502441 0.21926665306091309

Final encoder loss: 0.033738713006164996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08928704261779785 0.21921753883361816

Final encoder loss: 0.033629191605976626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08906126022338867 0.21913623809814453

Final encoder loss: 0.03406036532656857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08964323997497559 0.21925950050354004

Final encoder loss: 0.03374454932168647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0895376205444336 0.2190570831298828

Final encoder loss: 0.034769303646671086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08589577674865723 0.21600842475891113


Training case model
Final encoder loss: 0.20295484364032745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26268911361694336 0.05354475975036621

Final encoder loss: 0.1889011710882187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25893211364746094 0.052412986755371094

Final encoder loss: 0.19013655185699463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2596919536590576 0.051915884017944336

Final encoder loss: 0.19217714667320251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2594411373138428 0.05538678169250488

Final encoder loss: 0.18082088232040405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575674057006836 0.0519106388092041

Final encoder loss: 0.1919211596250534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554934024810791 0.05270957946777344

Final encoder loss: 0.09895812720060349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2603626251220703 0.05285763740539551

Final encoder loss: 0.08895136415958405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583794593811035 0.05287909507751465

Final encoder loss: 0.08508262783288956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2573714256286621 0.05109667778015137

Final encoder loss: 0.08427496999502182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2591280937194824 0.05353999137878418

Final encoder loss: 0.07671862095594406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27027392387390137 0.0522007942199707

Final encoder loss: 0.07990141212940216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25517868995666504 0.051436424255371094

Final encoder loss: 0.059288255870342255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583737373352051 0.05196022987365723

Final encoder loss: 0.053998809307813644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2597086429595947 0.05319380760192871

Final encoder loss: 0.052205510437488556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25859999656677246 0.05365133285522461

Final encoder loss: 0.05294780805706978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2587106227874756 0.053212642669677734

Final encoder loss: 0.04981837794184685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590498924255371 0.05294179916381836

Final encoder loss: 0.05143250524997711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561674118041992 0.052117109298706055

Final encoder loss: 0.04501328244805336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25765538215637207 0.05479860305786133

Final encoder loss: 0.04214858263731003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579483985900879 0.0528416633605957

Final encoder loss: 0.04107536002993584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25873351097106934 0.052017927169799805

Final encoder loss: 0.04216701537370682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26068997383117676 0.054885149002075195

Final encoder loss: 0.04106646403670311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2775394916534424 0.054526567459106445

Final encoder loss: 0.04115566611289978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563307285308838 0.05126047134399414

Final encoder loss: 0.0401340052485466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2601437568664551 0.05281233787536621

Final encoder loss: 0.03898301348090172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25873684883117676 0.05469822883605957

Final encoder loss: 0.038132358342409134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25742149353027344 0.05245709419250488

Final encoder loss: 0.03900296986103058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.268413782119751 0.0529627799987793

Final encoder loss: 0.03941715881228447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25960302352905273 0.05157113075256348

Final encoder loss: 0.0387285053730011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25491905212402344 0.05333232879638672

Final encoder loss: 0.03885313868522644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580099105834961 0.05203819274902344

Final encoder loss: 0.038027502596378326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583467960357666 0.05385613441467285

Final encoder loss: 0.03758933022618294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594578266143799 0.05411386489868164

Final encoder loss: 0.038354042917490005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571256160736084 0.05213212966918945

Final encoder loss: 0.03882512450218201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591524124145508 0.05152535438537598

Final encoder loss: 0.03804069757461548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25722336769104004 0.0506434440612793

Final encoder loss: 0.03745551407337189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578141689300537 0.05421805381774902

Final encoder loss: 0.036412667483091354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588472366333008 0.05234789848327637

Final encoder loss: 0.03619306907057762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25841259956359863 0.05310988426208496

Final encoder loss: 0.03673835098743439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589728832244873 0.052503347396850586

Final encoder loss: 0.03712984174489975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.258209228515625 0.05177783966064453

Final encoder loss: 0.03634786605834961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571687698364258 0.05336713790893555

Final encoder loss: 0.03626393899321556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26046013832092285 0.05139613151550293

Final encoder loss: 0.035369571298360825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25733423233032227 0.05261063575744629

Final encoder loss: 0.03502854332327843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575993537902832 0.05283331871032715

Final encoder loss: 0.03556967154145241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27486133575439453 0.052278995513916016

Final encoder loss: 0.036083243787288666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25893425941467285 0.05188155174255371

Final encoder loss: 0.03536197543144226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554638385772705 0.05411195755004883

Final encoder loss: 0.03568258136510849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25725412368774414 0.05162382125854492

Final encoder loss: 0.035015661269426346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25820279121398926 0.05329585075378418

Final encoder loss: 0.03473012521862984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2600889205932617 0.05541253089904785

Final encoder loss: 0.035268254578113556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577033042907715 0.053115129470825195

Final encoder loss: 0.035668909549713135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25744199752807617 0.053199052810668945

Final encoder loss: 0.03512509912252426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569873332977295 0.05149531364440918

Final encoder loss: 0.03531665727496147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25869321823120117 0.05337381362915039

Final encoder loss: 0.03444277495145798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579309940338135 0.05124163627624512

Final encoder loss: 0.03428199142217636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584822177886963 0.052100181579589844

Final encoder loss: 0.034912411123514175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26961827278137207 0.05029106140136719

Final encoder loss: 0.035301145166158676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26996541023254395 0.053974151611328125

Final encoder loss: 0.034655965864658356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25457334518432617 0.051373958587646484

Final encoder loss: 0.03493513539433479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25890636444091797 0.05362391471862793

Final encoder loss: 0.0342656746506691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25921177864074707 0.053785085678100586

Final encoder loss: 0.03377686068415642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25789451599121094 0.05362582206726074

Final encoder loss: 0.03433449938893318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577247619628906 0.05196046829223633

Final encoder loss: 0.03475829213857651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27069664001464844 0.05216646194458008

Final encoder loss: 0.034389808773994446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25667262077331543 0.052007436752319336

Final encoder loss: 0.03450097143650055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.261383056640625 0.05197453498840332

Final encoder loss: 0.03384695574641228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25940513610839844 0.05220341682434082

Final encoder loss: 0.03364614397287369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26885294914245605 0.05189228057861328

Final encoder loss: 0.03414580225944519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27833032608032227 0.053330421447753906

Final encoder loss: 0.03460456058382988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256148099899292 0.051910400390625

Final encoder loss: 0.03399783372879028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25396108627319336 0.04990863800048828

Final encoder loss: 0.03433309867978096
Final encoder loss: 0.03300813212990761
Final encoder loss: 0.031907208263874054
Final encoder loss: 0.03140702098608017
Final encoder loss: 0.030790243297815323
Final encoder loss: 0.02915833704173565

Training emognition model
Final encoder loss: 0.04036638381434036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.0811774730682373 0.2294914722442627

Final encoder loss: 0.039138562121278024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08060145378112793 0.22971320152282715

Final encoder loss: 0.03925687225770316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08034491539001465 0.2293086051940918

Final encoder loss: 0.036989910105643765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08104276657104492 0.23044967651367188

Final encoder loss: 0.039468226411941615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08191871643066406 0.23111557960510254

Final encoder loss: 0.03929035643424824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08080387115478516 0.23084545135498047

Final encoder loss: 0.04099379810809609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08220267295837402 0.2302258014678955

Final encoder loss: 0.040389208105086234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08093380928039551 0.2310781478881836

Final encoder loss: 0.04076710620535319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08104801177978516 0.23052310943603516

Final encoder loss: 0.03772609154548155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08103370666503906 0.23028206825256348

Final encoder loss: 0.03951756749271041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08198976516723633 0.23080945014953613

Final encoder loss: 0.03843093209745892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08274507522583008 0.23118042945861816

Final encoder loss: 0.03869975701432549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08210635185241699 0.23071050643920898

Final encoder loss: 0.039238520379164386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08284902572631836 0.23073816299438477

Final encoder loss: 0.038978419663055444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08102273941040039 0.23084068298339844

Final encoder loss: 0.04059876198837628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08080053329467773 0.22977805137634277


Training emognition model
Final encoder loss: 0.1935659795999527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2531149387359619 0.04887533187866211

Final encoder loss: 0.1949407309293747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24857544898986816 0.0496823787689209

Final encoder loss: 0.08303319662809372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24749255180358887 0.04852628707885742

Final encoder loss: 0.08211231976747513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24796199798583984 0.04880833625793457

Final encoder loss: 0.05523638799786568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24871277809143066 0.048433780670166016

Final encoder loss: 0.0536729171872139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24746036529541016 0.05042004585266113

Final encoder loss: 0.0442926399409771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24724030494689941 0.0493013858795166

Final encoder loss: 0.043287985026836395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24681806564331055 0.04975175857543945

Final encoder loss: 0.039170362055301666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24782514572143555 0.051451683044433594

Final encoder loss: 0.0386173278093338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24792242050170898 0.04912734031677246

Final encoder loss: 0.03671926632523537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24635767936706543 0.04870748519897461

Final encoder loss: 0.036502763628959656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2486112117767334 0.04886436462402344

Final encoder loss: 0.035948965698480606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24712562561035156 0.049912214279174805

Final encoder loss: 0.035749051719903946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24736905097961426 0.04923701286315918

Final encoder loss: 0.036145519465208054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24737906455993652 0.048917531967163086

Final encoder loss: 0.036080531775951385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2484874725341797 0.04871988296508789

Final encoder loss: 0.03631078824400902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24648332595825195 0.04960060119628906

Final encoder loss: 0.0365254282951355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24715352058410645 0.04995560646057129

Final encoder loss: 0.03655717521905899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24994349479675293 0.04847121238708496

Final encoder loss: 0.036854345351457596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24844789505004883 0.05016922950744629

Final encoder loss: 0.036692481487989426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24649405479431152 0.049195051193237305

Final encoder loss: 0.03679583966732025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2485346794128418 0.04851889610290527

Final encoder loss: 0.036684487015008926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24736499786376953 0.04803967475891113

Final encoder loss: 0.03678686171770096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463066577911377 0.04819512367248535

Final encoder loss: 0.03690430894494057
Final encoder loss: 0.035341355949640274

Training empatch model
Final encoder loss: 0.06322972462066165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07071161270141602 0.17238402366638184

Final encoder loss: 0.06255310186370414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.08326268196105957 0.17232632637023926

Final encoder loss: 0.056239119586258164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07047629356384277 0.17307472229003906

Final encoder loss: 0.0543992107262465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0708003044128418 0.17293739318847656

Final encoder loss: 0.05022675749537263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0706324577331543 0.17220401763916016

Final encoder loss: 0.05278185712854683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07069754600524902 0.17295217514038086

Final encoder loss: 0.049629930589351556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07035326957702637 0.17295408248901367

Final encoder loss: 0.049447681282038726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07017254829406738 0.1720564365386963

Final encoder loss: 0.036818483108016796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07061338424682617 0.17280077934265137

Final encoder loss: 0.03736892038068684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07064032554626465 0.17375707626342773

Final encoder loss: 0.03842086860427904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0708773136138916 0.17407488822937012

Final encoder loss: 0.037410871331414096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07221317291259766 0.17384719848632812

Final encoder loss: 0.04052951206644223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07119965553283691 0.1739959716796875

Final encoder loss: 0.039489167574117356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07152771949768066 0.17381072044372559

Final encoder loss: 0.04391713998178242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07226920127868652 0.17462968826293945

Final encoder loss: 0.038430810159885455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07077717781066895 0.17383265495300293


Training empatch model
Final encoder loss: 0.1711561232805252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17765259742736816 0.043582916259765625

Final encoder loss: 0.07861418277025223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17711710929870605 0.043901920318603516

Final encoder loss: 0.05645573511719704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17586112022399902 0.04373884201049805

Final encoder loss: 0.04643882438540459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17510771751403809 0.0443577766418457

Final encoder loss: 0.040911078453063965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17592072486877441 0.04341244697570801

Final encoder loss: 0.037574078887701035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752486228942871 0.04419445991516113

Final encoder loss: 0.03552982211112976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17591238021850586 0.0451662540435791

Final encoder loss: 0.03423856943845749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17568492889404297 0.044391632080078125

Final encoder loss: 0.03354089707136154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17677879333496094 0.04422879219055176

Final encoder loss: 0.03317723795771599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17607331275939941 0.04520773887634277

Final encoder loss: 0.03308437019586563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17566180229187012 0.04450178146362305

Final encoder loss: 0.03304678574204445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17496824264526367 0.04330801963806152

Final encoder loss: 0.03289684280753136

Training wesad model
Final encoder loss: 0.06241840600812483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07088756561279297 0.17247486114501953

Final encoder loss: 0.06304276854368948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07064604759216309 0.17291688919067383

Final encoder loss: 0.05712048825513881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07032608985900879 0.1720273494720459

Final encoder loss: 0.05864705620169125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07020688056945801 0.17239928245544434

Final encoder loss: 0.04306432707824249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07020854949951172 0.17230653762817383

Final encoder loss: 0.04193437007695426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0703270435333252 0.172377347946167

Final encoder loss: 0.04235224807220638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07036781311035156 0.17270541191101074

Final encoder loss: 0.040339384886731014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07027530670166016 0.17258071899414062

Final encoder loss: 0.03352477712494012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07043075561523438 0.17275595664978027

Final encoder loss: 0.03213697560020291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07033991813659668 0.17233705520629883

Final encoder loss: 0.03426491073585745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07042813301086426 0.17275667190551758

Final encoder loss: 0.03487576949406432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0705556869506836 0.1721651554107666

Final encoder loss: 0.029164407964055745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07023262977600098 0.1726987361907959

Final encoder loss: 0.02748543800580938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07072019577026367 0.1740109920501709

Final encoder loss: 0.027704530957121547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07103896141052246 0.1739819049835205

Final encoder loss: 0.028941095163748033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0713953971862793 0.17364048957824707


Training wesad model
Final encoder loss: 0.2156110256910324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10543584823608398 0.03358030319213867

Final encoder loss: 0.09437477588653564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10422611236572266 0.033203125

Final encoder loss: 0.06321585923433304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10477042198181152 0.032706260681152344

Final encoder loss: 0.04845239967107773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10407257080078125 0.03350543975830078

Final encoder loss: 0.04027983173727989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10429835319519043 0.03309917449951172

Final encoder loss: 0.03554172441363335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10422921180725098 0.0331418514251709

Final encoder loss: 0.032764848321676254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10402464866638184 0.03327322006225586

Final encoder loss: 0.031161373481154442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10413074493408203 0.033490896224975586

Final encoder loss: 0.030320793390274048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10434532165527344 0.0330352783203125

Final encoder loss: 0.029857134446501732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10434508323669434 0.03312849998474121

Final encoder loss: 0.029771262779831886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384654998779297 0.03334832191467285

Final encoder loss: 0.029850928112864494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10423469543457031 0.03248000144958496

Final encoder loss: 0.030399030074477196

Calculating loss for amigos model
	Full Pass 0.6831643581390381
numFreeParamsPath 18
Reconstruction loss values: 0.04267524182796478 0.0523553304374218

Calculating loss for dapper model
	Full Pass 0.15014100074768066
numFreeParamsPath 18
Reconstruction loss values: 0.037035468965768814 0.04024291783571243

Calculating loss for case model
	Full Pass 0.8610413074493408
numFreeParamsPath 18
Reconstruction loss values: 0.0495191290974617 0.05275295302271843

Calculating loss for emognition model
	Full Pass 0.29114794731140137
numFreeParamsPath 18
Reconstruction loss values: 0.053337402641773224 0.06089324131608009

Calculating loss for empatch model
	Full Pass 0.11650204658508301
numFreeParamsPath 18
Reconstruction loss values: 0.058402854949235916 0.06447644531726837

Calculating loss for wesad model
	Full Pass 0.0778043270111084
numFreeParamsPath 18
Reconstruction loss values: 0.06315980851650238 0.08570273965597153
Total loss calculation time: 3.85377836227417

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.86485743522644
Total epoch time: 143.9177885055542

Epoch: 25

Training emognition model
Final encoder loss: 0.05381310850457465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08927106857299805 0.28092002868652344

Final encoder loss: 0.050919500415109314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08327674865722656 0.2745521068572998

Final encoder loss: 0.052059888305942235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08360981941223145 0.2758018970489502

Final encoder loss: 0.049973158568422785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08441472053527832 0.27496910095214844

Final encoder loss: 0.046502212795203154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08278393745422363 0.27306079864501953

Final encoder loss: 0.04752797306698073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08237648010253906 0.27462077140808105

Final encoder loss: 0.047518389721016775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08284807205200195 0.2736237049102783

Final encoder loss: 0.047152591981485345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08267998695373535 0.2735788822174072

Final encoder loss: 0.04647143744042773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08290672302246094 0.27384281158447266

Final encoder loss: 0.047198399701165066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08254432678222656 0.2740182876586914

Final encoder loss: 0.04539856398678432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08260130882263184 0.27431249618530273

Final encoder loss: 0.04522038527388276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08260965347290039 0.2765164375305176

Final encoder loss: 0.04390605979602251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08304572105407715 0.2732396125793457

Final encoder loss: 0.04570916545513495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08252120018005371 0.2734375

Final encoder loss: 0.04851732881074454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08241462707519531 0.27372121810913086

Final encoder loss: 0.04796513629458821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08146095275878906 0.27274060249328613


Training case model
Final encoder loss: 0.04986709127123054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09105587005615234 0.26398587226867676

Final encoder loss: 0.04480344257427321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09085893630981445 0.26326417922973633

Final encoder loss: 0.043664012908609946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09116172790527344 0.2643728256225586

Final encoder loss: 0.0426362330734938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09093713760375977 0.2644522190093994

Final encoder loss: 0.040966935372878474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0908958911895752 0.26438045501708984

Final encoder loss: 0.040456115416709626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09086108207702637 0.26398301124572754

Final encoder loss: 0.04051385976172518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09125494956970215 0.26374173164367676

Final encoder loss: 0.03874839031692932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0905907154083252 0.2642176151275635

Final encoder loss: 0.03864655726056999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09104037284851074 0.26450157165527344

Final encoder loss: 0.038338104625169116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09070611000061035 0.2641310691833496

Final encoder loss: 0.03725901827706978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09111332893371582 0.26412534713745117

Final encoder loss: 0.03778798771116973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09110474586486816 0.2643616199493408

Final encoder loss: 0.038107001381937074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09151721000671387 0.263704776763916

Final encoder loss: 0.037195944201519955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09112381935119629 0.2641103267669678

Final encoder loss: 0.03750839217122797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09141302108764648 0.2636590003967285

Final encoder loss: 0.03596761575174786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08762145042419434 0.2602872848510742


Training dapper model
Final encoder loss: 0.045144150275086094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061426639556884766 0.14833831787109375

Final encoder loss: 0.03626849210034329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06099581718444824 0.14838337898254395

Final encoder loss: 0.03462670272484097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06489109992980957 0.1484973430633545

Final encoder loss: 0.03253014574122622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061521291732788086 0.14974498748779297

Final encoder loss: 0.03411221579075777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.060982465744018555 0.14945697784423828

Final encoder loss: 0.034187636307744226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06124687194824219 0.1491255760192871

Final encoder loss: 0.03351893468064941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06125950813293457 0.14948177337646484

Final encoder loss: 0.02811363179771243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06126570701599121 0.14870214462280273

Final encoder loss: 0.031794128341684734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06093931198120117 0.14873790740966797

Final encoder loss: 0.031612602251260294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06133103370666504 0.149428129196167

Final encoder loss: 0.03135714588474321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061530113220214844 0.15010428428649902

Final encoder loss: 0.03013642623991525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06167292594909668 0.14934015274047852

Final encoder loss: 0.02789908433119722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.061460018157958984 0.14922714233398438

Final encoder loss: 0.03581650922011019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06162834167480469 0.1491241455078125

Final encoder loss: 0.032754027742886195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06206035614013672 0.14918279647827148

Final encoder loss: 0.02897755853039017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06100034713745117 0.14870524406433105


Training amigos model
Final encoder loss: 0.04284375369141208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1081085205078125 0.38839197158813477

Final encoder loss: 0.04036017488306956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10784530639648438 0.3878626823425293

Final encoder loss: 0.04288409094582086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10808849334716797 0.38852453231811523

Final encoder loss: 0.04092540208663318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10826754570007324 0.38830089569091797

Final encoder loss: 0.03730911719584277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10815238952636719 0.3892796039581299

Final encoder loss: 0.039224316844112546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10822772979736328 0.3882119655609131

Final encoder loss: 0.034418601280619784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10817646980285645 0.3885669708251953

Final encoder loss: 0.04016611084212592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10824298858642578 0.3885610103607178

Final encoder loss: 0.03845908107491695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10813069343566895 0.38834691047668457

Final encoder loss: 0.0374680007788583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1081693172454834 0.3881864547729492

Final encoder loss: 0.04148748654476781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10776996612548828 0.3886728286743164

Final encoder loss: 0.042208050308808504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10829591751098633 0.3901224136352539

Final encoder loss: 0.03786574626157579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10830903053283691 0.3890223503112793

Final encoder loss: 0.03755786798792219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10825777053833008 0.38767147064208984

Final encoder loss: 0.04056722405642965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10786938667297363 0.38858819007873535

Final encoder loss: 0.037551921009080697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10290002822875977 0.38280248641967773


Training amigos model
Final encoder loss: 0.02768984036206952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10642337799072266 0.3413257598876953

Final encoder loss: 0.029182560266982103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1060645580291748 0.34108471870422363

Final encoder loss: 0.02890792502027518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1062169075012207 0.3412151336669922

Final encoder loss: 0.028445611524721217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1061866283416748 0.3413352966308594

Final encoder loss: 0.03038359121804811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10609579086303711 0.3411874771118164

Final encoder loss: 0.02661035553184131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10604500770568848 0.34136533737182617

Final encoder loss: 0.029799323631510048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10618233680725098 0.3410763740539551

Final encoder loss: 0.025786641131590564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10587620735168457 0.3413376808166504

Final encoder loss: 0.02858756089639031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10555768013000488 0.341113805770874

Final encoder loss: 0.027836116970288278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.1059885025024414 0.34120750427246094

Final encoder loss: 0.026681845133434908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10614991188049316 0.3410971164703369

Final encoder loss: 0.029882287005979592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10588288307189941 0.34099769592285156

Final encoder loss: 0.02970392025085818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10628795623779297 0.3417341709136963

Final encoder loss: 0.026904180309292387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10583758354187012 0.34117555618286133

Final encoder loss: 0.02885003531950991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10588574409484863 0.34093403816223145

Final encoder loss: 0.030070396623615228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10120010375976562 0.33735227584838867


Training amigos model
Final encoder loss: 0.1807645708322525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46287083625793457 0.0769355297088623

Final encoder loss: 0.1878187507390976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4769613742828369 0.07638883590698242

Final encoder loss: 0.18362945318222046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4688253402709961 0.07739567756652832

Final encoder loss: 0.07273170351982117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4693889617919922 0.07758784294128418

Final encoder loss: 0.07324513047933578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4696826934814453 0.07702898979187012

Final encoder loss: 0.06800779700279236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4637184143066406 0.07817530632019043

Final encoder loss: 0.044498782604932785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47347545623779297 0.07875418663024902

Final encoder loss: 0.04415661469101906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4763345718383789 0.07370376586914062

Final encoder loss: 0.04218316078186035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46764063835144043 0.0813589096069336

Final encoder loss: 0.03438200056552887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4691462516784668 0.07974529266357422

Final encoder loss: 0.034239280968904495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4691445827484131 0.0767061710357666

Final encoder loss: 0.03364703804254532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4647951126098633 0.07503032684326172

Final encoder loss: 0.03027496114373207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4694795608520508 0.0761423110961914

Final encoder loss: 0.0303347185254097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47805261611938477 0.07342648506164551

Final encoder loss: 0.030134597793221474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4724538326263428 0.0743412971496582

Final encoder loss: 0.029338698834180832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4702455997467041 0.08072590827941895

Final encoder loss: 0.029276879504323006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47295284271240234 0.08269143104553223

Final encoder loss: 0.029350105673074722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4651315212249756 0.07620048522949219

Final encoder loss: 0.030002236366271973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46767306327819824 0.07666277885437012

Final encoder loss: 0.029642565175890923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.468613862991333 0.07721376419067383

Final encoder loss: 0.029870862141251564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47356271743774414 0.07549333572387695

Final encoder loss: 0.030221324414014816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4738459587097168 0.07689547538757324

Final encoder loss: 0.029932862147688866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4732370376586914 0.07832551002502441

Final encoder loss: 0.030110271647572517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.465914249420166 0.0748605728149414

Final encoder loss: 0.029141997918486595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4698984622955322 0.07771801948547363

Final encoder loss: 0.02921188250184059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680812358856201 0.0771493911743164

Final encoder loss: 0.02923673950135708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47013401985168457 0.07718777656555176

Final encoder loss: 0.028720837086439133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47534704208374023 0.07606029510498047

Final encoder loss: 0.02874460071325302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4704318046569824 0.08548450469970703

Final encoder loss: 0.02900616265833378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4683680534362793 0.07902193069458008

Final encoder loss: 0.028389884158968925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46704888343811035 0.07568597793579102

Final encoder loss: 0.02832886390388012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46852779388427734 0.07695651054382324

Final encoder loss: 0.028841204941272736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46607208251953125 0.07504487037658691

Final encoder loss: 0.028752414509654045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4765191078186035 0.07509994506835938

Final encoder loss: 0.02817022055387497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47589993476867676 0.07481098175048828

Final encoder loss: 0.028856921941041946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46747326850891113 0.07810282707214355

Final encoder loss: 0.02821408025920391
Final encoder loss: 0.026584181934595108
Final encoder loss: 0.025808533653616905

Training dapper model
Final encoder loss: 0.024255560421466333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059876441955566406 0.10742640495300293

Final encoder loss: 0.026877816636682754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06025886535644531 0.10753965377807617

Final encoder loss: 0.026470181069089677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06003093719482422 0.10756540298461914

Final encoder loss: 0.032884359721487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059613704681396484 0.10827875137329102

Final encoder loss: 0.023998325018356383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06105184555053711 0.10790181159973145

Final encoder loss: 0.024634684496157345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060016632080078125 0.10741639137268066

Final encoder loss: 0.02688476734336231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.060698509216308594 0.10778665542602539

Final encoder loss: 0.024155806970958812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06011700630187988 0.1073002815246582

Final encoder loss: 0.023307944176178482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05995464324951172 0.10803985595703125

Final encoder loss: 0.024417534012917533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06093168258666992 0.10781741142272949

Final encoder loss: 0.023756122496589243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060624122619628906 0.10763406753540039

Final encoder loss: 0.02383955786090335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06000661849975586 0.10748124122619629

Final encoder loss: 0.022611160398536063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06084918975830078 0.10825538635253906

Final encoder loss: 0.022836899485639158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06117582321166992 0.10793948173522949

Final encoder loss: 0.02254436033219148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0601041316986084 0.1074211597442627

Final encoder loss: 0.021933669776845154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05927705764770508 0.10732197761535645


Training dapper model
Final encoder loss: 0.20241929590702057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11992549896240234 0.03505373001098633

Final encoder loss: 0.20819376409053802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11612820625305176 0.033564090728759766

Final encoder loss: 0.07554247230291367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11503028869628906 0.03364896774291992

Final encoder loss: 0.07548941671848297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11522841453552246 0.033733367919921875

Final encoder loss: 0.04535778611898422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11570906639099121 0.03350067138671875

Final encoder loss: 0.04407699406147003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11548471450805664 0.033812522888183594

Final encoder loss: 0.03292195126414299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11503720283508301 0.033490896224975586

Final encoder loss: 0.03215368464589119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11526846885681152 0.033521175384521484

Final encoder loss: 0.0269122663885355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11495304107666016 0.033711910247802734

Final encoder loss: 0.02655653841793537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1151130199432373 0.034394264221191406

Final encoder loss: 0.02396000549197197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11509513854980469 0.03358602523803711

Final encoder loss: 0.023732546716928482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11524701118469238 0.03346395492553711

Final encoder loss: 0.022528978064656258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11500406265258789 0.03383016586303711

Final encoder loss: 0.022486981004476547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563801765441895 0.03396129608154297

Final encoder loss: 0.022087909281253815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11513495445251465 0.03342461585998535

Final encoder loss: 0.02205508202314377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11532831192016602 0.03417563438415527

Final encoder loss: 0.022278310731053352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11551475524902344 0.0338435173034668

Final encoder loss: 0.0219531562179327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1149442195892334 0.03370189666748047

Final encoder loss: 0.022807875648140907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11537742614746094 0.03334641456604004

Final encoder loss: 0.022193247452378273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1156764030456543 0.03413558006286621

Final encoder loss: 0.022969907149672508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11504840850830078 0.03346753120422363

Final encoder loss: 0.02249363251030445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11612963676452637 0.033588409423828125

Final encoder loss: 0.022812005132436752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1162252426147461 0.03342247009277344

Final encoder loss: 0.022826898843050003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563301086425781 0.03437447547912598

Final encoder loss: 0.022692108526825905
Final encoder loss: 0.02128458395600319

Training case model
Final encoder loss: 0.03534931401404514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0894918441772461 0.2190263271331787

Final encoder loss: 0.03406945467498017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.089599609375 0.21903562545776367

Final encoder loss: 0.033993694539952254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0897376537322998 0.21920013427734375

Final encoder loss: 0.03362147795030592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09048128128051758 0.21927762031555176

Final encoder loss: 0.03413604871652929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08928442001342773 0.2189619541168213

Final encoder loss: 0.03304850545861361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09001994132995605 0.21909093856811523

Final encoder loss: 0.03250328160739324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08983159065246582 0.21926045417785645

Final encoder loss: 0.033863086664170046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08954095840454102 0.21898627281188965

Final encoder loss: 0.03397667550359409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08926796913146973 0.21905231475830078

Final encoder loss: 0.03241458795942881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08917927742004395 0.21921849250793457

Final encoder loss: 0.03330474518453979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08962821960449219 0.21903610229492188

Final encoder loss: 0.03331948282603129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08985137939453125 0.2189621925354004

Final encoder loss: 0.03201423342658424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08896040916442871 0.21909523010253906

Final encoder loss: 0.032569856800335265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08911561965942383 0.21922588348388672

Final encoder loss: 0.03233641234737278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08930635452270508 0.21922779083251953

Final encoder loss: 0.03318087458446098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08621835708618164 0.21564960479736328


Training case model
Final encoder loss: 0.2029656618833542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2618448734283447 0.05200600624084473

Final encoder loss: 0.18892338871955872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25897741317749023 0.05108475685119629

Final encoder loss: 0.19014614820480347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2574176788330078 0.05086541175842285

Final encoder loss: 0.19218604266643524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571227550506592 0.05210399627685547

Final encoder loss: 0.18080343306064606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25729966163635254 0.05299973487854004

Final encoder loss: 0.19192460179328918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25200510025024414 0.05115866661071777

Final encoder loss: 0.09918315708637238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578294277191162 0.0527188777923584

Final encoder loss: 0.0892365351319313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2561473846435547 0.05116415023803711

Final encoder loss: 0.08543512225151062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2562546730041504 0.05228066444396973

Final encoder loss: 0.08446600288152695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26743507385253906 0.05116748809814453

Final encoder loss: 0.07706037908792496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25696372985839844 0.05172157287597656

Final encoder loss: 0.0799117162823677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540268898010254 0.05071520805358887

Final encoder loss: 0.05933786928653717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25836896896362305 0.05258965492248535

Final encoder loss: 0.054112136363983154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26029133796691895 0.05249357223510742

Final encoder loss: 0.05226676166057587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25803637504577637 0.053919315338134766

Final encoder loss: 0.052954018115997314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.259052038192749 0.05193901062011719

Final encoder loss: 0.050031743943691254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25751233100891113 0.052767038345336914

Final encoder loss: 0.05123843997716904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559831142425537 0.05235028266906738

Final encoder loss: 0.044667426496744156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576284408569336 0.05450916290283203

Final encoder loss: 0.042062025517225266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258286714553833 0.051685333251953125

Final encoder loss: 0.04088296368718147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2569291591644287 0.05256843566894531

Final encoder loss: 0.04194445535540581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2596242427825928 0.055258750915527344

Final encoder loss: 0.0409625880420208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25722646713256836 0.053774356842041016

Final encoder loss: 0.04081616550683975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551155090332031 0.05307459831237793

Final encoder loss: 0.03962782397866249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27153491973876953 0.05247235298156738

Final encoder loss: 0.03830844908952713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25733304023742676 0.05312681198120117

Final encoder loss: 0.03771885111927986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576267719268799 0.0523686408996582

Final encoder loss: 0.03868349269032478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25872087478637695 0.052956581115722656

Final encoder loss: 0.03877805918455124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25922179222106934 0.051059722900390625

Final encoder loss: 0.03822219371795654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25449156761169434 0.05365729331970215

Final encoder loss: 0.03815026581287384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2692258358001709 0.05218052864074707

Final encoder loss: 0.03734354302287102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25737929344177246 0.052176713943481445

Final encoder loss: 0.03723130375146866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25992536544799805 0.05416297912597656

Final encoder loss: 0.03771764412522316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575063705444336 0.05180764198303223

Final encoder loss: 0.03850509226322174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2582578659057617 0.05301380157470703

Final encoder loss: 0.037768784910440445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25708937644958496 0.05111551284790039

Final encoder loss: 0.036826662719249725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580680847167969 0.052863121032714844

Final encoder loss: 0.03580223768949509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25769948959350586 0.052236318588256836

Final encoder loss: 0.035784970968961716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586848735809326 0.05271291732788086

Final encoder loss: 0.036105237901210785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25879383087158203 0.052886009216308594

Final encoder loss: 0.03673739358782768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2598896026611328 0.05320453643798828

Final encoder loss: 0.03581184148788452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2536170482635498 0.05215716361999512

Final encoder loss: 0.035707611590623856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.259319543838501 0.0521852970123291

Final encoder loss: 0.03474806249141693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2580265998840332 0.05262446403503418

Final encoder loss: 0.03458426520228386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572488784790039 0.05265378952026367

Final encoder loss: 0.035195622593164444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2690913677215576 0.05273699760437012

Final encoder loss: 0.035880520939826965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585265636444092 0.052069664001464844

Final encoder loss: 0.03476712480187416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25511693954467773 0.053894758224487305

Final encoder loss: 0.03491748869419098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25847911834716797 0.05368638038635254

Final encoder loss: 0.034304216504096985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2591865062713623 0.05429553985595703

Final encoder loss: 0.03410973772406578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2692410945892334 0.05430889129638672

Final encoder loss: 0.03459132835268974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25788164138793945 0.05277442932128906

Final encoder loss: 0.03514772281050682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2570514678955078 0.052865028381347656

Final encoder loss: 0.03424884378910065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2575507164001465 0.0532221794128418

Final encoder loss: 0.03448508679866791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25737571716308594 0.05326080322265625

Final encoder loss: 0.03396150469779968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583584785461426 0.05149221420288086

Final encoder loss: 0.03365820646286011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2589282989501953 0.05380558967590332

Final encoder loss: 0.034183304756879807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25989365577697754 0.05269670486450195

Final encoder loss: 0.03475746512413025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25912928581237793 0.05434417724609375

Final encoder loss: 0.034285008907318115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557253837585449 0.05214118957519531

Final encoder loss: 0.034161217510700226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25994873046875 0.052434444427490234

Final encoder loss: 0.03337748721241951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2576608657836914 0.05359697341918945

Final encoder loss: 0.03334496542811394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25823068618774414 0.05211067199707031

Final encoder loss: 0.03359364718198776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2686939239501953 0.051433563232421875

Final encoder loss: 0.03426043689250946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26006054878234863 0.05178689956665039

Final encoder loss: 0.033433448523283005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25357818603515625 0.05376291275024414

Final encoder loss: 0.03389054536819458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2590153217315674 0.05210471153259277

Final encoder loss: 0.03321995213627815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2587010860443115 0.05300450325012207

Final encoder loss: 0.03297730162739754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.259113073348999 0.0532221794128418

Final encoder loss: 0.033407457172870636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577941417694092 0.05218076705932617

Final encoder loss: 0.034283895045518875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591266632080078 0.05250740051269531

Final encoder loss: 0.03344512730836868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562885284423828 0.05243945121765137

Final encoder loss: 0.033542897552251816
Final encoder loss: 0.032340049743652344
Final encoder loss: 0.03140706941485405
Final encoder loss: 0.030715057626366615
Final encoder loss: 0.03020191565155983
Final encoder loss: 0.028386641293764114

Training emognition model
Final encoder loss: 0.04026646166074582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08244991302490234 0.2307894229888916

Final encoder loss: 0.038871011429610046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08089494705200195 0.23098492622375488

Final encoder loss: 0.0390528114939273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08257150650024414 0.23135781288146973

Final encoder loss: 0.03695323332351822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08148312568664551 0.2307136058807373

Final encoder loss: 0.0370327895353311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0823049545288086 0.23073935508728027

Final encoder loss: 0.036299674569349134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08121204376220703 0.23096823692321777

Final encoder loss: 0.03712571937756215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08152985572814941 0.23070788383483887

Final encoder loss: 0.03726327819119192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08121013641357422 0.23074626922607422

Final encoder loss: 0.03918725995899882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08116483688354492 0.2320234775543213

Final encoder loss: 0.039051038851740924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08093500137329102 0.2308344841003418

Final encoder loss: 0.039536456271531205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08121728897094727 0.23051238059997559

Final encoder loss: 0.03796180580720399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08147549629211426 0.23084616661071777

Final encoder loss: 0.038332213682757695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0816497802734375 0.23083114624023438

Final encoder loss: 0.037450149877619576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08209395408630371 0.23111915588378906

Final encoder loss: 0.037510098235650054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08129763603210449 0.23102545738220215

Final encoder loss: 0.03828269122011112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08144927024841309 0.23074126243591309


Training emognition model
Final encoder loss: 0.19356614351272583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25275659561157227 0.04949665069580078

Final encoder loss: 0.1949668526649475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24799752235412598 0.04887795448303223

Final encoder loss: 0.0834672823548317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24935317039489746 0.04977250099182129

Final encoder loss: 0.08223110437393188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24868226051330566 0.049566030502319336

Final encoder loss: 0.05541154742240906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477571964263916 0.04884696006774902

Final encoder loss: 0.05371297895908356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247941255569458 0.0496983528137207

Final encoder loss: 0.044150423258543015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24813508987426758 0.04902338981628418

Final encoder loss: 0.043150149285793304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24690914154052734 0.049855709075927734

Final encoder loss: 0.03893158212304115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24672555923461914 0.04951596260070801

Final encoder loss: 0.038319557905197144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24837398529052734 0.04910922050476074

Final encoder loss: 0.03638633340597153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24693942070007324 0.05121111869812012

Final encoder loss: 0.03596341609954834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24723458290100098 0.04905986785888672

Final encoder loss: 0.035462044179439545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494356632232666 0.04937934875488281

Final encoder loss: 0.03515982627868652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24686288833618164 0.050034523010253906

Final encoder loss: 0.03535545617341995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24738359451293945 0.04904317855834961

Final encoder loss: 0.03541237860918045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24783754348754883 0.04936957359313965

Final encoder loss: 0.03560931980609894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24649691581726074 0.04954195022583008

Final encoder loss: 0.035698726773262024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2460482120513916 0.04951000213623047

Final encoder loss: 0.03598970174789429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24723577499389648 0.05025649070739746

Final encoder loss: 0.03612963482737541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24746155738830566 0.05018901824951172

Final encoder loss: 0.035998303443193436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2486114501953125 0.04900836944580078

Final encoder loss: 0.03591785952448845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24730563163757324 0.048598289489746094

Final encoder loss: 0.03595064952969551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24742412567138672 0.04829001426696777

Final encoder loss: 0.03610740602016449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24590635299682617 0.05002450942993164

Final encoder loss: 0.03571105748414993
Final encoder loss: 0.03457489609718323

Training empatch model
Final encoder loss: 0.06364618739408757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07086706161499023 0.17269015312194824

Final encoder loss: 0.05954381662328137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07094168663024902 0.17282915115356445

Final encoder loss: 0.05115310553271949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07075238227844238 0.17308998107910156

Final encoder loss: 0.052644101749462116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07085227966308594 0.17295360565185547

Final encoder loss: 0.0507900620571016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0706171989440918 0.17298531532287598

Final encoder loss: 0.049469357873096274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07054567337036133 0.1730353832244873

Final encoder loss: 0.04436654255703753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07095551490783691 0.17259621620178223

Final encoder loss: 0.04784191000999097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.06999897956848145 0.172074556350708

Final encoder loss: 0.038128417977160066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07066464424133301 0.17297124862670898

Final encoder loss: 0.03626848996438413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07082009315490723 0.17261123657226562

Final encoder loss: 0.03695470073840541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07063603401184082 0.17336130142211914

Final encoder loss: 0.03751212681635413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07144832611083984 0.17399024963378906

Final encoder loss: 0.0362303821699108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0723569393157959 0.1744694709777832

Final encoder loss: 0.04284004840714711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0713658332824707 0.1741032600402832

Final encoder loss: 0.03576175661523506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07171106338500977 0.1743183135986328

Final encoder loss: 0.03925366940237755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07149243354797363 0.17359399795532227


Training empatch model
Final encoder loss: 0.17117276787757874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17776274681091309 0.04339718818664551

Final encoder loss: 0.07894229143857956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.177046537399292 0.04445910453796387

Final encoder loss: 0.05665574222803116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17671823501586914 0.04297447204589844

Final encoder loss: 0.04652463644742966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17452406883239746 0.04310727119445801

Final encoder loss: 0.04085174575448036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17681431770324707 0.04290437698364258

Final encoder loss: 0.037399258464574814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17644715309143066 0.044400930404663086

Final encoder loss: 0.03520782291889191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17631173133850098 0.043325185775756836

Final encoder loss: 0.033900339156389236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752617359161377 0.04450583457946777

Final encoder loss: 0.03311661258339882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17658209800720215 0.0444638729095459

Final encoder loss: 0.03273564949631691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17439055442810059 0.04366803169250488

Final encoder loss: 0.032537396997213364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17693710327148438 0.04300546646118164

Final encoder loss: 0.032447535544633865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758573055267334 0.04389667510986328

Final encoder loss: 0.03229425475001335

Training wesad model
Final encoder loss: 0.06287837723933916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07142424583435059 0.1737525463104248

Final encoder loss: 0.05770030778119319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0712435245513916 0.1741476058959961

Final encoder loss: 0.05511604433736964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07099390029907227 0.17350554466247559

Final encoder loss: 0.0571775855097444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07118821144104004 0.17418718338012695

Final encoder loss: 0.03984239454775721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07139277458190918 0.17369556427001953

Final encoder loss: 0.038049671507014694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07108402252197266 0.1740410327911377

Final encoder loss: 0.04197141216246225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07227325439453125 0.17407941818237305

Final encoder loss: 0.039535897261184574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07129931449890137 0.17322754859924316

Final encoder loss: 0.03156248660133851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07236242294311523 0.17423534393310547

Final encoder loss: 0.03019642364999695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07187652587890625 0.17395877838134766

Final encoder loss: 0.032331438587353364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07140994071960449 0.17416763305664062

Final encoder loss: 0.03413762559728305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07099747657775879 0.1749742031097412

Final encoder loss: 0.02646924843853632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07100486755371094 0.17348074913024902

Final encoder loss: 0.026135151795478016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07073831558227539 0.17370986938476562

Final encoder loss: 0.02743916341160455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07116913795471191 0.1741776466369629

Final encoder loss: 0.026670689312086566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07182073593139648 0.1738896369934082


Training wesad model
Final encoder loss: 0.21558777987957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10420393943786621 0.032801151275634766

Final encoder loss: 0.09466693550348282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10476160049438477 0.03355288505554199

Final encoder loss: 0.06334283202886581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10467314720153809 0.03332328796386719

Final encoder loss: 0.048414770513772964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384249687194824 0.03325819969177246

Final encoder loss: 0.04006988927721977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10345911979675293 0.03325319290161133

Final encoder loss: 0.03513861075043678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10557222366333008 0.0338287353515625

Final encoder loss: 0.03220473974943161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10364222526550293 0.0337069034576416

Final encoder loss: 0.030396323651075363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10373258590698242 0.03289508819580078

Final encoder loss: 0.029330627992749214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10455107688903809 0.033533334732055664

Final encoder loss: 0.02881021425127983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10514640808105469 0.032274484634399414

Final encoder loss: 0.028800377622246742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10447549819946289 0.03302431106567383

Final encoder loss: 0.029022445902228355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10421276092529297 0.03344106674194336

Final encoder loss: 0.029509255662560463

Calculating loss for amigos model
	Full Pass 0.6849877834320068
numFreeParamsPath 18
Reconstruction loss values: 0.04184013977646828 0.05214444175362587

Calculating loss for dapper model
	Full Pass 0.15123271942138672
numFreeParamsPath 18
Reconstruction loss values: 0.03538871556520462 0.038893140852451324

Calculating loss for case model
	Full Pass 0.9108643531799316
numFreeParamsPath 18
Reconstruction loss values: 0.049039676785469055 0.05174752324819565

Calculating loss for emognition model
	Full Pass 0.2923436164855957
numFreeParamsPath 18
Reconstruction loss values: 0.052222203463315964 0.05976362153887749

Calculating loss for empatch model
	Full Pass 0.10424184799194336
numFreeParamsPath 18
Reconstruction loss values: 0.057209234684705734 0.06366582959890366

Calculating loss for wesad model
	Full Pass 0.07711052894592285
numFreeParamsPath 18
Reconstruction loss values: 0.06322350353002548 0.08474516868591309
Total loss calculation time: 3.9109675884246826

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.881155490875244
Total epoch time: 144.06962752342224

Epoch: 26

Training dapper model
Final encoder loss: 0.033863361396712595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06593608856201172 0.15699315071105957

Final encoder loss: 0.030851152103015932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06322908401489258 0.15236473083496094

Final encoder loss: 0.031513131515352594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06205439567565918 0.15038228034973145

Final encoder loss: 0.03398688055614469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06319427490234375 0.1510143280029297

Final encoder loss: 0.030451841546582734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0764932632446289 0.15097641944885254

Final encoder loss: 0.03229493301409607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0624394416809082 0.15143346786499023

Final encoder loss: 0.033761966292279594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062342166900634766 0.15163969993591309

Final encoder loss: 0.032395271396590826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06234574317932129 0.1510000228881836

Final encoder loss: 0.028636499444948307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06225085258483887 0.15123820304870605

Final encoder loss: 0.02938226587469729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06228327751159668 0.14930415153503418

Final encoder loss: 0.03325302529904371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061363935470581055 0.14902949333190918

Final encoder loss: 0.030838716387628435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061203956604003906 0.14851951599121094

Final encoder loss: 0.026657073064847308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06133246421813965 0.14858651161193848

Final encoder loss: 0.03034222055645448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06135869026184082 0.14884352684020996

Final encoder loss: 0.03254839431802588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06108522415161133 0.14926791191101074

Final encoder loss: 0.03279925484327594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06499457359313965 0.14817070960998535


Training emognition model
Final encoder loss: 0.052246267684801694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0823831558227539 0.2730882167816162

Final encoder loss: 0.05098412057376718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08237075805664062 0.2734241485595703

Final encoder loss: 0.050739180149801125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08265304565429688 0.2740299701690674

Final encoder loss: 0.047623102925866787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08272528648376465 0.2734544277191162

Final encoder loss: 0.04759972966299485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08240747451782227 0.27359485626220703

Final encoder loss: 0.04722989390717063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0828404426574707 0.27405714988708496

Final encoder loss: 0.04598037541316286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08329510688781738 0.2754087448120117

Final encoder loss: 0.04770282146870978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0835416316986084 0.2755401134490967

Final encoder loss: 0.04692303989081701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08367300033569336 0.2751920223236084

Final encoder loss: 0.04984681627005181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08357858657836914 0.27545857429504395

Final encoder loss: 0.046279635177476575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0832967758178711 0.2749319076538086

Final encoder loss: 0.04585846100574348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08369159698486328 0.27527666091918945

Final encoder loss: 0.044259893149692996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08359432220458984 0.2761363983154297

Final encoder loss: 0.0455530886741488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08387088775634766 0.27556896209716797

Final encoder loss: 0.045232875168461334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08406686782836914 0.2759077548980713

Final encoder loss: 0.04646490323065983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08277726173400879 0.2734227180480957


Training amigos model
Final encoder loss: 0.043877171594301864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10738682746887207 0.3888680934906006

Final encoder loss: 0.03993278527373404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10957074165344238 0.39061880111694336

Final encoder loss: 0.03908645833199931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10817384719848633 0.38868236541748047

Final encoder loss: 0.03722906597634186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10847878456115723 0.38968467712402344

Final encoder loss: 0.039031073591666486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10845375061035156 0.3890695571899414

Final encoder loss: 0.035845239095874316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10792088508605957 0.3898470401763916

Final encoder loss: 0.04002625120121976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10954666137695312 0.3904268741607666

Final encoder loss: 0.040935007386337874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10846924781799316 0.3891780376434326

Final encoder loss: 0.03591515168395749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10889124870300293 0.3896365165710449

Final encoder loss: 0.03691861313543144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10791778564453125 0.3889496326446533

Final encoder loss: 0.03942264975324535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10827302932739258 0.38918113708496094

Final encoder loss: 0.04125776991491238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10851263999938965 0.38911008834838867

Final encoder loss: 0.03959088939211028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10778498649597168 0.38906025886535645

Final encoder loss: 0.0372153173304453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10827350616455078 0.3884875774383545

Final encoder loss: 0.03862601977191659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10772824287414551 0.38912057876586914

Final encoder loss: 0.041196250790872004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10259032249450684 0.3834073543548584


Training case model
Final encoder loss: 0.049574477774084295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09109997749328613 0.26373958587646484

Final encoder loss: 0.045014403566378446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09091949462890625 0.2638695240020752

Final encoder loss: 0.04372732127212911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0912632942199707 0.2643470764160156

Final encoder loss: 0.04281584337614823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09095501899719238 0.26493334770202637

Final encoder loss: 0.04162260941915755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09150862693786621 0.2649245262145996

Final encoder loss: 0.04138777525296034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09117269515991211 0.2646501064300537

Final encoder loss: 0.040428065923120156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09148621559143066 0.26447415351867676

Final encoder loss: 0.03894466132291956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09090232849121094 0.26482295989990234

Final encoder loss: 0.03920151669420347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09143185615539551 0.2639458179473877

Final encoder loss: 0.03755612518113526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09133386611938477 0.26502513885498047

Final encoder loss: 0.03601480281837684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09146714210510254 0.26459431648254395

Final encoder loss: 0.03733521396037567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0914614200592041 0.2656588554382324

Final encoder loss: 0.03681281921125941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0924830436706543 0.2652263641357422

Final encoder loss: 0.03730613671390896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09089088439941406 0.26480603218078613

Final encoder loss: 0.036107021486024814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09162616729736328 0.2640061378479004

Final encoder loss: 0.036147679738634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08755755424499512 0.2619357109069824


Training amigos model
Final encoder loss: 0.028182700097484945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10607099533081055 0.34120774269104004

Final encoder loss: 0.029605279876718446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10617613792419434 0.3415181636810303

Final encoder loss: 0.02665944741465072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10551857948303223 0.341327428817749

Final encoder loss: 0.026290786699234302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10602593421936035 0.34053754806518555

Final encoder loss: 0.027885589759425313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10570192337036133 0.3409147262573242

Final encoder loss: 0.030112625989242893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10595941543579102 0.34075450897216797

Final encoder loss: 0.02855982377445539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10590314865112305 0.34117913246154785

Final encoder loss: 0.028237271981391445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10600137710571289 0.3408541679382324

Final encoder loss: 0.02857773607694112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10537457466125488 0.3407316207885742

Final encoder loss: 0.028171861313524747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10574769973754883 0.34072232246398926

Final encoder loss: 0.028254668124858107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10574889183044434 0.34179115295410156

Final encoder loss: 0.02937741547139858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10587573051452637 0.3408336639404297

Final encoder loss: 0.027413112640402516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10551309585571289 0.34076929092407227

Final encoder loss: 0.028257810443098948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1056814193725586 0.34081053733825684

Final encoder loss: 0.028414531308120494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10546207427978516 0.3407320976257324

Final encoder loss: 0.027968017106959304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10060429573059082 0.3386998176574707


Training amigos model
Final encoder loss: 0.18077079951763153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4647941589355469 0.07563376426696777

Final encoder loss: 0.18783749639987946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46016907691955566 0.07494544982910156

Final encoder loss: 0.18363258242607117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45790886878967285 0.07461094856262207

Final encoder loss: 0.07277316600084305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4628794193267822 0.07455945014953613

Final encoder loss: 0.07364339381456375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4590790271759033 0.0726480484008789

Final encoder loss: 0.06804568320512772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45946764945983887 0.07494473457336426

Final encoder loss: 0.04427078738808632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4596278667449951 0.07401514053344727

Final encoder loss: 0.044032011181116104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4598860740661621 0.0741887092590332

Final encoder loss: 0.04208868741989136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4561142921447754 0.07251596450805664

Final encoder loss: 0.034141961485147476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4605863094329834 0.0751645565032959

Final encoder loss: 0.033747244626283646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4590616226196289 0.07617926597595215

Final encoder loss: 0.03314216062426567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45703625679016113 0.07465028762817383

Final encoder loss: 0.02999713271856308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4588277339935303 0.0741419792175293

Final encoder loss: 0.029627302661538124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46108222007751465 0.07545757293701172

Final encoder loss: 0.029605770483613014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4568171501159668 0.07515263557434082

Final encoder loss: 0.028809376060962677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4592728614807129 0.0756368637084961

Final encoder loss: 0.028491651639342308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4592142105102539 0.07497119903564453

Final encoder loss: 0.028637422248721123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4574611186981201 0.07404279708862305

Final encoder loss: 0.0291335079818964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4614565372467041 0.07378101348876953

Final encoder loss: 0.02880573645234108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4615345001220703 0.07499122619628906

Final encoder loss: 0.02894137054681778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45646071434020996 0.07481718063354492

Final encoder loss: 0.029418159276247025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46109676361083984 0.07545638084411621

Final encoder loss: 0.029186930507421494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45862507820129395 0.07610607147216797

Final encoder loss: 0.029553594067692757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45671558380126953 0.074188232421875

Final encoder loss: 0.02867681160569191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4710831642150879 0.08078145980834961

Final encoder loss: 0.028526058420538902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46881723403930664 0.07735037803649902

Final encoder loss: 0.028781775385141373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46371984481811523 0.0760202407836914

Final encoder loss: 0.02827395685017109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47297191619873047 0.07818722724914551

Final encoder loss: 0.02803626097738743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4759256839752197 0.07546830177307129

Final encoder loss: 0.02813861519098282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4674646854400635 0.08329892158508301

Final encoder loss: 0.027864597737789154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47238898277282715 0.08125066757202148

Final encoder loss: 0.02741268090903759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46556854248046875 0.07679128646850586

Final encoder loss: 0.027957171201705933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4653925895690918 0.07617759704589844

Final encoder loss: 0.02785678021609783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46941208839416504 0.07789134979248047

Final encoder loss: 0.02764495648443699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47641992568969727 0.07397270202636719

Final encoder loss: 0.028319809585809708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47303128242492676 0.07340192794799805

Final encoder loss: 0.0276136826723814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4703552722930908 0.0792849063873291

Final encoder loss: 0.02694564312696457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4710545539855957 0.08160686492919922

Final encoder loss: 0.027856850996613503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4674546718597412 0.07480859756469727

Final encoder loss: 0.027659518644213676
Final encoder loss: 0.025820577517151833
Final encoder loss: 0.025030605494976044

Training dapper model
Final encoder loss: 0.024958649155450097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06122851371765137 0.1084752082824707

Final encoder loss: 0.023995586608513718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06091737747192383 0.10674595832824707

Final encoder loss: 0.022831847775484512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05979180335998535 0.10737419128417969

Final encoder loss: 0.02278007235923255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.060131072998046875 0.10730504989624023

Final encoder loss: 0.025441503976317455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05989432334899902 0.10689377784729004

Final encoder loss: 0.025302270547606344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0609745979309082 0.10821032524108887

Final encoder loss: 0.024727576687775758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.06240344047546387 0.1073157787322998

Final encoder loss: 0.02377904531069412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05978989601135254 0.10761356353759766

Final encoder loss: 0.024853111701170622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05996441841125488 0.10750079154968262

Final encoder loss: 0.022892473234338295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05988192558288574 0.10733151435852051

Final encoder loss: 0.022737620740608384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060813188552856445 0.10857772827148438

Final encoder loss: 0.02490183568460828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.061415672302246094 0.10773468017578125

Final encoder loss: 0.0213548325853023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05968141555786133 0.10757040977478027

Final encoder loss: 0.022251838518302487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06029081344604492 0.10753107070922852

Final encoder loss: 0.02450164492349255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05982041358947754 0.1073768138885498

Final encoder loss: 0.023907622756159784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05955767631530762 0.10809993743896484


Training dapper model
Final encoder loss: 0.20242996513843536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.12088990211486816 0.033931732177734375

Final encoder loss: 0.2081904411315918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1160585880279541 0.0341496467590332

Final encoder loss: 0.07539883255958557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11659741401672363 0.034548282623291016

Final encoder loss: 0.07694601267576218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11628842353820801 0.03369760513305664

Final encoder loss: 0.04452960565686226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1183619499206543 0.03507804870605469

Final encoder loss: 0.04435001686215401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11832690238952637 0.03291034698486328

Final encoder loss: 0.03198849782347679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1171104907989502 0.0342249870300293

Final encoder loss: 0.03186254948377609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1169426441192627 0.03400897979736328

Final encoder loss: 0.026019643992185593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11630105972290039 0.034569740295410156

Final encoder loss: 0.025989161804318428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11882996559143066 0.035234689712524414

Final encoder loss: 0.023049866780638695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11854076385498047 0.03380084037780762

Final encoder loss: 0.023033352568745613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11638426780700684 0.03401041030883789

Final encoder loss: 0.021633578464388847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11584901809692383 0.03386235237121582

Final encoder loss: 0.021601669490337372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11556792259216309 0.03430533409118652

Final encoder loss: 0.021088210865855217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11884284019470215 0.03516364097595215

Final encoder loss: 0.02118067257106304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11783790588378906 0.03372335433959961

Final encoder loss: 0.021176258102059364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11679482460021973 0.03517031669616699

Final encoder loss: 0.021026285365223885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11545252799987793 0.0339357852935791

Final encoder loss: 0.021530088037252426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11581254005432129 0.03419780731201172

Final encoder loss: 0.02139582671225071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11821818351745605 0.03464984893798828

Final encoder loss: 0.02159738540649414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11716437339782715 0.03364086151123047

Final encoder loss: 0.02135528437793255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11638784408569336 0.033940792083740234

Final encoder loss: 0.021560927852988243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11651444435119629 0.03453636169433594

Final encoder loss: 0.021822545677423477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11570262908935547 0.03452634811401367

Final encoder loss: 0.021416425704956055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11898422241210938 0.034773826599121094

Final encoder loss: 0.021601945161819458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11707830429077148 0.03374028205871582

Final encoder loss: 0.021492784842848778
Final encoder loss: 0.020093407481908798

Training case model
Final encoder loss: 0.03191728125660851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0892188549041748 0.2194981575012207

Final encoder loss: 0.031675539497698794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09056758880615234 0.219343900680542

Final encoder loss: 0.03137165966781678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08963751792907715 0.21891379356384277

Final encoder loss: 0.031959705811142526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08972549438476562 0.21941924095153809

Final encoder loss: 0.031948821040068576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.09051084518432617 0.21885466575622559

Final encoder loss: 0.03135810084667995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08952045440673828 0.21897172927856445

Final encoder loss: 0.03309614687719484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.0895078182220459 0.2192540168762207

Final encoder loss: 0.031230612498621378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08888840675354004 0.21894478797912598

Final encoder loss: 0.03137069712340065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08929562568664551 0.2188098430633545

Final encoder loss: 0.03269459695159881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09090089797973633 0.2196485996246338

Final encoder loss: 0.03175296807769054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08969449996948242 0.2189805507659912

Final encoder loss: 0.03170536346532687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0897212028503418 0.21919870376586914

Final encoder loss: 0.03253127042689662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.09055471420288086 0.21952605247497559

Final encoder loss: 0.03143899980243107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08989810943603516 0.21893048286437988

Final encoder loss: 0.031155214112099632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08906269073486328 0.21940231323242188

Final encoder loss: 0.031165182068292215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08909869194030762 0.21617507934570312


Training case model
Final encoder loss: 0.20295828580856323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2632591724395752 0.05428934097290039

Final encoder loss: 0.18889975547790527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2591123580932617 0.0533907413482666

Final encoder loss: 0.19016429781913757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25887346267700195 0.0531461238861084

Final encoder loss: 0.1921858787536621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.259868860244751 0.05144453048706055

Final encoder loss: 0.18080583214759827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2682511806488037 0.052039146423339844

Final encoder loss: 0.19193482398986816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25652027130126953 0.05226445198059082

Final encoder loss: 0.09862164407968521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592780590057373 0.05280947685241699

Final encoder loss: 0.08896029740571976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2622950077056885 0.051843881607055664

Final encoder loss: 0.08512675762176514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26969194412231445 0.0528714656829834

Final encoder loss: 0.08386421948671341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2737727165222168 0.050470829010009766

Final encoder loss: 0.0770588219165802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27856922149658203 0.05172324180603027

Final encoder loss: 0.07984670996665955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26030826568603516 0.051427602767944336

Final encoder loss: 0.05844273045659065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25989532470703125 0.05235576629638672

Final encoder loss: 0.05355826020240784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27185916900634766 0.05240178108215332

Final encoder loss: 0.051698073744773865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26928234100341797 0.052215576171875

Final encoder loss: 0.05218738690018654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2811260223388672 0.053264617919921875

Final encoder loss: 0.049466051161289215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25928521156311035 0.05291914939880371

Final encoder loss: 0.05086290091276169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569308280944824 0.05381965637207031

Final encoder loss: 0.04367925599217415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2595069408416748 0.0523989200592041

Final encoder loss: 0.04123254865407944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25678133964538574 0.05215907096862793

Final encoder loss: 0.04002302512526512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26769375801086426 0.05174374580383301

Final encoder loss: 0.04087356477975845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2727503776550293 0.05044889450073242

Final encoder loss: 0.040336430072784424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2572321891784668 0.05231142044067383

Final encoder loss: 0.04037855565547943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25464892387390137 0.05179572105407715

Final encoder loss: 0.038565121591091156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2668771743774414 0.05140042304992676

Final encoder loss: 0.03741535544395447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2671167850494385 0.052542924880981445

Final encoder loss: 0.03666550666093826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25545835494995117 0.05191326141357422

Final encoder loss: 0.03742529824376106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.259432315826416 0.05135774612426758

Final encoder loss: 0.03821522369980812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576768398284912 0.05315232276916504

Final encoder loss: 0.03765334561467171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25559282302856445 0.05258750915527344

Final encoder loss: 0.03729570657014847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26095056533813477 0.052023887634277344

Final encoder loss: 0.036521073430776596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25879931449890137 0.051915645599365234

Final encoder loss: 0.036562900990247726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25734901428222656 0.05141782760620117

Final encoder loss: 0.03683033958077431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2585909366607666 0.053427934646606445

Final encoder loss: 0.0379389151930809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28565311431884766 0.05240797996520996

Final encoder loss: 0.03695073351264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569007873535156 0.05166959762573242

Final encoder loss: 0.03570471331477165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25742363929748535 0.05224013328552246

Final encoder loss: 0.03493431955575943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25832080841064453 0.05298566818237305

Final encoder loss: 0.03485703468322754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25766849517822266 0.05225801467895508

Final encoder loss: 0.03502977639436722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26825571060180664 0.05200791358947754

Final encoder loss: 0.035733357071876526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28299856185913086 0.05164504051208496

Final encoder loss: 0.03507782146334648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559549808502197 0.05262351036071777

Final encoder loss: 0.03455726429820061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576446533203125 0.05155634880065918

Final encoder loss: 0.033841248601675034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2570369243621826 0.05203413963317871

Final encoder loss: 0.0334915854036808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581038475036621 0.050515174865722656

Final encoder loss: 0.03417243808507919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2723820209503174 0.0519566535949707

Final encoder loss: 0.03493715077638626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2672436237335205 0.05058646202087402

Final encoder loss: 0.03401654213666916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542552947998047 0.050858497619628906

Final encoder loss: 0.033905815333127975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566671371459961 0.052188873291015625

Final encoder loss: 0.033342912793159485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25794291496276855 0.051427602767944336

Final encoder loss: 0.03309204429388046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2673988342285156 0.05260014533996582

Final encoder loss: 0.03335477411746979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2563138008117676 0.05199289321899414

Final encoder loss: 0.0343204140663147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2729628086090088 0.053641557693481445

Final encoder loss: 0.03377114608883858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25359201431274414 0.05051541328430176

Final encoder loss: 0.03353669494390488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25823259353637695 0.05252718925476074

Final encoder loss: 0.03302135318517685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2590923309326172 0.052298784255981445

Final encoder loss: 0.03283342719078064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26885271072387695 0.05526852607727051

Final encoder loss: 0.03307545557618141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27443623542785645 0.05334639549255371

Final encoder loss: 0.03413539007306099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27695369720458984 0.05216050148010254

Final encoder loss: 0.03332873433828354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565009593963623 0.05183601379394531

Final encoder loss: 0.03302561864256859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578256130218506 0.05429506301879883

Final encoder loss: 0.032634489238262177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27428317070007324 0.052102088928222656

Final encoder loss: 0.03220859915018082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586982250213623 0.0533909797668457

Final encoder loss: 0.0325278602540493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2698550224304199 0.05240988731384277

Final encoder loss: 0.033336564898490906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581937313079834 0.05304145812988281

Final encoder loss: 0.03248191997408867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561972141265869 0.05048060417175293

Final encoder loss: 0.03274577856063843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25794100761413574 0.053072214126586914

Final encoder loss: 0.03239818289875984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26009559631347656 0.051644086837768555

Final encoder loss: 0.031942885369062424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25679874420166016 0.05251884460449219

Final encoder loss: 0.03238397091627121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25917625427246094 0.052488088607788086

Final encoder loss: 0.03319298475980759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26035642623901367 0.0513453483581543

Final encoder loss: 0.03237451985478401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25484275817871094 0.05306053161621094

Final encoder loss: 0.03254405036568642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573864459991455 0.05217385292053223

Final encoder loss: 0.03206370770931244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25865960121154785 0.05448412895202637

Final encoder loss: 0.03178854286670685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.269287109375 0.05230259895324707

Final encoder loss: 0.031796589493751526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27808284759521484 0.054969072341918945

Final encoder loss: 0.03264455124735832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25909900665283203 0.05205249786376953

Final encoder loss: 0.032285869121551514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564859390258789 0.05209612846374512

Final encoder loss: 0.03233688697218895
Final encoder loss: 0.03130776435136795
Final encoder loss: 0.030138442292809486
Final encoder loss: 0.029531512409448624
Final encoder loss: 0.029224596917629242
Final encoder loss: 0.02745123766362667

Training emognition model
Final encoder loss: 0.0363566673092332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08283686637878418 0.2307283878326416

Final encoder loss: 0.03695185387162036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08102893829345703 0.23040056228637695

Final encoder loss: 0.03650109707586546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08163189888000488 0.23077130317687988

Final encoder loss: 0.03894631161815094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08113861083984375 0.23079490661621094

Final encoder loss: 0.03703885330854878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08120942115783691 0.23028802871704102

Final encoder loss: 0.03669195228719829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08157157897949219 0.230316162109375

Final encoder loss: 0.036432460777993315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08078598976135254 0.23073887825012207

Final encoder loss: 0.03729912233343149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08074021339416504 0.23077988624572754

Final encoder loss: 0.036524871043301564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08133411407470703 0.23057246208190918

Final encoder loss: 0.03638763933029049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08171868324279785 0.23110055923461914

Final encoder loss: 0.0362896486588964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08131027221679688 0.2306678295135498

Final encoder loss: 0.03752468379021241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08242988586425781 0.23103857040405273

Final encoder loss: 0.03693414975996341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0813302993774414 0.23096585273742676

Final encoder loss: 0.03698153796885167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08202910423278809 0.23089981079101562

Final encoder loss: 0.035535119201092175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08117055892944336 0.2309103012084961

Final encoder loss: 0.03672973685041398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.09301018714904785 0.22977590560913086


Training emognition model
Final encoder loss: 0.19357675313949585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.251373291015625 0.04854130744934082

Final encoder loss: 0.19495907425880432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24805498123168945 0.04939866065979004

Final encoder loss: 0.08402979373931885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24963021278381348 0.04848170280456543

Final encoder loss: 0.08252114802598953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2494511604309082 0.04952549934387207

Final encoder loss: 0.05558883398771286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2488701343536377 0.04903912544250488

Final encoder loss: 0.053675614297389984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477715015411377 0.04834580421447754

Final encoder loss: 0.04396282881498337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24882268905639648 0.04842400550842285

Final encoder loss: 0.0428573414683342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24770736694335938 0.050545692443847656

Final encoder loss: 0.038493186235427856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24796199798583984 0.04919552803039551

Final encoder loss: 0.03779631480574608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469491958618164 0.05025291442871094

Final encoder loss: 0.035666417330503464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24896883964538574 0.04960179328918457

Final encoder loss: 0.035245269536972046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24732589721679688 0.04915881156921387

Final encoder loss: 0.034529708325862885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24995803833007812 0.049428701400756836

Final encoder loss: 0.034158479422330856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24834895133972168 0.051314592361450195

Final encoder loss: 0.03444512560963631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2466583251953125 0.049630165100097656

Final encoder loss: 0.03419888764619827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25063467025756836 0.048245906829833984

Final encoder loss: 0.03467027097940445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24748921394348145 0.049102067947387695

Final encoder loss: 0.034692686051130295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24703121185302734 0.04906320571899414

Final encoder loss: 0.03484612703323364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24729347229003906 0.04843878746032715

Final encoder loss: 0.03509122133255005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24850249290466309 0.05020880699157715

Final encoder loss: 0.03498712554574013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24742889404296875 0.050726890563964844

Final encoder loss: 0.034827183932065964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24777793884277344 0.049324750900268555

Final encoder loss: 0.03498106077313423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24878692626953125 0.04783034324645996

Final encoder loss: 0.03496099263429642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24664068222045898 0.04949069023132324

Final encoder loss: 0.034942079335451126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24782991409301758 0.0493617057800293

Final encoder loss: 0.03493775799870491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24666929244995117 0.04839348793029785

Final encoder loss: 0.03488941118121147
Final encoder loss: 0.03371991589665413

Training empatch model
Final encoder loss: 0.05761105933384139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07100033760070801 0.17398309707641602

Final encoder loss: 0.05636042555146036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.08644366264343262 0.17408323287963867

Final encoder loss: 0.052250504948520946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07137131690979004 0.17387652397155762

Final encoder loss: 0.049320173684837876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0720059871673584 0.17413067817687988

Final encoder loss: 0.050682493385999776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07175683975219727 0.17436981201171875

Final encoder loss: 0.05206361991171368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07085776329040527 0.174208402633667

Final encoder loss: 0.04424259435218158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07230830192565918 0.1735372543334961

Final encoder loss: 0.04850203851642765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07057380676269531 0.17363500595092773

Final encoder loss: 0.033608988533133884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07187747955322266 0.17383980751037598

Final encoder loss: 0.03484760434374274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07145905494689941 0.17382407188415527

Final encoder loss: 0.03549509108843961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07132458686828613 0.17469358444213867

Final encoder loss: 0.03402230996901274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07116246223449707 0.17348361015319824

Final encoder loss: 0.03866187230468653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07081937789916992 0.1738874912261963

Final encoder loss: 0.036462319852958294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07159566879272461 0.17401337623596191

Final encoder loss: 0.036034976854135155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07099556922912598 0.1742382049560547

Final encoder loss: 0.035344964424655255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07201194763183594 0.17409205436706543


Training empatch model
Final encoder loss: 0.17114205658435822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17787718772888184 0.0439152717590332

Final encoder loss: 0.07934259623289108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17681312561035156 0.04305887222290039

Final encoder loss: 0.05678648129105568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17422914505004883 0.043706655502319336

Final encoder loss: 0.04633864387869835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17580747604370117 0.04300665855407715

Final encoder loss: 0.04043862968683243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1756141185760498 0.04363417625427246

Final encoder loss: 0.036861129105091095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1775670051574707 0.04399871826171875

Final encoder loss: 0.034644193947315216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1753711700439453 0.04315996170043945

Final encoder loss: 0.0332418754696846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752305030822754 0.043463706970214844

Final encoder loss: 0.032387349754571915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1751108169555664 0.04385948181152344

Final encoder loss: 0.03190307691693306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754910945892334 0.044548749923706055

Final encoder loss: 0.03164812549948692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17450428009033203 0.04374241828918457

Final encoder loss: 0.031617067754268646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17453265190124512 0.04562067985534668

Final encoder loss: 0.031494516879320145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17564654350280762 0.043450355529785156

Final encoder loss: 0.03148621320724487

Training wesad model
Final encoder loss: 0.06642762089432855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07147574424743652 0.17361903190612793

Final encoder loss: 0.05602216608508305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07096052169799805 0.1738278865814209

Final encoder loss: 0.057344091401996554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07247114181518555 0.17414021492004395

Final encoder loss: 0.055436174970542984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07091116905212402 0.1736741065979004

Final encoder loss: 0.040970211449374024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07121896743774414 0.17388176918029785

Final encoder loss: 0.04052426744533761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07095956802368164 0.17399191856384277

Final encoder loss: 0.03890569611122111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07113862037658691 0.1739485263824463

Final encoder loss: 0.037892449967207704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.072357177734375 0.17361116409301758

Final encoder loss: 0.032509040466857694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.070953369140625 0.17406153678894043

Final encoder loss: 0.030837242554972836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07139706611633301 0.17415452003479004

Final encoder loss: 0.03175908259964602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07130098342895508 0.17390966415405273

Final encoder loss: 0.03042438335294143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0706474781036377 0.17369890213012695

Final encoder loss: 0.024533742137157765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07109642028808594 0.17375946044921875

Final encoder loss: 0.026341394516477438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07080841064453125 0.17415809631347656

Final encoder loss: 0.02691665924856076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07242679595947266 0.17457962036132812

Final encoder loss: 0.028034729890519677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07146239280700684 0.17368030548095703


Training wesad model
Final encoder loss: 0.21563920378684998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10614776611328125 0.03351616859436035

Final encoder loss: 0.09562724083662033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10524463653564453 0.03283572196960449

Final encoder loss: 0.06401746720075607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10414505004882812 0.03373146057128906

Final encoder loss: 0.04881828650832176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10372281074523926 0.03326153755187988

Final encoder loss: 0.04016952961683273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10592150688171387 0.03417491912841797

Final encoder loss: 0.03497392311692238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10379910469055176 0.03315091133117676

Final encoder loss: 0.031792499125003815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10425090789794922 0.03340315818786621

Final encoder loss: 0.029822364449501038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10404396057128906 0.03362298011779785

Final encoder loss: 0.028625810518860817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10566258430480957 0.033300161361694336

Final encoder loss: 0.028027724474668503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10379576683044434 0.03267812728881836

Final encoder loss: 0.027968844398856163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10379648208618164 0.03350400924682617

Final encoder loss: 0.028480932116508484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10427093505859375 0.03403449058532715

Final encoder loss: 0.02939341589808464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10528445243835449 0.032347679138183594

Final encoder loss: 0.030440544709563255

Calculating loss for amigos model
	Full Pass 0.6855008602142334
numFreeParamsPath 18
Reconstruction loss values: 0.03963644430041313 0.04958658665418625

Calculating loss for dapper model
	Full Pass 0.15190505981445312
numFreeParamsPath 18
Reconstruction loss values: 0.034418366849422455 0.03832347318530083

Calculating loss for case model
	Full Pass 0.909529447555542
numFreeParamsPath 18
Reconstruction loss values: 0.04646512493491173 0.04922877252101898

Calculating loss for emognition model
	Full Pass 0.29112839698791504
numFreeParamsPath 18
Reconstruction loss values: 0.04994550347328186 0.057869575917720795

Calculating loss for empatch model
	Full Pass 0.10446834564208984
numFreeParamsPath 18
Reconstruction loss values: 0.054549071937799454 0.06121218577027321

Calculating loss for wesad model
	Full Pass 0.0765538215637207
numFreeParamsPath 18
Reconstruction loss values: 0.05905921012163162 0.08040609210729599
Total loss calculation time: 3.946619749069214

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.91443395614624
Total epoch time: 150.72764325141907

Epoch: 27

Training emognition model
Final encoder loss: 0.049720749781414955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08913421630859375 0.28795289993286133

Final encoder loss: 0.04780329425061197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08390641212463379 0.2751626968383789

Final encoder loss: 0.04720514733986092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08285403251647949 0.27627062797546387

Final encoder loss: 0.04386544409032945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.0837087631225586 0.2752804756164551

Final encoder loss: 0.045467800866632824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08429169654846191 0.2744760513305664

Final encoder loss: 0.04617346159079768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08341717720031738 0.2754547595977783

Final encoder loss: 0.0435514520129752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08353090286254883 0.275576114654541

Final encoder loss: 0.043858542421484556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0837407112121582 0.27463769912719727

Final encoder loss: 0.04484753715265614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08316731452941895 0.2759370803833008

Final encoder loss: 0.0444561747281298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08374691009521484 0.2756311893463135

Final encoder loss: 0.0444069228958724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08459210395812988 0.2749300003051758

Final encoder loss: 0.04219025294949754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08338046073913574 0.2750215530395508

Final encoder loss: 0.043575569324560824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08365535736083984 0.27576494216918945

Final encoder loss: 0.044200090603905194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08445286750793457 0.2745535373687744

Final encoder loss: 0.04427812953970301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08310985565185547 0.27521371841430664

Final encoder loss: 0.042196180488329696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08259892463684082 0.27483367919921875


Training amigos model
Final encoder loss: 0.04155693099357156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10912561416625977 0.3888075351715088

Final encoder loss: 0.03882802352099876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.11272525787353516 0.390305757522583

Final encoder loss: 0.039371881983563596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10866284370422363 0.3894059658050537

Final encoder loss: 0.03453391072417769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10852861404418945 0.39011168479919434

Final encoder loss: 0.03965180035719449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10805201530456543 0.38852977752685547

Final encoder loss: 0.03673392793870444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10785531997680664 0.3894379138946533

Final encoder loss: 0.03483645408209775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10841608047485352 0.38898444175720215

Final encoder loss: 0.036863378936577734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10853838920593262 0.39037132263183594

Final encoder loss: 0.03636929916306762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.1087188720703125 0.389406681060791

Final encoder loss: 0.03603628613669362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1079399585723877 0.39128875732421875

Final encoder loss: 0.03638505261804371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10896658897399902 0.3890368938446045

Final encoder loss: 0.036728174727569445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10847616195678711 0.38909912109375

Final encoder loss: 0.0389964254811345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1076517105102539 0.38948822021484375

Final encoder loss: 0.03376136876175014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10821676254272461 0.3877253532409668

Final encoder loss: 0.03476642460556277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.1075277328491211 0.38805699348449707

Final encoder loss: 0.039714534736749636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.1021580696105957 0.3823373317718506


Training case model
Final encoder loss: 0.04883096715469163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09087467193603516 0.26413631439208984

Final encoder loss: 0.0433433920214249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09053325653076172 0.26308131217956543

Final encoder loss: 0.041197899361139184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0908656120300293 0.2635807991027832

Final encoder loss: 0.04048711390256764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09060049057006836 0.26376891136169434

Final encoder loss: 0.038895082390106216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09079885482788086 0.2644951343536377

Final encoder loss: 0.038916400969492705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09105920791625977 0.2658102512359619

Final encoder loss: 0.03779453199819123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09150934219360352 0.26525139808654785

Final encoder loss: 0.03737138008559701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0912024974822998 0.2651386260986328

Final encoder loss: 0.03688654074788445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09120655059814453 0.26463747024536133

Final encoder loss: 0.03628896600798809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09648990631103516 0.26471948623657227

Final encoder loss: 0.03669915494184296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0918722152709961 0.2646484375

Final encoder loss: 0.036156287552847596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09127140045166016 0.26529550552368164

Final encoder loss: 0.03466921793408363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09118008613586426 0.2651708126068115

Final encoder loss: 0.03620948589902377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09130334854125977 0.2651960849761963

Final encoder loss: 0.034676361069836896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09184074401855469 0.265472412109375

Final encoder loss: 0.03514662671215254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08893871307373047 0.26250386238098145


Training dapper model
Final encoder loss: 0.038245909224874426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06256103515625 0.15071916580200195

Final encoder loss: 0.03320166616501999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06182456016540527 0.1498558521270752

Final encoder loss: 0.03135927419039751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06310009956359863 0.1502065658569336

Final encoder loss: 0.032717991697032586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06148838996887207 0.1508190631866455

Final encoder loss: 0.029450476388196874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.061615943908691406 0.14861345291137695

Final encoder loss: 0.03556207327030411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0605473518371582 0.14896678924560547

Final encoder loss: 0.031560523417653384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.060613393783569336 0.1482985019683838

Final encoder loss: 0.02851329392436003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06069493293762207 0.14884495735168457

Final encoder loss: 0.028589558873649293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06081581115722656 0.14901041984558105

Final encoder loss: 0.031158683323897114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06080818176269531 0.14882969856262207

Final encoder loss: 0.02902772444046564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.060559988021850586 0.1480562686920166

Final encoder loss: 0.02904046015257035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06126666069030762 0.14775609970092773

Final encoder loss: 0.029150587785187664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0604705810546875 0.1481645107269287

Final encoder loss: 0.030210918951380884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06079530715942383 0.14885497093200684

Final encoder loss: 0.027274277772882896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06053805351257324 0.14954137802124023

Final encoder loss: 0.025486207199363297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06102800369262695 0.1484370231628418


Training amigos model
Final encoder loss: 0.026617510103713663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10574936866760254 0.3406071662902832

Final encoder loss: 0.028389076470804145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10578036308288574 0.3408052921295166

Final encoder loss: 0.028665431169577812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10566377639770508 0.34139466285705566

Final encoder loss: 0.028129881948445144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10626077651977539 0.3414759635925293

Final encoder loss: 0.028209686689397755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10589075088500977 0.3415374755859375

Final encoder loss: 0.02876568035169264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10621953010559082 0.3413066864013672

Final encoder loss: 0.02655463868782584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10631442070007324 0.3415682315826416

Final encoder loss: 0.02540650511259009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10637402534484863 0.3416309356689453

Final encoder loss: 0.027903851063172162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10596942901611328 0.3414437770843506

Final encoder loss: 0.027658121978901586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10626673698425293 0.34160947799682617

Final encoder loss: 0.03036447567822202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10608172416687012 0.34140467643737793

Final encoder loss: 0.027436100823682474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10638785362243652 0.34132885932922363

Final encoder loss: 0.027667391408348545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10540175437927246 0.34174442291259766

Final encoder loss: 0.027372023743092784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10752987861633301 0.341538667678833

Final encoder loss: 0.027008928839196936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10601949691772461 0.34133291244506836

Final encoder loss: 0.028912805827162276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10107231140136719 0.3378112316131592


Training amigos model
Final encoder loss: 0.18076427280902863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4604659080505371 0.0790703296661377

Final encoder loss: 0.18782465159893036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46831321716308594 0.07790112495422363

Final encoder loss: 0.18362866342067719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46251845359802246 0.0730123519897461

Final encoder loss: 0.07348542660474777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46148157119750977 0.07491374015808105

Final encoder loss: 0.07507450878620148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46044182777404785 0.0757758617401123

Final encoder loss: 0.06909973174333572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46163153648376465 0.07546472549438477

Final encoder loss: 0.044335585087537766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45655250549316406 0.07439732551574707

Final encoder loss: 0.04484336078166962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45625853538513184 0.07413196563720703

Final encoder loss: 0.04236125946044922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4562711715698242 0.07250308990478516

Final encoder loss: 0.03382508084177971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45403504371643066 0.07503652572631836

Final encoder loss: 0.034230586141347885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4579203128814697 0.0787971019744873

Final encoder loss: 0.03314337879419327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45661091804504395 0.0731821060180664

Final encoder loss: 0.029477182775735855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.454683780670166 0.07499265670776367

Final encoder loss: 0.029822437092661858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4568824768066406 0.07569360733032227

Final encoder loss: 0.029232226312160492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4575479030609131 0.07696771621704102

Final encoder loss: 0.028079599142074585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4565308094024658 0.07703876495361328

Final encoder loss: 0.02831934206187725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4550774097442627 0.07441544532775879

Final encoder loss: 0.028021490201354027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45716381072998047 0.07576251029968262

Final encoder loss: 0.02834787778556347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45463991165161133 0.07531356811523438

Final encoder loss: 0.028366422280669212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45679712295532227 0.0756680965423584

Final encoder loss: 0.02858654037117958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4574146270751953 0.07471656799316406

Final encoder loss: 0.028677769005298615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45636653900146484 0.07368230819702148

Final encoder loss: 0.028490222990512848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4561929702758789 0.07492375373840332

Final encoder loss: 0.028827523812651634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4568333625793457 0.07517027854919434

Final encoder loss: 0.02779613994061947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45624780654907227 0.07457542419433594

Final encoder loss: 0.02800043113529682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4561476707458496 0.07533526420593262

Final encoder loss: 0.02787124738097191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45719075202941895 0.07358431816101074

Final encoder loss: 0.02714592032134533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45748162269592285 0.07465839385986328

Final encoder loss: 0.027399690821766853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4568946361541748 0.07446622848510742

Final encoder loss: 0.02743212692439556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45673227310180664 0.07397270202636719

Final encoder loss: 0.02712828852236271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45772528648376465 0.07573103904724121

Final encoder loss: 0.02702922560274601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4557785987854004 0.07700872421264648

Final encoder loss: 0.027136584743857384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4567265510559082 0.07394170761108398

Final encoder loss: 0.02726748213171959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45797204971313477 0.07414817810058594

Final encoder loss: 0.02684219367802143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4565455913543701 0.0767669677734375

Final encoder loss: 0.027250004932284355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45682621002197266 0.07433152198791504

Final encoder loss: 0.02704193815588951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45630478858947754 0.07567906379699707

Final encoder loss: 0.026463458314538002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4551064968109131 0.0761256217956543

Final encoder loss: 0.02681204490363598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45848560333251953 0.07433462142944336

Final encoder loss: 0.02655820921063423
Final encoder loss: 0.02514614537358284
Final encoder loss: 0.024411065503954887

Training dapper model
Final encoder loss: 0.022189066605152612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05952572822570801 0.1064157485961914

Final encoder loss: 0.021648113781981067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059021949768066406 0.10616302490234375

Final encoder loss: 0.023532391418354115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05977678298950195 0.10599017143249512

Final encoder loss: 0.024333200955210074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05892348289489746 0.10614228248596191

Final encoder loss: 0.022995740342235345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05915045738220215 0.10658597946166992

Final encoder loss: 0.021128202516134995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05923771858215332 0.10577178001403809

Final encoder loss: 0.02342486862262912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.058870553970336914 0.10657191276550293

Final encoder loss: 0.020043576164662066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.0595393180847168 0.10664844512939453

Final encoder loss: 0.019745657931526345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05919241905212402 0.10617828369140625

Final encoder loss: 0.02034282510847631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05912423133850098 0.10680842399597168

Final encoder loss: 0.02196216444202821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05905461311340332 0.10653328895568848

Final encoder loss: 0.023566299410166507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05898141860961914 0.10656118392944336

Final encoder loss: 0.02184270189974339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05873727798461914 0.10654377937316895

Final encoder loss: 0.023133657230009885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.058699846267700195 0.10653471946716309

Final encoder loss: 0.020412517836024033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05915331840515137 0.10723114013671875

Final encoder loss: 0.022742877366389077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05877351760864258 0.10597801208496094


Training dapper model
Final encoder loss: 0.20245316624641418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1168220043182373 0.034268856048583984

Final encoder loss: 0.2081976979970932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11448860168457031 0.033782243728637695

Final encoder loss: 0.07582337409257889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11363387107849121 0.03411149978637695

Final encoder loss: 0.07685942947864532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11490273475646973 0.03374814987182617

Final encoder loss: 0.04498421773314476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11419320106506348 0.033742427825927734

Final encoder loss: 0.04422799497842789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11436700820922852 0.033693552017211914

Final encoder loss: 0.03211107477545738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11444258689880371 0.03340554237365723

Final encoder loss: 0.03154335170984268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11425232887268066 0.033489227294921875

Final encoder loss: 0.025955457240343094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1141357421875 0.033600807189941406

Final encoder loss: 0.025578318163752556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1143348217010498 0.03318333625793457

Final encoder loss: 0.022805126383900642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11392068862915039 0.03385281562805176

Final encoder loss: 0.022510815411806107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11400914192199707 0.03343558311462402

Final encoder loss: 0.021323995664715767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11437869071960449 0.03369617462158203

Final encoder loss: 0.02103508822619915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1141965389251709 0.0330815315246582

Final encoder loss: 0.020795051008462906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1138455867767334 0.032958984375

Final encoder loss: 0.020450668409466743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11434149742126465 0.03352212905883789

Final encoder loss: 0.020921412855386734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11420679092407227 0.03392815589904785

Final encoder loss: 0.020316733047366142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11394357681274414 0.033841609954833984

Final encoder loss: 0.020846018567681313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11464834213256836 0.03379011154174805

Final encoder loss: 0.02036202885210514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1136164665222168 0.03318905830383301

Final encoder loss: 0.02109736204147339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11478018760681152 0.03358769416809082

Final encoder loss: 0.020788410678505898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1142277717590332 0.033188819885253906

Final encoder loss: 0.02116149105131626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11438226699829102 0.0335078239440918

Final encoder loss: 0.021157195791602135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11667513847351074 0.034218788146972656

Final encoder loss: 0.02090352401137352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11567831039428711 0.034130096435546875

Final encoder loss: 0.021070122718811035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11445283889770508 0.03392457962036133

Final encoder loss: 0.020865879952907562
Final encoder loss: 0.019311359152197838

Training case model
Final encoder loss: 0.03211864930563609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08913159370422363 0.21894598007202148

Final encoder loss: 0.030941641162198965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08931446075439453 0.21888947486877441

Final encoder loss: 0.03188620627615424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09035992622375488 0.21918320655822754

Final encoder loss: 0.03152639358388746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08981847763061523 0.21933770179748535

Final encoder loss: 0.03145510388697557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08945918083190918 0.21938872337341309

Final encoder loss: 0.03138934418706268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09108233451843262 0.21940946578979492

Final encoder loss: 0.031541030776514156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09025168418884277 0.21896982192993164

Final encoder loss: 0.031149592063876342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08955717086791992 0.2193436622619629

Final encoder loss: 0.03144686024172606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08986473083496094 0.2189028263092041

Final encoder loss: 0.031392243430419686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08915972709655762 0.21892213821411133

Final encoder loss: 0.031411418047921386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09070777893066406 0.21952462196350098

Final encoder loss: 0.030825156152938328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08941411972045898 0.21912765502929688

Final encoder loss: 0.031182360782563544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0893247127532959 0.21888422966003418

Final encoder loss: 0.030762554885528034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09088683128356934 0.21924376487731934

Final encoder loss: 0.03107092519209071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08924078941345215 0.21918821334838867

Final encoder loss: 0.03105808525030764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08600306510925293 0.21587538719177246


Training case model
Final encoder loss: 0.202967569231987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26191043853759766 0.05203056335449219

Final encoder loss: 0.1889057159423828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571289539337158 0.055268287658691406

Final encoder loss: 0.19013521075248718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2597794532775879 0.052591562271118164

Final encoder loss: 0.19218024611473083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2582087516784668 0.05275535583496094

Final encoder loss: 0.1808178424835205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25954723358154297 0.0521845817565918

Final encoder loss: 0.1919182687997818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25675296783447266 0.051216840744018555

Final encoder loss: 0.0996309369802475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2594330310821533 0.05140805244445801

Final encoder loss: 0.08985057473182678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.260983943939209 0.05161571502685547

Final encoder loss: 0.08625790476799011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2599830627441406 0.0521540641784668

Final encoder loss: 0.08490978181362152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25852513313293457 0.053069353103637695

Final encoder loss: 0.07757517695426941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2624928951263428 0.05201387405395508

Final encoder loss: 0.08053766191005707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554483413696289 0.0520176887512207

Final encoder loss: 0.05911298468708992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26202964782714844 0.05226469039916992

Final encoder loss: 0.054013513028621674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25759363174438477 0.05236458778381348

Final encoder loss: 0.052310116589069366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25916266441345215 0.052175283432006836

Final encoder loss: 0.05267651751637459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580714225769043 0.05210471153259277

Final encoder loss: 0.04971212521195412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.260084867477417 0.052672386169433594

Final encoder loss: 0.050995953381061554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25536131858825684 0.05165576934814453

Final encoder loss: 0.04393947497010231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25823044776916504 0.05506086349487305

Final encoder loss: 0.04108200594782829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583622932434082 0.052014827728271484

Final encoder loss: 0.040209829807281494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25927019119262695 0.05343317985534668

Final encoder loss: 0.041063304990530014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579188346862793 0.052358150482177734

Final encoder loss: 0.04021301865577698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26145410537719727 0.05392718315124512

Final encoder loss: 0.040001753717660904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25560712814331055 0.05236077308654785

Final encoder loss: 0.038316499441862106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26111388206481934 0.05314350128173828

Final encoder loss: 0.03688758611679077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25928282737731934 0.05188751220703125

Final encoder loss: 0.03661102056503296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25867247581481934 0.05469870567321777

Final encoder loss: 0.03738562762737274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2587425708770752 0.05145382881164551

Final encoder loss: 0.037640929222106934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25687456130981445 0.05231165885925293

Final encoder loss: 0.03690857067704201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540700435638428 0.050477027893066406

Final encoder loss: 0.03653768450021744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568197250366211 0.05459165573120117

Final encoder loss: 0.03574860841035843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26022768020629883 0.053252220153808594

Final encoder loss: 0.03612043336033821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584571838378906 0.053856611251831055

Final encoder loss: 0.03634287416934967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575724124908447 0.052077293395996094

Final encoder loss: 0.03716248273849487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2596750259399414 0.052463531494140625

Final encoder loss: 0.03611596301198006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553231716156006 0.05101656913757324

Final encoder loss: 0.035137005150318146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2602677345275879 0.051795005798339844

Final encoder loss: 0.034476980566978455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26084351539611816 0.05166268348693848

Final encoder loss: 0.03429060056805611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25887537002563477 0.05251312255859375

Final encoder loss: 0.03474738821387291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26387596130371094 0.05135059356689453

Final encoder loss: 0.035148680210113525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26059603691101074 0.05232977867126465

Final encoder loss: 0.03435097262263298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25732851028442383 0.05241060256958008

Final encoder loss: 0.03402181714773178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578873634338379 0.05258440971374512

Final encoder loss: 0.03317064419388771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2613716125488281 0.05112409591674805

Final encoder loss: 0.03308834135532379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2591860294342041 0.05258321762084961

Final encoder loss: 0.0334465391933918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2606344223022461 0.054776668548583984

Final encoder loss: 0.0343719907104969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25875139236450195 0.05376100540161133

Final encoder loss: 0.033229079097509384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2586181163787842 0.053815364837646484

Final encoder loss: 0.033288147300481796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588050365447998 0.051607370376586914

Final encoder loss: 0.03264448419213295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2603137493133545 0.05436825752258301

Final encoder loss: 0.032520655542612076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25965213775634766 0.0519108772277832

Final encoder loss: 0.03294796496629715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26123619079589844 0.053037166595458984

Final encoder loss: 0.03366805240511894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25893449783325195 0.052904605865478516

Final encoder loss: 0.03282143175601959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25666093826293945 0.05283212661743164

Final encoder loss: 0.03277285397052765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25989580154418945 0.051432132720947266

Final encoder loss: 0.03221529349684715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.259641170501709 0.05560564994812012

Final encoder loss: 0.03223635256290436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25989508628845215 0.05334067344665527

Final encoder loss: 0.032386183738708496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25670766830444336 0.05311393737792969

Final encoder loss: 0.03333757445216179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2597832679748535 0.0523219108581543

Final encoder loss: 0.03223872184753418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25434350967407227 0.055292606353759766

Final encoder loss: 0.03242894262075424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2609415054321289 0.05416989326477051

Final encoder loss: 0.031975213438272476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583925724029541 0.05339241027832031

Final encoder loss: 0.03177604451775551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2589545249938965 0.0518643856048584

Final encoder loss: 0.03200988471508026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2598612308502197 0.050986528396606445

Final encoder loss: 0.032742708921432495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2587473392486572 0.054427146911621094

Final encoder loss: 0.03185882419347763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558939456939697 0.05163097381591797

Final encoder loss: 0.032160453498363495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25796985626220703 0.05030655860900879

Final encoder loss: 0.03143177554011345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25792479515075684 0.051621198654174805

Final encoder loss: 0.03143467381596565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2573864459991455 0.052031755447387695

Final encoder loss: 0.031596243381500244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572164535522461 0.05199074745178223

Final encoder loss: 0.032562922686338425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25644612312316895 0.0519101619720459

Final encoder loss: 0.031405288726091385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551894187927246 0.05026745796203613

Final encoder loss: 0.03179599717259407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577064037322998 0.05151033401489258

Final encoder loss: 0.03138361871242523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584102153778076 0.052530527114868164

Final encoder loss: 0.031134717166423798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25899600982666016 0.05305027961730957

Final encoder loss: 0.03145680949091911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25839686393737793 0.051360130310058594

Final encoder loss: 0.032136909663677216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2582526206970215 0.052507877349853516

Final encoder loss: 0.031454361975193024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556579113006592 0.05278635025024414

Final encoder loss: 0.031592823565006256
Final encoder loss: 0.03051297925412655
Final encoder loss: 0.0296440739184618
Final encoder loss: 0.028865037485957146
Final encoder loss: 0.028577808290719986
Final encoder loss: 0.02668941207230091

Training emognition model
Final encoder loss: 0.034866000964415465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08138847351074219 0.23054075241088867

Final encoder loss: 0.03792746201906454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08115720748901367 0.2303774356842041

Final encoder loss: 0.03473247812039914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08106493949890137 0.23095464706420898

Final encoder loss: 0.03344748846907185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08135747909545898 0.23033952713012695

Final encoder loss: 0.035846485623279925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0811471939086914 0.23050928115844727

Final encoder loss: 0.0344066943685403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08072757720947266 0.23046159744262695

Final encoder loss: 0.035826275461323165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08081507682800293 0.23054862022399902

Final encoder loss: 0.035485429783642206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08077263832092285 0.23058223724365234

Final encoder loss: 0.037478751641669986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08092689514160156 0.23088669776916504

Final encoder loss: 0.03599558745080687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08115649223327637 0.23052453994750977

Final encoder loss: 0.0374374256235398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08142328262329102 0.230421781539917

Final encoder loss: 0.036976771308756165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08196234703063965 0.23056387901306152

Final encoder loss: 0.03768954826489295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08066177368164062 0.22977685928344727

Final encoder loss: 0.03511834233429621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08085012435913086 0.23001766204833984

Final encoder loss: 0.03700886473280252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08078241348266602 0.2301185131072998

Final encoder loss: 0.035158907054626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08004331588745117 0.2292320728302002


Training emognition model
Final encoder loss: 0.1935521811246872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24958348274230957 0.04915976524353027

Final encoder loss: 0.1949814409017563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24715614318847656 0.04822421073913574

Final encoder loss: 0.08483786135911942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24807214736938477 0.04772758483886719

Final encoder loss: 0.08396337181329727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2461099624633789 0.048960208892822266

Final encoder loss: 0.05576523765921593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24704432487487793 0.04819798469543457

Final encoder loss: 0.05423982813954353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24595904350280762 0.04924178123474121

Final encoder loss: 0.04381170496344566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24678397178649902 0.04873347282409668

Final encoder loss: 0.04292866587638855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24892234802246094 0.04941511154174805

Final encoder loss: 0.038145046681165695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24877500534057617 0.04879045486450195

Final encoder loss: 0.03765823692083359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24826478958129883 0.048624277114868164

Final encoder loss: 0.0352700911462307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24875640869140625 0.05029630661010742

Final encoder loss: 0.0351000539958477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24799633026123047 0.04931759834289551

Final encoder loss: 0.03401477634906769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24993515014648438 0.04912710189819336

Final encoder loss: 0.03400596231222153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470695972442627 0.04965615272521973

Final encoder loss: 0.03379683196544647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24803590774536133 0.048616647720336914

Final encoder loss: 0.03386136144399643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24680638313293457 0.04881691932678223

Final encoder loss: 0.033934351056814194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485363483428955 0.04984283447265625

Final encoder loss: 0.0340680256485939
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741721153259277 0.049367427825927734

Final encoder loss: 0.0341271236538887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2490086555480957 0.04880833625793457

Final encoder loss: 0.034433748573064804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24819254875183105 0.050582170486450195

Final encoder loss: 0.03395523875951767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24855279922485352 0.04861307144165039

Final encoder loss: 0.03426814824342728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24879169464111328 0.04921126365661621

Final encoder loss: 0.03406474366784096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2486865520477295 0.04894828796386719

Final encoder loss: 0.034333061426877975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24654221534729004 0.049196481704711914

Final encoder loss: 0.03401884809136391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24875926971435547 0.04905867576599121

Final encoder loss: 0.03410351648926735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477126121520996 0.04829072952270508

Final encoder loss: 0.033975761383771896
Final encoder loss: 0.032842133194208145

Training empatch model
Final encoder loss: 0.05769074907067864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07103729248046875 0.17354893684387207

Final encoder loss: 0.051632465785663516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07159757614135742 0.17403507232666016

Final encoder loss: 0.049647435671692546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07130146026611328 0.17428803443908691

Final encoder loss: 0.049859281595631445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07121872901916504 0.17438840866088867

Final encoder loss: 0.046518734953099224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07192707061767578 0.17352700233459473

Final encoder loss: 0.046182825813046724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07095932960510254 0.17397022247314453

Final encoder loss: 0.046837759304182644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07192039489746094 0.17400050163269043

Final encoder loss: 0.0434440428536113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07109189033508301 0.17354369163513184

Final encoder loss: 0.03893152205143676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07133984565734863 0.17391347885131836

Final encoder loss: 0.03546654045452377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0712590217590332 0.17422127723693848

Final encoder loss: 0.03618655078316097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07126879692077637 0.17422080039978027

Final encoder loss: 0.03466391057104875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07204651832580566 0.1735372543334961

Final encoder loss: 0.03418651987519312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07094883918762207 0.17403435707092285

Final encoder loss: 0.03403020706407874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0724191665649414 0.17355060577392578

Final encoder loss: 0.03473634111164328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0713047981262207 0.1739654541015625

Final encoder loss: 0.03343992331073176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0706644058227539 0.1740555763244629


Training empatch model
Final encoder loss: 0.17116105556488037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1765286922454834 0.043779850006103516

Final encoder loss: 0.07951200753450394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758882999420166 0.04515242576599121

Final encoder loss: 0.05660578981041908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17446041107177734 0.04441189765930176

Final encoder loss: 0.045983221381902695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17642998695373535 0.04471755027770996

Final encoder loss: 0.03992893174290657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17432188987731934 0.043340444564819336

Final encoder loss: 0.036257270723581314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17579960823059082 0.04427504539489746

Final encoder loss: 0.03394757956266403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755678653717041 0.043723344802856445

Final encoder loss: 0.03249892219901085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17584800720214844 0.04458785057067871

Final encoder loss: 0.031537771224975586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17575597763061523 0.044303178787231445

Final encoder loss: 0.030941909179091454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1756882667541504 0.043973445892333984

Final encoder loss: 0.0306814294308424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17566871643066406 0.044234275817871094

Final encoder loss: 0.03047916479408741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17430472373962402 0.044869422912597656

Final encoder loss: 0.030366629362106323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17582941055297852 0.04353904724121094

Final encoder loss: 0.030267443507909775

Training wesad model
Final encoder loss: 0.06210035332402363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07184100151062012 0.17396903038024902

Final encoder loss: 0.058902016031003125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07163453102111816 0.17322063446044922

Final encoder loss: 0.05579398475624455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0707705020904541 0.17428851127624512

Final encoder loss: 0.05139954491219385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0711526870727539 0.17378878593444824

Final encoder loss: 0.041213018840990705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07119941711425781 0.17387866973876953

Final encoder loss: 0.038769569695239994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07164406776428223 0.17395591735839844

Final encoder loss: 0.03682329985553293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07068133354187012 0.17368173599243164

Final encoder loss: 0.03713505475520486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07254958152770996 0.17359638214111328

Final encoder loss: 0.030381984544196976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07102131843566895 0.17407655715942383

Final encoder loss: 0.028864156074310112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07041573524475098 0.17384600639343262

Final encoder loss: 0.031137230426546314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07110333442687988 0.17365217208862305

Final encoder loss: 0.03267412102480249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07121706008911133 0.17384910583496094

Final encoder loss: 0.025434911688095425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0722513198852539 0.17386531829833984

Final encoder loss: 0.02414296564200576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07076454162597656 0.17358851432800293

Final encoder loss: 0.026652765642343933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07178759574890137 0.17363810539245605

Final encoder loss: 0.025605714094412025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07117867469787598 0.17392253875732422


Training wesad model
Final encoder loss: 0.2155996859073639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10545921325683594 0.03337812423706055

Final encoder loss: 0.0957900807261467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1059110164642334 0.032849788665771484

Final encoder loss: 0.0636238157749176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10351753234863281 0.03324532508850098

Final encoder loss: 0.04813499376177788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10373640060424805 0.033061981201171875

Final encoder loss: 0.03935771435499191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10444188117980957 0.03406119346618652

Final encoder loss: 0.03420465439558029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1052396297454834 0.03269672393798828

Final encoder loss: 0.031121641397476196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393142700195312 0.03303384780883789

Final encoder loss: 0.029282819479703903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10345625877380371 0.033055782318115234

Final encoder loss: 0.028284618631005287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10488080978393555 0.03355836868286133

Final encoder loss: 0.02755565382540226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393810272216797 0.032991886138916016

Final encoder loss: 0.027271077036857605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10387611389160156 0.03250765800476074

Final encoder loss: 0.027186939492821693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409688949584961 0.03261971473693848

Final encoder loss: 0.027714746072888374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10505843162536621 0.03372645378112793

Final encoder loss: 0.0282264556735754

Calculating loss for amigos model
	Full Pass 0.6847453117370605
numFreeParamsPath 18
Reconstruction loss values: 0.03902588412165642 0.04857732355594635

Calculating loss for dapper model
	Full Pass 0.15091919898986816
numFreeParamsPath 18
Reconstruction loss values: 0.0311741903424263 0.03531404212117195

Calculating loss for case model
	Full Pass 0.8600454330444336
numFreeParamsPath 18
Reconstruction loss values: 0.04575563967227936 0.048784289509058

Calculating loss for emognition model
	Full Pass 0.2801644802093506
numFreeParamsPath 18
Reconstruction loss values: 0.04905763640999794 0.05747087299823761

Calculating loss for empatch model
	Full Pass 0.10480260848999023
numFreeParamsPath 18
Reconstruction loss values: 0.05276796966791153 0.05980094522237778

Calculating loss for wesad model
	Full Pass 0.07703924179077148
numFreeParamsPath 18
Reconstruction loss values: 0.057683102786540985 0.07905786484479904
Total loss calculation time: 3.840420722961426

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.9571850299835205
Total epoch time: 149.86225008964539

Epoch: 28

Training case model
Final encoder loss: 0.04343264017250934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09907245635986328 0.2748854160308838

Final encoder loss: 0.04166959857748518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09205412864685059 0.26601719856262207

Final encoder loss: 0.04010677095390568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0911099910736084 0.26505088806152344

Final encoder loss: 0.03847426005533116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09147477149963379 0.2656116485595703

Final encoder loss: 0.037468549948141756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09110140800476074 0.266298770904541

Final encoder loss: 0.03791848538776346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09162211418151855 0.2655770778656006

Final encoder loss: 0.036518211904211034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09226322174072266 0.264218807220459

Final encoder loss: 0.03752829927561877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09104681015014648 0.265087366104126

Final encoder loss: 0.035803602451697364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09149336814880371 0.2648935317993164

Final encoder loss: 0.03583687363174129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09460783004760742 0.2666592597961426

Final encoder loss: 0.03617017892357988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09119343757629395 0.26464176177978516

Final encoder loss: 0.03525022542803833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09090328216552734 0.26496267318725586

Final encoder loss: 0.03447311015932818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0916604995727539 0.2661592960357666

Final encoder loss: 0.03408125753327986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09129524230957031 0.2646358013153076

Final encoder loss: 0.03410265088416381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.0917348861694336 0.2651252746582031

Final encoder loss: 0.03355461370692603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08789658546447754 0.2627131938934326


Training emognition model
Final encoder loss: 0.047892558530418274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08348226547241211 0.2749979496002197

Final encoder loss: 0.050865732959687944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08496332168579102 0.27475953102111816

Final encoder loss: 0.04729748853414859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0832357406616211 0.2748219966888428

Final encoder loss: 0.046769080961265516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08317708969116211 0.2754967212677002

Final encoder loss: 0.04619186111482044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08403158187866211 0.27671098709106445

Final encoder loss: 0.04338720808592311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08365821838378906 0.27506017684936523

Final encoder loss: 0.045038780383595184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08450055122375488 0.27524590492248535

Final encoder loss: 0.04290055369628557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08326292037963867 0.275540828704834

Final encoder loss: 0.04525093821353726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08345508575439453 0.2755093574523926

Final encoder loss: 0.04537294724260028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08474874496459961 0.2760899066925049

Final encoder loss: 0.04383231959414719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0833427906036377 0.2749607563018799

Final encoder loss: 0.042160187596745335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08291888236999512 0.2755904197692871

Final encoder loss: 0.04211281689838545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08363795280456543 0.27664995193481445

Final encoder loss: 0.04064460959035199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08355236053466797 0.2756693363189697

Final encoder loss: 0.04296045586635671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08445453643798828 0.2748401165008545

Final encoder loss: 0.042946461556201465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0857231616973877 0.27434706687927246


Training amigos model
Final encoder loss: 0.03658957337512044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10830426216125488 0.38895726203918457

Final encoder loss: 0.0398906771450546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10761237144470215 0.38979363441467285

Final encoder loss: 0.03738755169099654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10880565643310547 0.3904550075531006

Final encoder loss: 0.035319433023937426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10982084274291992 0.38947486877441406

Final encoder loss: 0.03827015048857804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10866904258728027 0.38851094245910645

Final encoder loss: 0.03510713437227755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10805249214172363 0.3885183334350586

Final encoder loss: 0.036484294523884984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10811042785644531 0.38785386085510254

Final encoder loss: 0.03524904460484256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10760211944580078 0.38840484619140625

Final encoder loss: 0.036478884497919806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10771775245666504 0.3881075382232666

Final encoder loss: 0.033266961752295675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.12316298484802246 0.3873910903930664

Final encoder loss: 0.03885228685314315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10784006118774414 0.3880593776702881

Final encoder loss: 0.03700722011315796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10752320289611816 0.38934826850891113

Final encoder loss: 0.041277408973324256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10846209526062012 0.3892369270324707

Final encoder loss: 0.03669697901743041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10851240158081055 0.38941025733947754

Final encoder loss: 0.035040239125150814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10832381248474121 0.3892033100128174

Final encoder loss: 0.036508776309939234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10311555862426758 0.38378405570983887


Training dapper model
Final encoder loss: 0.03589381253455488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06279778480529785 0.15010404586791992

Final encoder loss: 0.02979057694655136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06182861328125 0.15065717697143555

Final encoder loss: 0.02868620758745258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06271553039550781 0.15142035484313965

Final encoder loss: 0.027516767010799983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06228160858154297 0.15060091018676758

Final encoder loss: 0.03264533775527302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06209850311279297 0.15117406845092773

Final encoder loss: 0.0325928346030209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.0630035400390625 0.15079236030578613

Final encoder loss: 0.027346425412234723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06217598915100098 0.15058374404907227

Final encoder loss: 0.03156666820488152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06234931945800781 0.15118074417114258

Final encoder loss: 0.02809279555043196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.0619204044342041 0.15100979804992676

Final encoder loss: 0.02515512008431557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06205391883850098 0.15157842636108398

Final encoder loss: 0.03221541601985189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06342315673828125 0.1508939266204834

Final encoder loss: 0.029050533718188474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06240200996398926 0.1504826545715332

Final encoder loss: 0.028356693776180084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06196951866149902 0.15110111236572266

Final encoder loss: 0.02441729065517006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06209540367126465 0.15061140060424805

Final encoder loss: 0.02925318691964644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06205320358276367 0.15097951889038086

Final encoder loss: 0.029418372001235808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06275367736816406 0.15033245086669922


Training amigos model
Final encoder loss: 0.026441397215436556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10623955726623535 0.3415558338165283

Final encoder loss: 0.0268022343627628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10605764389038086 0.3418867588043213

Final encoder loss: 0.028026449163354766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1067194938659668 0.34149694442749023

Final encoder loss: 0.024141142551391458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10695433616638184 0.3417699337005615

Final encoder loss: 0.02745733827365313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1063997745513916 0.34180760383605957

Final encoder loss: 0.026470993116968558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10637283325195312 0.3413848876953125

Final encoder loss: 0.02573340386259717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10621523857116699 0.34185266494750977

Final encoder loss: 0.028625546276085673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10610008239746094 0.34191036224365234

Final encoder loss: 0.028634779102922274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10666632652282715 0.34142446517944336

Final encoder loss: 0.02721456857837382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10671782493591309 0.3415684700012207

Final encoder loss: 0.02672526620955051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10594868659973145 0.34159016609191895

Final encoder loss: 0.023844791017231033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10635638236999512 0.34156203269958496

Final encoder loss: 0.027650765536147964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10641622543334961 0.3416318893432617

Final encoder loss: 0.03065702693794899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10686969757080078 0.34153199195861816

Final encoder loss: 0.030029866395578537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10720467567443848 0.34189367294311523

Final encoder loss: 0.024496839096395665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10141396522521973 0.33863234519958496


Training amigos model
Final encoder loss: 0.1807543933391571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4405686855316162 0.07532954216003418

Final encoder loss: 0.1878504455089569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47727370262145996 0.07343387603759766

Final encoder loss: 0.18362899124622345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.442476749420166 0.07592463493347168

Final encoder loss: 0.07399029284715652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.459331750869751 0.07470011711120605

Final encoder loss: 0.07511278986930847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.39440131187438965 0.07441949844360352

Final encoder loss: 0.06917738914489746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44351840019226074 0.07325887680053711

Final encoder loss: 0.0445798821747303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.40891551971435547 0.07494449615478516

Final encoder loss: 0.04473596066236496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45771312713623047 0.07596850395202637

Final encoder loss: 0.04236842319369316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42175936698913574 0.07442998886108398

Final encoder loss: 0.033766184002161026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45421886444091797 0.0749962329864502

Final encoder loss: 0.03405154123902321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4701392650604248 0.07887887954711914

Final encoder loss: 0.03305654972791672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4229862689971924 0.0755317211151123

Final encoder loss: 0.02922164462506771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45544886589050293 0.07434201240539551

Final encoder loss: 0.029613228514790535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47260093688964844 0.07791829109191895

Final encoder loss: 0.029043985530734062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44345927238464355 0.07662343978881836

Final encoder loss: 0.027806390076875687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4606022834777832 0.0741877555847168

Final encoder loss: 0.028047913685441017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43116188049316406 0.07388138771057129

Final encoder loss: 0.02756991796195507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4184751510620117 0.07253336906433105

Final encoder loss: 0.027827104553580284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4461634159088135 0.07513785362243652

Final encoder loss: 0.028014104813337326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4637730121612549 0.0768899917602539

Final encoder loss: 0.027682442218065262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45108866691589355 0.07757925987243652

Final encoder loss: 0.02827465906739235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46967482566833496 0.07648062705993652

Final encoder loss: 0.02853759191930294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.39560675621032715 0.0780479907989502

Final encoder loss: 0.0279877670109272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.435819149017334 0.07727527618408203

Final encoder loss: 0.0276341512799263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.40972208976745605 0.0758829116821289

Final encoder loss: 0.02802305668592453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.39482975006103516 0.08076190948486328

Final encoder loss: 0.02758249267935753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4190785884857178 0.07795000076293945

Final encoder loss: 0.026925427839159966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4481055736541748 0.07412934303283691

Final encoder loss: 0.027025418356060982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.458498477935791 0.07938814163208008

Final encoder loss: 0.027055950835347176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4541633129119873 0.07273721694946289

Final encoder loss: 0.026490064337849617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4682135581970215 0.07418155670166016

Final encoder loss: 0.026524150744080544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44857096672058105 0.07438325881958008

Final encoder loss: 0.02666959911584854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44486188888549805 0.07309746742248535

Final encoder loss: 0.026635807007551193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4632411003112793 0.07586359977722168

Final encoder loss: 0.026514718309044838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4604053497314453 0.07558298110961914

Final encoder loss: 0.026726754382252693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44414758682250977 0.07508134841918945

Final encoder loss: 0.02642058953642845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4602780342102051 0.07577252388000488

Final encoder loss: 0.026226716116070747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4527852535247803 0.07458114624023438

Final encoder loss: 0.02627108246088028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44321250915527344 0.0729520320892334

Final encoder loss: 0.026119939982891083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46172380447387695 0.07562851905822754

Final encoder loss: 0.02610582299530506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45925378799438477 0.07557487487792969

Final encoder loss: 0.025905219838023186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44326329231262207 0.0734102725982666

Final encoder loss: 0.025910258293151855
Final encoder loss: 0.02480624057352543
Final encoder loss: 0.023703765124082565

Training dapper model
Final encoder loss: 0.02205624817645257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05953216552734375 0.10637831687927246

Final encoder loss: 0.023975993421321087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05891871452331543 0.10626769065856934

Final encoder loss: 0.022423674984772057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0590817928314209 0.10612010955810547

Final encoder loss: 0.02236004602931393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0590662956237793 0.1064918041229248

Final encoder loss: 0.022874411985462858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05933380126953125 0.10631012916564941

Final encoder loss: 0.02107179889121832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05913233757019043 0.10654783248901367

Final encoder loss: 0.021930350671805772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059122323989868164 0.10657453536987305

Final encoder loss: 0.019986372175340723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.0589144229888916 0.10663223266601562

Final encoder loss: 0.028338476317758328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05950021743774414 0.10652565956115723

Final encoder loss: 0.02223921079395451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05919671058654785 0.10653877258300781

Final encoder loss: 0.021964202720882352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05937027931213379 0.10639810562133789

Final encoder loss: 0.020520363684785745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05914807319641113 0.10640501976013184

Final encoder loss: 0.023392156016339996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.0594782829284668 0.10715198516845703

Final encoder loss: 0.02417131796111062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059203147888183594 0.1061408519744873

Final encoder loss: 0.022884638226971305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0591278076171875 0.10687828063964844

Final encoder loss: 0.023500686800240103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05907464027404785 0.10597991943359375


Training dapper model
Final encoder loss: 0.20247389376163483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11456537246704102 0.0337977409362793

Final encoder loss: 0.20820093154907227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11577105522155762 0.03389906883239746

Final encoder loss: 0.0769893229007721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11259198188781738 0.03390336036682129

Final encoder loss: 0.07743890583515167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159212589263916 0.03361153602600098

Final encoder loss: 0.04546219855546951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11246085166931152 0.033872127532958984

Final encoder loss: 0.04441870376467705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11513161659240723 0.033635616302490234

Final encoder loss: 0.03234844282269478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11316156387329102 0.03380846977233887

Final encoder loss: 0.03163255751132965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11537313461303711 0.03391098976135254

Final encoder loss: 0.025989955291152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11290097236633301 0.03362774848937988

Final encoder loss: 0.025669435039162636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1156308650970459 0.03306269645690918

Final encoder loss: 0.022828763350844383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11313867568969727 0.03395652770996094

Final encoder loss: 0.022668976336717606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11539649963378906 0.03341555595397949

Final encoder loss: 0.021231533959507942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11287617683410645 0.03368330001831055

Final encoder loss: 0.021234650164842606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11504364013671875 0.03380012512207031

Final encoder loss: 0.02060881070792675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11231112480163574 0.034059762954711914

Final encoder loss: 0.020522858947515488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11610651016235352 0.033795833587646484

Final encoder loss: 0.020651787519454956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11270904541015625 0.03396105766296387

Final encoder loss: 0.02038576826453209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11561417579650879 0.034123897552490234

Final encoder loss: 0.020864907652139664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11224818229675293 0.03372907638549805

Final encoder loss: 0.020373918116092682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11496663093566895 0.033905029296875

Final encoder loss: 0.02085503563284874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11255216598510742 0.033959388732910156

Final encoder loss: 0.020682577043771744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11548089981079102 0.03380870819091797

Final encoder loss: 0.020867835730314255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11320161819458008 0.03319382667541504

Final encoder loss: 0.02079867199063301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11530184745788574 0.03362131118774414

Final encoder loss: 0.020572055131196976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1129157543182373 0.03346562385559082

Final encoder loss: 0.020794153213500977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559915542602539 0.03371071815490723

Final encoder loss: 0.02019241452217102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11188769340515137 0.03363299369812012

Final encoder loss: 0.020665546879172325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11573553085327148 0.03338050842285156

Final encoder loss: 0.01997934654355049
Final encoder loss: 0.018680788576602936

Training case model
Final encoder loss: 0.03298387356021317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08877682685852051 0.21833276748657227

Final encoder loss: 0.032939038828607874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08902525901794434 0.21837663650512695

Final encoder loss: 0.031751505130451656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08878040313720703 0.21863365173339844

Final encoder loss: 0.03164893677939245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08956217765808105 0.2182936668395996

Final encoder loss: 0.031541076600322904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0894632339477539 0.21835613250732422

Final encoder loss: 0.030927590322366364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0887751579284668 0.2182459831237793

Final encoder loss: 0.03155134713623603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.0889289379119873 0.21848511695861816

Final encoder loss: 0.030761450228486645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08858919143676758 0.21836066246032715

Final encoder loss: 0.03085763090333064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.10131335258483887 0.2183077335357666

Final encoder loss: 0.030467148527532278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08909368515014648 0.21840381622314453

Final encoder loss: 0.03225297565678071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.0893547534942627 0.2182297706604004

Final encoder loss: 0.031584284334970736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08928680419921875 0.21845674514770508

Final encoder loss: 0.03017643257983092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08867406845092773 0.2185680866241455

Final encoder loss: 0.03111134226544725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.0894627571105957 0.2185356616973877

Final encoder loss: 0.030592127079452414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08889317512512207 0.21860599517822266

Final encoder loss: 0.03123212540282889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08600974082946777 0.21502017974853516


Training case model
Final encoder loss: 0.2029588669538498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2620413303375244 0.0518183708190918

Final encoder loss: 0.18890832364559174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25701212882995605 0.05283403396606445

Final encoder loss: 0.1901520937681198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2672243118286133 0.05248308181762695

Final encoder loss: 0.19218866527080536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2677299976348877 0.05312919616699219

Final encoder loss: 0.18080368638038635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2572488784790039 0.050672292709350586

Final encoder loss: 0.19192259013652802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25530004501342773 0.051979780197143555

Final encoder loss: 0.09998054057359695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25736331939697266 0.05112290382385254

Final encoder loss: 0.08996370434761047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25893735885620117 0.05229544639587402

Final encoder loss: 0.08634626865386963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26586031913757324 0.051516056060791016

Final encoder loss: 0.08524561673402786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576327323913574 0.05240130424499512

Final encoder loss: 0.07791221886873245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2663123607635498 0.05141043663024902

Final encoder loss: 0.08048764616250992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25511932373046875 0.05249452590942383

Final encoder loss: 0.05926079675555229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25834131240844727 0.05093884468078613

Final encoder loss: 0.05404316261410713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25702476501464844 0.0517275333404541

Final encoder loss: 0.052347876131534576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570643424987793 0.05252671241760254

Final encoder loss: 0.05296188220381737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2655465602874756 0.05188775062561035

Final encoder loss: 0.049891307950019836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574918270111084 0.052178144454956055

Final encoder loss: 0.05128135532140732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547760009765625 0.05029129981994629

Final encoder loss: 0.04398602247238159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25774717330932617 0.05200481414794922

Final encoder loss: 0.0411539152264595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26769328117370605 0.05213284492492676

Final encoder loss: 0.040235765278339386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2663571834564209 0.052786827087402344

Final encoder loss: 0.04117482528090477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2565340995788574 0.05102849006652832

Final encoder loss: 0.03998419642448425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578716278076172 0.05231213569641113

Final encoder loss: 0.04004940763115883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25344276428222656 0.05073118209838867

Final encoder loss: 0.03844107314944267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2667734622955322 0.0516657829284668

Final encoder loss: 0.03696030378341675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564849853515625 0.05144524574279785

Final encoder loss: 0.036446116864681244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25830721855163574 0.0520777702331543

Final encoder loss: 0.03728577494621277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27673840522766113 0.05103564262390137

Final encoder loss: 0.037648897618055344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2681448459625244 0.05298447608947754

Final encoder loss: 0.03663460165262222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546377182006836 0.05292367935180664

Final encoder loss: 0.03649184852838516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25844335556030273 0.05205416679382324

Final encoder loss: 0.035609304904937744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25802087783813477 0.05373668670654297

Final encoder loss: 0.03590733930468559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2691366672515869 0.05309247970581055

Final encoder loss: 0.036095451563596725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26060962677001953 0.0539243221282959

Final encoder loss: 0.036734893918037415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25890469551086426 0.05251598358154297

Final encoder loss: 0.03589755669236183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557713985443115 0.052085161209106445

Final encoder loss: 0.03512122854590416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26056933403015137 0.05088543891906738

Final encoder loss: 0.03441224247217178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25637030601501465 0.05310797691345215

Final encoder loss: 0.03410518914461136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25947046279907227 0.05355334281921387

Final encoder loss: 0.03457176685333252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2677326202392578 0.052480459213256836

Final encoder loss: 0.0351681113243103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27030229568481445 0.05459260940551758

Final encoder loss: 0.034392934292554855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25511765480041504 0.05207371711730957

Final encoder loss: 0.03366905450820923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2625410556793213 0.05156087875366211

Final encoder loss: 0.033005353063344955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26804184913635254 0.05201578140258789

Final encoder loss: 0.032777298241853714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2894401550292969 0.05224251747131348

Final encoder loss: 0.03323608264327049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25803112983703613 0.05173301696777344

Final encoder loss: 0.03387044742703438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27039265632629395 0.05142378807067871

Final encoder loss: 0.033241212368011475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25431203842163086 0.05185866355895996

Final encoder loss: 0.03299587965011597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2611713409423828 0.05171823501586914

Final encoder loss: 0.03243241086602211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2822873592376709 0.05282187461853027

Final encoder loss: 0.03231902793049812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.29056835174560547 0.05095863342285156

Final encoder loss: 0.03264899179339409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25661444664001465 0.0535740852355957

Final encoder loss: 0.03352682292461395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2886466979980469 0.05106639862060547

Final encoder loss: 0.03255486115813255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25587964057922363 0.051854848861694336

Final encoder loss: 0.032583191990852356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26098179817199707 0.05233407020568848

Final encoder loss: 0.032057635486125946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25780153274536133 0.05291891098022461

Final encoder loss: 0.03201906010508537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.259493350982666 0.05650973320007324

Final encoder loss: 0.032311417162418365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25815582275390625 0.05224752426147461

Final encoder loss: 0.0328681655228138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25949740409851074 0.05568838119506836

Final encoder loss: 0.03195023909211159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255401611328125 0.052101850509643555

Final encoder loss: 0.03210876137018204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26026296615600586 0.05385589599609375

Final encoder loss: 0.03167935460805893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2596149444580078 0.05271744728088379

Final encoder loss: 0.03149869665503502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.260866641998291 0.05237698554992676

Final encoder loss: 0.03178689628839493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2598445415496826 0.05199432373046875

Final encoder loss: 0.032503142952919006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574942111968994 0.05402994155883789

Final encoder loss: 0.03177942335605621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25660157203674316 0.05033397674560547

Final encoder loss: 0.03179176524281502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577047348022461 0.053687334060668945

Final encoder loss: 0.03126002103090286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2598865032196045 0.052606821060180664

Final encoder loss: 0.031020406633615494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2676265239715576 0.05298566818237305

Final encoder loss: 0.03126247972249985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2606968879699707 0.05329465866088867

Final encoder loss: 0.032010648399591446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26760387420654297 0.055275678634643555

Final encoder loss: 0.03133662790060043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25726771354675293 0.05291604995727539

Final encoder loss: 0.031546562910079956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2579524517059326 0.05379223823547363

Final encoder loss: 0.03097604401409626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.259824275970459 0.05162358283996582

Final encoder loss: 0.030937092378735542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2670023441314697 0.05303215980529785

Final encoder loss: 0.031198829412460327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2824718952178955 0.050657033920288086

Final encoder loss: 0.03184503689408302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25885891914367676 0.05282759666442871

Final encoder loss: 0.031040940433740616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.256641149520874 0.050969839096069336

Final encoder loss: 0.03127741813659668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587618827819824 0.05213737487792969

Final encoder loss: 0.03079666756093502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2843170166015625 0.05222463607788086

Final encoder loss: 0.030731648206710815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26058292388916016 0.05187392234802246

Final encoder loss: 0.030933856964111328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26815080642700195 0.053041696548461914

Final encoder loss: 0.03170346841216087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25679969787597656 0.051197052001953125

Final encoder loss: 0.030852030962705612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25963330268859863 0.051711082458496094

Final encoder loss: 0.031164037063717842
Final encoder loss: 0.030116895213723183
Final encoder loss: 0.02915354073047638
Final encoder loss: 0.028540488332509995
Final encoder loss: 0.028126278892159462
Final encoder loss: 0.026412030681967735

Training emognition model
Final encoder loss: 0.03647335514142318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08148884773254395 0.23085451126098633

Final encoder loss: 0.03463481561716155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08261775970458984 0.2314286231994629

Final encoder loss: 0.03596173546446345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08167576789855957 0.23064684867858887

Final encoder loss: 0.034774065584154454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0813283920288086 0.23079276084899902

Final encoder loss: 0.0351822488723385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08237671852111816 0.23077893257141113

Final encoder loss: 0.037525115883775814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08135604858398438 0.23013639450073242

Final encoder loss: 0.034599109654954725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08160233497619629 0.23119759559631348

Final encoder loss: 0.03469421789952723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08055353164672852 0.2307875156402588

Final encoder loss: 0.035807032668028725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08088159561157227 0.23092341423034668

Final encoder loss: 0.03582156176389528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08286118507385254 0.23098540306091309

Final encoder loss: 0.03681587372474809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08087015151977539 0.23080039024353027

Final encoder loss: 0.03704825338158404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0808858871459961 0.23104524612426758

Final encoder loss: 0.034334025312551524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08246874809265137 0.2309269905090332

Final encoder loss: 0.035607711492519144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08142566680908203 0.2307894229888916

Final encoder loss: 0.033881006131837456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08129215240478516 0.23110628128051758

Final encoder loss: 0.034905605150254426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08278369903564453 0.23017406463623047


Training emognition model
Final encoder loss: 0.1935664564371109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502455711364746 0.04978036880493164

Final encoder loss: 0.19496595859527588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24872374534606934 0.04767584800720215

Final encoder loss: 0.08505742996931076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24819445610046387 0.0487971305847168

Final encoder loss: 0.08453516662120819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24848604202270508 0.048736572265625

Final encoder loss: 0.05567490682005882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484900951385498 0.04706740379333496

Final encoder loss: 0.05436057969927788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24850010871887207 0.051848411560058594

Final encoder loss: 0.04361622408032417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2481520175933838 0.04918384552001953

Final encoder loss: 0.04274493083357811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24785780906677246 0.05066823959350586

Final encoder loss: 0.037863075733184814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24870562553405762 0.048067569732666016

Final encoder loss: 0.03728683292865753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24780631065368652 0.05015897750854492

Final encoder loss: 0.034959323704242706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25096774101257324 0.048735618591308594

Final encoder loss: 0.03455277159810066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24963736534118652 0.04936361312866211

Final encoder loss: 0.0335824191570282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24846267700195312 0.049936771392822266

Final encoder loss: 0.03337492421269417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24814963340759277 0.04970073699951172

Final encoder loss: 0.033324163407087326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2465062141418457 0.0482792854309082

Final encoder loss: 0.0332566574215889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24701786041259766 0.048609018325805664

Final encoder loss: 0.03342633321881294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474205493927002 0.04868006706237793

Final encoder loss: 0.03355314955115318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24609780311584473 0.05019831657409668

Final encoder loss: 0.03352430462837219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24697589874267578 0.049043893814086914

Final encoder loss: 0.03375415503978729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24583005905151367 0.049318790435791016

Final encoder loss: 0.03350043296813965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24733471870422363 0.04933953285217285

Final encoder loss: 0.03359135612845421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464158535003662 0.04900932312011719

Final encoder loss: 0.033458907157182693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24840521812438965 0.049340009689331055

Final encoder loss: 0.03335428610444069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24727678298950195 0.04964780807495117

Final encoder loss: 0.03353683277964592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2475275993347168 0.04836249351501465

Final encoder loss: 0.03332226350903511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24706649780273438 0.049144744873046875

Final encoder loss: 0.033332910388708115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24835944175720215 0.0490720272064209

Final encoder loss: 0.033515218645334244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247300386428833 0.04901742935180664

Final encoder loss: 0.0330708883702755
Final encoder loss: 0.03206202760338783

Training empatch model
Final encoder loss: 0.05408267032041367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07103967666625977 0.17341065406799316

Final encoder loss: 0.05128839360253955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07154083251953125 0.17315077781677246

Final encoder loss: 0.05058428608293941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07139968872070312 0.17395639419555664

Final encoder loss: 0.048263257366205486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07106184959411621 0.17377710342407227

Final encoder loss: 0.04485700978602612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07104158401489258 0.17371702194213867

Final encoder loss: 0.047035017456597814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07203173637390137 0.17373061180114746

Final encoder loss: 0.0465360781744282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07140874862670898 0.17371916770935059

Final encoder loss: 0.04350661682412113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07067084312438965 0.1731884479522705

Final encoder loss: 0.036694434208025165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07144689559936523 0.1740262508392334

Final encoder loss: 0.03411481856569703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07111310958862305 0.17429041862487793

Final encoder loss: 0.034390112238555155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07105350494384766 0.1736922264099121

Final encoder loss: 0.03431038992473728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07099151611328125 0.17363548278808594

Final encoder loss: 0.035960606318434465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07146072387695312 0.17405247688293457

Final encoder loss: 0.03541234748195357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07103872299194336 0.1735668182373047

Final encoder loss: 0.03356001259348474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0715024471282959 0.17380237579345703

Final encoder loss: 0.03355580740687536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07082700729370117 0.17357730865478516


Training empatch model
Final encoder loss: 0.17116515338420868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17709016799926758 0.04360842704772949

Final encoder loss: 0.07997852563858032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504215240478516 0.042913198471069336

Final encoder loss: 0.05681166425347328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17529511451721191 0.0437464714050293

Final encoder loss: 0.0460343100130558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17458200454711914 0.042815446853637695

Final encoder loss: 0.039876583963632584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1741166114807129 0.04287314414978027

Final encoder loss: 0.036085594445466995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17400360107421875 0.04349875450134277

Final encoder loss: 0.033709634095430374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17401480674743652 0.04325127601623535

Final encoder loss: 0.03220599144697189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17421746253967285 0.042548179626464844

Final encoder loss: 0.03127703815698624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17441058158874512 0.042801856994628906

Final encoder loss: 0.03065231442451477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1732649803161621 0.042980194091796875

Final encoder loss: 0.03032301366329193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17449140548706055 0.04353833198547363

Final encoder loss: 0.03011835552752018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17382001876831055 0.0438535213470459

Final encoder loss: 0.030079757794737816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17454743385314941 0.04249453544616699

Final encoder loss: 0.029921850189566612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1738591194152832 0.042548418045043945

Final encoder loss: 0.02980993688106537

Training wesad model
Final encoder loss: 0.05463844821027882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07039737701416016 0.1730802059173584

Final encoder loss: 0.055530585028398395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07063698768615723 0.17254114151000977

Final encoder loss: 0.05393297541035277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07082080841064453 0.17291712760925293

Final encoder loss: 0.05309562356852458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07050919532775879 0.17241430282592773

Final encoder loss: 0.03651897153934477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07032465934753418 0.17371225357055664

Final encoder loss: 0.03713295124279726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07086348533630371 0.17331171035766602

Final encoder loss: 0.03730310894900742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07150816917419434 0.17412567138671875

Final encoder loss: 0.03819448682895182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07133221626281738 0.17351365089416504

Final encoder loss: 0.030010435910361986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0706949234008789 0.17406582832336426

Final encoder loss: 0.030249462116711763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07211041450500488 0.1739518642425537

Final encoder loss: 0.029172037462180705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0711209774017334 0.17398905754089355

Final encoder loss: 0.03052424158272975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07175040245056152 0.17415976524353027

Final encoder loss: 0.02344233252889578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07109713554382324 0.17342257499694824

Final encoder loss: 0.024794895175925574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07093930244445801 0.17425918579101562

Final encoder loss: 0.024585036507161113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0708620548248291 0.17339277267456055

Final encoder loss: 0.026395352202646084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07086873054504395 0.1740128993988037


Training wesad model
Final encoder loss: 0.21561869978904724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10549783706665039 0.03279876708984375

Final encoder loss: 0.09578938782215118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10454416275024414 0.03313112258911133

Final encoder loss: 0.06310821324586868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10432100296020508 0.03382730484008789

Final encoder loss: 0.047560591250658035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10468864440917969 0.034085988998413086

Final encoder loss: 0.03885572776198387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10467720031738281 0.033136606216430664

Final encoder loss: 0.033703599125146866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10339069366455078 0.033179521560668945

Final encoder loss: 0.0305772814899683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384321212768555 0.03339266777038574

Final encoder loss: 0.02864673174917698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10513806343078613 0.0339047908782959

Final encoder loss: 0.027506988495588303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042790412902832 0.03351998329162598

Final encoder loss: 0.02687816321849823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10514521598815918 0.03339362144470215

Final encoder loss: 0.026749638840556145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1039741039276123 0.03323054313659668

Final encoder loss: 0.02684687450528145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10520052909851074 0.03239083290100098

Final encoder loss: 0.027002569288015366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10439848899841309 0.033405303955078125

Final encoder loss: 0.027010520920157433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10383987426757812 0.0334017276763916

Final encoder loss: 0.027380671352148056

Calculating loss for amigos model
	Full Pass 0.6743025779724121
numFreeParamsPath 18
Reconstruction loss values: 0.03812072426080704 0.047716330736875534

Calculating loss for dapper model
	Full Pass 0.15192103385925293
numFreeParamsPath 18
Reconstruction loss values: 0.02998504787683487 0.03396362066268921

Calculating loss for case model
	Full Pass 0.8599040508270264
numFreeParamsPath 18
Reconstruction loss values: 0.04506490379571915 0.04780394956469536

Calculating loss for emognition model
	Full Pass 0.2820167541503906
numFreeParamsPath 18
Reconstruction loss values: 0.047966185957193375 0.05583195760846138

Calculating loss for empatch model
	Full Pass 0.10528087615966797
numFreeParamsPath 18
Reconstruction loss values: 0.05168088153004646 0.05897727981209755

Calculating loss for wesad model
	Full Pass 0.0770273208618164
numFreeParamsPath 18
Reconstruction loss values: 0.05505969002842903 0.07523417472839355
Total loss calculation time: 3.8408761024475098

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.903902053833008
Total epoch time: 156.05019426345825

Epoch: 29

Training amigos model
Final encoder loss: 0.04073821696484131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11674189567565918 0.39282774925231934

Final encoder loss: 0.03609874205672421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10812139511108398 0.38921451568603516

Final encoder loss: 0.035759876830269606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10854983329772949 0.38893628120422363

Final encoder loss: 0.03435282252154101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10857129096984863 0.389554500579834

Final encoder loss: 0.03290477769144434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10840392112731934 0.3895137310028076

Final encoder loss: 0.034144249851535975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10913324356079102 0.38889074325561523

Final encoder loss: 0.03400570588906278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1093907356262207 0.3899376392364502

Final encoder loss: 0.039139127253476194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10839676856994629 0.39005088806152344

Final encoder loss: 0.031203440849823464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10830831527709961 0.3891732692718506

Final encoder loss: 0.03632710154990662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10821700096130371 0.3890037536621094

Final encoder loss: 0.03304508811716965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10833001136779785 0.38976502418518066

Final encoder loss: 0.03648082252181067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10840702056884766 0.38954806327819824

Final encoder loss: 0.03476225832458313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10907936096191406 0.38851284980773926

Final encoder loss: 0.03368399130043994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10962748527526855 0.3898789882659912

Final encoder loss: 0.035054520292001665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10851454734802246 0.38997769355773926

Final encoder loss: 0.03639737450591926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10335016250610352 0.3842484951019287


Training emognition model
Final encoder loss: 0.04965891415703638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08322954177856445 0.2749457359313965

Final encoder loss: 0.047438909987951035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08362126350402832 0.27495837211608887

Final encoder loss: 0.04755520808207593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0831902027130127 0.2752413749694824

Final encoder loss: 0.044664016258084235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08369112014770508 0.274735689163208

Final encoder loss: 0.04338753611473392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08485078811645508 0.27575254440307617

Final encoder loss: 0.04444017464916169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0835728645324707 0.2753117084503174

Final encoder loss: 0.04194080958180819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08320069313049316 0.275146484375

Final encoder loss: 0.042331785286984946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08299851417541504 0.2764472961425781

Final encoder loss: 0.044231327806665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0835580825805664 0.27563905715942383

Final encoder loss: 0.04302768463679096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0850377082824707 0.2754654884338379

Final encoder loss: 0.04239249264073363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08340787887573242 0.27536749839782715

Final encoder loss: 0.04168063047137572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08367753028869629 0.27559328079223633

Final encoder loss: 0.043967077535502955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08438587188720703 0.2762792110443115

Final encoder loss: 0.040865645087441484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08353662490844727 0.27603816986083984

Final encoder loss: 0.04279476860352633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08413934707641602 0.2750678062438965

Final encoder loss: 0.04524773656251598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0825345516204834 0.2742934226989746


Training case model
Final encoder loss: 0.04466580095099242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09130311012268066 0.2647981643676758

Final encoder loss: 0.0408511538837952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09224867820739746 0.2653083801269531

Final encoder loss: 0.03985278458350663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09173965454101562 0.26518774032592773

Final encoder loss: 0.038454196633545594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09178566932678223 0.26543116569519043

Final encoder loss: 0.037879154341737865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09297394752502441 0.2664680480957031

Final encoder loss: 0.03702040638785675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09149861335754395 0.26512932777404785

Final encoder loss: 0.036710906871647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09162259101867676 0.26543760299682617

Final encoder loss: 0.036167219927469775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09197187423706055 0.265885591506958

Final encoder loss: 0.035206692953372094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.4393002986907959 0.2656216621398926

Final encoder loss: 0.0347807262959191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0916757583618164 0.2657959461212158

Final encoder loss: 0.03450476324713361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09195280075073242 0.2656400203704834

Final encoder loss: 0.034536859533188934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09274673461914062 0.26629185676574707

Final encoder loss: 0.03382994384171554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0919194221496582 0.2655911445617676

Final encoder loss: 0.03358629881904688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09148597717285156 0.2650277614593506

Final encoder loss: 0.03472781316682564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09256267547607422 0.26642632484436035

Final encoder loss: 0.03408789196297499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08848881721496582 0.26232051849365234


Training dapper model
Final encoder loss: 0.03324871487866291
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06322002410888672 0.1503767967224121

Final encoder loss: 0.03565688404522067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06282949447631836 0.15069937705993652

Final encoder loss: 0.02798344388606934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06291484832763672 0.15509557723999023

Final encoder loss: 0.028598008167958804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06211352348327637 0.14988923072814941

Final encoder loss: 0.02894592613834126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.062407732009887695 0.15050411224365234

Final encoder loss: 0.031689178851342516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06331968307495117 0.1518700122833252

Final encoder loss: 0.028018007403343103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06274127960205078 0.1508796215057373

Final encoder loss: 0.027632897730451904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06204581260681152 0.15108942985534668

Final encoder loss: 0.026367083043394888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06343722343444824 0.15075421333312988

Final encoder loss: 0.02772129087476254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06255102157592773 0.15013551712036133

Final encoder loss: 0.028577478487677506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06233811378479004 0.1518106460571289

Final encoder loss: 0.02658686021418842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061918020248413086 0.15141892433166504

Final encoder loss: 0.025353207729382383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06262636184692383 0.15017986297607422

Final encoder loss: 0.03047898020438147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06351327896118164 0.15166091918945312

Final encoder loss: 0.027943787110273132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.0623321533203125 0.15061569213867188

Final encoder loss: 0.021606663698954975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06250596046447754 0.15043234825134277


Training amigos model
Final encoder loss: 0.027461571669066577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10666584968566895 0.3412933349609375

Final encoder loss: 0.025204420258665563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10718846321105957 0.34172630310058594

Final encoder loss: 0.025934558962157215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1062009334564209 0.34189391136169434

Final encoder loss: 0.02519002388779134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10615086555480957 0.3416585922241211

Final encoder loss: 0.02676015003017717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10598278045654297 0.3418242931365967

Final encoder loss: 0.024394139137489727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10732865333557129 0.3418121337890625

Final encoder loss: 0.02665263624159865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10602831840515137 0.3421142101287842

Final encoder loss: 0.02689653921570693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10636329650878906 0.3418304920196533

Final encoder loss: 0.02418882706684936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10643243789672852 0.34159255027770996

Final encoder loss: 0.029153570310643393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10684394836425781 0.34137988090515137

Final encoder loss: 0.02943443477196751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1069951057434082 0.34267663955688477

Final encoder loss: 0.026122361600654776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10712242126464844 0.3416023254394531

Final encoder loss: 0.02680759387142732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10639691352844238 0.34192919731140137

Final encoder loss: 0.024775076358381058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10600781440734863 0.341494083404541

Final encoder loss: 0.02911010555649447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10719108581542969 0.34203147888183594

Final encoder loss: 0.025834610672890404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10119366645812988 0.33783841133117676


Training amigos model
Final encoder loss: 0.1807539016008377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46001100540161133 0.07660126686096191

Final encoder loss: 0.18783260881900787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46965980529785156 0.07582378387451172

Final encoder loss: 0.1836356520652771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4566535949707031 0.07605266571044922

Final encoder loss: 0.07419316470623016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45582079887390137 0.07745480537414551

Final encoder loss: 0.07560232281684875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45482563972473145 0.0789949893951416

Final encoder loss: 0.06951229274272919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4424607753753662 0.07291769981384277

Final encoder loss: 0.04452779144048691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4486384391784668 0.07608461380004883

Final encoder loss: 0.04482044652104378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.458740234375 0.07560086250305176

Final encoder loss: 0.04241214692592621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.443284273147583 0.07402324676513672

Final encoder loss: 0.033524882048368454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4601423740386963 0.07236814498901367

Final encoder loss: 0.03396400436758995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4485468864440918 0.07710838317871094

Final encoder loss: 0.033020321279764175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45093441009521484 0.07613348960876465

Final encoder loss: 0.028786221519112587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4578979015350342 0.07694196701049805

Final encoder loss: 0.029447341337800026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4694821834564209 0.07591724395751953

Final encoder loss: 0.02903813309967518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4529244899749756 0.0795443058013916

Final encoder loss: 0.027083704248070717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4747042655944824 0.07682204246520996

Final encoder loss: 0.027860188856720924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45461320877075195 0.07600212097167969

Final encoder loss: 0.027553284540772438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4509241580963135 0.07910752296447754

Final encoder loss: 0.027181511744856834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47359371185302734 0.0771327018737793

Final encoder loss: 0.027663949877023697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4525434970855713 0.07763099670410156

Final encoder loss: 0.027602393180131912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4528636932373047 0.07881832122802734

Final encoder loss: 0.027670124545693398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47124695777893066 0.07536840438842773

Final encoder loss: 0.027847861871123314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45380687713623047 0.0759737491607666

Final encoder loss: 0.027817469090223312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45119380950927734 0.08146381378173828

Final encoder loss: 0.02716309018433094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45654749870300293 0.07773160934448242

Final encoder loss: 0.027147071436047554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4689033031463623 0.07658982276916504

Final encoder loss: 0.027401000261306763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45249176025390625 0.0801844596862793

Final encoder loss: 0.026457400992512703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47412562370300293 0.0769805908203125

Final encoder loss: 0.026726149022579193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4516425132751465 0.07495379447937012

Final encoder loss: 0.026849964633584023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4533047676086426 0.08123302459716797

Final encoder loss: 0.02603220008313656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47265076637268066 0.07600259780883789

Final encoder loss: 0.026184817776083946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4461333751678467 0.07636284828186035

Final encoder loss: 0.02657533809542656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4421870708465576 0.07411694526672363

Final encoder loss: 0.026080073788762093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.459491491317749 0.0749514102935791

Final encoder loss: 0.02603033371269703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4456608295440674 0.0746004581451416

Final encoder loss: 0.026445651426911354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44283533096313477 0.07486891746520996

Final encoder loss: 0.025542063638567924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44720935821533203 0.07872915267944336

Final encoder loss: 0.025628240779042244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45602989196777344 0.07617545127868652

Final encoder loss: 0.026007676497101784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4513225555419922 0.0762784481048584

Final encoder loss: 0.025595150887966156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46887683868408203 0.07814693450927734

Final encoder loss: 0.025447098538279533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4542367458343506 0.07941508293151855

Final encoder loss: 0.025943683460354805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45217323303222656 0.07920670509338379

Final encoder loss: 0.025384820997714996
Final encoder loss: 0.02404656447470188
Final encoder loss: 0.023621786385774612

Training dapper model
Final encoder loss: 0.020304335242976795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.060101985931396484 0.10653066635131836

Final encoder loss: 0.02293578140403179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05890154838562012 0.10595822334289551

Final encoder loss: 0.021563045037083547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0588834285736084 0.10579943656921387

Final encoder loss: 0.019698650398718334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.058791399002075195 0.10578083992004395

Final encoder loss: 0.022546375660791148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05910229682922363 0.1062171459197998

Final encoder loss: 0.02214675658535026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05898690223693848 0.10585379600524902

Final encoder loss: 0.020019469444811373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05904507637023926 0.1060478687286377

Final encoder loss: 0.02015276383817714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05892372131347656 0.10577201843261719

Final encoder loss: 0.020688776580996597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059036970138549805 0.10636544227600098

Final encoder loss: 0.023683151877821725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05895590782165527 0.10617232322692871

Final encoder loss: 0.021813637476363088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05917215347290039 0.10578489303588867

Final encoder loss: 0.020119022797461732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05886411666870117 0.10609292984008789

Final encoder loss: 0.023046869550346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05904865264892578 0.10639619827270508

Final encoder loss: 0.01815543856429021
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059137582778930664 0.10599040985107422

Final encoder loss: 0.02197463205353799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059125423431396484 0.10611152648925781

Final encoder loss: 0.021377725269393417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05856728553771973 0.10624027252197266


Training dapper model
Final encoder loss: 0.20244450867176056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11364459991455078 0.03374052047729492

Final encoder loss: 0.2081918567419052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11546111106872559 0.0337831974029541

Final encoder loss: 0.07644228637218475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11235833168029785 0.03403210639953613

Final encoder loss: 0.07716891914606094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150214672088623 0.033576250076293945

Final encoder loss: 0.04484190791845322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11267900466918945 0.034906625747680664

Final encoder loss: 0.04396820068359375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11599111557006836 0.03474688529968262

Final encoder loss: 0.031690485775470734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11287546157836914 0.03386878967285156

Final encoder loss: 0.0310649611055851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11616992950439453 0.033628225326538086

Final encoder loss: 0.025464246049523354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11293530464172363 0.03463578224182129

Final encoder loss: 0.02501133270561695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11526846885681152 0.03363537788391113

Final encoder loss: 0.022334730252623558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11275291442871094 0.0341038703918457

Final encoder loss: 0.02197428233921528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11524701118469238 0.03372621536254883

Final encoder loss: 0.020797815173864365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11267733573913574 0.03418922424316406

Final encoder loss: 0.02047821134328842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11608266830444336 0.03400468826293945

Final encoder loss: 0.02008063904941082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11279916763305664 0.03436732292175293

Final encoder loss: 0.01980525441467762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11569571495056152 0.03441786766052246

Final encoder loss: 0.020032381638884544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1132962703704834 0.03403472900390625

Final encoder loss: 0.0195599477738142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559438705444336 0.03397941589355469

Final encoder loss: 0.020045708864927292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1131136417388916 0.03387618064880371

Final encoder loss: 0.01939317397773266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595749855041504 0.034256935119628906

Final encoder loss: 0.01988290064036846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11243081092834473 0.03355836868286133

Final encoder loss: 0.019625786691904068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1162712574005127 0.033548593521118164

Final encoder loss: 0.019830917939543724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11332130432128906 0.03394055366516113

Final encoder loss: 0.019902970641851425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11584925651550293 0.034253835678100586

Final encoder loss: 0.019699547439813614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11273574829101562 0.034544944763183594

Final encoder loss: 0.02007826790213585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11662602424621582 0.03355693817138672

Final encoder loss: 0.01941709965467453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11401581764221191 0.03351998329162598

Final encoder loss: 0.019556866958737373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11496257781982422 0.033453941345214844

Final encoder loss: 0.019188441336154938
Final encoder loss: 0.017710119485855103

Training case model
Final encoder loss: 0.031041124901001606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08931088447570801 0.2192373275756836

Final encoder loss: 0.03221356450132199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0906531810760498 0.2194383144378662

Final encoder loss: 0.03058256831077471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0909733772277832 0.21937155723571777

Final encoder loss: 0.02979826084589682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08966207504272461 0.2190539836883545

Final encoder loss: 0.03003622547758574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0892794132232666 0.21891021728515625

Final encoder loss: 0.030000493041631486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08970856666564941 0.21909356117248535

Final encoder loss: 0.02961961049120811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08954501152038574 0.21893692016601562

Final encoder loss: 0.030571921872746762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08897948265075684 0.2194228172302246

Final encoder loss: 0.030228465651397533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09070348739624023 0.21960234642028809

Final encoder loss: 0.03068097707492492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09079384803771973 0.21931862831115723

Final encoder loss: 0.030478118103930793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08924102783203125 0.2188577651977539

Final encoder loss: 0.029908503979795092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09050154685974121 0.21951818466186523

Final encoder loss: 0.029979466997733382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0892181396484375 0.21917390823364258

Final encoder loss: 0.029348744935761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08910322189331055 0.21924924850463867

Final encoder loss: 0.02944754028096887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08867096900939941 0.2187645435333252

Final encoder loss: 0.029545798904262417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08573722839355469 0.21570253372192383


Training case model
Final encoder loss: 0.20295950770378113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.260852575302124 0.05383610725402832

Final encoder loss: 0.1889134794473648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25845980644226074 0.05262589454650879

Final encoder loss: 0.19014506042003632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25687193870544434 0.0517115592956543

Final encoder loss: 0.19218474626541138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570040225982666 0.051258087158203125

Final encoder loss: 0.18081536889076233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2739896774291992 0.051635026931762695

Final encoder loss: 0.19192883372306824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25713467597961426 0.05090475082397461

Final encoder loss: 0.09973927587270737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2574424743652344 0.050954580307006836

Final encoder loss: 0.0901605412364006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25659942626953125 0.05075669288635254

Final encoder loss: 0.08620162308216095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2571444511413574 0.05196094512939453

Final encoder loss: 0.08524540066719055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26777219772338867 0.05149364471435547

Final encoder loss: 0.07773250341415405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25685834884643555 0.052143096923828125

Final encoder loss: 0.08062262833118439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255277156829834 0.05211138725280762

Final encoder loss: 0.05882559344172478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569694519042969 0.05276012420654297

Final encoder loss: 0.05385325849056244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2674834728240967 0.052582502365112305

Final encoder loss: 0.05198131129145622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2564659118652344 0.05212712287902832

Final encoder loss: 0.05276990681886673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25751328468322754 0.05139946937561035

Final encoder loss: 0.04962075129151344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25626039505004883 0.05226731300354004

Final encoder loss: 0.050808925181627274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547271251678467 0.05139636993408203

Final encoder loss: 0.04338154196739197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2556931972503662 0.052260637283325195

Final encoder loss: 0.04080675169825554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25594592094421387 0.052652597427368164

Final encoder loss: 0.03958575055003166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2673015594482422 0.05284619331359863

Final encoder loss: 0.04073547199368477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568044662475586 0.05301475524902344

Final encoder loss: 0.03973834589123726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2559983730316162 0.05204653739929199

Final encoder loss: 0.0397346206009388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25325608253479004 0.0523533821105957

Final encoder loss: 0.03762347251176834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26688218116760254 0.05215263366699219

Final encoder loss: 0.036459874361753464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568984031677246 0.05338907241821289

Final encoder loss: 0.035666659474372864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2661631107330322 0.05215620994567871

Final encoder loss: 0.036648333072662354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25615572929382324 0.05146527290344238

Final encoder loss: 0.03717106580734253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25600266456604004 0.05203604698181152

Final encoder loss: 0.036530155688524246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2533376216888428 0.05161476135253906

Final encoder loss: 0.03574986383318901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2557215690612793 0.05295586585998535

Final encoder loss: 0.03528270497918129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26682400703430176 0.0517270565032959

Final encoder loss: 0.035000186413526535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26627349853515625 0.05043172836303711

Final encoder loss: 0.03540594130754471
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25678443908691406 0.05253028869628906

Final encoder loss: 0.03644386678934097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2665231227874756 0.052954673767089844

Final encoder loss: 0.035646695643663406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25257062911987305 0.05218219757080078

Final encoder loss: 0.034250929951667786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26730966567993164 0.052164316177368164

Final encoder loss: 0.033625781536102295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25651073455810547 0.052365779876708984

Final encoder loss: 0.033431075513362885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563347816467285 0.052361488342285156

Final encoder loss: 0.033821091055870056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2562894821166992 0.0512700080871582

Final encoder loss: 0.03417176753282547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2679288387298584 0.05242633819580078

Final encoder loss: 0.033558040857315063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2526695728302002 0.0512089729309082

Final encoder loss: 0.03303276374936104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2561616897583008 0.0517888069152832

Final encoder loss: 0.03240589052438736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2673037052154541 0.05115985870361328

Final encoder loss: 0.03215976059436798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25492334365844727 0.05094432830810547

Final encoder loss: 0.032629743218421936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2669069766998291 0.05126309394836426

Final encoder loss: 0.033192891627550125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25693535804748535 0.05237936973571777

Final encoder loss: 0.03245546296238899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2530381679534912 0.05035400390625

Final encoder loss: 0.032147981226444244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25620532035827637 0.05275917053222656

Final encoder loss: 0.031818173825740814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2554447650909424 0.052469730377197266

Final encoder loss: 0.03146947920322418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26728296279907227 0.05248689651489258

Final encoder loss: 0.031959302723407745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2557408809661865 0.05218815803527832

Final encoder loss: 0.03269055485725403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25763940811157227 0.05297589302062988

Final encoder loss: 0.03192969039082527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25256896018981934 0.05195021629333496

Final encoder loss: 0.03181717172265053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2559330463409424 0.05205106735229492

Final encoder loss: 0.031419623643159866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25726795196533203 0.05258798599243164

Final encoder loss: 0.031200749799609184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26619482040405273 0.05202984809875488

Final encoder loss: 0.03156329318881035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2736239433288574 0.05217099189758301

Final encoder loss: 0.03245089575648308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26607346534729004 0.052762746810913086

Final encoder loss: 0.03154648840427399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544374465942383 0.052228450775146484

Final encoder loss: 0.031332828104496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26690006256103516 0.05278277397155762

Final encoder loss: 0.03093460574746132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566647529602051 0.052317142486572266

Final encoder loss: 0.030697893351316452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25536131858825684 0.051924943923950195

Final encoder loss: 0.03108360432088375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2676205635070801 0.053067684173583984

Final encoder loss: 0.03180145472288132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25638747215270996 0.05208468437194824

Final encoder loss: 0.030929865315556526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2529611587524414 0.052523136138916016

Final encoder loss: 0.03108309395611286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2561776638031006 0.05129647254943848

Final encoder loss: 0.030735701322555542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2737898826599121 0.05186057090759277

Final encoder loss: 0.03041955456137657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2668771743774414 0.052629709243774414

Final encoder loss: 0.030840009450912476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2563309669494629 0.050919532775878906

Final encoder loss: 0.03136167675256729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25873780250549316 0.05200839042663574

Final encoder loss: 0.03070874884724617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2529127597808838 0.05146956443786621

Final encoder loss: 0.03059290163218975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25531601905822754 0.052146196365356445

Final encoder loss: 0.030388886108994484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564432621002197 0.05152750015258789

Final encoder loss: 0.030185597017407417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26678967475891113 0.050849199295043945

Final encoder loss: 0.03034370206296444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2563173770904541 0.05093216896057129

Final encoder loss: 0.031206965446472168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2564225196838379 0.051375389099121094

Final encoder loss: 0.030405398458242416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25293707847595215 0.05084371566772461

Final encoder loss: 0.030573338270187378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.256397008895874 0.05153393745422363

Final encoder loss: 0.030150610953569412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2559993267059326 0.05162405967712402

Final encoder loss: 0.030077079311013222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27094101905822754 0.052021026611328125

Final encoder loss: 0.030296526849269867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27251648902893066 0.052987098693847656

Final encoder loss: 0.03098355419933796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27733898162841797 0.0514070987701416

Final encoder loss: 0.030195068567991257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2604503631591797 0.05152440071105957

Final encoder loss: 0.030241994187235832
Final encoder loss: 0.029442619532346725
Final encoder loss: 0.028324056416749954
Final encoder loss: 0.02768772467970848
Final encoder loss: 0.02743915654718876
Final encoder loss: 0.025747792795300484

Training emognition model
Final encoder loss: 0.036160027698691775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08222317695617676 0.23115086555480957

Final encoder loss: 0.035568228783441104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08390116691589355 0.23135685920715332

Final encoder loss: 0.03420391232691438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08209228515625 0.2308521270751953

Final encoder loss: 0.03397712166101927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08159804344177246 0.23102045059204102

Final encoder loss: 0.03337754374439044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08254194259643555 0.23123717308044434

Final encoder loss: 0.035744604162720475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08129072189331055 0.23058509826660156

Final encoder loss: 0.0326318312762365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08228659629821777 0.23123836517333984

Final encoder loss: 0.03367029293545732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08114743232727051 0.23105692863464355

Final encoder loss: 0.03357036785349901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08164238929748535 0.23130512237548828

Final encoder loss: 0.03360886548889048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08265042304992676 0.23126935958862305

Final encoder loss: 0.034799171807262795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08138203620910645 0.23067545890808105

Final encoder loss: 0.03523432222931877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08129262924194336 0.23087382316589355

Final encoder loss: 0.03242647434192644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08239173889160156 0.23138880729675293

Final encoder loss: 0.035355897558580505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08176565170288086 0.2308955192565918

Final encoder loss: 0.03512837272781635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08131170272827148 0.23111939430236816

Final encoder loss: 0.03369156097005216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08228492736816406 0.23010826110839844


Training emognition model
Final encoder loss: 0.19358116388320923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2511885166168213 0.04964327812194824

Final encoder loss: 0.1949668526649475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2503085136413574 0.04989457130432129

Final encoder loss: 0.08541511744260788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24940276145935059 0.04936480522155762

Final encoder loss: 0.08492910861968994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.250194787979126 0.04956197738647461

Final encoder loss: 0.055654771625995636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24872350692749023 0.049271583557128906

Final encoder loss: 0.05423765257000923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2487316131591797 0.053064584732055664

Final encoder loss: 0.043385569006204605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24827146530151367 0.04939603805541992

Final encoder loss: 0.04242278262972832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25081634521484375 0.051575660705566406

Final encoder loss: 0.037535421550273895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24831581115722656 0.04949474334716797

Final encoder loss: 0.03683468699455261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24739646911621094 0.05052685737609863

Final encoder loss: 0.03453310579061508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2490551471710205 0.0491175651550293

Final encoder loss: 0.03404266759753227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24842095375061035 0.05040264129638672

Final encoder loss: 0.033059388399124146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24961209297180176 0.04863452911376953

Final encoder loss: 0.03278796374797821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24862408638000488 0.04878544807434082

Final encoder loss: 0.03259415552020073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25304126739501953 0.048433780670166016

Final encoder loss: 0.03257589414715767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471921443939209 0.05016207695007324

Final encoder loss: 0.03260848671197891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2506535053253174 0.05068802833557129

Final encoder loss: 0.03275418281555176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24806857109069824 0.05009937286376953

Final encoder loss: 0.03284778818488121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2499532699584961 0.05008697509765625

Final encoder loss: 0.03293934091925621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24817180633544922 0.04999089241027832

Final encoder loss: 0.03283260390162468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24913644790649414 0.05084419250488281

Final encoder loss: 0.03287748247385025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24850749969482422 0.05076432228088379

Final encoder loss: 0.032591912895441055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24818110466003418 0.05065011978149414

Final encoder loss: 0.03286198154091835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2485029697418213 0.048726558685302734

Final encoder loss: 0.032368581742048264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24892616271972656 0.04940366744995117

Final encoder loss: 0.03263883292675018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24890542030334473 0.0495142936706543

Final encoder loss: 0.03245684877038002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2487502098083496 0.04898381233215332

Final encoder loss: 0.0326797291636467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25020408630371094 0.04953169822692871

Final encoder loss: 0.032255928963422775
Final encoder loss: 0.03130946308374405

Training empatch model
Final encoder loss: 0.050943915163257594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07245850563049316 0.1743314266204834

Final encoder loss: 0.0505451130341545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07202553749084473 0.17437124252319336

Final encoder loss: 0.048767482500328085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07137894630432129 0.17387795448303223

Final encoder loss: 0.04512113918694657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0726480484008789 0.17478704452514648

Final encoder loss: 0.04444781681830155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0713357925415039 0.17385268211364746

Final encoder loss: 0.04604636995913531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07126474380493164 0.17380404472351074

Final encoder loss: 0.04626202731366069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07169938087463379 0.1743757724761963

Final encoder loss: 0.04417333938956477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07191061973571777 0.17407679557800293

Final encoder loss: 0.03328370837293646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07136201858520508 0.1734621524810791

Final encoder loss: 0.031655274007712834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0715944766998291 0.17349696159362793

Final encoder loss: 0.032527056454058526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07257580757141113 0.17494678497314453

Final encoder loss: 0.03368209632556281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07226324081420898 0.17414474487304688

Final encoder loss: 0.03449740809101973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07160186767578125 0.174147367477417

Final encoder loss: 0.03691077040491843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07134509086608887 0.17440485954284668

Final encoder loss: 0.03394290828286772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07268166542053223 0.17447710037231445

Final encoder loss: 0.037330564527815614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07114648818969727 0.1736457347869873


Training empatch model
Final encoder loss: 0.1711624413728714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17778992652893066 0.04363608360290527

Final encoder loss: 0.07979804277420044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17874884605407715 0.04396176338195801

Final encoder loss: 0.056620411574840546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17610621452331543 0.044309377670288086

Final encoder loss: 0.04566607624292374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17633867263793945 0.043137550354003906

Final encoder loss: 0.03944672271609306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17819952964782715 0.04485726356506348

Final encoder loss: 0.03561541065573692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1768360137939453 0.044252872467041016

Final encoder loss: 0.033174026757478714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17436790466308594 0.044016361236572266

Final encoder loss: 0.03166112303733826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17704296112060547 0.044930219650268555

Final encoder loss: 0.030636128038167953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17534184455871582 0.04348301887512207

Final encoder loss: 0.029981208965182304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755213737487793 0.04346513748168945

Final encoder loss: 0.02949896641075611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17746210098266602 0.044820308685302734

Final encoder loss: 0.02931136265397072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17549753189086914 0.0438847541809082

Final encoder loss: 0.02914481796324253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17623567581176758 0.043512582778930664

Final encoder loss: 0.029100211337208748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1774752140045166 0.04462432861328125

Final encoder loss: 0.029031379148364067

Training wesad model
Final encoder loss: 0.052008040340272906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07194995880126953 0.17375731468200684

Final encoder loss: 0.05397797281198904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07124042510986328 0.17414498329162598

Final encoder loss: 0.04816687678038854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07212996482849121 0.1740436553955078

Final encoder loss: 0.052970781878244506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07142162322998047 0.17391657829284668

Final encoder loss: 0.037714024222205766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0712575912475586 0.17377877235412598

Final encoder loss: 0.034226980850684784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07153844833374023 0.17465949058532715

Final encoder loss: 0.035671511351460135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07372379302978516 0.17383742332458496

Final encoder loss: 0.033030097406681745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07143831253051758 0.17424392700195312

Final encoder loss: 0.027718611977271084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07115507125854492 0.1740248203277588

Final encoder loss: 0.027565186719290664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07247018814086914 0.17408275604248047

Final encoder loss: 0.028795202645693206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07130050659179688 0.17417144775390625

Final encoder loss: 0.02842706457623571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07115745544433594 0.1738901138305664

Final encoder loss: 0.024474195425231746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07147979736328125 0.17431211471557617

Final encoder loss: 0.02277141004998213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07391119003295898 0.17412805557250977

Final encoder loss: 0.02322511489257315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.08909344673156738 0.1740567684173584

Final encoder loss: 0.02373639044362629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07120895385742188 0.1744241714477539


Training wesad model
Final encoder loss: 0.21556377410888672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10697031021118164 0.03374290466308594

Final encoder loss: 0.09646153450012207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10531854629516602 0.033724308013916016

Final encoder loss: 0.06340504437685013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456585884094238 0.03316807746887207

Final encoder loss: 0.04731207713484764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10415887832641602 0.03285670280456543

Final encoder loss: 0.038397133350372314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10438084602355957 0.0334625244140625

Final encoder loss: 0.033166032284498215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10484957695007324 0.03442573547363281

Final encoder loss: 0.02995680272579193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10558962821960449 0.034410715103149414

Final encoder loss: 0.027874687686562538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442757606506348 0.033779144287109375

Final encoder loss: 0.02654661051928997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10465240478515625 0.0328059196472168

Final encoder loss: 0.025747094303369522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375618934631348 0.03310441970825195

Final encoder loss: 0.025418667122721672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10511922836303711 0.033646583557128906

Final encoder loss: 0.025462524965405464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10600399971008301 0.03394150733947754

Final encoder loss: 0.025767460465431213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10606861114501953 0.03360748291015625

Final encoder loss: 0.02611037716269493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10401272773742676 0.03350663185119629

Final encoder loss: 0.026202058419585228

Calculating loss for amigos model
	Full Pass 0.6719663143157959
numFreeParamsPath 18
Reconstruction loss values: 0.038320522755384445 0.04787931591272354

Calculating loss for dapper model
	Full Pass 0.1506955623626709
numFreeParamsPath 18
Reconstruction loss values: 0.028592269867658615 0.03368932753801346

Calculating loss for case model
	Full Pass 0.8541982173919678
numFreeParamsPath 18
Reconstruction loss values: 0.043982841074466705 0.04708973318338394

Calculating loss for emognition model
	Full Pass 0.27910399436950684
numFreeParamsPath 18
Reconstruction loss values: 0.047310344874858856 0.05491312965750694

Calculating loss for empatch model
	Full Pass 0.10416722297668457
numFreeParamsPath 18
Reconstruction loss values: 0.05098533630371094 0.058125339448451996

Calculating loss for wesad model
	Full Pass 0.07660984992980957
numFreeParamsPath 18
Reconstruction loss values: 0.05342935770750046 0.07451746612787247
Total loss calculation time: 3.787884473800659

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.5021185874938965
Total epoch time: 156.53524470329285

Epoch: 30

Training amigos model
Final encoder loss: 0.03816664473157695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11693763732910156 0.3896980285644531

Final encoder loss: 0.03562838784879938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10796213150024414 0.3891031742095947

Final encoder loss: 0.03335942985598082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10839676856994629 0.3894948959350586

Final encoder loss: 0.03374588804421051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10860538482666016 0.3891432285308838

Final encoder loss: 0.03389576379651612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10789871215820312 0.38999462127685547

Final encoder loss: 0.034082060632854946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10819029808044434 0.3911139965057373

Final encoder loss: 0.03752636630567501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1097557544708252 0.3888082504272461

Final encoder loss: 0.03465745575306734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10833978652954102 0.3890852928161621

Final encoder loss: 0.03266826584884266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10826778411865234 0.38909077644348145

Final encoder loss: 0.0339597546019544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10769438743591309 0.38893723487854004

Final encoder loss: 0.03716604584425687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10788631439208984 0.38846778869628906

Final encoder loss: 0.03583566486225942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10796999931335449 0.3881709575653076

Final encoder loss: 0.03489676535987099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10818862915039062 0.38861083984375

Final encoder loss: 0.03458369530789193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10803604125976562 0.38905954360961914

Final encoder loss: 0.03634631948899939
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.1079261302947998 0.38890528678894043

Final encoder loss: 0.03327844192124852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10284113883972168 0.38340234756469727


Training emognition model
Final encoder loss: 0.04910442738080467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08284449577331543 0.2732980251312256

Final encoder loss: 0.04504724756248143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08240365982055664 0.27553582191467285

Final encoder loss: 0.04608620725373244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08352828025817871 0.27533531188964844

Final encoder loss: 0.04425658565576158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08350205421447754 0.27611398696899414

Final encoder loss: 0.0426501961182159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08502650260925293 0.276747465133667

Final encoder loss: 0.04275262200737565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08364534378051758 0.2757537364959717

Final encoder loss: 0.04325765909647926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08322381973266602 0.27530694007873535

Final encoder loss: 0.04119298039662071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08367419242858887 0.2761056423187256

Final encoder loss: 0.042044720445398025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08331918716430664 0.2758030891418457

Final encoder loss: 0.04137852505011273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08497738838195801 0.27539896965026855

Final encoder loss: 0.04255955691772698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08361220359802246 0.2753472328186035

Final encoder loss: 0.04011107241600109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08380889892578125 0.2757589817047119

Final encoder loss: 0.039833984568663265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08473038673400879 0.27646851539611816

Final encoder loss: 0.040618861100879464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08375763893127441 0.27649569511413574

Final encoder loss: 0.04233111606515943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08413314819335938 0.275280237197876

Final encoder loss: 0.04362206381634434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08266735076904297 0.2747788429260254


Training dapper model
Final encoder loss: 0.028507947308625634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06265616416931152 0.15053892135620117

Final encoder loss: 0.02849295557008362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06224656105041504 0.1511678695678711

Final encoder loss: 0.029643147512489195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06310200691223145 0.15058684349060059

Final encoder loss: 0.029809354446742355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06225275993347168 0.1497948169708252

Final encoder loss: 0.026196014532395718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06230950355529785 0.15213680267333984

Final encoder loss: 0.025141193699905005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06241607666015625 0.15070009231567383

Final encoder loss: 0.025849043702679524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062345266342163086 0.1510000228881836

Final encoder loss: 0.028062127603448908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06357407569885254 0.15188169479370117

Final encoder loss: 0.02588430066793336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06258010864257812 0.15109848976135254

Final encoder loss: 0.025318106204540045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06249666213989258 0.15128254890441895

Final encoder loss: 0.027864273347679488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06360888481140137 0.1507103443145752

Final encoder loss: 0.026090706297079127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.062299251556396484 0.15083789825439453

Final encoder loss: 0.026083208347927232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062390804290771484 0.15226054191589355

Final encoder loss: 0.02560874912976842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06221628189086914 0.1511073112487793

Final encoder loss: 0.02710666035741182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06229829788208008 0.1505112648010254

Final encoder loss: 0.03095756805786349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06293463706970215 0.15134310722351074


Training case model
Final encoder loss: 0.0457379880963957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09175848960876465 0.26510119438171387

Final encoder loss: 0.041445258124783835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09267330169677734 0.26627016067504883

Final encoder loss: 0.03987103033963309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0921022891998291 0.2653956413269043

Final encoder loss: 0.03834140226835975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09137225151062012 0.2657346725463867

Final encoder loss: 0.037063590077702546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09239745140075684 0.26624131202697754

Final encoder loss: 0.0360401491496333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09171509742736816 0.2652466297149658

Final encoder loss: 0.036197162912698706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0921163558959961 0.26535630226135254

Final encoder loss: 0.03575425320064843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0914158821105957 0.26659393310546875

Final encoder loss: 0.03557434316432031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09211397171020508 0.26585960388183594

Final encoder loss: 0.03460966722591955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09256911277770996 0.26477861404418945

Final encoder loss: 0.03344993968530669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09178614616394043 0.2656729221343994

Final encoder loss: 0.03400657749929405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09176397323608398 0.26593756675720215

Final encoder loss: 0.03463868690655671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09297513961791992 0.26603102684020996

Final encoder loss: 0.03375884892261908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09169459342956543 0.2653071880340576

Final encoder loss: 0.03311242010905772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09134340286254883 0.26604390144348145

Final encoder loss: 0.033103061602803886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08901786804199219 0.26334095001220703


Training amigos model
Final encoder loss: 0.02605321015259534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10643458366394043 0.3416919708251953

Final encoder loss: 0.02349702055514699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10672569274902344 0.3416783809661865

Final encoder loss: 0.02477755061239709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10686993598937988 0.3416016101837158

Final encoder loss: 0.02575612422896673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10698652267456055 0.3419198989868164

Final encoder loss: 0.02482750711081811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1060185432434082 0.34159421920776367

Final encoder loss: 0.026499941598930314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10667300224304199 0.3418698310852051

Final encoder loss: 0.024711602577567004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10635876655578613 0.3417203426361084

Final encoder loss: 0.026825128813377005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10730433464050293 0.3418588638305664

Final encoder loss: 0.026205292967600918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10580897331237793 0.34186601638793945

Final encoder loss: 0.025008876436085903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10632896423339844 0.3415720462799072

Final encoder loss: 0.02592324708442397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10640668869018555 0.34216833114624023

Final encoder loss: 0.026585553181459498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10698318481445312 0.3420283794403076

Final encoder loss: 0.02562422999527079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10671019554138184 0.3418254852294922

Final encoder loss: 0.02688369398211636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10615849494934082 0.3415074348449707

Final encoder loss: 0.02573335509878401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1066749095916748 0.34185004234313965

Final encoder loss: 0.02720447791480326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10166788101196289 0.33901309967041016


Training amigos model
Final encoder loss: 0.18076445162296295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4538135528564453 0.08061957359313965

Final encoder loss: 0.18781235814094543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4540548324584961 0.07326197624206543

Final encoder loss: 0.18362754583358765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4536783695220947 0.07660746574401855

Final encoder loss: 0.07378590106964111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4620344638824463 0.07821822166442871

Final encoder loss: 0.07566860318183899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4526023864746094 0.07985496520996094

Final encoder loss: 0.06963978707790375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43741822242736816 0.07227015495300293

Final encoder loss: 0.04401930794119835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45491886138916016 0.07666444778442383

Final encoder loss: 0.04473967105150223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45328760147094727 0.07601213455200195

Final encoder loss: 0.04209993779659271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4494283199310303 0.07810139656066895

Final encoder loss: 0.03295323625206947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46621060371398926 0.07397174835205078

Final encoder loss: 0.033568087965250015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4557619094848633 0.07775020599365234

Final encoder loss: 0.03249343857169151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4471614360809326 0.07594633102416992

Final encoder loss: 0.02829485759139061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45296716690063477 0.07662677764892578

Final encoder loss: 0.028808772563934326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45171165466308594 0.08113265037536621

Final encoder loss: 0.028458626940846443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45159435272216797 0.07650470733642578

Final encoder loss: 0.026615599170327187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4646131992340088 0.07692122459411621

Final encoder loss: 0.02715848758816719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45267152786254883 0.0766913890838623

Final encoder loss: 0.026792114600539207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4490339756011963 0.08210492134094238

Final encoder loss: 0.026623865589499474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4702162742614746 0.0768430233001709

Final encoder loss: 0.026998450979590416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44901371002197266 0.07744216918945312

Final encoder loss: 0.026727180927991867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44612908363342285 0.0756833553314209

Final encoder loss: 0.027008015662431717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4652125835418701 0.08200716972351074

Final encoder loss: 0.027092227712273598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45778560638427734 0.07706809043884277

Final encoder loss: 0.027131319046020508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44652557373046875 0.07536125183105469

Final encoder loss: 0.02632606215775013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43991708755493164 0.07592892646789551

Final encoder loss: 0.02619839273393154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45157337188720703 0.0807490348815918

Final encoder loss: 0.026902539655566216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45267748832702637 0.07486867904663086

Final encoder loss: 0.025606997311115265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4676971435546875 0.07776284217834473

Final encoder loss: 0.025657903403043747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4538581371307373 0.07468819618225098

Final encoder loss: 0.02620282955467701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44868922233581543 0.07966327667236328

Final encoder loss: 0.025323335081338882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46851134300231934 0.07745027542114258

Final encoder loss: 0.02518487349152565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45272135734558105 0.07677769660949707

Final encoder loss: 0.025621073320508003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.447953462600708 0.07503414154052734

Final encoder loss: 0.025407083332538605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46837377548217773 0.07189512252807617

Final encoder loss: 0.025056064128875732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4523885250091553 0.07995271682739258

Final encoder loss: 0.025502508506178856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4499390125274658 0.07571649551391602

Final encoder loss: 0.024946073070168495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4563169479370117 0.07569551467895508

Final encoder loss: 0.024750908836722374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44329071044921875 0.07576584815979004

Final encoder loss: 0.02533865161240101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.440479040145874 0.07468485832214355

Final encoder loss: 0.024745507165789604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4551515579223633 0.0757746696472168

Final encoder loss: 0.024699490517377853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4424929618835449 0.07286357879638672

Final encoder loss: 0.02529900148510933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42741966247558594 0.07484054565429688

Final encoder loss: 0.02437155693769455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4509584903717041 0.08006095886230469

Final encoder loss: 0.02441319264471531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4660482406616211 0.07489585876464844

Final encoder loss: 0.025027791038155556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4503335952758789 0.07355356216430664

Final encoder loss: 0.024460095912218094
Final encoder loss: 0.02326066419482231
Final encoder loss: 0.022736690938472748

Training dapper model
Final encoder loss: 0.020993806021054228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.0610349178314209 0.10825443267822266

Final encoder loss: 0.02014593617842477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.0598149299621582 0.10771918296813965

Final encoder loss: 0.02052008320930108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06001734733581543 0.1075139045715332

Final encoder loss: 0.021194022068326303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0600130558013916 0.10823917388916016

Final encoder loss: 0.02057657598528936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060320377349853516 0.10781574249267578

Final encoder loss: 0.021406755681140432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060178279876708984 0.10728311538696289

Final encoder loss: 0.01920249790971533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0597681999206543 0.10766339302062988

Final encoder loss: 0.019852111077387907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06089210510253906 0.10734272003173828

Final encoder loss: 0.019444697099470356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06035590171813965 0.1076962947845459

Final encoder loss: 0.01911923431888823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.059889793395996094 0.1075754165649414

Final encoder loss: 0.019683116446629997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06105756759643555 0.10820341110229492

Final encoder loss: 0.02086361643848455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06013631820678711 0.10792708396911621

Final encoder loss: 0.02061659760252613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06011533737182617 0.10810279846191406

Final encoder loss: 0.019253489887066325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.061098575592041016 0.10790205001831055

Final encoder loss: 0.02131166351478541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05973625183105469 0.10718107223510742

Final encoder loss: 0.022978525223824982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059473276138305664 0.10695934295654297


Training dapper model
Final encoder loss: 0.20245082676410675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11737179756164551 0.035302162170410156

Final encoder loss: 0.208219513297081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11293745040893555 0.03454899787902832

Final encoder loss: 0.07906676828861237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1133718490600586 0.03436160087585449

Final encoder loss: 0.08001215010881424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11334609985351562 0.03541135787963867

Final encoder loss: 0.04620075225830078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11256575584411621 0.03466153144836426

Final encoder loss: 0.04531903937458992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11299300193786621 0.0340273380279541

Final encoder loss: 0.03230934217572212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11270809173583984 0.03513813018798828

Final encoder loss: 0.031616345047950745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1127467155456543 0.034276723861694336

Final encoder loss: 0.025606755167245865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11370110511779785 0.03459978103637695

Final encoder loss: 0.0251722801476717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11234211921691895 0.03510642051696777

Final encoder loss: 0.022192928940057755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11371898651123047 0.03399085998535156

Final encoder loss: 0.021872293204069138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11168551445007324 0.034600257873535156

Final encoder loss: 0.020532209426164627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11150121688842773 0.034645795822143555

Final encoder loss: 0.020201807841658592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11371779441833496 0.03411412239074707

Final encoder loss: 0.01958288624882698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11254549026489258 0.03444242477416992

Final encoder loss: 0.01932610757648945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1124117374420166 0.03412675857543945

Final encoder loss: 0.01915201172232628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11455845832824707 0.03412437438964844

Final encoder loss: 0.018924953415989876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11182045936584473 0.03422260284423828

Final encoder loss: 0.019039476290345192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11214780807495117 0.03426051139831543

Final encoder loss: 0.018672890961170197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11363649368286133 0.03536367416381836

Final encoder loss: 0.018994571641087532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11196112632751465 0.03431963920593262

Final encoder loss: 0.01852325350046158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11243867874145508 0.033982038497924805

Final encoder loss: 0.019211577251553535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11325311660766602 0.03503823280334473

Final encoder loss: 0.018470505252480507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11270976066589355 0.03465151786804199

Final encoder loss: 0.0190986767411232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11206698417663574 0.03412175178527832

Final encoder loss: 0.01853928528726101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11324143409729004 0.035015106201171875

Final encoder loss: 0.018702426925301552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11301398277282715 0.03406357765197754

Final encoder loss: 0.01860952191054821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11249423027038574 0.03435111045837402

Final encoder loss: 0.018588680773973465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11332368850708008 0.03570294380187988

Final encoder loss: 0.018614385277032852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11356425285339355 0.034270286560058594

Final encoder loss: 0.018480556085705757
Final encoder loss: 0.016914406791329384

Training case model
Final encoder loss: 0.02940083643390465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.09022307395935059 0.21949052810668945

Final encoder loss: 0.029867270520302386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08953475952148438 0.21912932395935059

Final encoder loss: 0.02979906363359809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09051132202148438 0.21932768821716309

Final encoder loss: 0.02948755013118363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08965015411376953 0.21930837631225586

Final encoder loss: 0.02920886783360373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08990669250488281 0.21891236305236816

Final encoder loss: 0.028593833362733412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0891273021697998 0.21884870529174805

Final encoder loss: 0.028584084669225282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08941936492919922 0.21913695335388184

Final encoder loss: 0.02881894242068651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08957719802856445 0.21931171417236328

Final encoder loss: 0.028795688510735846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08934140205383301 0.21912145614624023

Final encoder loss: 0.029093205189258217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08872079849243164 0.2184901237487793

Final encoder loss: 0.029166869897972295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08948540687561035 0.2184600830078125

Final encoder loss: 0.028453327187750715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08956241607666016 0.21847772598266602

Final encoder loss: 0.029204078654193884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08904695510864258 0.2185053825378418

Final encoder loss: 0.02833580450452095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08905172348022461 0.21846723556518555

Final encoder loss: 0.02886444728117998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08851003646850586 0.21834921836853027

Final encoder loss: 0.02834066019926681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08581709861755371 0.2155907154083252


Training case model
Final encoder loss: 0.20296621322631836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2601635456085205 0.051674604415893555

Final encoder loss: 0.18890999257564545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2731034755706787 0.05193972587585449

Final encoder loss: 0.1901516169309616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2734377384185791 0.05184173583984375

Final encoder loss: 0.19218634068965912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26761603355407715 0.05286884307861328

Final encoder loss: 0.18081700801849365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26872897148132324 0.052027225494384766

Final encoder loss: 0.1919158548116684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561168670654297 0.0515592098236084

Final encoder loss: 0.09965053200721741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583475112915039 0.053107500076293945

Final encoder loss: 0.0899600014090538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26822400093078613 0.05292463302612305

Final encoder loss: 0.08649387210607529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2694566249847412 0.05179572105407715

Final encoder loss: 0.08535750210285187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.258805513381958 0.05147433280944824

Final encoder loss: 0.0780583992600441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27853918075561523 0.052178382873535156

Final encoder loss: 0.0809977799654007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560765743255615 0.0515139102935791

Final encoder loss: 0.05856575816869736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2868502140045166 0.05338430404663086

Final encoder loss: 0.05349653214216232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2705802917480469 0.05370211601257324

Final encoder loss: 0.05186183378100395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27773618698120117 0.05206918716430664

Final encoder loss: 0.05257975682616234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26889610290527344 0.05229759216308594

Final encoder loss: 0.04943489283323288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2662358283996582 0.05170702934265137

Final encoder loss: 0.05075643211603165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543032169342041 0.050870656967163086

Final encoder loss: 0.04288053885102272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27288007736206055 0.0523531436920166

Final encoder loss: 0.0402703583240509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2676377296447754 0.05081653594970703

Final encoder loss: 0.039296723902225494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2670929431915283 0.0523068904876709

Final encoder loss: 0.04030491039156914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27437400817871094 0.05173039436340332

Final encoder loss: 0.03938901796936989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25630784034729004 0.052205562591552734

Final encoder loss: 0.03939107805490494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543215751647949 0.05063199996948242

Final encoder loss: 0.03688031807541847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25574421882629395 0.053511619567871094

Final encoder loss: 0.03586149215698242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2848834991455078 0.051749467849731445

Final encoder loss: 0.03519956022500992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25786638259887695 0.05351376533508301

Final encoder loss: 0.0360247977077961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2582104206085205 0.05271649360656738

Final encoder loss: 0.03668912872672081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2674539089202881 0.05375266075134277

Final encoder loss: 0.03574520722031593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25502514839172363 0.051856040954589844

Final encoder loss: 0.0349542610347271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2681245803833008 0.052358388900756836

Final encoder loss: 0.03464683145284653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26766157150268555 0.05359458923339844

Final encoder loss: 0.03451647236943245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2677121162414551 0.0533747673034668

Final encoder loss: 0.0346253365278244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25658726692199707 0.05329728126525879

Final encoder loss: 0.035811781883239746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26751208305358887 0.05181455612182617

Final encoder loss: 0.03499244898557663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550663948059082 0.05582880973815918

Final encoder loss: 0.03339462727308273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26092982292175293 0.05258011817932129

Final encoder loss: 0.03290116414427757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2741422653198242 0.053472280502319336

Final encoder loss: 0.03257681801915169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2561798095703125 0.05161786079406738

Final encoder loss: 0.032807495445013046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573089599609375 0.05097627639770508

Final encoder loss: 0.03366341441869736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2727980613708496 0.053340911865234375

Final encoder loss: 0.03293246030807495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25579309463500977 0.05228447914123535

Final encoder loss: 0.03208867833018303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25823545455932617 0.05198836326599121

Final encoder loss: 0.031703341752290726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28872156143188477 0.05172157287597656

Final encoder loss: 0.031459957361221313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26796913146972656 0.05191159248352051

Final encoder loss: 0.031769197434186935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27312135696411133 0.05227303504943848

Final encoder loss: 0.03261972963809967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2674105167388916 0.05372190475463867

Final encoder loss: 0.031917791813611984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25330471992492676 0.05260014533996582

Final encoder loss: 0.031245391815900803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27365851402282715 0.05336737632751465

Final encoder loss: 0.031058138236403465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26693296432495117 0.05134940147399902

Final encoder loss: 0.03091154806315899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2668306827545166 0.05259227752685547

Final encoder loss: 0.03110104240477085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26825451850891113 0.05149555206298828

Final encoder loss: 0.032035354524850845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26688241958618164 0.052208662033081055

Final encoder loss: 0.03116198256611824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542855739593506 0.050907135009765625

Final encoder loss: 0.03102203458547592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26640796661376953 0.05177164077758789

Final encoder loss: 0.03081018663942814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26638340950012207 0.05234670639038086

Final encoder loss: 0.030571358278393745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2739415168762207 0.052394866943359375

Final encoder loss: 0.030797729268670082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579796314239502 0.05257391929626465

Final encoder loss: 0.03180871531367302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2809877395629883 0.05257868766784668

Final encoder loss: 0.030754607170820236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25261974334716797 0.05118060111999512

Final encoder loss: 0.030513249337673187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2718231678009033 0.05269646644592285

Final encoder loss: 0.030266867950558662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26738905906677246 0.05183053016662598

Final encoder loss: 0.029844481498003006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27244114875793457 0.05186867713928223

Final encoder loss: 0.030040526762604713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2564108371734619 0.05160331726074219

Final encoder loss: 0.03104075789451599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2722959518432617 0.05186891555786133

Final encoder loss: 0.030154991894960403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25360989570617676 0.05044841766357422

Final encoder loss: 0.030124902725219727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2572000026702881 0.05293154716491699

Final encoder loss: 0.029940864071249962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2740027904510498 0.05180811882019043

Final encoder loss: 0.029714925214648247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2677602767944336 0.05068778991699219

Final encoder loss: 0.02988802082836628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2549021244049072 0.0512082576751709

Final encoder loss: 0.030745478346943855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575550079345703 0.05271124839782715

Final encoder loss: 0.030022501945495605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2525506019592285 0.05180954933166504

Final encoder loss: 0.029769737273454666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28028321266174316 0.05236935615539551

Final encoder loss: 0.02959311380982399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2737717628479004 0.05156350135803223

Final encoder loss: 0.029260454699397087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2672078609466553 0.05237698554992676

Final encoder loss: 0.02944050543010235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26643967628479004 0.05184793472290039

Final encoder loss: 0.03028593771159649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2682209014892578 0.05329442024230957

Final encoder loss: 0.02956272102892399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542686462402344 0.05133962631225586

Final encoder loss: 0.02974511869251728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2903168201446533 0.052139997482299805

Final encoder loss: 0.02959349751472473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2669250965118408 0.05137372016906738

Final encoder loss: 0.029296940192580223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2665083408355713 0.05247306823730469

Final encoder loss: 0.029391005635261536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26686525344848633 0.05333065986633301

Final encoder loss: 0.03032243624329567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2724313735961914 0.05402565002441406

Final encoder loss: 0.02949921414256096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25234317779541016 0.05158424377441406

Final encoder loss: 0.029506472870707512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2720775604248047 0.05211639404296875

Final encoder loss: 0.02917191945016384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2732827663421631 0.05162978172302246

Final encoder loss: 0.02890986017882824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27358365058898926 0.05209088325500488

Final encoder loss: 0.028940550982952118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2670755386352539 0.05219531059265137

Final encoder loss: 0.029871052131056786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25632524490356445 0.052034854888916016

Final encoder loss: 0.029098128899931908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2536756992340088 0.051927804946899414

Final encoder loss: 0.02930130623281002
Final encoder loss: 0.028497854247689247
Final encoder loss: 0.027590669691562653
Final encoder loss: 0.026798835024237633
Final encoder loss: 0.02661067806184292
Final encoder loss: 0.024945460259914398

Training emognition model
Final encoder loss: 0.034649279495778444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08118486404418945 0.2299823760986328

Final encoder loss: 0.032441459360336426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08070182800292969 0.22998833656311035

Final encoder loss: 0.034628973902753164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0810401439666748 0.22953057289123535

Final encoder loss: 0.03289895852505018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08064699172973633 0.22983908653259277

Final encoder loss: 0.03318628089914357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08065176010131836 0.22971248626708984

Final encoder loss: 0.03384129682860237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08077859878540039 0.22962021827697754

Final encoder loss: 0.03172919931459325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08082938194274902 0.229628324508667

Final encoder loss: 0.034914847561540265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08056497573852539 0.23014092445373535

Final encoder loss: 0.03528782706720712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08076858520507812 0.2296912670135498

Final encoder loss: 0.032251148513472716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08095169067382812 0.22929835319519043

Final encoder loss: 0.03477433219698977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08046174049377441 0.22917628288269043

Final encoder loss: 0.03347320976396437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08027791976928711 0.2297208309173584

Final encoder loss: 0.03224681193170103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08067202568054199 0.22955536842346191

Final encoder loss: 0.0331880035155528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08052349090576172 0.22971701622009277

Final encoder loss: 0.03480240048019636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08086299896240234 0.2296581268310547

Final encoder loss: 0.03321238371398641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07957649230957031 0.2290482521057129


Training emognition model
Final encoder loss: 0.19357264041900635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24940204620361328 0.049452781677246094

Final encoder loss: 0.1949814110994339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24586725234985352 0.049096107482910156

Final encoder loss: 0.0853368416428566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24658417701721191 0.04860496520996094

Final encoder loss: 0.08497636765241623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24601125717163086 0.04842066764831543

Final encoder loss: 0.055466242134571075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24684858322143555 0.04893779754638672

Final encoder loss: 0.054117996245622635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2462601661682129 0.04962611198425293

Final encoder loss: 0.04304349422454834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24631357192993164 0.04920840263366699

Final encoder loss: 0.04209184646606445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24580621719360352 0.047646522521972656

Final encoder loss: 0.037063393741846085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2460932731628418 0.048618316650390625

Final encoder loss: 0.03635439649224281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24618911743164062 0.04870414733886719

Final encoder loss: 0.03392462059855461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24680471420288086 0.049399614334106445

Final encoder loss: 0.03344833105802536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24579524993896484 0.0483551025390625

Final encoder loss: 0.032341327518224716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2470405101776123 0.04796719551086426

Final encoder loss: 0.032130900770425797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24547910690307617 0.04890251159667969

Final encoder loss: 0.03179559484124184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24649262428283691 0.05011487007141113

Final encoder loss: 0.031885623931884766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24601101875305176 0.049726247787475586

Final encoder loss: 0.031851835548877716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24672412872314453 0.048264265060424805

Final encoder loss: 0.03209769353270531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2458479404449463 0.04918074607849121

Final encoder loss: 0.032084036618471146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24654078483581543 0.04974079132080078

Final encoder loss: 0.03256585821509361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2455000877380371 0.049454450607299805

Final encoder loss: 0.032050516456365585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24857425689697266 0.049890756607055664

Final encoder loss: 0.03234918788075447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24750256538391113 0.05141043663024902

Final encoder loss: 0.03201783075928688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24900436401367188 0.04972100257873535

Final encoder loss: 0.03222297504544258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477257251739502 0.049336910247802734

Final encoder loss: 0.03151751682162285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24956965446472168 0.04864215850830078

Final encoder loss: 0.03189218416810036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24765920639038086 0.04986071586608887

Final encoder loss: 0.03182290121912956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2506999969482422 0.04860520362854004

Final encoder loss: 0.031979039311409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24759936332702637 0.05036330223083496

Final encoder loss: 0.0316891074180603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25004100799560547 0.051604270935058594

Final encoder loss: 0.03195410594344139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24598002433776855 0.05036497116088867

Final encoder loss: 0.03174058347940445
Final encoder loss: 0.030734295025467873

Training empatch model
Final encoder loss: 0.05129282720551986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07168006896972656 0.17421627044677734

Final encoder loss: 0.05128700290983541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07187318801879883 0.1743485927581787

Final encoder loss: 0.04762218323683872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07271504402160645 0.17438530921936035

Final encoder loss: 0.047617831544010995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07142376899719238 0.17415332794189453

Final encoder loss: 0.04233986600340508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07125282287597656 0.17391228675842285

Final encoder loss: 0.0464467638782075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07120156288146973 0.17510509490966797

Final encoder loss: 0.04303578581267491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07281661033630371 0.17406821250915527

Final encoder loss: 0.041485183926979134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07112932205200195 0.17373085021972656

Final encoder loss: 0.03217009639772477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07112741470336914 0.17424917221069336

Final encoder loss: 0.033285363807426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07275533676147461 0.17450904846191406

Final encoder loss: 0.030816538346298807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07156229019165039 0.1741771697998047

Final encoder loss: 0.03056595704626231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0713953971862793 0.1738576889038086

Final encoder loss: 0.033000130156238434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07102727890014648 0.17443513870239258

Final encoder loss: 0.03515349789916914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07297134399414062 0.17399287223815918

Final encoder loss: 0.032276691936730134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07176637649536133 0.17435336112976074

Final encoder loss: 0.03212584361964776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07086920738220215 0.17417478561401367


Training empatch model
Final encoder loss: 0.1711660623550415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1804664134979248 0.04315519332885742

Final encoder loss: 0.07988134771585464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758441925048828 0.04376959800720215

Final encoder loss: 0.05649685114622116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17587065696716309 0.04461932182312012

Final encoder loss: 0.0453944206237793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17731595039367676 0.04353809356689453

Final encoder loss: 0.039070285856723785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758415699005127 0.043450117111206055

Final encoder loss: 0.03515808656811714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17454814910888672 0.04412198066711426

Final encoder loss: 0.03266724571585655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17800593376159668 0.04361701011657715

Final encoder loss: 0.031085772439837456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17583036422729492 0.04384469985961914

Final encoder loss: 0.030051130801439285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17552542686462402 0.044069528579711914

Final encoder loss: 0.02940550073981285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17792725563049316 0.045105934143066406

Final encoder loss: 0.02901501953601837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471098899841309 0.04461383819580078

Final encoder loss: 0.028835220262408257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17612814903259277 0.04306316375732422

Final encoder loss: 0.028591420501470566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17688393592834473 0.04571962356567383

Final encoder loss: 0.028562571853399277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744825839996338 0.04405474662780762

Final encoder loss: 0.028478818014264107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1740865707397461 0.0444948673248291

Final encoder loss: 0.02843579649925232

Training wesad model
Final encoder loss: 0.05260409227635015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07365679740905762 0.1737520694732666

Final encoder loss: 0.04910656257122912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07165265083312988 0.17428088188171387

Final encoder loss: 0.04945214240987518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07120323181152344 0.17447948455810547

Final encoder loss: 0.04786386113205844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07252192497253418 0.17456626892089844

Final encoder loss: 0.033700695726152155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07113313674926758 0.17393064498901367

Final encoder loss: 0.03390930642469122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07123875617980957 0.17421913146972656

Final encoder loss: 0.03539532275176022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07154965400695801 0.1743929386138916

Final encoder loss: 0.03417680779949588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07353520393371582 0.17385053634643555

Final encoder loss: 0.02559817365407239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07193803787231445 0.17432117462158203

Final encoder loss: 0.026645389924563605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07128071784973145 0.17469406127929688

Final encoder loss: 0.027387953680368282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07283353805541992 0.1745738983154297

Final encoder loss: 0.028810904104519426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07126784324645996 0.17391252517700195

Final encoder loss: 0.022987599939164233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07122969627380371 0.17383718490600586

Final encoder loss: 0.02187980040174714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07099437713623047 0.1739053726196289

Final encoder loss: 0.021682613960191333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07319307327270508 0.17528057098388672

Final encoder loss: 0.024425779061176742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07132363319396973 0.17398953437805176


Training wesad model
Final encoder loss: 0.21560809016227722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10527324676513672 0.03312873840332031

Final encoder loss: 0.09722345322370529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10433840751647949 0.03448319435119629

Final encoder loss: 0.06366734951734543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1065208911895752 0.03437352180480957

Final encoder loss: 0.04729887470602989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393977165222168 0.03348135948181152

Final encoder loss: 0.03814611956477165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356330871582031 0.03386282920837402

Final encoder loss: 0.032697033137083054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10397052764892578 0.03330397605895996

Final encoder loss: 0.02932879514992237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10347533226013184 0.03291654586791992

Final encoder loss: 0.02722768858075142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1058506965637207 0.034285783767700195

Final encoder loss: 0.02589035965502262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10602617263793945 0.033434391021728516

Final encoder loss: 0.025064392015337944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1043391227722168 0.033319711685180664

Final encoder loss: 0.02466501109302044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357213020324707 0.033303022384643555

Final encoder loss: 0.024528341367840767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10318922996520996 0.03272271156311035

Final encoder loss: 0.024715499952435493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10413646697998047 0.033525705337524414

Final encoder loss: 0.02519528940320015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10622596740722656 0.034169912338256836

Final encoder loss: 0.025492506101727486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10606002807617188 0.032685279846191406

Final encoder loss: 0.025533895939588547

Calculating loss for amigos model
	Full Pass 0.6758894920349121
numFreeParamsPath 18
Reconstruction loss values: 0.03646846115589142 0.04652291536331177

Calculating loss for dapper model
	Full Pass 0.1547088623046875
numFreeParamsPath 18
Reconstruction loss values: 0.028463134542107582 0.03221442177891731

Calculating loss for case model
	Full Pass 0.9101636409759521
numFreeParamsPath 18
Reconstruction loss values: 0.0415252149105072 0.044933728873729706

Calculating loss for emognition model
	Full Pass 0.29275012016296387
numFreeParamsPath 18
Reconstruction loss values: 0.04570209980010986 0.05427531152963638

Calculating loss for empatch model
	Full Pass 0.10486173629760742
numFreeParamsPath 18
Reconstruction loss values: 0.04951317980885506 0.05631353706121445

Calculating loss for wesad model
	Full Pass 0.07747936248779297
numFreeParamsPath 18
Reconstruction loss values: 0.05175364762544632 0.07307542115449905
Total loss calculation time: 3.9636576175689697

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.005472660064697
Total epoch time: 163.56435203552246

Epoch: 31

Training case model
Final encoder loss: 0.04023851340521279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09907317161560059 0.27293872833251953

Final encoder loss: 0.03726793559340561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09244275093078613 0.26621437072753906

Final encoder loss: 0.036699024931879785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09137129783630371 0.26547670364379883

Final encoder loss: 0.03521240857075533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09151363372802734 0.26645588874816895

Final encoder loss: 0.03532545026988971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.0919501781463623 0.2653179168701172

Final encoder loss: 0.03487904030592676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0914759635925293 0.26607823371887207

Final encoder loss: 0.03490259375876999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.4077322483062744 0.26505136489868164

Final encoder loss: 0.03317446474817333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09220433235168457 0.2667815685272217

Final encoder loss: 0.03413493580688863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09177708625793457 0.2665395736694336

Final encoder loss: 0.034086663034093134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09177184104919434 0.2657804489135742

Final encoder loss: 0.031971764980209705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0924825668334961 0.2645406723022461

Final encoder loss: 0.03235191845430595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09173941612243652 0.2656707763671875

Final encoder loss: 0.03203403494766196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09334444999694824 0.26683926582336426

Final encoder loss: 0.032495528456439236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09224534034729004 0.2656385898590088

Final encoder loss: 0.03125322435428449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09280562400817871 0.2659487724304199

Final encoder loss: 0.03114737027218694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08917760848999023 0.2623603343963623


Training emognition model
Final encoder loss: 0.048230624417834954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08312797546386719 0.2751190662384033

Final encoder loss: 0.042109307076995145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08530855178833008 0.2746284008026123

Final encoder loss: 0.04321271438452131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08376908302307129 0.27601051330566406

Final encoder loss: 0.04289395603207002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08592796325683594 0.2765955924987793

Final encoder loss: 0.041977359513028306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08433270454406738 0.27568721771240234

Final encoder loss: 0.040588171185776935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08371114730834961 0.276200532913208

Final encoder loss: 0.03918716856195975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08360099792480469 0.2749161720275879

Final encoder loss: 0.04132733296881154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0836799144744873 0.2755246162414551

Final encoder loss: 0.040997118539067554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08538246154785156 0.27672600746154785

Final encoder loss: 0.04066702174194932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08406448364257812 0.27570295333862305

Final encoder loss: 0.04001699900299359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08472537994384766 0.2759575843811035

Final encoder loss: 0.040179572086059566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08388710021972656 0.2766580581665039

Final encoder loss: 0.04342518204848084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08324551582336426 0.2738819122314453

Final encoder loss: 0.040526954382131754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08286428451538086 0.2728550434112549

Final encoder loss: 0.03838343900209164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08280134201049805 0.27475619316101074

Final encoder loss: 0.0407486981698239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08263826370239258 0.27295970916748047


Training dapper model
Final encoder loss: 0.029875061274136975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06144356727600098 0.14770174026489258

Final encoder loss: 0.026786535728117202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06138110160827637 0.14882946014404297

Final encoder loss: 0.028368001693439605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06115221977233887 0.14938998222351074

Final encoder loss: 0.031313173826497016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061525583267211914 0.14906525611877441

Final encoder loss: 0.02601164243272083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.061128854751586914 0.14923405647277832

Final encoder loss: 0.026290979453502897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06142544746398926 0.1489706039428711

Final encoder loss: 0.02591703096645542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06144285202026367 0.14931678771972656

Final encoder loss: 0.027182514735364514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061487674713134766 0.14853572845458984

Final encoder loss: 0.0256503135209215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061125993728637695 0.14901328086853027

Final encoder loss: 0.028211702028932976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06152844429016113 0.1505594253540039

Final encoder loss: 0.02522044201085877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06208062171936035 0.14966535568237305

Final encoder loss: 0.02816406587558722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.0626382827758789 0.15087485313415527

Final encoder loss: 0.028138375991906984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06266069412231445 0.150099515914917

Final encoder loss: 0.025268170454833427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.061893463134765625 0.1506500244140625

Final encoder loss: 0.026827553752546588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06237173080444336 0.15031862258911133

Final encoder loss: 0.021756937313142308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06235003471374512 0.14964914321899414


Training amigos model
Final encoder loss: 0.035587917551868006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1087498664855957 0.3890080451965332

Final encoder loss: 0.03497981244500263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10833621025085449 0.38893866539001465

Final encoder loss: 0.03410870936291392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10835981369018555 0.38948988914489746

Final encoder loss: 0.03699591635876038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.1086416244506836 0.3886699676513672

Final encoder loss: 0.03703798151319457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.1084291934967041 0.38953495025634766

Final encoder loss: 0.03262713897643126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10860705375671387 0.3898427486419678

Final encoder loss: 0.03412374521902688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10827016830444336 0.38890838623046875

Final encoder loss: 0.035045595163199016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10846686363220215 0.38936686515808105

Final encoder loss: 0.03266263096933137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10776019096374512 0.389847993850708

Final encoder loss: 0.032717594556621585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10851740837097168 0.38932037353515625

Final encoder loss: 0.033284187568847344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10880589485168457 0.38921546936035156

Final encoder loss: 0.03578933023408772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10807991027832031 0.3895680904388428

Final encoder loss: 0.03405682343368768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10861730575561523 0.390261173248291

Final encoder loss: 0.03413761828763196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1085503101348877 0.38916492462158203

Final encoder loss: 0.035230127146731606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10859346389770508 0.3891921043395996

Final encoder loss: 0.03379026555425747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10317158699035645 0.3829350471496582


Training amigos model
Final encoder loss: 0.022816894946849242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10609722137451172 0.341533899307251

Final encoder loss: 0.026491524404564714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10579609870910645 0.3411104679107666

Final encoder loss: 0.023810669685257427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10585379600524902 0.3410634994506836

Final encoder loss: 0.02292012333392235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10632753372192383 0.3411240577697754

Final encoder loss: 0.024629605452270515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10590600967407227 0.341538667678833

Final encoder loss: 0.026323397322327798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1059408187866211 0.3411521911621094

Final encoder loss: 0.02554513957483225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10559964179992676 0.3408956527709961

Final encoder loss: 0.02321230338147572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10556268692016602 0.3413405418395996

Final encoder loss: 0.026873620691005882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10589790344238281 0.3424496650695801

Final encoder loss: 0.023851502549596663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10715484619140625 0.34154748916625977

Final encoder loss: 0.02305816690241992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1060178279876709 0.3417994976043701

Final encoder loss: 0.024524191243263882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10781645774841309 0.34146761894226074

Final encoder loss: 0.02654817648145107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1062018871307373 0.34194302558898926

Final encoder loss: 0.02534616696222487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10604166984558105 0.3414914608001709

Final encoder loss: 0.025736902866310227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10595107078552246 0.341522216796875

Final encoder loss: 0.023663789044569034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10159802436828613 0.33783578872680664


Training amigos model
Final encoder loss: 0.18076461553573608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45528435707092285 0.0812690258026123

Final encoder loss: 0.18783599138259888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46821045875549316 0.0736086368560791

Final encoder loss: 0.1836465746164322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4553961753845215 0.07587242126464844

Final encoder loss: 0.07394590973854065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4659554958343506 0.07335996627807617

Final encoder loss: 0.07547830790281296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45136141777038574 0.08089494705200195

Final encoder loss: 0.06964794546365738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45703601837158203 0.07171416282653809

Final encoder loss: 0.0441509410738945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46997570991516113 0.07595014572143555

Final encoder loss: 0.044770438224077225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4520277976989746 0.07562494277954102

Final encoder loss: 0.042120806872844696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45331287384033203 0.08009457588195801

Final encoder loss: 0.03297837823629379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4689908027648926 0.07353949546813965

Final encoder loss: 0.03364914283156395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45627617835998535 0.07646489143371582

Final encoder loss: 0.03245754912495613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45105791091918945 0.0762929916381836

Final encoder loss: 0.028140755370259285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4507334232330322 0.07632899284362793

Final encoder loss: 0.028963245451450348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45398831367492676 0.07560133934020996

Final encoder loss: 0.02810918539762497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45754361152648926 0.07480549812316895

Final encoder loss: 0.02642844058573246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4501030445098877 0.07611203193664551

Final encoder loss: 0.02704150602221489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4651796817779541 0.0794675350189209

Final encoder loss: 0.026515401899814606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45236802101135254 0.07576322555541992

Final encoder loss: 0.026389336213469505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4557185173034668 0.07768511772155762

Final encoder loss: 0.026857582852244377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45023441314697266 0.07577848434448242

Final encoder loss: 0.026613358408212662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45030832290649414 0.07575821876525879

Final encoder loss: 0.026571262627840042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46645355224609375 0.08162379264831543

Final encoder loss: 0.0269438698887825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4565598964691162 0.07585430145263672

Final encoder loss: 0.02692500688135624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4491698741912842 0.07540249824523926

Final encoder loss: 0.026372022926807404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4543447494506836 0.07554006576538086

Final encoder loss: 0.026120636612176895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4651458263397217 0.08136701583862305

Final encoder loss: 0.02639147825539112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45550966262817383 0.07349038124084473

Final encoder loss: 0.025588631629943848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46395134925842285 0.07408928871154785

Final encoder loss: 0.02540966123342514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4532132148742676 0.07575035095214844

Final encoder loss: 0.02564040571451187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45241236686706543 0.07854127883911133

Final encoder loss: 0.02490142174065113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45485997200012207 0.07503557205200195

Final encoder loss: 0.025072330608963966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45394158363342285 0.0755312442779541

Final encoder loss: 0.025327051058411598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44838666915893555 0.07701706886291504

Final encoder loss: 0.024827370420098305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4661564826965332 0.07994198799133301

Final encoder loss: 0.025100531056523323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45684385299682617 0.07588720321655273

Final encoder loss: 0.02529112622141838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45368409156799316 0.07463502883911133

Final encoder loss: 0.024519242346286774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46554994583129883 0.07585763931274414

Final encoder loss: 0.02463168278336525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45290255546569824 0.0783390998840332

Final encoder loss: 0.025002891197800636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45309901237487793 0.07575654983520508

Final encoder loss: 0.02433985285460949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4676170349121094 0.07601118087768555

Final encoder loss: 0.024428103119134903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44684267044067383 0.07412362098693848

Final encoder loss: 0.024607233703136444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45273900032043457 0.0803990364074707

Final encoder loss: 0.024360036477446556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4551730155944824 0.07541656494140625

Final encoder loss: 0.024288909509778023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.467789888381958 0.07652473449707031

Final encoder loss: 0.024573950096964836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4481081962585449 0.07493066787719727

Final encoder loss: 0.02409341000020504
Final encoder loss: 0.02310171164572239
Final encoder loss: 0.022220181301236153

Training dapper model
Final encoder loss: 0.021217874687584626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06126093864440918 0.10767841339111328

Final encoder loss: 0.020518344591517326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059612274169921875 0.10725688934326172

Final encoder loss: 0.022190840866301662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06001687049865723 0.10746073722839355

Final encoder loss: 0.02143578705599642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059441566467285156 0.10807108879089355

Final encoder loss: 0.020349003390315694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06074976921081543 0.10711002349853516

Final encoder loss: 0.02151533870383883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06013846397399902 0.10775136947631836

Final encoder loss: 0.01988486101602958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05992555618286133 0.10732126235961914

Final encoder loss: 0.018364234291174664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060801029205322266 0.10777425765991211

Final encoder loss: 0.022231005532133818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.0602726936340332 0.10763692855834961

Final encoder loss: 0.019272756877242645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06002640724182129 0.10748624801635742

Final encoder loss: 0.022319075840061597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059984445571899414 0.10797452926635742

Final encoder loss: 0.01991526914309012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06047964096069336 0.10741472244262695

Final encoder loss: 0.019330537027277227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06009650230407715 0.10762166976928711

Final encoder loss: 0.020373605378834477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06009793281555176 0.10805344581604004

Final encoder loss: 0.0204584332301682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06091785430908203 0.1080014705657959

Final encoder loss: 0.01750476595376234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05980682373046875 0.10732674598693848


Training dapper model
Final encoder loss: 0.20243877172470093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1151273250579834 0.034439802169799805

Final encoder loss: 0.20818127691745758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11778020858764648 0.03432655334472656

Final encoder loss: 0.07944043725728989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11278629302978516 0.03464865684509277

Final encoder loss: 0.08080413192510605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11575841903686523 0.03418731689453125

Final encoder loss: 0.04662250354886055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11349320411682129 0.03566384315490723

Final encoder loss: 0.04618702828884125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11576509475708008 0.034661293029785156

Final encoder loss: 0.03268297389149666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11327743530273438 0.03409934043884277

Final encoder loss: 0.032204825431108475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11399340629577637 0.034885406494140625

Final encoder loss: 0.025800611823797226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1139993667602539 0.0341184139251709

Final encoder loss: 0.025563431903719902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557269096374512 0.034491539001464844

Final encoder loss: 0.022174380719661713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11282014846801758 0.03430342674255371

Final encoder loss: 0.022126423195004463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11683177947998047 0.034749507904052734

Final encoder loss: 0.02031099796295166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11285185813903809 0.03501558303833008

Final encoder loss: 0.020313246175646782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11574745178222656 0.03432917594909668

Final encoder loss: 0.019416669383645058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1133584976196289 0.03539109230041504

Final encoder loss: 0.019390644505620003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11627745628356934 0.0346217155456543

Final encoder loss: 0.018937179818749428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11277055740356445 0.03425788879394531

Final encoder loss: 0.0188966803252697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11494612693786621 0.034868717193603516

Final encoder loss: 0.019012633711099625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11441469192504883 0.03403306007385254

Final encoder loss: 0.01883033849298954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1151421070098877 0.03482460975646973

Final encoder loss: 0.018633870407938957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11328363418579102 0.03440356254577637

Final encoder loss: 0.018775423988699913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11744809150695801 0.03466629981994629

Final encoder loss: 0.018846219405531883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11283516883850098 0.03413510322570801

Final encoder loss: 0.01870856247842312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11501216888427734 0.03414034843444824

Final encoder loss: 0.019109178334474564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1142127513885498 0.035370588302612305

Final encoder loss: 0.018768249079585075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11575055122375488 0.03388261795043945

Final encoder loss: 0.01864168606698513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11305975914001465 0.034319162368774414

Final encoder loss: 0.018692201003432274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1149911880493164 0.0346529483795166

Final encoder loss: 0.018174028024077415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1138153076171875 0.03451681137084961

Final encoder loss: 0.01850985735654831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527204513549805 0.03400135040283203

Final encoder loss: 0.017891624942421913
Final encoder loss: 0.016935547813773155

Training case model
Final encoder loss: 0.030450599965737384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08977794647216797 0.21904277801513672

Final encoder loss: 0.029934864057840682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08933424949645996 0.21915626525878906

Final encoder loss: 0.02854565471258833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08904242515563965 0.2191939353942871

Final encoder loss: 0.029142675652249903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09015870094299316 0.21928024291992188

Final encoder loss: 0.029543217651524768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08920693397521973 0.2193593978881836

Final encoder loss: 0.028991102986045625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09015297889709473 0.21934294700622559

Final encoder loss: 0.029004408335145702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08990216255187988 0.2191023826599121

Final encoder loss: 0.0297587433441027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.0901341438293457 0.21911835670471191

Final encoder loss: 0.028344324709929185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08957362174987793 0.21928930282592773

Final encoder loss: 0.029573346123049113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0902254581451416 0.21953749656677246

Final encoder loss: 0.02872825976435942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08963394165039062 0.2191927433013916

Final encoder loss: 0.02958561851909967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08969902992248535 0.21941685676574707

Final encoder loss: 0.028889412199607976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08985090255737305 0.21911263465881348

Final encoder loss: 0.02839223542325903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09042644500732422 0.21956586837768555

Final encoder loss: 0.029044625573250597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0891423225402832 0.2193152904510498

Final encoder loss: 0.028493538832274786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08618283271789551 0.21615839004516602


Training case model
Final encoder loss: 0.2029564380645752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2601656913757324 0.05138349533081055

Final encoder loss: 0.18891184031963348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2800142765045166 0.052202463150024414

Final encoder loss: 0.19015993177890778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2564566135406494 0.05253410339355469

Final encoder loss: 0.19219070672988892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28000950813293457 0.051981210708618164

Final encoder loss: 0.18082201480865479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2564098834991455 0.052132368087768555

Final encoder loss: 0.19192609190940857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545630931854248 0.05124950408935547

Final encoder loss: 0.10074446350336075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2667512893676758 0.05238485336303711

Final encoder loss: 0.09096510708332062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26685166358947754 0.05227208137512207

Final encoder loss: 0.08695577830076218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26708126068115234 0.05164790153503418

Final encoder loss: 0.08612751215696335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26816368103027344 0.05186200141906738

Final encoder loss: 0.07854613661766052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27613234519958496 0.05232095718383789

Final encoder loss: 0.08153314143419266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25649452209472656 0.052489280700683594

Final encoder loss: 0.05948234349489212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2681999206542969 0.05351400375366211

Final encoder loss: 0.05424274876713753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2757561206817627 0.053653717041015625

Final encoder loss: 0.052302245050668716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586207389831543 0.05271410942077637

Final encoder loss: 0.052929800003767014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2682633399963379 0.0529024600982666

Final encoder loss: 0.0496450699865818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26993775367736816 0.050827741622924805

Final encoder loss: 0.05121854320168495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560868263244629 0.05265522003173828

Final encoder loss: 0.04349350556731224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27671241760253906 0.05409121513366699

Final encoder loss: 0.040584564208984375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27022743225097656 0.05311012268066406

Final encoder loss: 0.03961023688316345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2776825428009033 0.05159115791320801

Final encoder loss: 0.040473345667123795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25939393043518066 0.054076194763183594

Final encoder loss: 0.039354801177978516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2740905284881592 0.053446054458618164

Final encoder loss: 0.03970020264387131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549769878387451 0.05216789245605469

Final encoder loss: 0.03739558905363083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27486538887023926 0.052481651306152344

Final encoder loss: 0.03588302060961723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27063822746276855 0.05273079872131348

Final encoder loss: 0.03529917821288109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28763461112976074 0.05470442771911621

Final encoder loss: 0.03606090694665909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.268573522567749 0.05091714859008789

Final encoder loss: 0.03631502017378807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2687053680419922 0.053505897521972656

Final encoder loss: 0.03586657717823982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25586533546447754 0.05236625671386719

Final encoder loss: 0.03500192612409592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27838563919067383 0.05392193794250488

Final encoder loss: 0.034330081194639206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2750976085662842 0.053557395935058594

Final encoder loss: 0.03430981934070587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26901936531066895 0.05204463005065918

Final encoder loss: 0.034534353762865067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28516173362731934 0.051665306091308594

Final encoder loss: 0.03507896885275841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25789761543273926 0.054010629653930664

Final encoder loss: 0.03447593003511429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25683164596557617 0.05271744728088379

Final encoder loss: 0.033532366156578064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573971748352051 0.054544687271118164

Final encoder loss: 0.03278310224413872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.267911434173584 0.05191755294799805

Final encoder loss: 0.032377663999795914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2823903560638428 0.05268740653991699

Final encoder loss: 0.032937053591012955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26038217544555664 0.0529325008392334

Final encoder loss: 0.0332660898566246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2694368362426758 0.05355429649353027

Final encoder loss: 0.032644134014844894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542896270751953 0.05178666114807129

Final encoder loss: 0.0321611613035202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588672637939453 0.05204486846923828

Final encoder loss: 0.03132539615035057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2951021194458008 0.05243253707885742

Final encoder loss: 0.031165534630417824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2694864273071289 0.05202627182006836

Final encoder loss: 0.0315670408308506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2738685607910156 0.05378007888793945

Final encoder loss: 0.03232447803020477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2752079963684082 0.05436539649963379

Final encoder loss: 0.03146699443459511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541651725769043 0.052733659744262695

Final encoder loss: 0.03148895874619484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.272418737411499 0.05292558670043945

Final encoder loss: 0.03071499615907669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26708531379699707 0.051938772201538086

Final encoder loss: 0.030338000506162643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27320075035095215 0.05259346961975098

Final encoder loss: 0.03075825236737728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2728290557861328 0.051787614822387695

Final encoder loss: 0.031548574566841125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26581406593322754 0.05254721641540527

Final encoder loss: 0.03083563596010208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25342440605163574 0.052496910095214844

Final encoder loss: 0.0309084951877594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2671370506286621 0.052609920501708984

Final encoder loss: 0.030391627922654152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28068971633911133 0.0532076358795166

Final encoder loss: 0.030281614512205124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25757908821105957 0.05250191688537598

Final encoder loss: 0.030347371473908424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28450703620910645 0.053757429122924805

Final encoder loss: 0.03116369992494583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2686343193054199 0.05364036560058594

Final encoder loss: 0.03039552830159664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255704402923584 0.0526885986328125

Final encoder loss: 0.03045537881553173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2684202194213867 0.05376458168029785

Final encoder loss: 0.02995845302939415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28525233268737793 0.052824974060058594

Final encoder loss: 0.029755139723420143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2682919502258301 0.05287766456604004

Final encoder loss: 0.029942240566015244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2771263122558594 0.05380105972290039

Final encoder loss: 0.030649304389953613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26839518547058105 0.052442312240600586

Final encoder loss: 0.029870551079511642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556004524230957 0.05225372314453125

Final encoder loss: 0.030152123421430588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582259178161621 0.05280303955078125

Final encoder loss: 0.02956387586891651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27426576614379883 0.05249929428100586

Final encoder loss: 0.029370635747909546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588920593261719 0.053591012954711914

Final encoder loss: 0.029547780752182007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2796900272369385 0.0520930290222168

Final encoder loss: 0.030389534309506416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25687265396118164 0.05273890495300293

Final encoder loss: 0.029584411531686783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539389133453369 0.050997257232666016

Final encoder loss: 0.029849007725715637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2734379768371582 0.052762508392333984

Final encoder loss: 0.0294395312666893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2567782402038574 0.05267214775085449

Final encoder loss: 0.028990959748625755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27324533462524414 0.052643537521362305

Final encoder loss: 0.029195427894592285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25568485260009766 0.051354169845581055

Final encoder loss: 0.029998989775776863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2722346782684326 0.052961111068725586

Final encoder loss: 0.029218608513474464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2527730464935303 0.051138877868652344

Final encoder loss: 0.029569897800683975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568964958190918 0.053295135498046875

Final encoder loss: 0.029042087495326996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2674529552459717 0.05359339714050293

Final encoder loss: 0.028939127922058105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2668793201446533 0.05304980278015137

Final encoder loss: 0.028983252122998238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2748677730560303 0.0538785457611084

Final encoder loss: 0.0297749862074852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2671668529510498 0.05300402641296387

Final encoder loss: 0.029011709615588188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543785572052002 0.05090785026550293

Final encoder loss: 0.029405627399683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.274641752243042 0.0519099235534668

Final encoder loss: 0.028966590762138367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2677922248840332 0.05282473564147949

Final encoder loss: 0.02855774015188217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2674567699432373 0.05117011070251465

Final encoder loss: 0.028861042112112045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2876615524291992 0.05202054977416992

Final encoder loss: 0.029459843412041664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26615047454833984 0.053458452224731445

Final encoder loss: 0.028861794620752335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552800178527832 0.052496910095214844

Final encoder loss: 0.029193203896284103
Final encoder loss: 0.028227921575307846
Final encoder loss: 0.027258573099970818
Final encoder loss: 0.026514464989304543
Final encoder loss: 0.02628382295370102
Final encoder loss: 0.02460898831486702

Training emognition model
Final encoder loss: 0.03265332448719977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08262491226196289 0.23139190673828125

Final encoder loss: 0.03235577066565668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08251166343688965 0.23122000694274902

Final encoder loss: 0.033331839487118695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08096861839294434 0.23035693168640137

Final encoder loss: 0.03334532529450094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08069205284118652 0.23088669776916504

Final encoder loss: 0.03284813726933647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08109903335571289 0.23052334785461426

Final encoder loss: 0.03315080305496659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0810239315032959 0.23048615455627441

Final encoder loss: 0.03340746530020571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08104729652404785 0.23030376434326172

Final encoder loss: 0.03254287845509549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08081340789794922 0.22998332977294922

Final encoder loss: 0.0321414279182752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08255219459533691 0.23044729232788086

Final encoder loss: 0.03392450222831049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08091115951538086 0.23061752319335938

Final encoder loss: 0.034128689907287464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08135557174682617 0.23065495491027832

Final encoder loss: 0.0319939074347426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08068633079528809 0.22980690002441406

Final encoder loss: 0.033745668846933474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08069300651550293 0.22997713088989258

Final encoder loss: 0.03366891915851136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08063817024230957 0.22972607612609863

Final encoder loss: 0.0318115488947717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0807943344116211 0.23075628280639648

Final encoder loss: 0.03302100673752428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08022189140319824 0.22896194458007812


Training emognition model
Final encoder loss: 0.19357062876224518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24924898147583008 0.04847002029418945

Final encoder loss: 0.1949668973684311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24837088584899902 0.04911184310913086

Final encoder loss: 0.08558560907840729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24797582626342773 0.04873156547546387

Final encoder loss: 0.08490587770938873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24728870391845703 0.04818320274353027

Final encoder loss: 0.05534563958644867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24817252159118652 0.050078630447387695

Final encoder loss: 0.05381789058446884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471938133239746 0.04823160171508789

Final encoder loss: 0.04276738688349724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24735808372497559 0.04896187782287598

Final encoder loss: 0.04170281067490578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24638605117797852 0.049341678619384766

Final encoder loss: 0.03666078299283981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467501163482666 0.04850506782531738

Final encoder loss: 0.03598235547542572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464134693145752 0.04933285713195801

Final encoder loss: 0.033540401607751846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2480158805847168 0.04900860786437988

Final encoder loss: 0.0330372229218483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2454357147216797 0.04940295219421387

Final encoder loss: 0.03208300471305847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.246812105178833 0.04944181442260742

Final encoder loss: 0.03165033087134361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24562358856201172 0.0496373176574707

Final encoder loss: 0.03156669810414314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474355697631836 0.049302101135253906

Final encoder loss: 0.031320519745349884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24602437019348145 0.04816269874572754

Final encoder loss: 0.031592536717653275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24725866317749023 0.04890727996826172

Final encoder loss: 0.031560495495796204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2459402084350586 0.049263954162597656

Final encoder loss: 0.031420376151800156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474205493927002 0.04836130142211914

Final encoder loss: 0.031801771372556686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24623990058898926 0.04889941215515137

Final encoder loss: 0.03133852034807205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24704241752624512 0.0492556095123291

Final encoder loss: 0.031532227993011475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464890480041504 0.04884934425354004

Final encoder loss: 0.03127294033765793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472071647644043 0.04872846603393555

Final encoder loss: 0.03131934255361557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2453289031982422 0.04840230941772461

Final encoder loss: 0.031353943049907684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24766159057617188 0.04885101318359375

Final encoder loss: 0.03120614029467106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24626398086547852 0.04880499839782715

Final encoder loss: 0.031250569969415665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24663615226745605 0.048421382904052734

Final encoder loss: 0.031314361840486526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24654221534729004 0.04932665824890137

Final encoder loss: 0.030791955068707466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2462785243988037 0.04850196838378906

Final encoder loss: 0.031204828992486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24638843536376953 0.04858732223510742

Final encoder loss: 0.03065403364598751
Final encoder loss: 0.030017532408237457

Training empatch model
Final encoder loss: 0.05198022518882405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07095813751220703 0.1731889247894287

Final encoder loss: 0.04803500506636683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07087945938110352 0.17272710800170898

Final encoder loss: 0.04494552260131188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07103443145751953 0.1729273796081543

Final encoder loss: 0.044137451137390454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07079386711120605 0.17278838157653809

Final encoder loss: 0.04571295860264475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07112288475036621 0.17303729057312012

Final encoder loss: 0.03960423906584871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07062721252441406 0.17273306846618652

Final encoder loss: 0.041879574268197516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07068133354187012 0.172987699508667

Final encoder loss: 0.04158679389100645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07033300399780273 0.17257094383239746

Final encoder loss: 0.03168363851066437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07111883163452148 0.17304158210754395

Final encoder loss: 0.03444983463901962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0709989070892334 0.17304039001464844

Final encoder loss: 0.029591536083819286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07104969024658203 0.17285966873168945

Final encoder loss: 0.03210048555323649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07097887992858887 0.17288923263549805

Final encoder loss: 0.032163660845415505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0709986686706543 0.17247486114501953

Final encoder loss: 0.030113782026102067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07065916061401367 0.17260503768920898

Final encoder loss: 0.03357611443077312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07107782363891602 0.17252421379089355

Final encoder loss: 0.030554563183138792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07058596611022949 0.17279624938964844


Training empatch model
Final encoder loss: 0.17115767300128937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17665314674377441 0.043643951416015625

Final encoder loss: 0.07979898899793625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17509222030639648 0.043663740158081055

Final encoder loss: 0.05628481134772301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17522072792053223 0.043137550354003906

Final encoder loss: 0.0451524518430233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17479681968688965 0.04337716102600098

Final encoder loss: 0.03876990079879761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17490792274475098 0.04374980926513672

Final encoder loss: 0.03480932489037514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17517328262329102 0.04346752166748047

Final encoder loss: 0.03226899355649948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17494654655456543 0.043259382247924805

Final encoder loss: 0.030715670436620712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748945713043213 0.0429685115814209

Final encoder loss: 0.02973959967494011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17501378059387207 0.04361438751220703

Final encoder loss: 0.029140925034880638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17480015754699707 0.042986392974853516

Final encoder loss: 0.028677741065621376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17402887344360352 0.044165611267089844

Final encoder loss: 0.02834874577820301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752316951751709 0.04311084747314453

Final encoder loss: 0.0281937625259161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17483878135681152 0.04333996772766113

Final encoder loss: 0.028095005080103874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17528939247131348 0.04329323768615723

Final encoder loss: 0.028044257313013077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17441463470458984 0.04361987113952637

Final encoder loss: 0.02783520333468914

Training wesad model
Final encoder loss: 0.05122586619797783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07084536552429199 0.17302989959716797

Final encoder loss: 0.05035627942930508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07070016860961914 0.17286396026611328

Final encoder loss: 0.048504690803590815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07053446769714355 0.17327499389648438

Final encoder loss: 0.04384731942009299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07071185111999512 0.17297124862670898

Final encoder loss: 0.03319100439493652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07059931755065918 0.17275190353393555

Final encoder loss: 0.03083196855204472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07015347480773926 0.17304372787475586

Final encoder loss: 0.034226421623537104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07118105888366699 0.17282605171203613

Final encoder loss: 0.034444847689163366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07104134559631348 0.17299866676330566

Final encoder loss: 0.025267374766019684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07064294815063477 0.17300033569335938

Final encoder loss: 0.026727477180942767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07079267501831055 0.17278003692626953

Final encoder loss: 0.026818638823034948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07055187225341797 0.17315387725830078

Final encoder loss: 0.02697609601965046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07077765464782715 0.1728816032409668

Final encoder loss: 0.021378013067194106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07077884674072266 0.17259883880615234

Final encoder loss: 0.022347173482704052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07025527954101562 0.1728987693786621

Final encoder loss: 0.02163427716773591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710446834564209 0.17279934883117676

Final encoder loss: 0.023496985750593686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07050395011901855 0.17342138290405273


Training wesad model
Final encoder loss: 0.21561922132968903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10411524772644043 0.033899545669555664

Final encoder loss: 0.09637105464935303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10463285446166992 0.03368782997131348

Final encoder loss: 0.06338884681463242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10332465171813965 0.03261113166809082

Final encoder loss: 0.04732047766447067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10368537902832031 0.03310418128967285

Final encoder loss: 0.03823745623230934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10406303405761719 0.03319406509399414

Final encoder loss: 0.032794613391160965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10342621803283691 0.03319144248962402

Final encoder loss: 0.029393894597887993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10371279716491699 0.03267717361450195

Final encoder loss: 0.02711830474436283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037604808807373 0.03321433067321777

Final encoder loss: 0.02565399743616581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10359001159667969 0.03326988220214844

Final encoder loss: 0.024788470938801765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356521606445312 0.032950639724731445

Final encoder loss: 0.02438664622604847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1032857894897461 0.03255271911621094

Final encoder loss: 0.024262476712465286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038360595703125 0.03319430351257324

Final encoder loss: 0.024283992126584053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10345339775085449 0.033032894134521484

Final encoder loss: 0.024502966552972794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393571853637695 0.03289175033569336

Final encoder loss: 0.02480802871286869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10349845886230469 0.03325462341308594

Final encoder loss: 0.025033412501215935

Calculating loss for amigos model
	Full Pass 0.6817283630371094
numFreeParamsPath 18
Reconstruction loss values: 0.03499080240726471 0.04537137225270271

Calculating loss for dapper model
	Full Pass 0.15060901641845703
numFreeParamsPath 18
Reconstruction loss values: 0.028156377375125885 0.032623566687107086

Calculating loss for case model
	Full Pass 0.8606488704681396
numFreeParamsPath 18
Reconstruction loss values: 0.041530970484018326 0.04455627128481865

Calculating loss for emognition model
	Full Pass 0.29079747200012207
numFreeParamsPath 18
Reconstruction loss values: 0.044336624443531036 0.05259575694799423

Calculating loss for empatch model
	Full Pass 0.10465645790100098
numFreeParamsPath 18
Reconstruction loss values: 0.04792829975485802 0.054626137018203735

Calculating loss for wesad model
	Full Pass 0.07698822021484375
numFreeParamsPath 18
Reconstruction loss values: 0.050106827169656754 0.072871133685112
Total loss calculation time: 3.8692626953125

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.611732721328735
Total epoch time: 163.67742729187012

Epoch: 32

Training emognition model
Final encoder loss: 0.042496586065137766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08988356590270996 0.2836170196533203

Final encoder loss: 0.04003928767424456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08475422859191895 0.2765817642211914

Final encoder loss: 0.04382696448260638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08404040336608887 0.275587797164917

Final encoder loss: 0.042226497265569096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08352422714233398 0.27608156204223633

Final encoder loss: 0.040933992074468455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08438324928283691 0.2744455337524414

Final encoder loss: 0.04059992057895457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08382654190063477 0.2755725383758545

Final encoder loss: 0.04262006672251703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08493661880493164 0.276517391204834

Final encoder loss: 0.039941162947654194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08365702629089355 0.27602529525756836

Final encoder loss: 0.03955924228414516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08356428146362305 0.2763376235961914

Final encoder loss: 0.04073926096955123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08305001258850098 0.27454137802124023

Final encoder loss: 0.040226603742259656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08353543281555176 0.2760944366455078

Final encoder loss: 0.03979533277432674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08534383773803711 0.27683472633361816

Final encoder loss: 0.04008545083424819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08388447761535645 0.2756001949310303

Final encoder loss: 0.03819734370582616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08429527282714844 0.2767629623413086

Final encoder loss: 0.039439162539735034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08376216888427734 0.27576684951782227

Final encoder loss: 0.039630900156733484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08274149894714355 0.27480292320251465


Training dapper model
Final encoder loss: 0.02727194572139964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.0639340877532959 0.15006327629089355

Final encoder loss: 0.026964562215087542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.062310218811035156 0.15088605880737305

Final encoder loss: 0.025324152714335673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06223893165588379 0.1511518955230713

Final encoder loss: 0.02783274649737816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.48026227951049805 0.1514759063720703

Final encoder loss: 0.02665132288132815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.062374114990234375 0.1507871150970459

Final encoder loss: 0.025270890764891817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062380313873291016 0.15195178985595703

Final encoder loss: 0.02603651635080491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06405234336853027 0.15244555473327637

Final encoder loss: 0.02573020824653819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06301474571228027 0.15159845352172852

Final encoder loss: 0.026906820711934527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06261730194091797 0.15086603164672852

Final encoder loss: 0.0244087134992494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06249260902404785 0.15204524993896484

Final encoder loss: 0.02484973571212355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06350374221801758 0.15151023864746094

Final encoder loss: 0.025565448591658477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06282520294189453 0.15152764320373535

Final encoder loss: 0.023380575924182915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06255292892456055 0.1511385440826416

Final encoder loss: 0.021146572931366497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06245017051696777 0.15248918533325195

Final encoder loss: 0.026800338880284238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06412696838378906 0.15151667594909668

Final encoder loss: 0.028153988713569004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06223106384277344 0.150160551071167


Training case model
Final encoder loss: 0.04142266947689211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09147047996520996 0.26503872871398926

Final encoder loss: 0.03916304912126878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09294748306274414 0.26641416549682617

Final encoder loss: 0.03800838700428168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09216523170471191 0.2651989459991455

Final encoder loss: 0.03703260520579732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.0927276611328125 0.2666497230529785

Final encoder loss: 0.035685125685794024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09134078025817871 0.2653772830963135

Final encoder loss: 0.03503144637092992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09123039245605469 0.26558995246887207

Final encoder loss: 0.0347921734818307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0921330451965332 0.2649412155151367

Final encoder loss: 0.034178975601569954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09142470359802246 0.26546192169189453

Final encoder loss: 0.03409560425608941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09264016151428223 0.2666053771972656

Final encoder loss: 0.03277959122167207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.0916600227355957 0.26525402069091797

Final encoder loss: 0.032787552881518876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09130120277404785 0.2668302059173584

Final encoder loss: 0.03186051438060156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09148716926574707 0.26577019691467285

Final encoder loss: 0.03200091707161252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09128952026367188 0.2655062675476074

Final encoder loss: 0.031952210300575885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09329700469970703 0.26567602157592773

Final encoder loss: 0.031488216834650876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09178638458251953 0.2652313709259033

Final encoder loss: 0.03176941026672408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.09059619903564453 0.2630307674407959


Training amigos model
Final encoder loss: 0.03660662163854422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10852980613708496 0.3893859386444092

Final encoder loss: 0.03471565165887204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.11024832725524902 0.3908803462982178

Final encoder loss: 0.03657193036769098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10881829261779785 0.3898751735687256

Final encoder loss: 0.03293486156775243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.11054158210754395 0.39265871047973633

Final encoder loss: 0.034072954505414574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10831284523010254 0.38971853256225586

Final encoder loss: 0.03409841055319944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10810232162475586 0.38913846015930176

Final encoder loss: 0.03259657601463004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.11183381080627441 0.39150404930114746

Final encoder loss: 0.03310002850367579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10828542709350586 0.3897991180419922

Final encoder loss: 0.03248172541249275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10995006561279297 0.39013242721557617

Final encoder loss: 0.03133028632021694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10841941833496094 0.39014768600463867

Final encoder loss: 0.035724547269902776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10914754867553711 0.38898181915283203

Final encoder loss: 0.03193135945022126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.108184814453125 0.38961195945739746

Final encoder loss: 0.03247304846977158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10844779014587402 0.38950514793395996

Final encoder loss: 0.03239338772981059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10897016525268555 0.3924248218536377

Final encoder loss: 0.03415628478590066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10935163497924805 0.389523983001709

Final encoder loss: 0.0324085865499966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10498666763305664 0.38512444496154785


Training amigos model
Final encoder loss: 0.025403882666620196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10615992546081543 0.341846227645874

Final encoder loss: 0.02211917045279324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10825967788696289 0.3415050506591797

Final encoder loss: 0.02531139014428268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10594701766967773 0.3417658805847168

Final encoder loss: 0.02360258377011878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10719418525695801 0.34159135818481445

Final encoder loss: 0.024088334872599475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10613656044006348 0.3420090675354004

Final encoder loss: 0.025879927070509106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10715460777282715 0.34142374992370605

Final encoder loss: 0.023238713294910927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10645294189453125 0.34213805198669434

Final encoder loss: 0.023035591656580413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1062171459197998 0.3414750099182129

Final encoder loss: 0.024032319353930458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10608863830566406 0.3416731357574463

Final encoder loss: 0.025227608550730488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10575389862060547 0.34165501594543457

Final encoder loss: 0.02566198754765623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10596156120300293 0.3415851593017578

Final encoder loss: 0.026986307321597895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10631465911865234 0.34139227867126465

Final encoder loss: 0.024525932674472926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10634398460388184 0.3415992259979248

Final encoder loss: 0.023537106350085255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10612010955810547 0.3413856029510498

Final encoder loss: 0.025796016433093164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1057431697845459 0.3418312072753906

Final encoder loss: 0.02440990695855308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10115551948547363 0.33838605880737305


Training amigos model
Final encoder loss: 0.1807602494955063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45819568634033203 0.07639908790588379

Final encoder loss: 0.1878231167793274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47669482231140137 0.07703995704650879

Final encoder loss: 0.18363137543201447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45812010765075684 0.07639527320861816

Final encoder loss: 0.07360530644655228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4618532657623291 0.07564830780029297

Final encoder loss: 0.07496334612369537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45217156410217285 0.07419824600219727

Final encoder loss: 0.06916001439094543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4311530590057373 0.07393431663513184

Final encoder loss: 0.04387691244482994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44842100143432617 0.07652044296264648

Final encoder loss: 0.044172052294015884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4585423469543457 0.07215380668640137

Final encoder loss: 0.04190005734562874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44387221336364746 0.07520580291748047

Final encoder loss: 0.032751817256212234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45323705673217773 0.07200431823730469

Final encoder loss: 0.033209335058927536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.452897310256958 0.07567429542541504

Final encoder loss: 0.03225211426615715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4483942985534668 0.07881331443786621

Final encoder loss: 0.027903784066438675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.454420804977417 0.07537269592285156

Final encoder loss: 0.028490286320447922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4692525863647461 0.07894468307495117

Final encoder loss: 0.02793736942112446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4507102966308594 0.07518172264099121

Final encoder loss: 0.02610737830400467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45450544357299805 0.07762718200683594

Final encoder loss: 0.02666388265788555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4688692092895508 0.07569289207458496

Final encoder loss: 0.02634315937757492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44898080825805664 0.07524371147155762

Final encoder loss: 0.02621336467564106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4539186954498291 0.07505559921264648

Final encoder loss: 0.026186157017946243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46890974044799805 0.0748751163482666

Final encoder loss: 0.026093710213899612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4339604377746582 0.0772089958190918

Final encoder loss: 0.02626083604991436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45309901237487793 0.07361936569213867

Final encoder loss: 0.02605293318629265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45955514907836914 0.07622051239013672

Final encoder loss: 0.026388462632894516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44414734840393066 0.07426977157592773

Final encoder loss: 0.025722438469529152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46132802963256836 0.07632040977478027

Final encoder loss: 0.025696996599435806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44794631004333496 0.07445549964904785

Final encoder loss: 0.025838816538453102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44349217414855957 0.07477998733520508

Final encoder loss: 0.025163762271404266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4556858539581299 0.08013916015625

Final encoder loss: 0.02515360340476036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47157859802246094 0.07391738891601562

Final encoder loss: 0.025464951992034912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4548017978668213 0.07404613494873047

Final encoder loss: 0.024435319006443024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45589780807495117 0.07640242576599121

Final encoder loss: 0.024580417200922966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46909022331237793 0.08053708076477051

Final encoder loss: 0.024973636493086815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45481443405151367 0.07310199737548828

Final encoder loss: 0.02444026991724968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45825934410095215 0.07568812370300293

Final encoder loss: 0.024330701678991318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4537038803100586 0.0750436782836914

Final encoder loss: 0.024720577523112297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4530353546142578 0.08099532127380371

Final encoder loss: 0.024607691913843155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4553196430206299 0.0745995044708252

Final encoder loss: 0.02398337610065937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4582388401031494 0.07598185539245605

Final encoder loss: 0.024412069469690323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.452251672744751 0.07406830787658691

Final encoder loss: 0.02416180446743965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4558556079864502 0.08074474334716797

Final encoder loss: 0.023779509589076042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4541342258453369 0.07390046119689941

Final encoder loss: 0.024387897923588753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4576590061187744 0.07503390312194824

Final encoder loss: 0.023794714361429214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45451807975769043 0.07461881637573242

Final encoder loss: 0.023458758369088173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46944403648376465 0.07849478721618652

Final encoder loss: 0.023979373276233673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45378541946411133 0.07388043403625488

Final encoder loss: 0.02356111817061901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4742732048034668 0.07797384262084961

Final encoder loss: 0.023379629477858543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45646190643310547 0.07544755935668945

Final encoder loss: 0.023742837831377983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4517197608947754 0.08067607879638672

Final encoder loss: 0.02363043837249279
Final encoder loss: 0.022280514240264893
Final encoder loss: 0.021666357293725014

Training dapper model
Final encoder loss: 0.021305575464413074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06054377555847168 0.10725927352905273

Final encoder loss: 0.019337237135251636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059943437576293945 0.10709738731384277

Final encoder loss: 0.02101318918246724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06021928787231445 0.10776734352111816

Final encoder loss: 0.0218054392672299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06078505516052246 0.10776758193969727

Final encoder loss: 0.019585380770942358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05975341796875 0.10783648490905762

Final encoder loss: 0.020490084299695527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05974745750427246 0.10668635368347168

Final encoder loss: 0.020581111456273758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.060178518295288086 0.10780000686645508

Final encoder loss: 0.01975788838569568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059595584869384766 0.10739994049072266

Final encoder loss: 0.019799716201889184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.060331106185913086 0.10749626159667969

Final encoder loss: 0.0207457611462934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.060048818588256836 0.10802316665649414

Final encoder loss: 0.018289197369890115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060949087142944336 0.1070399284362793

Final encoder loss: 0.018257643895776894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060189008712768555 0.10755681991577148

Final encoder loss: 0.017816605316413745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05980229377746582 0.10725021362304688

Final encoder loss: 0.018069700190974796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.060436248779296875 0.10816717147827148

Final encoder loss: 0.01921783537757262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0598139762878418 0.10753273963928223

Final encoder loss: 0.017417539985675745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05951571464538574 0.10700297355651855


Training dapper model
Final encoder loss: 0.2024519443511963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11510324478149414 0.03504371643066406

Final encoder loss: 0.20821942389011383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11709928512573242 0.033548593521118164

Final encoder loss: 0.0796108990907669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11246204376220703 0.03446340560913086

Final encoder loss: 0.08125410974025726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11623072624206543 0.0354156494140625

Final encoder loss: 0.04663769155740738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11428070068359375 0.03423595428466797

Final encoder loss: 0.046527035534381866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11624789237976074 0.034425973892211914

Final encoder loss: 0.032610151916742325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11388158798217773 0.03421759605407715

Final encoder loss: 0.0323118157684803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11798214912414551 0.03484368324279785

Final encoder loss: 0.02565007284283638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11271023750305176 0.034670352935791016

Final encoder loss: 0.025491822510957718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11650609970092773 0.034394264221191406

Final encoder loss: 0.022019362077116966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11356639862060547 0.03461956977844238

Final encoder loss: 0.021940680220723152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11637568473815918 0.034134626388549805

Final encoder loss: 0.020157793536782265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11292362213134766 0.03444409370422363

Final encoder loss: 0.020076077431440353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557483673095703 0.0347440242767334

Final encoder loss: 0.019224418327212334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11418724060058594 0.03407120704650879

Final encoder loss: 0.018954847007989883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11632609367370605 0.03434324264526367

Final encoder loss: 0.018690800294280052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11272954940795898 0.03453183174133301

Final encoder loss: 0.01856975071132183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11803555488586426 0.034589290618896484

Final encoder loss: 0.01845306158065796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11320900917053223 0.034964799880981445

Final encoder loss: 0.018542645499110222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11592864990234375 0.03408479690551758

Final encoder loss: 0.018341412767767906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11370730400085449 0.03477072715759277

Final encoder loss: 0.01861109770834446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11705493927001953 0.03367114067077637

Final encoder loss: 0.01835625432431698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11272644996643066 0.03414106369018555

Final encoder loss: 0.018602818250656128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11552119255065918 0.03534746170043945

Final encoder loss: 0.018685059621930122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11482095718383789 0.03377485275268555

Final encoder loss: 0.01863234117627144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563944816589355 0.03408551216125488

Final encoder loss: 0.018264930695295334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1121978759765625 0.03403282165527344

Final encoder loss: 0.018361961469054222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11798787117004395 0.033972740173339844

Final encoder loss: 0.018405582755804062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11323785781860352 0.03494071960449219

Final encoder loss: 0.017999345436692238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11564254760742188 0.03399181365966797

Final encoder loss: 0.017446689307689667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11397957801818848 0.034921884536743164

Final encoder loss: 0.017686540260910988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11617183685302734 0.03438425064086914

Final encoder loss: 0.01729401759803295
Final encoder loss: 0.01637323573231697

Training case model
Final encoder loss: 0.029407125772251456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.09048223495483398 0.21919655799865723

Final encoder loss: 0.028720265517927668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08915996551513672 0.21901392936706543

Final encoder loss: 0.028561729183858853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09074997901916504 0.21946930885314941

Final encoder loss: 0.028429336030167264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08974361419677734 0.2193152904510498

Final encoder loss: 0.02881626782716205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.09030008316040039 0.21885085105895996

Final encoder loss: 0.027789877153487737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08906316757202148 0.21950364112854004

Final encoder loss: 0.027544604758766387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08939218521118164 0.21924877166748047

Final encoder loss: 0.028154048439491045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08932924270629883 0.21930599212646484

Final encoder loss: 0.028422632966892465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08910942077636719 0.21915245056152344

Final encoder loss: 0.028477270110102714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08931088447570801 0.21941161155700684

Final encoder loss: 0.02846156989500092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.0898277759552002 0.21915555000305176

Final encoder loss: 0.028168500410658402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09050226211547852 0.21912264823913574

Final encoder loss: 0.028718037770878002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08959555625915527 0.21893095970153809

Final encoder loss: 0.02823513691900723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09097027778625488 0.21925997734069824

Final encoder loss: 0.02773891290508178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08924651145935059 0.21953296661376953

Final encoder loss: 0.02841640170784751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08801770210266113 0.21614551544189453


Training case model
Final encoder loss: 0.20298141241073608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2633662223815918 0.05145621299743652

Final encoder loss: 0.18889565765857697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582883834838867 0.05336737632751465

Final encoder loss: 0.1901385635137558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25784945487976074 0.05153608322143555

Final encoder loss: 0.19219423830509186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27611875534057617 0.05286860466003418

Final encoder loss: 0.18080493807792664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576625347137451 0.055017709732055664

Final encoder loss: 0.19192147254943848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25504565238952637 0.05237150192260742

Final encoder loss: 0.10104469209909439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2589080333709717 0.05128598213195801

Final encoder loss: 0.09134873002767563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2604057788848877 0.05229377746582031

Final encoder loss: 0.08765796571969986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26621389389038086 0.05334043502807617

Final encoder loss: 0.08642309904098511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2673771381378174 0.05416107177734375

Final encoder loss: 0.07869740575551987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27430295944213867 0.05301308631896973

Final encoder loss: 0.08174986392259598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25708532333374023 0.052300453186035156

Final encoder loss: 0.059573717415332794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25756144523620605 0.054390668869018555

Final encoder loss: 0.05457139015197754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2586400508880615 0.05094599723815918

Final encoder loss: 0.05244438350200653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25824618339538574 0.05276370048522949

Final encoder loss: 0.052900511771440506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26915526390075684 0.05582284927368164

Final encoder loss: 0.04963883012533188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28489041328430176 0.05345940589904785

Final encoder loss: 0.05130380764603615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552034854888916 0.05226469039916992

Final encoder loss: 0.043396878987550735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26003098487854004 0.05313277244567871

Final encoder loss: 0.04077170044183731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2589426040649414 0.05227780342102051

Final encoder loss: 0.03925910219550133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25809359550476074 0.05329775810241699

Final encoder loss: 0.04025363549590111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2595493793487549 0.05309915542602539

Final encoder loss: 0.038777634501457214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2603328227996826 0.05153322219848633

Final encoder loss: 0.039648186415433884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550983428955078 0.05235719680786133

Final encoder loss: 0.03706048056483269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25783729553222656 0.05349373817443848

Final encoder loss: 0.0358622781932354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2671487331390381 0.05243325233459473

Final encoder loss: 0.034651704132556915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25984811782836914 0.05225515365600586

Final encoder loss: 0.03579593449831009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2676386833190918 0.053893089294433594

Final encoder loss: 0.03560890257358551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2764246463775635 0.05270028114318848

Final encoder loss: 0.03553905338048935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25613856315612793 0.05165863037109375

Final encoder loss: 0.03489624336361885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25868654251098633 0.0534818172454834

Final encoder loss: 0.034298885613679886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2673940658569336 0.05258321762084961

Final encoder loss: 0.03362904116511345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2583587169647217 0.05323505401611328

Final encoder loss: 0.03428106755018234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2604377269744873 0.051621437072753906

Final encoder loss: 0.03504874184727669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25690460205078125 0.05440163612365723

Final encoder loss: 0.0340324342250824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25544309616088867 0.05276370048522949

Final encoder loss: 0.03295676410198212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580533027648926 0.051868438720703125

Final encoder loss: 0.0325162447988987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2593526840209961 0.052169084548950195

Final encoder loss: 0.03224476799368858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2744779586791992 0.053345441818237305

Final encoder loss: 0.03245672211050987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26813554763793945 0.05341148376464844

Final encoder loss: 0.03296283632516861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25755953788757324 0.05276942253112793

Final encoder loss: 0.03244100138545036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557563781738281 0.05403542518615723

Final encoder loss: 0.03179321810603142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25719785690307617 0.052878618240356445

Final encoder loss: 0.03114539198577404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2732996940612793 0.052594900131225586

Final encoder loss: 0.03090343065559864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25920534133911133 0.051940202713012695

Final encoder loss: 0.030978195369243622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26781249046325684 0.05203819274902344

Final encoder loss: 0.0317571721971035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2588038444519043 0.05339241027832031

Final encoder loss: 0.031231703236699104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564094066619873 0.051753997802734375

Final encoder loss: 0.03097938559949398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25868797302246094 0.05333852767944336

Final encoder loss: 0.030593564733862877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588765621185303 0.052179574966430664

Final encoder loss: 0.030081428587436676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26993441581726074 0.05273914337158203

Final encoder loss: 0.030361443758010864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.265045166015625 0.05159878730773926

Final encoder loss: 0.031168436631560326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2673685550689697 0.0528864860534668

Final encoder loss: 0.030435791239142418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25457000732421875 0.05165505409240723

Final encoder loss: 0.0303619597107172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25490713119506836 0.05211949348449707

Final encoder loss: 0.030098648741841316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27246880531311035 0.051367759704589844

Final encoder loss: 0.02962612360715866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25711631774902344 0.05397963523864746

Final encoder loss: 0.02999022975564003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2563450336456299 0.052599430084228516

Final encoder loss: 0.030706889927387238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256150484085083 0.05212903022766113

Final encoder loss: 0.029881753027439117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25488901138305664 0.05244159698486328

Final encoder loss: 0.029840592294931412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.268643856048584 0.05164074897766113

Final encoder loss: 0.02968505024909973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258206844329834 0.052294254302978516

Final encoder loss: 0.02923455275595188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2597835063934326 0.052782297134399414

Final encoder loss: 0.029358554631471634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25662922859191895 0.0536649227142334

Final encoder loss: 0.030092811211943626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576611042022705 0.052626848220825195

Final encoder loss: 0.02951841428875923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25416016578674316 0.05261397361755371

Final encoder loss: 0.02957390435039997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25870585441589355 0.05249357223510742

Final encoder loss: 0.029198193922638893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690117359161377 0.05484604835510254

Final encoder loss: 0.02888559363782406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27404165267944336 0.05374288558959961

Final encoder loss: 0.029005499556660652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580256462097168 0.05277752876281738

Final encoder loss: 0.02983449585735798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578771114349365 0.05310225486755371

Final encoder loss: 0.029159875586628914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560923099517822 0.05120110511779785

Final encoder loss: 0.02924676612019539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25776076316833496 0.05291461944580078

Final encoder loss: 0.029053034260869026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2580714225769043 0.0537571907043457

Final encoder loss: 0.028602320700883865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25856471061706543 0.05313301086425781

Final encoder loss: 0.028725167736411095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2595176696777344 0.051129817962646484

Final encoder loss: 0.029329633340239525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2573111057281494 0.05304217338562012

Final encoder loss: 0.028950801119208336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25582170486450195 0.05208945274353027

Final encoder loss: 0.029019447043538094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2670142650604248 0.051690101623535156

Final encoder loss: 0.028797674924135208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25924134254455566 0.05182695388793945

Final encoder loss: 0.028407270088791847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2677440643310547 0.05373191833496094

Final encoder loss: 0.028595536947250366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26736998558044434 0.05296015739440918

Final encoder loss: 0.02926773391664028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26688289642333984 0.052786827087402344

Final encoder loss: 0.02854737639427185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556328773498535 0.05210614204406738

Final encoder loss: 0.028811853379011154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2590939998626709 0.0516507625579834

Final encoder loss: 0.02856937050819397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2675745487213135 0.05533599853515625

Final encoder loss: 0.028233371675014496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25750255584716797 0.0522465705871582

Final encoder loss: 0.028230510652065277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2676732540130615 0.05312705039978027

Final encoder loss: 0.029014328494668007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2592594623565674 0.05242562294006348

Final encoder loss: 0.02844521403312683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25483012199401855 0.05296945571899414

Final encoder loss: 0.028677940368652344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25712108612060547 0.05271339416503906

Final encoder loss: 0.028390973806381226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257765531539917 0.051972389221191406

Final encoder loss: 0.027888508513569832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2686290740966797 0.05203604698181152

Final encoder loss: 0.02809670753777027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2755599021911621 0.05205559730529785

Final encoder loss: 0.028810016810894012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578892707824707 0.05564737319946289

Final encoder loss: 0.02831944078207016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550349235534668 0.05100440979003906

Final encoder loss: 0.02841936983168125
Final encoder loss: 0.027757935225963593
Final encoder loss: 0.026614205911755562
Final encoder loss: 0.025822265073657036
Final encoder loss: 0.025519054383039474
Final encoder loss: 0.02408759668469429

Training emognition model
Final encoder loss: 0.0340616232781255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08132195472717285 0.22960686683654785

Final encoder loss: 0.034355388479614654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08054232597351074 0.2290358543395996

Final encoder loss: 0.03138820694538339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08064818382263184 0.2297220230102539

Final encoder loss: 0.031352447049990476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08061885833740234 0.2298583984375

Final encoder loss: 0.032112826641541664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08082795143127441 0.22945094108581543

Final encoder loss: 0.03294279621497422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08054304122924805 0.22978734970092773

Final encoder loss: 0.032550668362355616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08061814308166504 0.22967147827148438

Final encoder loss: 0.030895096793484216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08048820495605469 0.23002958297729492

Final encoder loss: 0.033438776416213886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.0811164379119873 0.23095369338989258

Final encoder loss: 0.03213023075542222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08114790916442871 0.23076939582824707

Final encoder loss: 0.03211274577650934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08138036727905273 0.23099160194396973

Final encoder loss: 0.032088183539497786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08165121078491211 0.23086285591125488

Final encoder loss: 0.03309376026161783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08213949203491211 0.23110508918762207

Final encoder loss: 0.032028613380687916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08102202415466309 0.23081564903259277

Final encoder loss: 0.03296542698558885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08088111877441406 0.23046398162841797

Final encoder loss: 0.03423722036457819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08070564270019531 0.23017239570617676


Training emognition model
Final encoder loss: 0.19356292486190796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25090861320495605 0.05032825469970703

Final encoder loss: 0.1949623078107834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24724054336547852 0.0493924617767334

Final encoder loss: 0.08638722449541092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24727392196655273 0.05120205879211426

Final encoder loss: 0.08596163243055344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470569610595703 0.05015063285827637

Final encoder loss: 0.05570404231548309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485060691833496 0.04932093620300293

Final encoder loss: 0.054275769740343094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470541000366211 0.05054664611816406

Final encoder loss: 0.04276517406105995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24719452857971191 0.049813032150268555

Final encoder loss: 0.04181753844022751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471156120300293 0.05012702941894531

Final encoder loss: 0.0365329273045063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24694180488586426 0.04862046241760254

Final encoder loss: 0.03596130758523941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2451612949371338 0.04900360107421875

Final encoder loss: 0.03334265202283859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24776506423950195 0.04758000373840332

Final encoder loss: 0.032958194613456726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24602031707763672 0.049040794372558594

Final encoder loss: 0.03176719322800636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24646687507629395 0.0479433536529541

Final encoder loss: 0.03149475157260895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24604511260986328 0.04813241958618164

Final encoder loss: 0.031125973910093307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24701857566833496 0.04769754409790039

Final encoder loss: 0.03104734793305397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24566340446472168 0.048458099365234375

Final encoder loss: 0.030928632244467735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24659466743469238 0.04845142364501953

Final encoder loss: 0.031080665066838264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2452259063720703 0.04825711250305176

Final encoder loss: 0.03079902194440365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478961944580078 0.04918622970581055

Final encoder loss: 0.031369708478450775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24707627296447754 0.04998350143432617

Final encoder loss: 0.03083786368370056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24854660034179688 0.048955440521240234

Final encoder loss: 0.03113299421966076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2481098175048828 0.04971575736999512

Final encoder loss: 0.030724378302693367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24823212623596191 0.04925966262817383

Final encoder loss: 0.03106502629816532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2480626106262207 0.050112009048461914

Final encoder loss: 0.030632534995675087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494199275970459 0.04898571968078613

Final encoder loss: 0.030947037041187286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24840807914733887 0.04971051216125488

Final encoder loss: 0.030519485473632812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24805831909179688 0.04883718490600586

Final encoder loss: 0.0308826956897974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24904251098632812 0.05009341239929199

Final encoder loss: 0.0304779801517725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24839019775390625 0.04758501052856445

Final encoder loss: 0.03073785826563835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.246171236038208 0.050522804260253906

Final encoder loss: 0.030331388115882874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2518947124481201 0.05055403709411621

Final encoder loss: 0.030630216002464294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472689151763916 0.050315141677856445

Final encoder loss: 0.030265722423791885
Final encoder loss: 0.02927526831626892

Training empatch model
Final encoder loss: 0.045817548889554124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07065248489379883 0.17356586456298828

Final encoder loss: 0.04835916250611918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07109451293945312 0.17293930053710938

Final encoder loss: 0.043510719868220075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07305741310119629 0.17447853088378906

Final encoder loss: 0.044632721288856765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0720818042755127 0.17426776885986328

Final encoder loss: 0.04529135682385558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07251405715942383 0.17450451850891113

Final encoder loss: 0.040373922978244645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07100629806518555 0.1741020679473877

Final encoder loss: 0.0428602712592736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0713052749633789 0.17399144172668457

Final encoder loss: 0.04008900276127773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07200479507446289 0.17395305633544922

Final encoder loss: 0.027912272174234042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07056665420532227 0.17341279983520508

Final encoder loss: 0.032693164075049706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07063150405883789 0.17391705513000488

Final encoder loss: 0.030864099515931744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07164192199707031 0.1730339527130127

Final encoder loss: 0.029993929051535737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07081484794616699 0.17326807975769043

Final encoder loss: 0.029195389072280475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07104039192199707 0.17328834533691406

Final encoder loss: 0.03206203408190997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07223987579345703 0.17354536056518555

Final encoder loss: 0.03141965774029568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07045459747314453 0.1736299991607666

Final encoder loss: 0.03289219986093551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07066512107849121 0.17244815826416016


Training empatch model
Final encoder loss: 0.1711578667163849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17124581336975098 0.044091224670410156

Final encoder loss: 0.07984790951013565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1740269660949707 0.04412031173706055

Final encoder loss: 0.05615660920739174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17006325721740723 0.04293489456176758

Final encoder loss: 0.04491604119539261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17450523376464844 0.04400825500488281

Final encoder loss: 0.03856688737869263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17009830474853516 0.04370832443237305

Final encoder loss: 0.03464590385556221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17386722564697266 0.043058156967163086

Final encoder loss: 0.03210868313908577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16911888122558594 0.04362034797668457

Final encoder loss: 0.030427727848291397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744537353515625 0.043556928634643555

Final encoder loss: 0.029413368552923203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16982460021972656 0.04441118240356445

Final encoder loss: 0.02878425642848015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17572569847106934 0.04417777061462402

Final encoder loss: 0.028338763862848282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17050981521606445 0.043950557708740234

Final encoder loss: 0.028074992820620537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17516732215881348 0.04402494430541992

Final encoder loss: 0.02786361053586006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16963458061218262 0.04362082481384277

Final encoder loss: 0.02769879624247551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17451167106628418 0.04468369483947754

Final encoder loss: 0.027501365169882774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17005205154418945 0.04353642463684082

Final encoder loss: 0.02740194834768772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17458009719848633 0.043495893478393555

Final encoder loss: 0.02735994942486286

Training wesad model
Final encoder loss: 0.05071330608704546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07092452049255371 0.1725451946258545

Final encoder loss: 0.047885457698643906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07052350044250488 0.17240405082702637

Final encoder loss: 0.045314623570008795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0704038143157959 0.1723475456237793

Final encoder loss: 0.04766629552502856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07042741775512695 0.17294859886169434

Final encoder loss: 0.030800804640564377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07075691223144531 0.17278623580932617

Final encoder loss: 0.0334672203521459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0702664852142334 0.1727128028869629

Final encoder loss: 0.03179455032838357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0703897476196289 0.17296743392944336

Final encoder loss: 0.03480071971868888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07078194618225098 0.17280793190002441

Final encoder loss: 0.027124902005375585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07080984115600586 0.17216062545776367

Final encoder loss: 0.02505768534216436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07010173797607422 0.1723332405090332

Final encoder loss: 0.026602767560389227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07043790817260742 0.17282533645629883

Final encoder loss: 0.025831987498012878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07062244415283203 0.17266583442687988

Final encoder loss: 0.019653957430086303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07032918930053711 0.17229461669921875

Final encoder loss: 0.02178369284389695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07028412818908691 0.1728980541229248

Final encoder loss: 0.02210196571040335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07076239585876465 0.17268872261047363

Final encoder loss: 0.02269042772795053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07036757469177246 0.1727280616760254


Training wesad model
Final encoder loss: 0.21561230719089508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1053321361541748 0.032735347747802734

Final encoder loss: 0.09700185060501099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10533022880554199 0.033251285552978516

Final encoder loss: 0.06345229595899582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409379005432129 0.03335070610046387

Final encoder loss: 0.04708172008395195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10419893264770508 0.03303718566894531

Final encoder loss: 0.03789108991622925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10341167449951172 0.03278613090515137

Final encoder loss: 0.03242071717977524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1032114028930664 0.03319191932678223

Final encoder loss: 0.029023202136158943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10392975807189941 0.03315591812133789

Final encoder loss: 0.026819972321391106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10331344604492188 0.03234696388244629

Final encoder loss: 0.025412332266569138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10358929634094238 0.032984018325805664

Final encoder loss: 0.024574702605605125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388350486755371 0.032532453536987305

Final encoder loss: 0.024114932864904404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10382509231567383 0.03273272514343262

Final encoder loss: 0.0239839069545269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10368013381958008 0.03323769569396973

Final encoder loss: 0.02414180524647236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10398697853088379 0.03261590003967285

Final encoder loss: 0.024272501468658447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1036231517791748 0.03236699104309082

Final encoder loss: 0.024437198415398598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10392236709594727 0.03261232376098633

Final encoder loss: 0.02431298978626728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10346269607543945 0.03326559066772461

Final encoder loss: 0.024214284494519234

Calculating loss for amigos model
	Full Pass 0.681978702545166
numFreeParamsPath 18
Reconstruction loss values: 0.03357085958123207 0.044494107365608215

Calculating loss for dapper model
	Full Pass 0.15125584602355957
numFreeParamsPath 18
Reconstruction loss values: 0.027804236859083176 0.03188389539718628

Calculating loss for case model
	Full Pass 0.9049956798553467
numFreeParamsPath 18
Reconstruction loss values: 0.04065153747797012 0.043408844619989395

Calculating loss for emognition model
	Full Pass 0.2905447483062744
numFreeParamsPath 18
Reconstruction loss values: 0.043647728860378265 0.0517115481197834

Calculating loss for empatch model
	Full Pass 0.10433697700500488
numFreeParamsPath 18
Reconstruction loss values: 0.04698890447616577 0.05473923310637474

Calculating loss for wesad model
	Full Pass 0.07686018943786621
numFreeParamsPath 18
Reconstruction loss values: 0.04800426587462425 0.06971580535173416
Total loss calculation time: 3.925421714782715

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.559076309204102
Total epoch time: 169.7124001979828

Epoch: 33

Training dapper model
Final encoder loss: 0.027838922580734816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06535124778747559 0.15642023086547852

Final encoder loss: 0.029384899379897286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061464548110961914 0.14919352531433105

Final encoder loss: 0.025362822586960758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06116437911987305 0.14995217323303223

Final encoder loss: 0.024871000617589505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06182074546813965 0.1501905918121338

Final encoder loss: 0.029096222056489405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06218409538269043 0.14997386932373047

Final encoder loss: 0.02510260260352117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06174778938293457 0.14941143989562988

Final encoder loss: 0.02378377000317359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06126070022583008 0.15008020401000977

Final encoder loss: 0.026536303487707965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06138968467712402 0.14927053451538086

Final encoder loss: 0.023948976620343492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.062071800231933594 0.14902281761169434

Final encoder loss: 0.023495569335842095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06153988838195801 0.1499953269958496

Final encoder loss: 0.024824182308028714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06212162971496582 0.14906764030456543

Final encoder loss: 0.024466532156111224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06113314628601074 0.1499028205871582

Final encoder loss: 0.0237313691386512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062210798263549805 0.14995503425598145

Final encoder loss: 0.023744024968676773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06143450736999512 0.1500253677368164

Final encoder loss: 0.02313051926536851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06160402297973633 0.14946579933166504

Final encoder loss: 0.022275252881527964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.48368287086486816 0.1500706672668457


Training emognition model
Final encoder loss: 0.04632708136838142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08298110961914062 0.27366113662719727

Final encoder loss: 0.04168376084048671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08325815200805664 0.2749056816101074

Final encoder loss: 0.04226539236041181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08352255821228027 0.27425408363342285

Final encoder loss: 0.04134096335051714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08359527587890625 0.2742586135864258

Final encoder loss: 0.04073526259832123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0833120346069336 0.2742934226989746

Final encoder loss: 0.04089459741726674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08314371109008789 0.27454710006713867

Final encoder loss: 0.03995236483632686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0833883285522461 0.2745833396911621

Final encoder loss: 0.03894496854760413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08329558372497559 0.2745535373687744

Final encoder loss: 0.039328195466885006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08355832099914551 0.2745044231414795

Final encoder loss: 0.03909715329011548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08334851264953613 0.27410888671875

Final encoder loss: 0.039347933177787155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08305001258850098 0.27470993995666504

Final encoder loss: 0.03841697911766478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08345174789428711 0.27413034439086914

Final encoder loss: 0.0367780214164463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08329367637634277 0.2732264995574951

Final encoder loss: 0.04020386093352763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08271145820617676 0.2729926109313965

Final encoder loss: 0.04059047212891404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0829622745513916 0.2743957042694092

Final encoder loss: 0.03677751654810703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0827629566192627 0.2736778259277344


Training case model
Final encoder loss: 0.040016970459224956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0913383960723877 0.2641327381134033

Final encoder loss: 0.03684298123594342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09082150459289551 0.26460957527160645

Final encoder loss: 0.036364801395083406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09145402908325195 0.2650125026702881

Final encoder loss: 0.03496660368188453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09120321273803711 0.2650761604309082

Final encoder loss: 0.03412922545744922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09147071838378906 0.2651937007904053

Final encoder loss: 0.03396940154331988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09150481224060059 0.26436328887939453

Final encoder loss: 0.03277121017116023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09091520309448242 0.2647287845611572

Final encoder loss: 0.033497960145378196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09131956100463867 0.2648348808288574

Final encoder loss: 0.03278132786479818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09135770797729492 0.2639150619506836

Final encoder loss: 0.032642752068692625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09118771553039551 0.2640109062194824

Final encoder loss: 0.03161702640822883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09220457077026367 0.27069592475891113

Final encoder loss: 0.0320686513551472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0916893482208252 0.2657737731933594

Final encoder loss: 0.03208309150734706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09193277359008789 0.2652297019958496

Final encoder loss: 0.03144175177944486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09098124504089355 0.2650184631347656

Final encoder loss: 0.029693361398207743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09306669235229492 0.2671811580657959

Final encoder loss: 0.031688012226614964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08938241004943848 0.26226067543029785


Training amigos model
Final encoder loss: 0.03689876478286721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10902142524719238 0.39032602310180664

Final encoder loss: 0.035069910138452946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10872888565063477 0.38998961448669434

Final encoder loss: 0.032078735920510894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10980105400085449 0.391770601272583

Final encoder loss: 0.032293149415502494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10841727256774902 0.3895587921142578

Final encoder loss: 0.032511051197547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10911130905151367 0.3885815143585205

Final encoder loss: 0.035443302301555756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10819339752197266 0.39171457290649414

Final encoder loss: 0.03278748426069029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10922360420227051 0.38984251022338867

Final encoder loss: 0.033548235837723914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10921430587768555 0.39113378524780273

Final encoder loss: 0.03453348113396702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.1087489128112793 0.38887763023376465

Final encoder loss: 0.031082710002758324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10993671417236328 0.391282320022583

Final encoder loss: 0.032267923575076005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1082007884979248 0.38987112045288086

Final encoder loss: 0.03109807805640829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10930848121643066 0.3897433280944824

Final encoder loss: 0.03268549591442211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10852622985839844 0.3895235061645508

Final encoder loss: 0.034205845239278554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10833311080932617 0.3895275592803955

Final encoder loss: 0.03132499113922745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10888934135437012 0.3910858631134033

Final encoder loss: 0.03134684531509773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10345339775085449 0.3845536708831787


Training amigos model
Final encoder loss: 0.02501852347685211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10843205451965332 0.3417806625366211

Final encoder loss: 0.02369108236521899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10619235038757324 0.3421146869659424

Final encoder loss: 0.0233452230182137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10792326927185059 0.34166598320007324

Final encoder loss: 0.02346137721601022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10634160041809082 0.3421292304992676

Final encoder loss: 0.021130541019323178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10677170753479004 0.3416554927825928

Final encoder loss: 0.02339830772057056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10616827011108398 0.34192824363708496

Final encoder loss: 0.025974879451008008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10690093040466309 0.3415360450744629

Final encoder loss: 0.024653791140640233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10591006278991699 0.3417210578918457

Final encoder loss: 0.024463473563930003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10590624809265137 0.34151721000671387

Final encoder loss: 0.024789876151333236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10612630844116211 0.3415546417236328

Final encoder loss: 0.022587402734747918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10583949089050293 0.3416118621826172

Final encoder loss: 0.023787905925562045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10628271102905273 0.34155702590942383

Final encoder loss: 0.021412848305543537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10634803771972656 0.34137725830078125

Final encoder loss: 0.025097327034743176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10625004768371582 0.3417348861694336

Final encoder loss: 0.02349160188437015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10613203048706055 0.34180259704589844

Final encoder loss: 0.023606438689259567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10129332542419434 0.3384058475494385


Training amigos model
Final encoder loss: 0.18076051771640778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4743232727050781 0.0811314582824707

Final encoder loss: 0.18782415986061096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4703938961029053 0.08305549621582031

Final encoder loss: 0.18363305926322937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47049856185913086 0.07497715950012207

Final encoder loss: 0.07373588532209396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4707043170928955 0.07684135437011719

Final encoder loss: 0.07541219145059586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4711179733276367 0.07658219337463379

Final encoder loss: 0.06925976276397705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4742732048034668 0.07512903213500977

Final encoder loss: 0.043950509279966354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4751315116882324 0.08097481727600098

Final encoder loss: 0.04452211782336235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4727661609649658 0.07764935493469238

Final encoder loss: 0.041845131665468216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46541476249694824 0.07654714584350586

Final encoder loss: 0.03267868980765343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46937131881713867 0.07557177543640137

Final encoder loss: 0.03326880559325218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4689669609069824 0.07543826103210449

Final encoder loss: 0.03206610307097435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4743211269378662 0.07764124870300293

Final encoder loss: 0.027767391875386238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47582244873046875 0.07588005065917969

Final encoder loss: 0.028437912464141846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47458910942077637 0.07822012901306152

Final encoder loss: 0.027682892978191376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46668529510498047 0.0812382698059082

Final encoder loss: 0.025887513533234596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4680478572845459 0.07599401473999023

Final encoder loss: 0.026644887402653694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46781134605407715 0.07528853416442871

Final encoder loss: 0.02586880698800087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46675753593444824 0.07508063316345215

Final encoder loss: 0.025663260370492935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47893500328063965 0.07531332969665527

Final encoder loss: 0.026136653497815132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47431039810180664 0.08092331886291504

Final encoder loss: 0.025834472849965096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46901869773864746 0.07802224159240723

Final encoder loss: 0.02610018476843834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4695594310760498 0.07647442817687988

Final encoder loss: 0.02623697929084301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47036170959472656 0.07436656951904297

Final encoder loss: 0.026192300021648407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4649381637573242 0.07536649703979492

Final encoder loss: 0.025386905297636986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4764525890350342 0.07694530487060547

Final encoder loss: 0.02546033449470997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4753103256225586 0.07207322120666504

Final encoder loss: 0.02585558220744133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4705367088317871 0.08121895790100098

Final encoder loss: 0.024775777012109756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47130799293518066 0.07955479621887207

Final encoder loss: 0.024818604812026024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4675016403198242 0.07483983039855957

Final encoder loss: 0.025033891201019287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4622783660888672 0.07649445533752441

Final encoder loss: 0.024263082072138786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47542405128479004 0.0759890079498291

Final encoder loss: 0.02447493001818657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47341179847717285 0.08157896995544434

Final encoder loss: 0.024584293365478516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45732593536376953 0.07477259635925293

Final encoder loss: 0.023988546803593636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45861363410949707 0.07483720779418945

Final encoder loss: 0.024367738515138626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45852231979370117 0.07137227058410645

Final encoder loss: 0.02441284991800785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45718884468078613 0.07580208778381348

Final encoder loss: 0.023812411352992058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45908594131469727 0.07528948783874512

Final encoder loss: 0.023821938782930374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685492515563965 0.07363438606262207

Final encoder loss: 0.024144371971488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654359817504883 0.07377266883850098

Final encoder loss: 0.023914890363812447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46874070167541504 0.07504153251647949

Final encoder loss: 0.023672673851251602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46950697898864746 0.07584643363952637

Final encoder loss: 0.02390376292169094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46715354919433594 0.07454061508178711

Final encoder loss: 0.02355494350194931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46788525581359863 0.07758259773254395

Final encoder loss: 0.023580415174365044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47031474113464355 0.07523560523986816

Final encoder loss: 0.02373693697154522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46528196334838867 0.07322406768798828

Final encoder loss: 0.02342751808464527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4690854549407959 0.07214617729187012

Final encoder loss: 0.023180538788437843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4678373336791992 0.07714200019836426

Final encoder loss: 0.02354595810174942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4675900936126709 0.07299232482910156

Final encoder loss: 0.023202896118164062
Final encoder loss: 0.021992618218064308
Final encoder loss: 0.02149113267660141

Training dapper model
Final encoder loss: 0.021357955464886433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05996346473693848 0.10668611526489258

Final encoder loss: 0.02057162654398524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05907630920410156 0.10648012161254883

Final encoder loss: 0.020231186154104946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05959010124206543 0.10585355758666992

Final encoder loss: 0.018610016673885857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0594019889831543 0.10677337646484375

Final encoder loss: 0.01939581068955477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05930924415588379 0.1064608097076416

Final encoder loss: 0.020251002247095664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0597071647644043 0.10662841796875

Final encoder loss: 0.021414839896133613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05923652648925781 0.1062769889831543

Final encoder loss: 0.01868469958506874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05912160873413086 0.1068112850189209

Final encoder loss: 0.017051544640776208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059497833251953125 0.10659623146057129

Final encoder loss: 0.01879311332756605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05940604209899902 0.10703873634338379

Final encoder loss: 0.018199867204297424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059419870376586914 0.10727334022521973

Final encoder loss: 0.01809721798548063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05984759330749512 0.10671734809875488

Final encoder loss: 0.019624309352265695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05917835235595703 0.10629487037658691

Final encoder loss: 0.019313649573502293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05950665473937988 0.10663533210754395

Final encoder loss: 0.018953237794366612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05955839157104492 0.1060023307800293

Final encoder loss: 0.01607302763448788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059052467346191406 0.10610103607177734


Training dapper model
Final encoder loss: 0.2024451196193695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11764907836914062 0.03392910957336426

Final encoder loss: 0.20818789303302765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11510729789733887 0.033715248107910156

Final encoder loss: 0.08153305947780609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11579751968383789 0.03395819664001465

Final encoder loss: 0.08299560099840164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11510038375854492 0.03373312950134277

Final encoder loss: 0.04808687046170235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11536598205566406 0.03380274772644043

Final encoder loss: 0.04752124473452568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11819005012512207 0.0343928337097168

Final encoder loss: 0.03334988281130791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11570119857788086 0.03426790237426758

Final encoder loss: 0.032956503331661224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11806678771972656 0.03523850440979004

Final encoder loss: 0.02601446956396103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1159355640411377 0.03435945510864258

Final encoder loss: 0.02583128586411476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11616039276123047 0.03433084487915039

Final encoder loss: 0.022261159494519234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11669349670410156 0.03513669967651367

Final encoder loss: 0.022153954952955246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11741781234741211 0.0339813232421875

Final encoder loss: 0.020166467875242233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11611270904541016 0.035094261169433594

Final encoder loss: 0.02022000029683113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11555194854736328 0.035343170166015625

Final encoder loss: 0.018962077796459198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11846446990966797 0.03448915481567383

Final encoder loss: 0.01908561959862709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11682963371276855 0.03439164161682129

Final encoder loss: 0.018492968752980232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11548829078674316 0.03436017036437988

Final encoder loss: 0.018435830250382423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11905670166015625 0.033887386322021484

Final encoder loss: 0.018549034371972084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11625933647155762 0.03412199020385742

Final encoder loss: 0.01809142902493477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11587381362915039 0.034787893295288086

Final encoder loss: 0.018538499251008034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11757826805114746 0.034325599670410156

Final encoder loss: 0.018069513142108917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11594533920288086 0.03507208824157715

Final encoder loss: 0.018192119896411896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11614060401916504 0.03446841239929199

Final encoder loss: 0.018029818311333656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11735701560974121 0.03470921516418457

Final encoder loss: 0.018169734627008438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.116790771484375 0.033635854721069336

Final encoder loss: 0.018274469301104546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11683201789855957 0.034330129623413086

Final encoder loss: 0.017814379185438156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11623334884643555 0.03466963768005371

Final encoder loss: 0.018114304170012474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11787104606628418 0.034177303314208984

Final encoder loss: 0.017521118745207787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1167459487915039 0.034071922302246094

Final encoder loss: 0.017567114904522896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11614179611206055 0.03484630584716797

Final encoder loss: 0.017313726246356964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11817169189453125 0.03488564491271973

Final encoder loss: 0.017244797199964523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11587858200073242 0.03443646430969238

Final encoder loss: 0.01723782904446125
Final encoder loss: 0.016243064776062965

Training case model
Final encoder loss: 0.028377915292898553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0905764102935791 0.21933817863464355

Final encoder loss: 0.028323840982349316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08922553062438965 0.21908950805664062

Final encoder loss: 0.028148621106364142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08985042572021484 0.21917080879211426

Final encoder loss: 0.027559411451481948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08969497680664062 0.219529390335083

Final encoder loss: 0.02811826245828959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08936929702758789 0.21932363510131836

Final encoder loss: 0.027371066041075816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08973979949951172 0.21912479400634766

Final encoder loss: 0.02772400147853925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08968305587768555 0.21930217742919922

Final encoder loss: 0.027700425920852628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08951139450073242 0.21928000450134277

Final encoder loss: 0.02734144464170543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08926200866699219 0.21940016746520996

Final encoder loss: 0.02738044540062103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0895700454711914 0.21961402893066406

Final encoder loss: 0.026891969929048604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.0898895263671875 0.21920204162597656

Final encoder loss: 0.027527709623890062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09088540077209473 0.2193610668182373

Final encoder loss: 0.027142096457222455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08922982215881348 0.21946263313293457

Final encoder loss: 0.0273336592455046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09056830406188965 0.2193741798400879

Final encoder loss: 0.027869412955517644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08906435966491699 0.21935486793518066

Final encoder loss: 0.026854304873560306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08662724494934082 0.21571707725524902


Training case model
Final encoder loss: 0.2029593288898468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26431918144226074 0.053275346755981445

Final encoder loss: 0.18890349566936493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2765336036682129 0.0547945499420166

Final encoder loss: 0.19014081358909607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2573971748352051 0.05245018005371094

Final encoder loss: 0.19219517707824707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27585721015930176 0.05311322212219238

Final encoder loss: 0.1808108687400818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.29039788246154785 0.05412101745605469

Final encoder loss: 0.19193656742572784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549259662628174 0.05213165283203125

Final encoder loss: 0.10122041404247284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2687246799468994 0.053412675857543945

Final encoder loss: 0.09142116457223892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694969177246094 0.05302023887634277

Final encoder loss: 0.08737219125032425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2792971134185791 0.05113029479980469

Final encoder loss: 0.08642666041851044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26801633834838867 0.05499696731567383

Final encoder loss: 0.07870987802743912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2753467559814453 0.05211830139160156

Final encoder loss: 0.08209780603647232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562065124511719 0.052825212478637695

Final encoder loss: 0.0594656802713871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.277759313583374 0.05160999298095703

Final encoder loss: 0.05421585589647293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2768285274505615 0.054970502853393555

Final encoder loss: 0.052081380039453506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257580041885376 0.052098751068115234

Final encoder loss: 0.052554789930582047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2753942012786865 0.052926063537597656

Final encoder loss: 0.049464575946331024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27745842933654785 0.05249810218811035

Final encoder loss: 0.05118473991751671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25511598587036133 0.053719282150268555

Final encoder loss: 0.043113939464092255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2684440612792969 0.05265998840332031

Final encoder loss: 0.04024511203169823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2745201587677002 0.05329084396362305

Final encoder loss: 0.03893917798995972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2700691223144531 0.05257081985473633

Final encoder loss: 0.039757370948791504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27657055854797363 0.05564379692077637

Final encoder loss: 0.03886110708117485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2688424587249756 0.052776336669921875

Final encoder loss: 0.039114583283662796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556025981903076 0.05366945266723633

Final encoder loss: 0.03665225952863693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2715268135070801 0.05367016792297363

Final encoder loss: 0.03513062372803688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2856404781341553 0.05529332160949707

Final encoder loss: 0.03439774736762047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26947689056396484 0.05235004425048828

Final encoder loss: 0.03525351360440254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27426671981811523 0.05239462852478027

Final encoder loss: 0.03546394035220146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27044200897216797 0.051225900650024414

Final encoder loss: 0.03502858057618141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553868293762207 0.05241227149963379

Final encoder loss: 0.03424308821558952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2688307762145996 0.052884817123413086

Final encoder loss: 0.03340546786785126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2897067070007324 0.054065704345703125

Final encoder loss: 0.03313747048377991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2711372375488281 0.05173540115356445

Final encoder loss: 0.03371488302946091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26751041412353516 0.053673744201660156

Final encoder loss: 0.034502603113651276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2779366970062256 0.054228782653808594

Final encoder loss: 0.034024883061647415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255631685256958 0.0517880916595459

Final encoder loss: 0.03255876153707504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580862045288086 0.053292274475097656

Final encoder loss: 0.03168632462620735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2766306400299072 0.052635908126831055

Final encoder loss: 0.03125785291194916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2748708724975586 0.051552772521972656

Final encoder loss: 0.03158975765109062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2905607223510742 0.05372309684753418

Final encoder loss: 0.03252803906798363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2692108154296875 0.054874420166015625

Final encoder loss: 0.031966906040906906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25502657890319824 0.052442073822021484

Final encoder loss: 0.031183617189526558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2676513195037842 0.05310535430908203

Final encoder loss: 0.030534792691469193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2710294723510742 0.05202436447143555

Final encoder loss: 0.0300032589584589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2843177318572998 0.05424952507019043

Final encoder loss: 0.03047148510813713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2684001922607422 0.05426836013793945

Final encoder loss: 0.031198637560009956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27454710006713867 0.053096771240234375

Final encoder loss: 0.030729906633496284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2574608325958252 0.05128645896911621

Final encoder loss: 0.030325954779982567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.277935266494751 0.05466032028198242

Final encoder loss: 0.029736846685409546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2763502597808838 0.0529322624206543

Final encoder loss: 0.02942384034395218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25804829597473145 0.05147385597229004

Final encoder loss: 0.029682081192731857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2778356075286865 0.05332756042480469

Final encoder loss: 0.03049032762646675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581796646118164 0.054335832595825195

Final encoder loss: 0.029746120795607567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548940181732178 0.05230832099914551

Final encoder loss: 0.02983020432293415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27560877799987793 0.052510976791381836

Final encoder loss: 0.02938644215464592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26988911628723145 0.05201244354248047

Final encoder loss: 0.02911187894642353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2745072841644287 0.05383896827697754

Final encoder loss: 0.02933979406952858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25806164741516113 0.053339242935180664

Final encoder loss: 0.030318988487124443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2755272388458252 0.05183768272399902

Final encoder loss: 0.029612114652991295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25455331802368164 0.05163979530334473

Final encoder loss: 0.02936488203704357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2752363681793213 0.05324554443359375

Final encoder loss: 0.028878552839159966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694976329803467 0.052628278732299805

Final encoder loss: 0.028440196067094803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27614712715148926 0.05292201042175293

Final encoder loss: 0.028679102659225464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25786757469177246 0.05318880081176758

Final encoder loss: 0.029545150697231293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2776985168457031 0.05373024940490723

Final encoder loss: 0.02891591563820839
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25641727447509766 0.0527651309967041

Final encoder loss: 0.02899097464978695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2693455219268799 0.05200552940368652

Final encoder loss: 0.02864467166364193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27782654762268066 0.05289459228515625

Final encoder loss: 0.028106996789574623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.267549991607666 0.05381512641906738

Final encoder loss: 0.028424443677067757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26886606216430664 0.053247928619384766

Final encoder loss: 0.029435979202389717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27742648124694824 0.05274820327758789

Final encoder loss: 0.028617361560463905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545647621154785 0.05538129806518555

Final encoder loss: 0.028662143275141716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2676975727081299 0.053768157958984375

Final encoder loss: 0.02827683836221695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2754817008972168 0.05410432815551758

Final encoder loss: 0.027929505333304405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2779977321624756 0.05099225044250488

Final encoder loss: 0.02808929607272148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26855015754699707 0.05449223518371582

Final encoder loss: 0.028825320303440094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27550673484802246 0.052079200744628906

Final encoder loss: 0.02820507436990738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255765438079834 0.05317807197570801

Final encoder loss: 0.028559289872646332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28843259811401367 0.05290341377258301

Final encoder loss: 0.028224393725395203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584402561187744 0.05465435981750488

Final encoder loss: 0.027663758024573326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2686009407043457 0.051378488540649414

Final encoder loss: 0.027826925739645958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2743947505950928 0.05374884605407715

Final encoder loss: 0.028885208070278168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.278458833694458 0.053655385971069336

Final encoder loss: 0.02813391201198101
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25424933433532715 0.05313849449157715

Final encoder loss: 0.028232881799340248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28585124015808105 0.05355668067932129

Final encoder loss: 0.027801860123872757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26907801628112793 0.05232739448547363

Final encoder loss: 0.02747766673564911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2889842987060547 0.05196213722229004

Final encoder loss: 0.02764681912958622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26824331283569336 0.053035736083984375

Final encoder loss: 0.028423642739653587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28618383407592773 0.05344510078430176

Final encoder loss: 0.027858154848217964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25443458557128906 0.05278921127319336

Final encoder loss: 0.028117341920733452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2793755531311035 0.052376508712768555

Final encoder loss: 0.027796689420938492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2779691219329834 0.0531458854675293

Final encoder loss: 0.027273911982774734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26862454414367676 0.052985191345214844

Final encoder loss: 0.02747870422899723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26827383041381836 0.051459550857543945

Final encoder loss: 0.02842433750629425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27995800971984863 0.051367998123168945

Final encoder loss: 0.027787011116743088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537229061126709 0.05381035804748535

Final encoder loss: 0.02785726822912693
Final encoder loss: 0.026999088004231453
Final encoder loss: 0.025960741564631462
Final encoder loss: 0.025170747190713882
Final encoder loss: 0.025029907003045082
Final encoder loss: 0.023549774661660194

Training emognition model
Final encoder loss: 0.031948693982195096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08139228820800781 0.23086810111999512

Final encoder loss: 0.03259574694660046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0824136734008789 0.2307569980621338

Final encoder loss: 0.03301070032819782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0816798210144043 0.23111605644226074

Final encoder loss: 0.03141558377885156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.09305357933044434 0.23052406311035156

Final encoder loss: 0.031262079850493285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08087515830993652 0.22973990440368652

Final encoder loss: 0.031110335209053885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08066582679748535 0.22980761528015137

Final encoder loss: 0.031709300742541406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08092045783996582 0.22902345657348633

Final encoder loss: 0.031728608857934215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08053469657897949 0.22987127304077148

Final encoder loss: 0.030801142603535434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08077764511108398 0.22982311248779297

Final encoder loss: 0.031152960735600033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08088111877441406 0.22977209091186523

Final encoder loss: 0.031335268712146605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08097290992736816 0.22995281219482422

Final encoder loss: 0.03250038494999671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08040237426757812 0.22982096672058105

Final encoder loss: 0.03042427431257109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08057928085327148 0.2295536994934082

Final encoder loss: 0.030588098694662114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08048152923583984 0.2296433448791504

Final encoder loss: 0.0318457823218166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08049869537353516 0.22957158088684082

Final encoder loss: 0.0327896228220076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0796670913696289 0.22882771492004395


Training emognition model
Final encoder loss: 0.1935594230890274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2518124580383301 0.04959559440612793

Final encoder loss: 0.194967582821846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2493131160736084 0.049218177795410156

Final encoder loss: 0.08588149398565292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24974393844604492 0.048697471618652344

Final encoder loss: 0.08573919534683228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24836206436157227 0.05045914649963379

Final encoder loss: 0.055421486496925354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24889779090881348 0.04891037940979004

Final encoder loss: 0.05422734469175339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24876713752746582 0.05073380470275879

Final encoder loss: 0.04251833260059357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24915361404418945 0.0517725944519043

Final encoder loss: 0.04172549769282341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24842572212219238 0.04895305633544922

Final encoder loss: 0.036191217601299286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24696564674377441 0.049416542053222656

Final encoder loss: 0.03572177141904831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24799561500549316 0.04985785484313965

Final encoder loss: 0.03283952921628952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24856352806091309 0.048819541931152344

Final encoder loss: 0.032545145601034164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24749541282653809 0.04962491989135742

Final encoder loss: 0.031187845394015312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24962234497070312 0.048750877380371094

Final encoder loss: 0.030955439433455467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24827075004577637 0.04988884925842285

Final encoder loss: 0.030594877898693085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24804472923278809 0.05135822296142578

Final encoder loss: 0.030456362292170525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24843358993530273 0.048933982849121094

Final encoder loss: 0.030504904687404633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24877381324768066 0.04882240295410156

Final encoder loss: 0.030515184625983238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24940228462219238 0.05172920227050781

Final encoder loss: 0.030447708442807198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24728894233703613 0.04892897605895996

Final encoder loss: 0.03066817857325077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477433681488037 0.04945683479309082

Final encoder loss: 0.030277511104941368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2495260238647461 0.04894876480102539

Final encoder loss: 0.030547240749001503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24749398231506348 0.050557613372802734

Final encoder loss: 0.030101610347628593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.248138427734375 0.049253225326538086

Final encoder loss: 0.030451564118266106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24993562698364258 0.05048084259033203

Final encoder loss: 0.03004518151283264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24937868118286133 0.048511505126953125

Final encoder loss: 0.0301378071308136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24814152717590332 0.05161285400390625

Final encoder loss: 0.030299901962280273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467508316040039 0.04814863204956055

Final encoder loss: 0.03025522269308567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24856162071228027 0.049835205078125

Final encoder loss: 0.030067715793848038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24962902069091797 0.04987835884094238

Final encoder loss: 0.030088583007454872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24753284454345703 0.0495607852935791

Final encoder loss: 0.02991211973130703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24925899505615234 0.048607587814331055

Final encoder loss: 0.03010382689535618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24926233291625977 0.05006289482116699

Final encoder loss: 0.02956676483154297
Final encoder loss: 0.02867860347032547

Training empatch model
Final encoder loss: 0.04579536320463258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07164216041564941 0.17414236068725586

Final encoder loss: 0.044148798111793404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.071990966796875 0.1739368438720703

Final encoder loss: 0.042496288026395546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07131671905517578 0.17427706718444824

Final encoder loss: 0.043636787186512635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07209253311157227 0.17437148094177246

Final encoder loss: 0.04042514341421129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07113194465637207 0.17408227920532227

Final encoder loss: 0.046615319700260706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07157468795776367 0.17404413223266602

Final encoder loss: 0.040933829796988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07159137725830078 0.1741182804107666

Final encoder loss: 0.03961018665439188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07074308395385742 0.17374181747436523

Final encoder loss: 0.029924483519977476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07111310958862305 0.1737534999847412

Final encoder loss: 0.03158078346305552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07125282287597656 0.17577338218688965

Final encoder loss: 0.02905590267775833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07224321365356445 0.1734774112701416

Final encoder loss: 0.03233853671915265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0706946849822998 0.1743626594543457

Final encoder loss: 0.03146925881789409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0721578598022461 0.17393088340759277

Final encoder loss: 0.030858983804161612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07095527648925781 0.17382574081420898

Final encoder loss: 0.029820325104359877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07196521759033203 0.1744093894958496

Final encoder loss: 0.031117684862601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0708014965057373 0.17380785942077637


Training empatch model
Final encoder loss: 0.17115667462348938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17672228813171387 0.04401230812072754

Final encoder loss: 0.08012642711400986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17412495613098145 0.04415726661682129

Final encoder loss: 0.05596930906176567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1749117374420166 0.04336142539978027

Final encoder loss: 0.044516898691654205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17409086227416992 0.04345560073852539

Final encoder loss: 0.03806969150900841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504072189331055 0.04403543472290039

Final encoder loss: 0.03410425782203674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17395257949829102 0.042983055114746094

Final encoder loss: 0.03153441101312637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17415857315063477 0.04397916793823242

Final encoder loss: 0.02980327047407627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755237579345703 0.04381847381591797

Final encoder loss: 0.0287205558270216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17414045333862305 0.04340982437133789

Final encoder loss: 0.02807239070534706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17471694946289062 0.04424166679382324

Final encoder loss: 0.027633296325802803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17494678497314453 0.04310297966003418

Final encoder loss: 0.027354300022125244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17421197891235352 0.04318428039550781

Final encoder loss: 0.027037281543016434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17430615425109863 0.04305529594421387

Final encoder loss: 0.02691807970404625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752629280090332 0.04381966590881348

Final encoder loss: 0.026765093207359314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17633676528930664 0.043306827545166016

Final encoder loss: 0.026718242093920708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17522454261779785 0.04443860054016113

Final encoder loss: 0.026597242802381516

Training wesad model
Final encoder loss: 0.050074273902036795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07107710838317871 0.17365431785583496

Final encoder loss: 0.04492940231426808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0714423656463623 0.1753995418548584

Final encoder loss: 0.04372810775356732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07185006141662598 0.17412424087524414

Final encoder loss: 0.04397107451283593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07113361358642578 0.17348504066467285

Final encoder loss: 0.0324264244881614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0709383487701416 0.17379260063171387

Final encoder loss: 0.03142041323228354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0852515697479248 0.1743459701538086

Final encoder loss: 0.031209635181985464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07072114944458008 0.17391657829284668

Final encoder loss: 0.030914196475803433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07134103775024414 0.173370361328125

Final encoder loss: 0.024219175652016554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07084512710571289 0.1746082305908203

Final encoder loss: 0.024276015312375495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07081055641174316 0.1739788055419922

Final encoder loss: 0.024526542658292218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07204437255859375 0.17381978034973145

Final encoder loss: 0.02531428179111998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07094407081604004 0.17395353317260742

Final encoder loss: 0.019836150884128968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0710763931274414 0.17410659790039062

Final encoder loss: 0.02039160977287869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07117915153503418 0.1740434169769287

Final encoder loss: 0.020108076167387835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07096433639526367 0.17365789413452148

Final encoder loss: 0.020874890612304627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07210731506347656 0.17371749877929688


Training wesad model
Final encoder loss: 0.21561317145824432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10646510124206543 0.03377699851989746

Final encoder loss: 0.09723090380430222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10468244552612305 0.03318476676940918

Final encoder loss: 0.06318655610084534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10434627532958984 0.03365755081176758

Final encoder loss: 0.04663306847214699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10572981834411621 0.032747745513916016

Final encoder loss: 0.037365593016147614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10410857200622559 0.03350114822387695

Final encoder loss: 0.031830448657274246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10433816909790039 0.03360700607299805

Final encoder loss: 0.02838720940053463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10400891304016113 0.03339958190917969

Final encoder loss: 0.026196541264653206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10353374481201172 0.03338193893432617

Final encoder loss: 0.024736639112234116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10473322868347168 0.03290677070617676

Final encoder loss: 0.023783309385180473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10350465774536133 0.03275656700134277

Final encoder loss: 0.023297151550650597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10451173782348633 0.0328669548034668

Final encoder loss: 0.023130197077989578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10326623916625977 0.033419132232666016

Final encoder loss: 0.023181697353720665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10295701026916504 0.03321719169616699

Final encoder loss: 0.023296773433685303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10407018661499023 0.03311467170715332

Final encoder loss: 0.023402707651257515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10318517684936523 0.03316783905029297

Final encoder loss: 0.023466801270842552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10438227653503418 0.03326749801635742

Final encoder loss: 0.02348705567419529

Calculating loss for amigos model
	Full Pass 0.6841464042663574
numFreeParamsPath 18
Reconstruction loss values: 0.03362945839762688 0.043876029551029205

Calculating loss for dapper model
	Full Pass 0.1509847640991211
numFreeParamsPath 18
Reconstruction loss values: 0.028037505224347115 0.0321357287466526

Calculating loss for case model
	Full Pass 0.8546061515808105
numFreeParamsPath 18
Reconstruction loss values: 0.03971843793988228 0.04300772771239281

Calculating loss for emognition model
	Full Pass 0.2813844680786133
numFreeParamsPath 18
Reconstruction loss values: 0.04313931241631508 0.050546564161777496

Calculating loss for empatch model
	Full Pass 0.10507798194885254
numFreeParamsPath 18
Reconstruction loss values: 0.04578476771712303 0.05258393660187721

Calculating loss for wesad model
	Full Pass 0.07723879814147949
numFreeParamsPath 18
Reconstruction loss values: 0.046602796763181686 0.06772460043430328
Total loss calculation time: 3.8173255920410156

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.571152687072754
Total epoch time: 171.38177156448364

Epoch: 34

Training dapper model
Final encoder loss: 0.029118900765479606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06534028053283691 0.15336132049560547

Final encoder loss: 0.028051532161186787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061841726303100586 0.15197992324829102

Final encoder loss: 0.02703363555445306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.061731815338134766 0.14973092079162598

Final encoder loss: 0.023869188028869563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06192445755004883 0.14985871315002441

Final encoder loss: 0.024438956575442528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06186103820800781 0.15022993087768555

Final encoder loss: 0.024372167261955242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062076568603515625 0.15024113655090332

Final encoder loss: 0.02299436716638368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0619964599609375 0.14973711967468262

Final encoder loss: 0.02515015107880212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06170988082885742 0.15074777603149414

Final encoder loss: 0.02292121430249103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.0633542537689209 0.15275049209594727

Final encoder loss: 0.025971635153412734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06265592575073242 0.15232348442077637

Final encoder loss: 0.023999432381715962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06325078010559082 0.15372943878173828

Final encoder loss: 0.025710943922517847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06172490119934082 0.1506209373474121

Final encoder loss: 0.02299763449622117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.062081336975097656 0.15096354484558105

Final encoder loss: 0.02450295371650134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06339764595031738 0.1508486270904541

Final encoder loss: 0.025298855903905904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06192278861999512 0.15171527862548828

Final encoder loss: 0.03144155965053921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.061425209045410156 0.14936137199401855


Training emognition model
Final encoder loss: 0.04196791515487741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08347511291503906 0.27365779876708984

Final encoder loss: 0.041937467430520535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08289456367492676 0.2745933532714844

Final encoder loss: 0.04147632660851928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08361649513244629 0.27570128440856934

Final encoder loss: 0.04081593738581206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08272123336791992 0.27568674087524414

Final encoder loss: 0.042000918325924785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08324790000915527 0.2751181125640869

Final encoder loss: 0.04064180454722865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08333945274353027 0.2739534378051758

Final encoder loss: 0.04057634316846204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08256292343139648 0.27454566955566406

Final encoder loss: 0.03746178146466667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08317947387695312 0.2752237319946289

Final encoder loss: 0.03897551012649324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08289885520935059 0.27383875846862793

Final encoder loss: 0.03853270698776848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08299636840820312 0.2747230529785156

Final encoder loss: 0.03976615481227457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0830373764038086 0.27333831787109375

Final encoder loss: 0.03852199633692325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0824272632598877 0.273967981338501

Final encoder loss: 0.040184739922455325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0825345516204834 0.272702693939209

Final encoder loss: 0.03770859237639792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08225011825561523 0.27417707443237305

Final encoder loss: 0.03951750131135008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0833740234375 0.273942232131958

Final encoder loss: 0.03731785973294458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08206558227539062 0.27362680435180664


Training case model
Final encoder loss: 0.04016607722284093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0910184383392334 0.26369667053222656

Final encoder loss: 0.03746013709747008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09116935729980469 0.26505517959594727

Final encoder loss: 0.035317346137115825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0917654037475586 0.2659156322479248

Final encoder loss: 0.03511895166173679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09073185920715332 0.26461029052734375

Final encoder loss: 0.03493469169457995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09145569801330566 0.2643768787384033

Final encoder loss: 0.03303304502418145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09160184860229492 0.2644808292388916

Final encoder loss: 0.032682563791890995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09082198143005371 0.2640111446380615

Final encoder loss: 0.03312156435336659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09106063842773438 0.26525187492370605

Final encoder loss: 0.03155939179300289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0912175178527832 0.2646479606628418

Final encoder loss: 0.03142361679425992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09072327613830566 0.2641611099243164

Final encoder loss: 0.03083192966269519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09080195426940918 0.26381850242614746

Final encoder loss: 0.03134178234511068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09094524383544922 0.264157772064209

Final encoder loss: 0.03155259135051185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09082269668579102 0.2642524242401123

Final encoder loss: 0.031880102819958865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09098124504089355 0.26372623443603516

Final encoder loss: 0.030992552663111683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.0908811092376709 0.26411938667297363

Final encoder loss: 0.030599316359050093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08740496635437012 0.2606213092803955


Training amigos model
Final encoder loss: 0.034039309382903574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10800743103027344 0.3892183303833008

Final encoder loss: 0.030421278888678176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.1082150936126709 0.38729333877563477

Final encoder loss: 0.03381271440382071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10764622688293457 0.38788938522338867

Final encoder loss: 0.03509466704551407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10776901245117188 0.38799405097961426

Final encoder loss: 0.034867285752600875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10763001441955566 0.38840818405151367

Final encoder loss: 0.03223608039676833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10771656036376953 0.3879077434539795

Final encoder loss: 0.032330586058123195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1076960563659668 0.3880624771118164

Final encoder loss: 0.032633593414552686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10778212547302246 0.3880736827850342

Final encoder loss: 0.03348514773840034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10753941535949707 0.3877856731414795

Final encoder loss: 0.03256035968964369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10774993896484375 0.38813018798828125

Final encoder loss: 0.033290669694716196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10797762870788574 0.3882761001586914

Final encoder loss: 0.03148265509188576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10767626762390137 0.38861703872680664

Final encoder loss: 0.03182455773314178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1078176498413086 0.38829517364501953

Final encoder loss: 0.033923018763338225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10838842391967773 0.3884587287902832

Final encoder loss: 0.02994327580896069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10760807991027832 0.3882570266723633

Final encoder loss: 0.03162057428660282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10247421264648438 0.3830525875091553


Training amigos model
Final encoder loss: 0.023080990923779875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1059114933013916 0.3411295413970947

Final encoder loss: 0.023352284336995547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10580730438232422 0.341184139251709

Final encoder loss: 0.02260922060734121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10589599609375 0.34096598625183105

Final encoder loss: 0.024811009767374975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10609650611877441 0.3412344455718994

Final encoder loss: 0.025026820872340753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1055457592010498 0.3412010669708252

Final encoder loss: 0.023374918959938977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1061410903930664 0.3410763740539551

Final encoder loss: 0.02392667273581893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10582780838012695 0.34116387367248535

Final encoder loss: 0.023850313262972135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10580873489379883 0.34095335006713867

Final encoder loss: 0.024268373070730324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10559558868408203 0.34097719192504883

Final encoder loss: 0.02206981400555108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10581350326538086 0.34089183807373047

Final encoder loss: 0.023556435590912705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1059725284576416 0.34097957611083984

Final encoder loss: 0.02474651272461553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10597944259643555 0.3409605026245117

Final encoder loss: 0.022233527195658624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10596036911010742 0.3409290313720703

Final encoder loss: 0.02383377352568218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10568881034851074 0.3408796787261963

Final encoder loss: 0.021192656763469646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1054987907409668 0.34104323387145996

Final encoder loss: 0.02346163268713307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10059118270874023 0.3368544578552246


Training amigos model
Final encoder loss: 0.18078206479549408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4461667537689209 0.07544922828674316

Final encoder loss: 0.18782731890678406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44412970542907715 0.07400393486022949

Final encoder loss: 0.18363018333911896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4264645576477051 0.0747826099395752

Final encoder loss: 0.07413070648908615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4553999900817871 0.07350754737854004

Final encoder loss: 0.07515081018209457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45493316650390625 0.0748751163482666

Final encoder loss: 0.06920722872018814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4266929626464844 0.07292032241821289

Final encoder loss: 0.04415149986743927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44381046295166016 0.07526040077209473

Final encoder loss: 0.04429937154054642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4573495388031006 0.07476520538330078

Final encoder loss: 0.041741035878658295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43994855880737305 0.07336544990539551

Final encoder loss: 0.032704051584005356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45521068572998047 0.07564878463745117

Final encoder loss: 0.03314049169421196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44980454444885254 0.07590460777282715

Final encoder loss: 0.031848207116127014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4228544235229492 0.07425212860107422

Final encoder loss: 0.027754832059144974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45589733123779297 0.07635116577148438

Final encoder loss: 0.02838103659451008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.41006898880004883 0.07863306999206543

Final encoder loss: 0.02751440741121769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44738149642944336 0.07719564437866211

Final encoder loss: 0.02587634138762951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46564698219299316 0.07830667495727539

Final encoder loss: 0.0263119637966156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45099616050720215 0.07580423355102539

Final encoder loss: 0.02582859992980957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44150590896606445 0.07410597801208496

Final encoder loss: 0.02581418864428997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45725369453430176 0.07675981521606445

Final encoder loss: 0.025735439732670784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46764183044433594 0.08026695251464844

Final encoder loss: 0.025509783998131752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4503505229949951 0.07577013969421387

Final encoder loss: 0.02600995823740959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45311498641967773 0.07721185684204102

Final encoder loss: 0.025734419003129005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4494795799255371 0.07504677772521973

Final encoder loss: 0.025387128815054893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4544398784637451 0.07504582405090332

Final encoder loss: 0.025088412687182426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45336413383483887 0.08098483085632324

Final encoder loss: 0.025373311713337898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.467393159866333 0.0808858871459961

Final encoder loss: 0.02500314638018608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4481966495513916 0.07509589195251465

Final encoder loss: 0.02433806285262108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.450650691986084 0.07358217239379883

Final encoder loss: 0.024846691638231277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43501830101013184 0.07754635810852051

Final encoder loss: 0.024466464295983315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4553685188293457 0.07456088066101074

Final encoder loss: 0.0239721667021513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4684770107269287 0.08542299270629883

Final encoder loss: 0.023728348314762115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4509150981903076 0.08180999755859375

Final encoder loss: 0.02395615540444851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4486255645751953 0.07347846031188965

Final encoder loss: 0.024000423029065132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44934964179992676 0.07482099533081055

Final encoder loss: 0.02374296821653843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4130537509918213 0.07526445388793945

Final encoder loss: 0.023889798671007156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42536425590515137 0.07491874694824219

Final encoder loss: 0.023659683763980865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4499807357788086 0.08020949363708496

Final encoder loss: 0.023349344730377197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46720385551452637 0.07716035842895508

Final encoder loss: 0.02363571897149086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4326016902923584 0.07484769821166992

Final encoder loss: 0.023447612300515175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44968652725219727 0.07657432556152344

Final encoder loss: 0.02341998741030693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45914268493652344 0.07552957534790039

Final encoder loss: 0.02335045114159584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4236576557159424 0.08265137672424316

Final encoder loss: 0.023166600614786148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4527285099029541 0.08288955688476562

Final encoder loss: 0.022793976590037346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4538917541503906 0.07621312141418457

Final encoder loss: 0.02321675978600979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4281139373779297 0.07614636421203613

Final encoder loss: 0.023139217868447304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45830368995666504 0.07537674903869629

Final encoder loss: 0.023010151460766792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44223737716674805 0.07427740097045898

Final encoder loss: 0.023022880777716637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4478113651275635 0.07777643203735352

Final encoder loss: 0.022965488955378532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4644451141357422 0.07604670524597168

Final encoder loss: 0.02269762009382248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46670985221862793 0.07727742195129395

Final encoder loss: 0.022961605340242386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4198436737060547 0.07409977912902832

Final encoder loss: 0.022694488987326622
Final encoder loss: 0.021650217473506927
Final encoder loss: 0.02075355313718319

Training dapper model
Final encoder loss: 0.019268331810996374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06016826629638672 0.1074380874633789

Final encoder loss: 0.01933528745331851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06002664566040039 0.10791420936584473

Final encoder loss: 0.0185851267943161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.061254024505615234 0.10854458808898926

Final encoder loss: 0.019708091197317017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06100058555603027 0.10750818252563477

Final encoder loss: 0.019365485462652744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059682607650756836 0.10798811912536621

Final encoder loss: 0.019730886686044207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05984163284301758 0.10746550559997559

Final encoder loss: 0.019350339519426565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05984759330749512 0.10775089263916016

Final encoder loss: 0.016960829470508327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.061304330825805664 0.10872530937194824

Final encoder loss: 0.01868587336518384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.062125444412231445 0.10687136650085449

Final encoder loss: 0.01801335597609696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05994701385498047 0.10754680633544922

Final encoder loss: 0.018723506464818216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.0599513053894043 0.1078343391418457

Final encoder loss: 0.018433364147099562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05958223342895508 0.10737419128417969

Final encoder loss: 0.01882579868495501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06071805953979492 0.10833358764648438

Final encoder loss: 0.01837735149366843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.0617070198059082 0.10753154754638672

Final encoder loss: 0.018805128025239117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059800148010253906 0.10753440856933594

Final encoder loss: 0.01544119862573492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059990644454956055 0.10676836967468262


Training dapper model
Final encoder loss: 0.2024489939212799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11431670188903809 0.03407716751098633

Final encoder loss: 0.2082194834947586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11830806732177734 0.03527355194091797

Final encoder loss: 0.08197931200265884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11517119407653809 0.03425884246826172

Final encoder loss: 0.0832965075969696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11629962921142578 0.03440403938293457

Final encoder loss: 0.04809533432126045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11269640922546387 0.034002065658569336

Final encoder loss: 0.04744572192430496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11613607406616211 0.03383517265319824

Final encoder loss: 0.03330671042203903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11506867408752441 0.03512406349182129

Final encoder loss: 0.03290444239974022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11809277534484863 0.03309059143066406

Final encoder loss: 0.025968054309487343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11341309547424316 0.03412365913391113

Final encoder loss: 0.025864021852612495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11630606651306152 0.03451943397521973

Final encoder loss: 0.022040724754333496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11286258697509766 0.03415989875793457

Final encoder loss: 0.022078830748796463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1185147762298584 0.03456521034240723

Final encoder loss: 0.01989842765033245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11461234092712402 0.03399014472961426

Final encoder loss: 0.01995805837213993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1161644458770752 0.034223318099975586

Final encoder loss: 0.018890563398599625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11290740966796875 0.03438425064086914

Final encoder loss: 0.018769467249512672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11588788032531738 0.03463435173034668

Final encoder loss: 0.01822381094098091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11500048637390137 0.03513288497924805

Final encoder loss: 0.017993006855249405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11831855773925781 0.03328227996826172

Final encoder loss: 0.01780524104833603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11333107948303223 0.0341951847076416

Final encoder loss: 0.01774522103369236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11546111106872559 0.03435492515563965

Final encoder loss: 0.017680222168564796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11330461502075195 0.034125566482543945

Final encoder loss: 0.017871448770165443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11791515350341797 0.03467559814453125

Final encoder loss: 0.01768958382308483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11433243751525879 0.03394818305969238

Final encoder loss: 0.018297282978892326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11636686325073242 0.03412890434265137

Final encoder loss: 0.01775594986975193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11330342292785645 0.0339047908782959

Final encoder loss: 0.018282020464539528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11613273620605469 0.03418254852294922

Final encoder loss: 0.017635570839047432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11507558822631836 0.035856008529663086

Final encoder loss: 0.017984984442591667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1178579330444336 0.03399324417114258

Final encoder loss: 0.01734011247754097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11283326148986816 0.034197092056274414

Final encoder loss: 0.017391150817275047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11592435836791992 0.034336090087890625

Final encoder loss: 0.016863862052559853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1131136417388916 0.0347445011138916

Final encoder loss: 0.01706831529736519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11792707443237305 0.03531360626220703

Final encoder loss: 0.01642243005335331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11432552337646484 0.03390383720397949

Final encoder loss: 0.016765525564551353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11598348617553711 0.03420543670654297

Final encoder loss: 0.016289252787828445
Final encoder loss: 0.015662765130400658

Training case model
Final encoder loss: 0.028596881476782604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08934545516967773 0.2194366455078125

Final encoder loss: 0.028117866107664483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.089569091796875 0.21933603286743164

Final encoder loss: 0.027729579695257932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08979463577270508 0.2198324203491211

Final encoder loss: 0.027217123515737466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09002161026000977 0.21940159797668457

Final encoder loss: 0.027136634183552458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.09078764915466309 0.21939659118652344

Final encoder loss: 0.027424321951227175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08884096145629883 0.21826958656311035

Final encoder loss: 0.026921942004760578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08887767791748047 0.21835899353027344

Final encoder loss: 0.027584396345011388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08887434005737305 0.21815204620361328

Final encoder loss: 0.027274572930117504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08899283409118652 0.2181849479675293

Final encoder loss: 0.02637617604574262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08881211280822754 0.21855783462524414

Final encoder loss: 0.026462382774697882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08913564682006836 0.2184898853302002

Final encoder loss: 0.027465926776866188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08938956260681152 0.21841907501220703

Final encoder loss: 0.027074121799957957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08877158164978027 0.21845030784606934

Final encoder loss: 0.02760334791677674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08947324752807617 0.2183704376220703

Final encoder loss: 0.026833662677301418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08870387077331543 0.21853971481323242

Final encoder loss: 0.02688665999067671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08565926551818848 0.21480464935302734


Training case model
Final encoder loss: 0.20296891033649445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2610154151916504 0.051493167877197266

Final encoder loss: 0.18891732394695282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25753045082092285 0.05224609375

Final encoder loss: 0.19014985859394073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25728631019592285 0.05193209648132324

Final encoder loss: 0.1921946406364441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25679492950439453 0.05324721336364746

Final encoder loss: 0.1808130145072937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25749969482421875 0.05274796485900879

Final encoder loss: 0.19193783402442932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540566921234131 0.05132126808166504

Final encoder loss: 0.10111338645219803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25763559341430664 0.05231738090515137

Final encoder loss: 0.09137406945228577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588670253753662 0.05113363265991211

Final encoder loss: 0.08753596991300583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25858020782470703 0.05151224136352539

Final encoder loss: 0.08628284931182861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25707507133483887 0.05155134201049805

Final encoder loss: 0.07900655269622803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25792741775512695 0.05193209648132324

Final encoder loss: 0.08192086219787598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25420451164245605 0.05188322067260742

Final encoder loss: 0.05936257541179657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25668978691101074 0.05183243751525879

Final encoder loss: 0.05410323292016983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568244934082031 0.051381587982177734

Final encoder loss: 0.05220089480280876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25795888900756836 0.05188798904418945

Final encoder loss: 0.05260530859231949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584853172302246 0.05227255821228027

Final encoder loss: 0.04959927871823311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2569301128387451 0.0515437126159668

Final encoder loss: 0.05119961500167847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25386667251586914 0.0529179573059082

Final encoder loss: 0.043009430170059204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25804877281188965 0.05430102348327637

Final encoder loss: 0.04013735055923462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582519054412842 0.05212712287902832

Final encoder loss: 0.03898480534553528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2578399181365967 0.05240774154663086

Final encoder loss: 0.03971724584698677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572505474090576 0.051824092864990234

Final encoder loss: 0.038803812116384506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2567920684814453 0.05235767364501953

Final encoder loss: 0.039173197001218796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25301265716552734 0.0516664981842041

Final encoder loss: 0.036502063274383545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573587894439697 0.05095696449279785

Final encoder loss: 0.0351421982049942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564864158630371 0.0513613224029541

Final encoder loss: 0.03435896709561348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25872039794921875 0.05144834518432617

Final encoder loss: 0.034891288727521896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2569105625152588 0.05160975456237793

Final encoder loss: 0.03543119505047798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25605154037475586 0.05185580253601074

Final encoder loss: 0.03490111604332924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541804313659668 0.051360368728637695

Final encoder loss: 0.03412943705916405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25676560401916504 0.05185127258300781

Final encoder loss: 0.03360483795404434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568228244781494 0.05156970024108887

Final encoder loss: 0.03327373042702675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2583656311035156 0.05156373977661133

Final encoder loss: 0.033414628356695175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2595086097717285 0.05285978317260742

Final encoder loss: 0.034420501440763474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25981950759887695 0.051703691482543945

Final encoder loss: 0.03362664952874184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25518226623535156 0.05432748794555664

Final encoder loss: 0.03228634595870972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25760936737060547 0.052689313888549805

Final encoder loss: 0.031699810177087784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.259563684463501 0.05212664604187012

Final encoder loss: 0.03129340335726738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588658332824707 0.051653385162353516

Final encoder loss: 0.03160834684967995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.256486177444458 0.054084062576293945

Final encoder loss: 0.03216401860117912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25853753089904785 0.0504460334777832

Final encoder loss: 0.0315077044069767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25486135482788086 0.05222773551940918

Final encoder loss: 0.03092106804251671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25924086570739746 0.05543637275695801

Final encoder loss: 0.03022739477455616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25752782821655273 0.052523136138916016

Final encoder loss: 0.02980540134012699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25772833824157715 0.05274796485900879

Final encoder loss: 0.030241472646594048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581808567047119 0.0516362190246582

Final encoder loss: 0.03121398575603962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25780582427978516 0.05345749855041504

Final encoder loss: 0.030287548899650574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539548873901367 0.05174684524536133

Final encoder loss: 0.029900193214416504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583613395690918 0.053284406661987305

Final encoder loss: 0.029635824263095856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25911569595336914 0.051993608474731445

Final encoder loss: 0.029189879074692726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570643424987793 0.052391767501831055

Final encoder loss: 0.029254091903567314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2582993507385254 0.05191946029663086

Final encoder loss: 0.030324064195156097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25972795486450195 0.050913095474243164

Final encoder loss: 0.029415443539619446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25403785705566406 0.05289340019226074

Final encoder loss: 0.029478298500180244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.256425142288208 0.05135941505432129

Final encoder loss: 0.02918504923582077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26071643829345703 0.05118131637573242

Final encoder loss: 0.02878887578845024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257671594619751 0.051026105880737305

Final encoder loss: 0.029037101194262505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25777387619018555 0.055520057678222656

Final encoder loss: 0.029886595904827118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25739312171936035 0.05385112762451172

Final encoder loss: 0.0289946086704731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562072277069092 0.0532536506652832

Final encoder loss: 0.028920503333210945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592480182647705 0.05587172508239746

Final encoder loss: 0.028653955087065697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25736212730407715 0.05182671546936035

Final encoder loss: 0.02821105159819126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576904296875 0.05148601531982422

Final encoder loss: 0.02841990254819393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.258195161819458 0.051971435546875

Final encoder loss: 0.02920953929424286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25732970237731934 0.05367922782897949

Final encoder loss: 0.028452575206756592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542898654937744 0.050751447677612305

Final encoder loss: 0.028691019862890244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578752040863037 0.05354499816894531

Final encoder loss: 0.028303783386945724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2587103843688965 0.05196118354797363

Final encoder loss: 0.027787981554865837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570626735687256 0.05269575119018555

Final encoder loss: 0.028133923187851906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25788164138793945 0.05247902870178223

Final encoder loss: 0.029129119589924812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2592923641204834 0.052607059478759766

Final encoder loss: 0.02821640856564045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555980682373047 0.05345892906188965

Final encoder loss: 0.028314730152487755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580084800720215 0.05115342140197754

Final encoder loss: 0.02796241082251072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2591261863708496 0.05173301696777344

Final encoder loss: 0.02747572772204876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2592043876647949 0.052631378173828125

Final encoder loss: 0.027703430503606796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573540210723877 0.05243539810180664

Final encoder loss: 0.028658825904130936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2586226463317871 0.05280303955078125

Final encoder loss: 0.027606427669525146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547023296356201 0.05164980888366699

Final encoder loss: 0.028106307610869408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25964784622192383 0.05594038963317871

Final encoder loss: 0.027894558385014534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25775623321533203 0.05182647705078125

Final encoder loss: 0.02740921452641487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25799012184143066 0.051483869552612305

Final encoder loss: 0.02754691056907177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590782642364502 0.05185079574584961

Final encoder loss: 0.02837246097624302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25700950622558594 0.052516937255859375

Final encoder loss: 0.02768627554178238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25409388542175293 0.05274009704589844

Final encoder loss: 0.027790267020463943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581460475921631 0.05243110656738281

Final encoder loss: 0.027576453983783722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25861048698425293 0.05182313919067383

Final encoder loss: 0.02710488624870777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570154666900635 0.05259585380554199

Final encoder loss: 0.027287831529974937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584846019744873 0.051587820053100586

Final encoder loss: 0.02818916365504265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574031352996826 0.05234050750732422

Final encoder loss: 0.027409203350543976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25609612464904785 0.053633928298950195

Final encoder loss: 0.027814524248242378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569406032562256 0.051990509033203125

Final encoder loss: 0.027355017140507698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583308219909668 0.05190420150756836

Final encoder loss: 0.027015957981348038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25942325592041016 0.051892757415771484

Final encoder loss: 0.0271811094135046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574634552001953 0.05260038375854492

Final encoder loss: 0.0280027873814106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25783276557922363 0.052587032318115234

Final encoder loss: 0.027266209945082664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25481367111206055 0.05163073539733887

Final encoder loss: 0.027537046000361443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588388919830322 0.05554771423339844

Final encoder loss: 0.0272066593170166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573974132537842 0.05263471603393555

Final encoder loss: 0.026767738163471222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575399875640869 0.052420616149902344

Final encoder loss: 0.02687775157392025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25885748863220215 0.052077293395996094

Final encoder loss: 0.027762779965996742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25731635093688965 0.05299496650695801

Final encoder loss: 0.02693326398730278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542915344238281 0.05091381072998047

Final encoder loss: 0.027415933087468147
Final encoder loss: 0.026595372706651688
Final encoder loss: 0.025520943105220795
Final encoder loss: 0.02479613572359085
Final encoder loss: 0.024710705503821373
Final encoder loss: 0.023146584630012512

Training emognition model
Final encoder loss: 0.03159554809351726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08130073547363281 0.2306385040283203

Final encoder loss: 0.029212523661740566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08124756813049316 0.2309722900390625

Final encoder loss: 0.03283515743843193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08167243003845215 0.23102760314941406

Final encoder loss: 0.03253905360852183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08185243606567383 0.23120617866516113

Final encoder loss: 0.030961142223436565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08152365684509277 0.23131513595581055

Final encoder loss: 0.032753634419368925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08174443244934082 0.23128533363342285

Final encoder loss: 0.031854888999150587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08140730857849121 0.2313220500946045

Final encoder loss: 0.031001407602241838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08241438865661621 0.2308809757232666

Final encoder loss: 0.03112559274657334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08098578453063965 0.23102116584777832

Final encoder loss: 0.032061515932068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08169341087341309 0.2305755615234375

Final encoder loss: 0.03233863257287785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08119368553161621 0.23096299171447754

Final encoder loss: 0.03164847841773343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0809319019317627 0.23075103759765625

Final encoder loss: 0.031164535130813898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08089303970336914 0.23103094100952148

Final encoder loss: 0.03238536315004414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08097386360168457 0.23087358474731445

Final encoder loss: 0.03061346535537997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0817711353302002 0.2310490608215332

Final encoder loss: 0.03112053599152264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08069419860839844 0.22988486289978027


Training emognition model
Final encoder loss: 0.19356612861156464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25051259994506836 0.04984569549560547

Final encoder loss: 0.1949499398469925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2480146884918213 0.04874062538146973

Final encoder loss: 0.08584347367286682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24761700630187988 0.048726558685302734

Final encoder loss: 0.0856441855430603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24880290031433105 0.0500035285949707

Final encoder loss: 0.055480167269706726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24994969367980957 0.04954981803894043

Final encoder loss: 0.05439362674951553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24729371070861816 0.05159592628479004

Final encoder loss: 0.042559631168842316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496018409729004 0.048619747161865234

Final encoder loss: 0.04182102903723717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2499861717224121 0.04836726188659668

Final encoder loss: 0.03623243048787117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24794411659240723 0.05069613456726074

Final encoder loss: 0.0357661247253418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24794435501098633 0.049469709396362305

Final encoder loss: 0.03274335712194443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24852991104125977 0.04821300506591797

Final encoder loss: 0.032548706978559494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24877333641052246 0.050170183181762695

Final encoder loss: 0.03101441264152527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24823260307312012 0.04929709434509277

Final encoder loss: 0.03088897280395031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24805355072021484 0.04897284507751465

Final encoder loss: 0.03028915822505951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25020670890808105 0.05053067207336426

Final encoder loss: 0.030329080298542976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463240623474121 0.048517465591430664

Final encoder loss: 0.03030000813305378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24950551986694336 0.04899191856384277

Final encoder loss: 0.030284976586699486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24789166450500488 0.04947090148925781

Final encoder loss: 0.03030780889093876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24926114082336426 0.049909353256225586

Final encoder loss: 0.030465712770819664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24789166450500488 0.04891061782836914

Final encoder loss: 0.03021838702261448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496480941772461 0.0479130744934082

Final encoder loss: 0.030372099950909615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24771356582641602 0.050725698471069336

Final encoder loss: 0.02985684759914875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494347095489502 0.048439741134643555

Final encoder loss: 0.030376892536878586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24954795837402344 0.04863905906677246

Final encoder loss: 0.02974816784262657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24798226356506348 0.05070161819458008

Final encoder loss: 0.030136588960886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24807119369506836 0.05090045928955078

Final encoder loss: 0.029685845598578453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482461929321289 0.048073768615722656

Final encoder loss: 0.030082562938332558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24701929092407227 0.05091500282287598

Final encoder loss: 0.029699668288230896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24870586395263672 0.04814624786376953

Final encoder loss: 0.029798325151205063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24743390083312988 0.04854178428649902

Final encoder loss: 0.029693445190787315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2505364418029785 0.051122188568115234

Final encoder loss: 0.029740141704678535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24708080291748047 0.04805731773376465

Final encoder loss: 0.029451332986354828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24875664710998535 0.04912257194519043

Final encoder loss: 0.02941388264298439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24763202667236328 0.048195600509643555

Final encoder loss: 0.0292049590498209
Final encoder loss: 0.028445081785321236

Training empatch model
Final encoder loss: 0.04840818798716092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07120418548583984 0.17417263984680176

Final encoder loss: 0.04333753018581953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0712897777557373 0.17391753196716309

Final encoder loss: 0.04172474597122007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07099366188049316 0.17417693138122559

Final encoder loss: 0.04497780774290805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07085847854614258 0.17383313179016113

Final encoder loss: 0.04183430447528731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07191109657287598 0.17381000518798828

Final encoder loss: 0.041358894974671774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0711827278137207 0.17412066459655762

Final encoder loss: 0.038814999851050275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07211065292358398 0.1745445728302002

Final encoder loss: 0.04073063133334592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07092809677124023 0.17371106147766113

Final encoder loss: 0.030619267919075283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07146739959716797 0.1744701862335205

Final encoder loss: 0.03055994443840956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07134294509887695 0.1736152172088623

Final encoder loss: 0.03195588180917781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07208728790283203 0.17438769340515137

Final encoder loss: 0.03068471015301872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07116079330444336 0.17374134063720703

Final encoder loss: 0.028992089258719673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07215046882629395 0.17386531829833984

Final encoder loss: 0.027200953251614382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07120394706726074 0.17378497123718262

Final encoder loss: 0.030040937139175154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07103586196899414 0.17400288581848145

Final encoder loss: 0.030000531364309035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07133889198303223 0.17419838905334473


Training empatch model
Final encoder loss: 0.17117862403392792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17297983169555664 0.04440569877624512

Final encoder loss: 0.07974856346845627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746692657470703 0.04358506202697754

Final encoder loss: 0.055698640644550323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1700296401977539 0.04331398010253906

Final encoder loss: 0.04432779923081398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17356061935424805 0.04316353797912598

Final encoder loss: 0.03788522258400917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1694347858428955 0.04409527778625488

Final encoder loss: 0.033940061926841736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1738591194152832 0.042749881744384766

Final encoder loss: 0.031340740621089935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16915321350097656 0.0430147647857666

Final encoder loss: 0.02964090183377266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17420744895935059 0.04244709014892578

Final encoder loss: 0.028460482135415077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1690382957458496 0.043039560317993164

Final encoder loss: 0.027692800387740135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17398381233215332 0.043526411056518555

Final encoder loss: 0.027196915820240974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16880559921264648 0.04261279106140137

Final encoder loss: 0.026919040828943253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17387819290161133 0.04281044006347656

Final encoder loss: 0.02675219066441059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.16910338401794434 0.04357171058654785

Final encoder loss: 0.02665817365050316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1740105152130127 0.04341459274291992

Final encoder loss: 0.02648364193737507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1694478988647461 0.0442662239074707

Final encoder loss: 0.026427706703543663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.174393892288208 0.0427403450012207

Final encoder loss: 0.026329299435019493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1703338623046875 0.0441133975982666

Final encoder loss: 0.026260243728756905

Training wesad model
Final encoder loss: 0.048213256787337996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07098507881164551 0.17398643493652344

Final encoder loss: 0.042610128898262996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07188177108764648 0.17387056350708008

Final encoder loss: 0.045141325875136654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07096457481384277 0.17401385307312012

Final encoder loss: 0.04120261593554396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07213521003723145 0.17435383796691895

Final encoder loss: 0.03225461818870292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07141399383544922 0.1730051040649414

Final encoder loss: 0.028751749047536548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0713655948638916 0.17442774772644043

Final encoder loss: 0.031871001544972834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07135581970214844 0.17392277717590332

Final encoder loss: 0.030219632086723985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07133078575134277 0.17454290390014648

Final encoder loss: 0.02508417988569816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0705869197845459 0.17367291450500488

Final encoder loss: 0.02308516752628542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07078099250793457 0.17410755157470703

Final encoder loss: 0.02476522128150747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07199788093566895 0.17370343208312988

Final encoder loss: 0.02375563315643304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07132816314697266 0.1740119457244873

Final encoder loss: 0.018791835063289096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07166814804077148 0.17472505569458008

Final encoder loss: 0.020539018428645518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07068872451782227 0.17389607429504395

Final encoder loss: 0.019815561470135135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07150626182556152 0.17438435554504395

Final encoder loss: 0.021725421709544213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0715482234954834 0.1737508773803711


Training wesad model
Final encoder loss: 0.21560512483119965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10694527626037598 0.03411722183227539

Final encoder loss: 0.09692908823490143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10416483879089355 0.03337383270263672

Final encoder loss: 0.06326014548540115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10451149940490723 0.03341364860534668

Final encoder loss: 0.046643417328596115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038212776184082 0.03328514099121094

Final encoder loss: 0.0372408926486969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10511541366577148 0.03354477882385254

Final encoder loss: 0.031602296978235245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356402397155762 0.0329439640045166

Final encoder loss: 0.028108876198530197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10374569892883301 0.032840728759765625

Final encoder loss: 0.02586270309984684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10350418090820312 0.03365468978881836

Final encoder loss: 0.024377930909395218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10467815399169922 0.03255486488342285

Final encoder loss: 0.02338993549346924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1033792495727539 0.0328068733215332

Final encoder loss: 0.022875988855957985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10420036315917969 0.033084869384765625

Final encoder loss: 0.022656654939055443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1046295166015625 0.033469438552856445

Final encoder loss: 0.02265046536922455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10402297973632812 0.032839298248291016

Final encoder loss: 0.022700797766447067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10455751419067383 0.03332948684692383

Final encoder loss: 0.022815866395831108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1045994758605957 0.03361701965332031

Final encoder loss: 0.02314002625644207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10551786422729492 0.03325462341308594

Final encoder loss: 0.023378923535346985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375595092773438 0.03341841697692871

Final encoder loss: 0.02331952378153801

Calculating loss for amigos model
	Full Pass 0.6405603885650635
numFreeParamsPath 18
Reconstruction loss values: 0.03195822238922119 0.041970569640398026

Calculating loss for dapper model
	Full Pass 0.15294146537780762
numFreeParamsPath 18
Reconstruction loss values: 0.027047669515013695 0.03081822395324707

Calculating loss for case model
	Full Pass 0.8569087982177734
numFreeParamsPath 18
Reconstruction loss values: 0.038498301059007645 0.04160214960575104

Calculating loss for emognition model
	Full Pass 0.28025007247924805
numFreeParamsPath 18
Reconstruction loss values: 0.04186374694108963 0.04956207051873207

Calculating loss for empatch model
	Full Pass 0.10547566413879395
numFreeParamsPath 18
Reconstruction loss values: 0.04407261684536934 0.05187268927693367

Calculating loss for wesad model
	Full Pass 0.07767152786254883
numFreeParamsPath 18
Reconstruction loss values: 0.045404672622680664 0.0638047531247139
Total loss calculation time: 4.226673603057861

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.625276803970337
Total epoch time: 175.15832018852234

Epoch: 35

Training emognition model
Final encoder loss: 0.04024156335187272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08912158012390137 0.2826650142669678

Final encoder loss: 0.039073801563619684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08299541473388672 0.2749485969543457

Final encoder loss: 0.03991590369448847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0847158432006836 0.27632927894592285

Final encoder loss: 0.03768352109395585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08417749404907227 0.2758190631866455

Final encoder loss: 0.039001281145834524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08351945877075195 0.27335262298583984

Final encoder loss: 0.03934348753810745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08256983757019043 0.2747530937194824

Final encoder loss: 0.038284673251024165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08307123184204102 0.273632287979126

Final encoder loss: 0.037944116586001594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08334779739379883 0.2730751037597656

Final encoder loss: 0.03743744920088483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08258318901062012 0.2732057571411133

Final encoder loss: 0.03835645196700744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0822446346282959 0.2739531993865967

Final encoder loss: 0.038496651463853865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08215928077697754 0.27330970764160156

Final encoder loss: 0.03761965964594659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08251714706420898 0.27294468879699707

Final encoder loss: 0.03606814230985685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08229732513427734 0.27886319160461426

Final encoder loss: 0.037740194134174856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08245396614074707 0.27475595474243164

Final encoder loss: 0.037603020600665865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08333921432495117 0.27504777908325195

Final encoder loss: 0.037279254612204514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08313679695129395 0.2742655277252197


Training case model
Final encoder loss: 0.03955151258311721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09100604057312012 0.2663004398345947

Final encoder loss: 0.03574505156624646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0912628173828125 0.26555657386779785

Final encoder loss: 0.03435204803489244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09202861785888672 0.26479172706604004

Final encoder loss: 0.03372370204530787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09141659736633301 0.26508235931396484

Final encoder loss: 0.0321767815152705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09181356430053711 0.2655797004699707

Final encoder loss: 0.032328908764107496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0922706127166748 0.26601624488830566

Final encoder loss: 0.03249736760284509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09295797348022461 0.26488590240478516

Final encoder loss: 0.03170793127703581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09241986274719238 0.2653803825378418

Final encoder loss: 0.030665601037152105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09177851676940918 0.2654407024383545

Final encoder loss: 0.030190355143821977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09157109260559082 0.26595139503479004

Final encoder loss: 0.031240994745297204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09139823913574219 0.2677614688873291

Final encoder loss: 0.030644327098972406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0913841724395752 0.26515674591064453

Final encoder loss: 0.030647997839511606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09109210968017578 0.2647402286529541

Final encoder loss: 0.029399535200095986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09089446067810059 0.2640700340270996

Final encoder loss: 0.0302252662413669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.0915217399597168 0.26494431495666504

Final encoder loss: 0.029816983904447918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08793067932128906 0.2600259780883789


Training amigos model
Final encoder loss: 0.0332226329366475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10782146453857422 0.38752150535583496

Final encoder loss: 0.03071851190654202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10739517211914062 0.3877832889556885

Final encoder loss: 0.028042615914095004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10738015174865723 0.3878920078277588

Final encoder loss: 0.031814187522306435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10733580589294434 0.38760995864868164

Final encoder loss: 0.030611584367674024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10778689384460449 0.388073205947876

Final encoder loss: 0.0338611033762526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1073617935180664 0.387908935546875

Final encoder loss: 0.032200511257990415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10764336585998535 0.3884456157684326

Final encoder loss: 0.03197523348951359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10833311080932617 0.38939452171325684

Final encoder loss: 0.03408906111299581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10843539237976074 0.38995957374572754

Final encoder loss: 0.029188960435884443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10795021057128906 0.38854336738586426

Final encoder loss: 0.029651867084862407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10826897621154785 0.38989949226379395

Final encoder loss: 0.032357048656749675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10775470733642578 0.38982415199279785

Final encoder loss: 0.032760554181384155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10814452171325684 0.3890399932861328

Final encoder loss: 0.031800465009275095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10871577262878418 0.38901615142822266

Final encoder loss: 0.028882565804588423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10828661918640137 0.38861608505249023

Final encoder loss: 0.030810212122038154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10390901565551758 0.38510894775390625


Training dapper model
Final encoder loss: 0.026380084901395946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06142854690551758 0.14810514450073242

Final encoder loss: 0.026050680363364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061960458755493164 0.15001487731933594

Final encoder loss: 0.027849483025407698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06168866157531738 0.15285563468933105

Final encoder loss: 0.026824422703957462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06357598304748535 0.15205740928649902

Final encoder loss: 0.025126328189675685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06373929977416992 0.15073466300964355

Final encoder loss: 0.024887860225228493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06170034408569336 0.1494739055633545

Final encoder loss: 0.027270612887086956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.061946868896484375 0.15009164810180664

Final encoder loss: 0.024479744028382197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061820268630981445 0.15000605583190918

Final encoder loss: 0.023688823019901734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061649322509765625 0.14991140365600586

Final encoder loss: 0.0224041659905419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06168508529663086 0.15033245086669922

Final encoder loss: 0.022181338024935902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06161046028137207 0.15009093284606934

Final encoder loss: 0.025588826215511454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061728715896606445 0.14966106414794922

Final encoder loss: 0.022533569363130886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.061517953872680664 0.15108156204223633

Final encoder loss: 0.023292191573922764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06333208084106445 0.1525743007659912

Final encoder loss: 0.024957211008672275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06305360794067383 0.15268182754516602

Final encoder loss: 0.028228984721207634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06306004524230957 0.15055441856384277


Training amigos model
Final encoder loss: 0.021752989869903677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10593962669372559 0.3416023254394531

Final encoder loss: 0.025516004202891292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10661864280700684 0.34161806106567383

Final encoder loss: 0.02421455055360447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10618185997009277 0.34134888648986816

Final encoder loss: 0.022125594902561798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.1061553955078125 0.3412771224975586

Final encoder loss: 0.024853483531946354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10734963417053223 0.3415334224700928

Final encoder loss: 0.02532817460537419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1066734790802002 0.3413426876068115

Final encoder loss: 0.022921915812551638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10592913627624512 0.34135866165161133

Final encoder loss: 0.02224188941384187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10604667663574219 0.3412024974822998

Final encoder loss: 0.023536065598561015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10596799850463867 0.3412461280822754

Final encoder loss: 0.023636405330485062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10633730888366699 0.34139156341552734

Final encoder loss: 0.022631876614837954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10599899291992188 0.34119367599487305

Final encoder loss: 0.02350625172223849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.1057744026184082 0.34158945083618164

Final encoder loss: 0.02448417607363551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1068730354309082 0.3419301509857178

Final encoder loss: 0.02238157841197737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.5568974018096924 0.34174442291259766

Final encoder loss: 0.0234611745064523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10622620582580566 0.34137439727783203

Final encoder loss: 0.02433266191886944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10083985328674316 0.3379998207092285


Training amigos model
Final encoder loss: 0.18077561259269714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44803905487060547 0.07451367378234863

Final encoder loss: 0.1878337413072586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45925211906433105 0.07666850090026855

Final encoder loss: 0.18364956974983215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.430147647857666 0.07372665405273438

Final encoder loss: 0.07430925965309143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4562501907348633 0.07397150993347168

Final encoder loss: 0.0756148025393486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.442781925201416 0.07370209693908691

Final encoder loss: 0.0694013386964798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44350171089172363 0.07409381866455078

Final encoder loss: 0.044188253581523895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45651769638061523 0.07480955123901367

Final encoder loss: 0.044529981911182404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44274044036865234 0.07415390014648438

Final encoder loss: 0.04180781543254852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4437375068664551 0.0776219367980957

Final encoder loss: 0.032753195613622665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4562723636627197 0.0732579231262207

Final encoder loss: 0.0332595556974411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44824934005737305 0.07468247413635254

Final encoder loss: 0.031864382326602936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4433591365814209 0.07389235496520996

Final encoder loss: 0.027691679075360298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4572482109069824 0.07434463500976562

Final encoder loss: 0.02827998623251915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4300076961517334 0.07491493225097656

Final encoder loss: 0.027409052476286888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4453449249267578 0.07352972030639648

Final encoder loss: 0.025592638179659843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4560394287109375 0.07476162910461426

Final encoder loss: 0.026103714480996132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4445979595184326 0.0776519775390625

Final encoder loss: 0.025556838139891624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44535160064697266 0.07349300384521484

Final encoder loss: 0.02540457807481289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4560399055480957 0.07494997978210449

Final encoder loss: 0.025517353788018227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44330453872680664 0.0736093521118164

Final encoder loss: 0.025311453267931938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44483494758605957 0.07578349113464355

Final encoder loss: 0.025621989741921425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4559147357940674 0.0736229419708252

Final encoder loss: 0.025475183501839638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44504570960998535 0.07376861572265625

Final encoder loss: 0.02536100149154663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4297604560852051 0.07532429695129395

Final encoder loss: 0.0248396098613739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.444408655166626 0.07359004020690918

Final encoder loss: 0.02484739013016224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45477890968322754 0.0713815689086914

Final encoder loss: 0.024907950311899185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43128538131713867 0.07455682754516602

Final encoder loss: 0.024325257167220116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44408273696899414 0.07141518592834473

Final encoder loss: 0.024319086223840714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45714855194091797 0.0753011703491211

Final encoder loss: 0.024282732978463173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4441838264465332 0.07324767112731934

Final encoder loss: 0.023966006934642792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4581878185272217 0.07527637481689453

Final encoder loss: 0.023691387847065926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44494199752807617 0.07545948028564453

Final encoder loss: 0.023822588846087456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4449613094329834 0.07443737983703613

Final encoder loss: 0.023778144270181656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4432830810546875 0.07329225540161133

Final encoder loss: 0.02349197305738926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45531797409057617 0.07435798645019531

Final encoder loss: 0.023758692666888237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4463491439819336 0.07456159591674805

Final encoder loss: 0.023255284875631332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45797014236450195 0.0738978385925293

Final encoder loss: 0.023102451115846634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4437429904937744 0.07376670837402344

Final encoder loss: 0.023380892351269722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44449520111083984 0.0739283561706543

Final encoder loss: 0.023198990151286125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46384549140930176 0.07606720924377441

Final encoder loss: 0.02312699519097805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4534468650817871 0.0756387710571289

Final encoder loss: 0.02314591221511364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4503817558288574 0.07573986053466797

Final encoder loss: 0.022899137809872627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47256040573120117 0.07327771186828613

Final encoder loss: 0.022691475227475166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4562981128692627 0.07558679580688477

Final encoder loss: 0.022847019135951996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4538571834564209 0.07892823219299316

Final encoder loss: 0.022741788998246193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46460437774658203 0.0757443904876709

Final encoder loss: 0.022411564365029335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45305633544921875 0.07620692253112793

Final encoder loss: 0.022860918194055557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4504818916320801 0.07629537582397461

Final encoder loss: 0.02271539717912674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47293877601623535 0.07379317283630371

Final encoder loss: 0.021962249651551247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45650482177734375 0.07435727119445801

Final encoder loss: 0.02265944518148899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4538602828979492 0.07846450805664062

Final encoder loss: 0.022617928683757782
Final encoder loss: 0.021213149651885033
Final encoder loss: 0.020519327372312546

Training dapper model
Final encoder loss: 0.01838315238878645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.061498165130615234 0.1070559024810791

Final encoder loss: 0.01968628583208676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059899091720581055 0.10786676406860352

Final encoder loss: 0.019468645788900698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059972524642944336 0.1075284481048584

Final encoder loss: 0.020391819958220363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06030988693237305 0.10705399513244629

Final encoder loss: 0.02049272857551255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060845375061035156 0.10816383361816406

Final encoder loss: 0.016766387606682275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0627293586730957 0.10699200630187988

Final encoder loss: 0.020473154445566803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059819698333740234 0.10779285430908203

Final encoder loss: 0.0183242308520043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05981183052062988 0.10766291618347168

Final encoder loss: 0.01794060249511076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059980154037475586 0.10734248161315918

Final encoder loss: 0.01997001878337126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06129956245422363 0.10822343826293945

Final encoder loss: 0.01578189950340739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06187891960144043 0.10807251930236816

Final encoder loss: 0.01943064430078932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.059561729431152344 0.1074519157409668

Final encoder loss: 0.017918159467683617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.060259103775024414 0.10779786109924316

Final encoder loss: 0.017565197508765302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06001472473144531 0.10719633102416992

Final encoder loss: 0.017860964560552212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06041598320007324 0.10864377021789551

Final encoder loss: 0.017495809992858047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.060721635818481445 0.10792255401611328


Training dapper model
Final encoder loss: 0.2024347484111786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11463332176208496 0.03435873985290527

Final encoder loss: 0.2082163393497467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11609148979187012 0.03412008285522461

Final encoder loss: 0.0812930017709732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11388373374938965 0.03470110893249512

Final encoder loss: 0.08285893499851227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11812210083007812 0.03601980209350586

Final encoder loss: 0.047907255589962006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11598968505859375 0.03416848182678223

Final encoder loss: 0.04766792431473732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11585092544555664 0.03415727615356445

Final encoder loss: 0.03330989554524422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.113433837890625 0.03471255302429199

Final encoder loss: 0.033173318952322006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11519956588745117 0.0345301628112793

Final encoder loss: 0.026084240525960922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11585783958435059 0.03551506996154785

Final encoder loss: 0.025956887751817703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11897134780883789 0.03419804573059082

Final encoder loss: 0.02217012830078602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11314105987548828 0.03480267524719238

Final encoder loss: 0.022023793309926987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11689186096191406 0.03415799140930176

Final encoder loss: 0.019904382526874542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11301231384277344 0.03448629379272461

Final encoder loss: 0.019921110942959785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11783170700073242 0.03551149368286133

Final encoder loss: 0.018607549369335175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11632704734802246 0.034745216369628906

Final encoder loss: 0.018685849383473396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11626386642456055 0.03461909294128418

Final encoder loss: 0.017974453046917915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11275529861450195 0.034822702407836914

Final encoder loss: 0.01798945665359497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11571907997131348 0.034148454666137695

Final encoder loss: 0.017721951007843018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1150505542755127 0.03505659103393555

Final encoder loss: 0.017649365589022636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11850452423095703 0.03388214111328125

Final encoder loss: 0.017764870077371597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11406159400939941 0.03465986251831055

Final encoder loss: 0.017736146226525307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11607503890991211 0.03455042839050293

Final encoder loss: 0.017980452626943588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11365318298339844 0.034845829010009766

Final encoder loss: 0.017888426780700684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.119171142578125 0.035277605056762695

Final encoder loss: 0.017615756019949913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11518359184265137 0.034223079681396484

Final encoder loss: 0.017919333651661873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11565542221069336 0.03442025184631348

Final encoder loss: 0.017470700666308403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11343240737915039 0.034381866455078125

Final encoder loss: 0.017649417743086815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11571550369262695 0.03429865837097168

Final encoder loss: 0.017319994047284126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11480236053466797 0.03482341766357422

Final encoder loss: 0.017396336421370506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1181793212890625 0.0342714786529541

Final encoder loss: 0.016727514564990997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11370491981506348 0.034288883209228516

Final encoder loss: 0.016881626099348068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11604070663452148 0.03459978103637695

Final encoder loss: 0.01641104370355606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11318230628967285 0.03423953056335449

Final encoder loss: 0.016383683308959007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1183779239654541 0.0347285270690918

Final encoder loss: 0.016387054696679115
Final encoder loss: 0.015440622344613075

Training case model
Final encoder loss: 0.028408893145143582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08971810340881348 0.21907734870910645

Final encoder loss: 0.02792858354057061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08928751945495605 0.21956872940063477

Final encoder loss: 0.027797151632986083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09026598930358887 0.21935129165649414

Final encoder loss: 0.02782348607031973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08927035331726074 0.21922063827514648

Final encoder loss: 0.0263887825803404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.089935302734375 0.21925711631774902

Final encoder loss: 0.026812753069600515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08919000625610352 0.21948695182800293

Final encoder loss: 0.02642460426428416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08927536010742188 0.21935701370239258

Final encoder loss: 0.026796722538732328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.09073996543884277 0.2193770408630371

Final encoder loss: 0.027058414300161034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08983206748962402 0.21924471855163574

Final encoder loss: 0.02583555141286366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08963990211486816 0.21940135955810547

Final encoder loss: 0.026983138483248542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09049844741821289 0.2195899486541748

Final encoder loss: 0.02715527905981238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08998727798461914 0.21917033195495605

Final encoder loss: 0.02648993893492237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0895087718963623 0.21935057640075684

Final encoder loss: 0.02680079747720181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09059333801269531 0.2194194793701172

Final encoder loss: 0.026412736248832048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08981084823608398 0.21918416023254395

Final encoder loss: 0.02676242267570125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08690667152404785 0.21692419052124023


Training case model
Final encoder loss: 0.20296147465705872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26290082931518555 0.05154561996459961

Final encoder loss: 0.18889868259429932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25943875312805176 0.0536656379699707

Final encoder loss: 0.1901583969593048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575523853302002 0.05269265174865723

Final encoder loss: 0.19217446446418762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26073360443115234 0.0535731315612793

Final encoder loss: 0.18081742525100708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575857639312744 0.05312657356262207

Final encoder loss: 0.19191978871822357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2572314739227295 0.05238771438598633

Final encoder loss: 0.10132130980491638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2691688537597656 0.052544355392456055

Final encoder loss: 0.09123671054840088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25989842414855957 0.0534520149230957

Final encoder loss: 0.08769449591636658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25864744186401367 0.052544355392456055

Final encoder loss: 0.08638374507427216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589712142944336 0.05405163764953613

Final encoder loss: 0.07874473184347153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25827622413635254 0.053836822509765625

Final encoder loss: 0.08208874613046646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25679612159729004 0.0530703067779541

Final encoder loss: 0.05955108255147934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25858116149902344 0.052814483642578125

Final encoder loss: 0.05408399552106857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26836395263671875 0.05520772933959961

Final encoder loss: 0.05232076719403267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588486671447754 0.05335378646850586

Final encoder loss: 0.0525803379714489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577064037322998 0.05434775352478027

Final encoder loss: 0.04938991740345955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26012396812438965 0.053473711013793945

Final encoder loss: 0.05127084255218506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560770511627197 0.05243182182312012

Final encoder loss: 0.043046195060014725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2702045440673828 0.05204963684082031

Final encoder loss: 0.040005963295698166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26871633529663086 0.05154609680175781

Final encoder loss: 0.038884371519088745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594587802886963 0.05393671989440918

Final encoder loss: 0.03956225886940956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2685072422027588 0.05212092399597168

Final encoder loss: 0.03843299299478531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591817378997803 0.05230093002319336

Final encoder loss: 0.03914841264486313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549304962158203 0.05155801773071289

Final encoder loss: 0.036394041031599045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26207900047302246 0.05185842514038086

Final encoder loss: 0.0348401740193367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573990821838379 0.05204057693481445

Final encoder loss: 0.03393164649605751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.260392427444458 0.05180978775024414

Final encoder loss: 0.03461609408259392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583456039428711 0.051766395568847656

Final encoder loss: 0.035081394016742706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26042842864990234 0.052228689193725586

Final encoder loss: 0.03484978526830673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25486087799072266 0.051981210708618164

Final encoder loss: 0.03397534415125847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2604963779449463 0.05570650100708008

Final encoder loss: 0.03313209488987923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2706153392791748 0.051909446716308594

Final encoder loss: 0.03274684026837349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26078009605407715 0.05621337890625

Final encoder loss: 0.032999202609062195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25859642028808594 0.05158066749572754

Final encoder loss: 0.033881936222314835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25931501388549805 0.054547786712646484

Final encoder loss: 0.03326862305402756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555677890777588 0.051851749420166016

Final encoder loss: 0.03211342170834541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26021361351013184 0.054152488708496094

Final encoder loss: 0.03118855319917202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584807872772217 0.0535430908203125

Final encoder loss: 0.030924838036298752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25886011123657227 0.053151607513427734

Final encoder loss: 0.031181100755929947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2588155269622803 0.051930904388427734

Final encoder loss: 0.03189405798912048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25769805908203125 0.05550384521484375

Final encoder loss: 0.03133435174822807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25466465950012207 0.05275678634643555

Final encoder loss: 0.030509155243635178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25952863693237305 0.05304551124572754

Final encoder loss: 0.029919389635324478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583122253417969 0.0526583194732666

Final encoder loss: 0.029421530663967133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2603566646575928 0.053720712661743164

Final encoder loss: 0.029653072357177734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2566518783569336 0.05459308624267578

Final encoder loss: 0.030698370188474655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26895833015441895 0.051560401916503906

Final encoder loss: 0.03006180003285408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534337043762207 0.051671504974365234

Final encoder loss: 0.029682187363505363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566511631011963 0.05095410346984863

Final encoder loss: 0.029137441888451576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26660680770874023 0.051120758056640625

Final encoder loss: 0.028811873868107796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25673985481262207 0.05185389518737793

Final encoder loss: 0.029076477512717247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.256547212600708 0.05164003372192383

Final encoder loss: 0.030021637678146362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25575923919677734 0.05193161964416504

Final encoder loss: 0.029184848070144653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25307703018188477 0.05148172378540039

Final encoder loss: 0.029285969212651253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25664496421813965 0.052115440368652344

Final encoder loss: 0.02893184684216976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2595193386077881 0.05326199531555176

Final encoder loss: 0.028423499315977097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2687184810638428 0.05340099334716797

Final encoder loss: 0.028499038890004158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25710225105285645 0.052262306213378906

Final encoder loss: 0.029607921838760376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26787519454956055 0.05234503746032715

Final encoder loss: 0.028744513168931007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25413966178894043 0.05111360549926758

Final encoder loss: 0.02857913263142109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581508159637451 0.0536189079284668

Final encoder loss: 0.02823645807802677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25750112533569336 0.05247378349304199

Final encoder loss: 0.027881184592843056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2578744888305664 0.05316638946533203

Final encoder loss: 0.027931613847613335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25652432441711426 0.05185079574584961

Final encoder loss: 0.02885850891470909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576942443847656 0.05101752281188965

Final encoder loss: 0.028164410963654518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25450873374938965 0.052004098892211914

Final encoder loss: 0.028366299346089363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25647950172424316 0.05283188819885254

Final encoder loss: 0.028009258210659027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25760865211486816 0.05373787879943848

Final encoder loss: 0.027423590421676636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26684045791625977 0.052396535873413086

Final encoder loss: 0.027518657967448235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579665184020996 0.05291342735290527

Final encoder loss: 0.02861933968961239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25701260566711426 0.05213117599487305

Final encoder loss: 0.028041431680321693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538752555847168 0.05256175994873047

Final encoder loss: 0.02797868475317955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2575714588165283 0.05244255065917969

Final encoder loss: 0.027638740837574005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2673659324645996 0.05144548416137695

Final encoder loss: 0.02728719636797905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2571566104888916 0.053109169006347656

Final encoder loss: 0.027338597923517227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575724124908447 0.05134010314941406

Final encoder loss: 0.02819434553384781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25766539573669434 0.05290937423706055

Final encoder loss: 0.02740364708006382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537384033203125 0.052794456481933594

Final encoder loss: 0.027811642736196518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25608396530151367 0.051575660705566406

Final encoder loss: 0.02749980054795742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25671839714050293 0.05172610282897949

Final encoder loss: 0.027043448761105537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25536561012268066 0.05168795585632324

Final encoder loss: 0.027148505672812462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635623931884766 0.05190896987915039

Final encoder loss: 0.028100425377488136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2667419910430908 0.05213570594787598

Final encoder loss: 0.027450958266854286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2525506019592285 0.05175518989562988

Final encoder loss: 0.0274954941123724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2567291259765625 0.05171823501586914

Final encoder loss: 0.027192121371626854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26636552810668945 0.05294346809387207

Final encoder loss: 0.026812920346856117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25747203826904297 0.05212283134460449

Final encoder loss: 0.02681714855134487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2560265064239502 0.051869869232177734

Final encoder loss: 0.02779994159936905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25895214080810547 0.0528416633605957

Final encoder loss: 0.027064630761742592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546660900115967 0.0516357421875

Final encoder loss: 0.027418989688158035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2605128288269043 0.05197739601135254

Final encoder loss: 0.02720734477043152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571418285369873 0.05352425575256348

Final encoder loss: 0.02655695751309395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2679932117462158 0.05149984359741211

Final encoder loss: 0.026665739715099335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580373287200928 0.052065372467041016

Final encoder loss: 0.02764877676963806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25885510444641113 0.051409244537353516

Final encoder loss: 0.027097409591078758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25473952293395996 0.054380178451538086

Final encoder loss: 0.027107752859592438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26827120780944824 0.05244278907775879

Final encoder loss: 0.026834357529878616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690107822418213 0.052161216735839844

Final encoder loss: 0.026511240750551224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25928425788879395 0.05387759208679199

Final encoder loss: 0.02653598226606846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2585022449493408 0.05241727828979492

Final encoder loss: 0.027366021648049355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26849889755249023 0.05223488807678223

Final encoder loss: 0.02665535733103752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25689125061035156 0.05188870429992676

Final encoder loss: 0.027163071557879448
Final encoder loss: 0.02620452269911766
Final encoder loss: 0.025125231593847275
Final encoder loss: 0.024396590888500214
Final encoder loss: 0.024374766275286674
Final encoder loss: 0.022839495912194252

Training emognition model
Final encoder loss: 0.03208908560958332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.0823662281036377 0.23112988471984863

Final encoder loss: 0.031188440442000556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0813283920288086 0.2307119369506836

Final encoder loss: 0.030850959225068857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08275914192199707 0.23092412948608398

Final encoder loss: 0.03074870967510454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08108019828796387 0.230926513671875

Final encoder loss: 0.031538048476249136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08294057846069336 0.23070597648620605

Final encoder loss: 0.031136434424905445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08131670951843262 0.23138117790222168

Final encoder loss: 0.030260287497252885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08246707916259766 0.23067021369934082

Final encoder loss: 0.030643070093604084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08131718635559082 0.23080897331237793

Final encoder loss: 0.029857265057140155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08135437965393066 0.23075079917907715

Final encoder loss: 0.031167037755789995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08172154426574707 0.23066186904907227

Final encoder loss: 0.030963486808816436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08155655860900879 0.23090648651123047

Final encoder loss: 0.03023970188409338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08167338371276855 0.2311093807220459

Final encoder loss: 0.03000732889604585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08137822151184082 0.23043203353881836

Final encoder loss: 0.03227423042120507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.0822899341583252 0.23116087913513184

Final encoder loss: 0.030807365278424065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08136892318725586 0.2311415672302246

Final encoder loss: 0.029360523794965447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0813453197479248 0.23057985305786133


Training emognition model
Final encoder loss: 0.19356022775173187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25256967544555664 0.05011463165283203

Final encoder loss: 0.1949579119682312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24698400497436523 0.04941678047180176

Final encoder loss: 0.08551650494337082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2493736743927002 0.04881596565246582

Final encoder loss: 0.08504500985145569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2490859031677246 0.051804304122924805

Final encoder loss: 0.05520061403512955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24882769584655762 0.0493621826171875

Final encoder loss: 0.0539034865796566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24831676483154297 0.04939723014831543

Final encoder loss: 0.04231536015868187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2492523193359375 0.04807114601135254

Final encoder loss: 0.041426558047533035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477409839630127 0.049405574798583984

Final encoder loss: 0.035972416400909424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24791646003723145 0.04813361167907715

Final encoder loss: 0.03537416085600853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2482454776763916 0.04796934127807617

Final encoder loss: 0.03258848935365677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24727129936218262 0.05118298530578613

Final encoder loss: 0.03209775313735008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474677562713623 0.04874992370605469

Final encoder loss: 0.03086840733885765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24982380867004395 0.049092769622802734

Final encoder loss: 0.03043447434902191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24718189239501953 0.05037689208984375

Final encoder loss: 0.03022342175245285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24818682670593262 0.049463748931884766

Final encoder loss: 0.029864072799682617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475264072418213 0.050693511962890625

Final encoder loss: 0.030018072575330734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496662139892578 0.050522565841674805

Final encoder loss: 0.029825959354639053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24698448181152344 0.04897022247314453

Final encoder loss: 0.02979322336614132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24948644638061523 0.04882693290710449

Final encoder loss: 0.03002270683646202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24854755401611328 0.050846099853515625

Final encoder loss: 0.02977326326072216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24808096885681152 0.048061370849609375

Final encoder loss: 0.029768209904432297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2490227222442627 0.04960203170776367

Final encoder loss: 0.02964501455426216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24876093864440918 0.049407005310058594

Final encoder loss: 0.02971860021352768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247100830078125 0.04947161674499512

Final encoder loss: 0.029398327693343163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485358715057373 0.04871225357055664

Final encoder loss: 0.029490461573004723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24854731559753418 0.04885053634643555

Final encoder loss: 0.02940337173640728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24869179725646973 0.05016350746154785

Final encoder loss: 0.02952934242784977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24833893775939941 0.049458980560302734

Final encoder loss: 0.02924305573105812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24940228462219238 0.04921364784240723

Final encoder loss: 0.0292491614818573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24623632431030273 0.05149102210998535

Final encoder loss: 0.02914910390973091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25006961822509766 0.04984140396118164

Final encoder loss: 0.029201464727520943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24802088737487793 0.0499272346496582

Final encoder loss: 0.028997201472520828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24847841262817383 0.05088329315185547

Final encoder loss: 0.02898341789841652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24797487258911133 0.048616647720336914

Final encoder loss: 0.02880639210343361
Final encoder loss: 0.02783317118883133

Training empatch model
Final encoder loss: 0.04536262559607377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07227158546447754 0.17411279678344727

Final encoder loss: 0.04308306947406103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0716712474822998 0.17369842529296875

Final encoder loss: 0.04201761633596559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07097721099853516 0.17440485954284668

Final encoder loss: 0.04081831705332503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07106375694274902 0.17378449440002441

Final encoder loss: 0.0420620182226059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07114291191101074 0.17384910583496094

Final encoder loss: 0.037565970353504646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07157158851623535 0.17353057861328125

Final encoder loss: 0.04300867695797292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07116961479187012 0.17404818534851074

Final encoder loss: 0.03732765370227652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07241964340209961 0.1737358570098877

Final encoder loss: 0.027923782143006042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07137417793273926 0.1737372875213623

Final encoder loss: 0.02997852084742699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07118940353393555 0.17420268058776855

Final encoder loss: 0.029274213204017917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07130002975463867 0.1738452911376953

Final encoder loss: 0.027437923002524903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07128047943115234 0.1740717887878418

Final encoder loss: 0.03021792785783041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07242417335510254 0.1738584041595459

Final encoder loss: 0.028275964179275588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07131838798522949 0.17381572723388672

Final encoder loss: 0.030825027900035624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07221746444702148 0.17412447929382324

Final encoder loss: 0.02957496588602504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07101321220397949 0.17404937744140625


Training empatch model
Final encoder loss: 0.1711723506450653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1778552532196045 0.04288339614868164

Final encoder loss: 0.07976410537958145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17458486557006836 0.04301881790161133

Final encoder loss: 0.05554994195699692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1759779453277588 0.044460296630859375

Final encoder loss: 0.044030893594026566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17423033714294434 0.043399810791015625

Final encoder loss: 0.03749028965830803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761171817779541 0.044362545013427734

Final encoder loss: 0.033470120280981064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1740865707397461 0.04343843460083008

Final encoder loss: 0.0308705922216177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754777431488037 0.04423689842224121

Final encoder loss: 0.029144510626792908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17472076416015625 0.0435788631439209

Final encoder loss: 0.028033021837472916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17589592933654785 0.04390573501586914

Final encoder loss: 0.02731495536863804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17403244972229004 0.04386472702026367

Final encoder loss: 0.02688921056687832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17512130737304688 0.044814348220825195

Final encoder loss: 0.02655881457030773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17522764205932617 0.04356956481933594

Final encoder loss: 0.026334742084145546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17422103881835938 0.04446268081665039

Final encoder loss: 0.026101933792233467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1751997470855713 0.04357767105102539

Final encoder loss: 0.025964008644223213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17527437210083008 0.04462885856628418

Final encoder loss: 0.025864064693450928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745164394378662 0.043221235275268555

Final encoder loss: 0.025693489238619804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17496275901794434 0.044623374938964844

Final encoder loss: 0.025732148438692093

Training wesad model
Final encoder loss: 0.048610350410315896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0716867446899414 0.17361736297607422

Final encoder loss: 0.04585458679981295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07156252861022949 0.17414307594299316

Final encoder loss: 0.04258988549504872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07140707969665527 0.1741480827331543

Final encoder loss: 0.04312020899555322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07133889198303223 0.17416143417358398

Final encoder loss: 0.030181947163616885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07217717170715332 0.17391753196716309

Final encoder loss: 0.030848500130672713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0713355541229248 0.1736891269683838

Final encoder loss: 0.030718206054837605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07226300239562988 0.17415714263916016

Final encoder loss: 0.030136112636439366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07159876823425293 0.17342138290405273

Final encoder loss: 0.024374423078288986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07110071182250977 0.1746213436126709

Final encoder loss: 0.023922372822402616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07097482681274414 0.17351579666137695

Final encoder loss: 0.02259669629384251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07105183601379395 0.173614501953125

Final encoder loss: 0.023871107274580857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07195734977722168 0.17400431632995605

Final encoder loss: 0.018808221950184674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07148981094360352 0.17380070686340332

Final encoder loss: 0.019890319589273515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07244563102722168 0.17444276809692383

Final encoder loss: 0.020341625611404752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07132220268249512 0.17392325401306152

Final encoder loss: 0.020139356787017084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07101130485534668 0.17400741577148438


Training wesad model
Final encoder loss: 0.21557877957820892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10583305358886719 0.0334172248840332

Final encoder loss: 0.09708867222070694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10445761680603027 0.033788442611694336

Final encoder loss: 0.06327157467603683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10462427139282227 0.033544301986694336

Final encoder loss: 0.046676624566316605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10578584671020508 0.033162593841552734

Final encoder loss: 0.03723384812474251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10405874252319336 0.033409833908081055

Final encoder loss: 0.031547099351882935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10425972938537598 0.03343677520751953

Final encoder loss: 0.027979286387562752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10437393188476562 0.033852577209472656

Final encoder loss: 0.025680480524897575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10549736022949219 0.033254384994506836

Final encoder loss: 0.024175943806767464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10461068153381348 0.033637285232543945

Final encoder loss: 0.023221254348754883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10485506057739258 0.03332209587097168

Final encoder loss: 0.022714674472808838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10485243797302246 0.03429055213928223

Final encoder loss: 0.022592296823859215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10449433326721191 0.034238576889038086

Final encoder loss: 0.022588297724723816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10443711280822754 0.03304290771484375

Final encoder loss: 0.0225167665630579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10448360443115234 0.033014774322509766

Final encoder loss: 0.022430885583162308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10533523559570312 0.03380870819091797

Final encoder loss: 0.0223627258092165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10496711730957031 0.03332853317260742

Final encoder loss: 0.022493332624435425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10531115531921387 0.03391432762145996

Final encoder loss: 0.02283293940126896

Calculating loss for amigos model
	Full Pass 0.6564035415649414
numFreeParamsPath 18
Reconstruction loss values: 0.031766898930072784 0.041722942143678665

Calculating loss for dapper model
	Full Pass 0.15305018424987793
numFreeParamsPath 18
Reconstruction loss values: 0.025465102866292 0.029466576874256134

Calculating loss for case model
	Full Pass 0.8574850559234619
numFreeParamsPath 18
Reconstruction loss values: 0.038614265620708466 0.04173767566680908

Calculating loss for emognition model
	Full Pass 0.28112173080444336
numFreeParamsPath 18
Reconstruction loss values: 0.04170369356870651 0.049208518117666245

Calculating loss for empatch model
	Full Pass 0.10503292083740234
numFreeParamsPath 18
Reconstruction loss values: 0.043434176594018936 0.05025719478726387

Calculating loss for wesad model
	Full Pass 0.07835102081298828
numFreeParamsPath 18
Reconstruction loss values: 0.044296469539403915 0.06520697474479675
Total loss calculation time: 3.788867712020874

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.099001169204712
Total epoch time: 176.15616297721863

Epoch: 36

Training dapper model
Final encoder loss: 0.024155689371643148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06721186637878418 0.15882086753845215

Final encoder loss: 0.024432628617531718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06281328201293945 0.15203261375427246

Final encoder loss: 0.02405188756845166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062297821044921875 0.15089988708496094

Final encoder loss: 0.02505617357577729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.0630190372467041 0.1509871482849121

Final encoder loss: 0.02335851690144057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06263160705566406 0.15127301216125488

Final encoder loss: 0.021111072159026134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06242203712463379 0.15156269073486328

Final encoder loss: 0.023079858619362726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06522011756896973 0.15088963508605957

Final encoder loss: 0.026223196228635743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06194043159484863 0.15014147758483887

Final encoder loss: 0.023263119713057445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06248927116394043 0.15190982818603516

Final encoder loss: 0.022835450789060886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06182146072387695 0.15116643905639648

Final encoder loss: 0.022967665999931934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06203031539916992 0.15093517303466797

Final encoder loss: 0.02123540271134374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06334185600280762 0.15163040161132812

Final encoder loss: 0.022590579034857616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06223797798156738 0.1511983871459961

Final encoder loss: 0.023316453054400745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.0621485710144043 0.15090465545654297

Final encoder loss: 0.021448660969797983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06313586235046387 0.15025854110717773

Final encoder loss: 0.025779309163100568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.0619351863861084 0.1496279239654541


Training amigos model
Final encoder loss: 0.030188631332721132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10886764526367188 0.3902890682220459

Final encoder loss: 0.032056790607742504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10830545425415039 0.3895289897918701

Final encoder loss: 0.03143715944311681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10814118385314941 0.38903212547302246

Final encoder loss: 0.02959218604539815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10841822624206543 0.3901495933532715

Final encoder loss: 0.033715145194064565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10880088806152344 0.39042186737060547

Final encoder loss: 0.031144941238040517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10965490341186523 0.3907790184020996

Final encoder loss: 0.03202400040925363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10920190811157227 0.3899216651916504

Final encoder loss: 0.03030964535247184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10794687271118164 0.3878006935119629

Final encoder loss: 0.032040500732811894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10748934745788574 0.38787221908569336

Final encoder loss: 0.0302085419875424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10797977447509766 0.38829636573791504

Final encoder loss: 0.028297872178082932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.1076359748840332 0.39086294174194336

Final encoder loss: 0.03226837383760391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10840415954589844 0.3875617980957031

Final encoder loss: 0.030252805408198136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10783100128173828 0.3874802589416504

Final encoder loss: 0.03110398247941776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1082305908203125 0.3880152702331543

Final encoder loss: 0.02906457090255857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.1078958511352539 0.3888356685638428

Final encoder loss: 0.03179442950701581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10345339775085449 0.3841540813446045


Training case model
Final encoder loss: 0.0394480680043462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09150958061218262 0.265399694442749

Final encoder loss: 0.03665490948431691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09130978584289551 0.26500558853149414

Final encoder loss: 0.034389512542852135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09156584739685059 0.26593470573425293

Final encoder loss: 0.03386391459251274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09137344360351562 0.26723599433898926

Final encoder loss: 0.03208384503909134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09163331985473633 0.2651050090789795

Final encoder loss: 0.03199359860501841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0920557975769043 0.26588916778564453

Final encoder loss: 0.03268844013290653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0914146900177002 0.26554298400878906

Final encoder loss: 0.03155423801829913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09134459495544434 0.26571011543273926

Final encoder loss: 0.03141397183843046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0922400951385498 0.2667086124420166

Final encoder loss: 0.031019780874902914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09170031547546387 0.26489877700805664

Final encoder loss: 0.030725986529010204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09095358848571777 0.2650947570800781

Final encoder loss: 0.030863633903740466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09210658073425293 0.2661569118499756

Final encoder loss: 0.02975645741741208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09131479263305664 0.26485228538513184

Final encoder loss: 0.02967158997424334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09132218360900879 0.2661163806915283

Final encoder loss: 0.029994293526875493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09246039390563965 0.2664828300476074

Final encoder loss: 0.03031955859651897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08844304084777832 0.262312650680542


Training emognition model
Final encoder loss: 0.0418165154794448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08334064483642578 0.27578163146972656

Final encoder loss: 0.0388914117245451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08430171012878418 0.2760462760925293

Final encoder loss: 0.04283293914125922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08353781700134277 0.27542948722839355

Final encoder loss: 0.03937563139831681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08353018760681152 0.2748708724975586

Final encoder loss: 0.03844014965231492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08416056632995605 0.2765021324157715

Final encoder loss: 0.03888402177887541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0836784839630127 0.2757594585418701

Final encoder loss: 0.03628425760377649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08348560333251953 0.27588915824890137

Final encoder loss: 0.03830384479219433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08356237411499023 0.27524638175964355

Final encoder loss: 0.03757949032183632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08365321159362793 0.27593398094177246

Final encoder loss: 0.03871813820357498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08420252799987793 0.2755582332611084

Final encoder loss: 0.039970843803618164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0835726261138916 0.27706289291381836

Final encoder loss: 0.03528970325424002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08400082588195801 0.2759737968444824

Final encoder loss: 0.03770188675127047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08436203002929688 0.27520132064819336

Final encoder loss: 0.038904071025816514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08351731300354004 0.2766115665435791

Final encoder loss: 0.03766177152257397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0836796760559082 0.2753574848175049

Final encoder loss: 0.03648653543589162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08816194534301758 0.2746925354003906


Training amigos model
Final encoder loss: 0.022951967602857278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10668635368347168 0.34189701080322266

Final encoder loss: 0.0239342146644513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10628819465637207 0.34175729751586914

Final encoder loss: 0.021616079059961865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1064143180847168 0.3415377140045166

Final encoder loss: 0.023218105999578034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10647201538085938 0.3419167995452881

Final encoder loss: 0.02226897541054286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10661578178405762 0.3413991928100586

Final encoder loss: 0.02153292434611275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10712695121765137 0.3419029712677002

Final encoder loss: 0.023767243256438694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10638666152954102 0.341702938079834

Final encoder loss: 0.02295872461489646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10642695426940918 0.34171438217163086

Final encoder loss: 0.02286481869305638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10661077499389648 0.34174180030822754

Final encoder loss: 0.02282624687677533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10609841346740723 0.34181809425354004

Final encoder loss: 0.024668809657283203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10708069801330566 0.34177589416503906

Final encoder loss: 0.023466892839923183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10723304748535156 0.3417844772338867

Final encoder loss: 0.02366429190187395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10642886161804199 0.34178924560546875

Final encoder loss: 0.02294827326181973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10646915435791016 0.34155702590942383

Final encoder loss: 0.022973869172545392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10555839538574219 0.3408041000366211

Final encoder loss: 0.024631492069691577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.11773872375488281 0.3368816375732422


Training amigos model
Final encoder loss: 0.1807679384946823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46308279037475586 0.0740964412689209

Final encoder loss: 0.1878281682729721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4602329730987549 0.07538485527038574

Final encoder loss: 0.18364191055297852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45624637603759766 0.07499337196350098

Final encoder loss: 0.0746920257806778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.461423397064209 0.0727534294128418

Final encoder loss: 0.07639744877815247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4726529121398926 0.07675671577453613

Final encoder loss: 0.07054115831851959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46997761726379395 0.07418274879455566

Final encoder loss: 0.04407801106572151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4709169864654541 0.07665252685546875

Final encoder loss: 0.044559743255376816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47022175788879395 0.07717347145080566

Final encoder loss: 0.041911397129297256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46781349182128906 0.07865023612976074

Final encoder loss: 0.03247861936688423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47116518020629883 0.0809333324432373

Final encoder loss: 0.032944101840257645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4691760540008545 0.07439756393432617

Final encoder loss: 0.031706515699625015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46886134147644043 0.07325959205627441

Final encoder loss: 0.027330409735441208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4718172550201416 0.07394123077392578

Final encoder loss: 0.027857493609189987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46029186248779297 0.07379841804504395

Final encoder loss: 0.027129683643579483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4566359519958496 0.07465386390686035

Final encoder loss: 0.025195889174938202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46030640602111816 0.07478213310241699

Final encoder loss: 0.025678100064396858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4584507942199707 0.07342004776000977

Final encoder loss: 0.025219034403562546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4568147659301758 0.07733368873596191

Final encoder loss: 0.02487695962190628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4680609703063965 0.07565069198608398

Final encoder loss: 0.02516625076532364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47072458267211914 0.07571268081665039

Final encoder loss: 0.02478622831404209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46351099014282227 0.07442498207092285

Final encoder loss: 0.02458280697464943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4700758457183838 0.07565665245056152

Final encoder loss: 0.024962451308965683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46744871139526367 0.07225394248962402

Final encoder loss: 0.02473294548690319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4658682346343994 0.07748770713806152

Final encoder loss: 0.02427825704216957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45976901054382324 0.07528829574584961

Final encoder loss: 0.024333830922842026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47673821449279785 0.0736234188079834

Final encoder loss: 0.02429097332060337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46483302116394043 0.07709312438964844

Final encoder loss: 0.02343316748738289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4650731086730957 0.07556557655334473

Final encoder loss: 0.023458464071154594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47136664390563965 0.0779571533203125

Final encoder loss: 0.02373492531478405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45828819274902344 0.08066606521606445

Final encoder loss: 0.023238785564899445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46329665184020996 0.07339620590209961

Final encoder loss: 0.023106269538402557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46167683601379395 0.0789027214050293

Final encoder loss: 0.023330355063080788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45821332931518555 0.07441115379333496

Final encoder loss: 0.023116398602724075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.459761381149292 0.07521224021911621

Final encoder loss: 0.02316313050687313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4607698917388916 0.07301044464111328

Final encoder loss: 0.02309417724609375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4570424556732178 0.07413101196289062

Final encoder loss: 0.022783951833844185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46932220458984375 0.07520914077758789

Final encoder loss: 0.022843964397907257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4582858085632324 0.0728464126586914

Final encoder loss: 0.022874560207128525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45917463302612305 0.07402467727661133

Final encoder loss: 0.022323956713080406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.460066556930542 0.07709741592407227

Final encoder loss: 0.02240658551454544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4596259593963623 0.0744171142578125

Final encoder loss: 0.02272554486989975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4578878879547119 0.07454133033752441

Final encoder loss: 0.0222732312977314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46117305755615234 0.07283926010131836

Final encoder loss: 0.02204265259206295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45886802673339844 0.07430815696716309

Final encoder loss: 0.022454790771007538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4563605785369873 0.07343602180480957

Final encoder loss: 0.02228548750281334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4616520404815674 0.07481026649475098

Final encoder loss: 0.02198001742362976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4593238830566406 0.07225561141967773

Final encoder loss: 0.02233424223959446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45630860328674316 0.07410287857055664

Final encoder loss: 0.022200198844075203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4617905616760254 0.07569599151611328

Final encoder loss: 0.02201872132718563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46024560928344727 0.07572150230407715

Final encoder loss: 0.022359197959303856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4570012092590332 0.07373738288879395

Final encoder loss: 0.02172137051820755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4605751037597656 0.07439589500427246

Final encoder loss: 0.021937785670161247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4593331813812256 0.07354092597961426

Final encoder loss: 0.022024719044566154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45621585845947266 0.07624053955078125

Final encoder loss: 0.02171259932219982
Final encoder loss: 0.02043045312166214
Final encoder loss: 0.020008284598588943

Training dapper model
Final encoder loss: 0.020116478633299895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05981850624084473 0.1069650650024414

Final encoder loss: 0.021549878186699976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.07183408737182617 0.10617709159851074

Final encoder loss: 0.018101654104452695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05949211120605469 0.10712695121765137

Final encoder loss: 0.018900451221600615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059606313705444336 0.10605311393737793

Final encoder loss: 0.017397533476486225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05886220932006836 0.10783886909484863

Final encoder loss: 0.017979268175592866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0599055290222168 0.10599565505981445

Final encoder loss: 0.0192154600302993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05902671813964844 0.10610151290893555

Final encoder loss: 0.01839331169237592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05970406532287598 0.10669493675231934

Final encoder loss: 0.019335816428606305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.0594332218170166 0.1062164306640625

Final encoder loss: 0.017874301048876558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.059296607971191406 0.10703039169311523

Final encoder loss: 0.018896794653939817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05939888954162598 0.1063232421875

Final encoder loss: 0.018198093736082537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05929422378540039 0.10667634010314941

Final encoder loss: 0.016913819719769827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05955648422241211 0.10646677017211914

Final encoder loss: 0.016723879521608934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05961346626281738 0.1057894229888916

Final encoder loss: 0.018084225362826223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.059587955474853516 0.10634231567382812

Final encoder loss: 0.017443735056741384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05909919738769531 0.10676836967468262


Training dapper model
Final encoder loss: 0.20243024826049805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11799120903015137 0.03399252891540527

Final encoder loss: 0.20818741619586945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157679557800293 0.03411364555358887

Final encoder loss: 0.08163245767354965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11560535430908203 0.03411149978637695

Final encoder loss: 0.083136647939682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11500859260559082 0.03373003005981445

Final encoder loss: 0.048270083963871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11578607559204102 0.034242868423461914

Final encoder loss: 0.04798245429992676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563587188720703 0.03406190872192383

Final encoder loss: 0.03346562758088112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11502599716186523 0.03408646583557129

Final encoder loss: 0.03334348276257515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11634349822998047 0.03436636924743652

Final encoder loss: 0.026049133390188217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11540031433105469 0.034178733825683594

Final encoder loss: 0.02605406753718853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11620736122131348 0.03430318832397461

Final encoder loss: 0.02203850820660591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11857128143310547 0.03448653221130371

Final encoder loss: 0.02204359695315361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11554503440856934 0.03432631492614746

Final encoder loss: 0.019733190536499023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11501598358154297 0.03414034843444824

Final encoder loss: 0.019879665225744247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11576724052429199 0.03385567665100098

Final encoder loss: 0.01843409053981304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11528587341308594 0.03365039825439453

Final encoder loss: 0.01862999051809311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11511015892028809 0.0338747501373291

Final encoder loss: 0.017799578607082367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11594128608703613 0.03426337242126465

Final encoder loss: 0.01785566844046116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11501145362854004 0.034198760986328125

Final encoder loss: 0.01750698685646057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11612105369567871 0.033939361572265625

Final encoder loss: 0.017486773431301117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11611509323120117 0.03416728973388672

Final encoder loss: 0.017552774399518967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11512207984924316 0.03355002403259277

Final encoder loss: 0.01766933873295784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557483673095703 0.03492259979248047

Final encoder loss: 0.017656760290265083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11593127250671387 0.03402113914489746

Final encoder loss: 0.01814352720975876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11518168449401855 0.03430533409118652

Final encoder loss: 0.0175976250320673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1154336929321289 0.03368878364562988

Final encoder loss: 0.01788852922618389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11568021774291992 0.03384566307067871

Final encoder loss: 0.017543859779834747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11533927917480469 0.03393959999084473

Final encoder loss: 0.01740712672472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11525917053222656 0.0340576171875

Final encoder loss: 0.01734611950814724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11575865745544434 0.03414344787597656

Final encoder loss: 0.016838716343045235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11517953872680664 0.033532142639160156

Final encoder loss: 0.016584055498242378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11558032035827637 0.033586978912353516

Final encoder loss: 0.016717493534088135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11667203903198242 0.03551602363586426

Final encoder loss: 0.015947123989462852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1157691478729248 0.03371381759643555

Final encoder loss: 0.01645670272409916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11517167091369629 0.03407144546508789

Final encoder loss: 0.015794750303030014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11562490463256836 0.03407406806945801

Final encoder loss: 0.01626143790781498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11532187461853027 0.03411459922790527

Final encoder loss: 0.01596115157008171
Final encoder loss: 0.015286965295672417

Training case model
Final encoder loss: 0.026658726299779868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08907485008239746 0.21868109703063965

Final encoder loss: 0.027023604871963057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08890414237976074 0.2183094024658203

Final encoder loss: 0.02704556432655068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08896493911743164 0.2187941074371338

Final encoder loss: 0.02693339117641118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.0892798900604248 0.21875929832458496

Final encoder loss: 0.026692754538561698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0896005630493164 0.21855950355529785

Final encoder loss: 0.02647622617759548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08884429931640625 0.21821379661560059

Final encoder loss: 0.02641631022880204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08859443664550781 0.2186288833618164

Final encoder loss: 0.02637276267825259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.0887148380279541 0.2185666561126709

Final encoder loss: 0.02602524305711097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08873319625854492 0.21876001358032227

Final encoder loss: 0.026216881656466205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08859753608703613 0.21860265731811523

Final encoder loss: 0.02579433766891824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08917880058288574 0.21847319602966309

Final encoder loss: 0.025944429487215905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08934140205383301 0.2187035083770752

Final encoder loss: 0.02574327576869006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.0886993408203125 0.21866154670715332

Final encoder loss: 0.026097436308393514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08918166160583496 0.21845316886901855

Final encoder loss: 0.026453128944195003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08886289596557617 0.21880674362182617

Final encoder loss: 0.026647863157917295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08614587783813477 0.21530652046203613


Training case model
Final encoder loss: 0.20295827090740204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26187705993652344 0.05230903625488281

Final encoder loss: 0.18890459835529327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25657081604003906 0.052709341049194336

Final encoder loss: 0.19015184044837952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2579498291015625 0.05238652229309082

Final encoder loss: 0.1921856552362442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27259397506713867 0.05316352844238281

Final encoder loss: 0.1808195412158966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.258831262588501 0.05292034149169922

Final encoder loss: 0.1919207125902176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2568552494049072 0.05040454864501953

Final encoder loss: 0.10179243236780167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2693064212799072 0.05384182929992676

Final encoder loss: 0.09139508754014969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27587032318115234 0.05287957191467285

Final encoder loss: 0.08797328174114227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2704885005950928 0.05312967300415039

Final encoder loss: 0.0866251066327095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2700538635253906 0.05222821235656738

Final encoder loss: 0.07903913408517838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581923007965088 0.0530543327331543

Final encoder loss: 0.08211852610111237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2575523853302002 0.05274796485900879

Final encoder loss: 0.05973814055323601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25870513916015625 0.05423569679260254

Final encoder loss: 0.05394826829433441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.3019232749938965 0.05262875556945801

Final encoder loss: 0.05237768962979317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25812745094299316 0.05240035057067871

Final encoder loss: 0.052651941776275635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27120089530944824 0.05357193946838379

Final encoder loss: 0.049495358020067215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2777280807495117 0.053536415100097656

Final encoder loss: 0.0510721392929554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25774526596069336 0.05182147026062012

Final encoder loss: 0.04290663078427315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25755953788757324 0.05189800262451172

Final encoder loss: 0.039732158184051514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26310276985168457 0.05311274528503418

Final encoder loss: 0.038752827793359756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2687795162200928 0.05288386344909668

Final encoder loss: 0.03948044776916504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2596919536590576 0.05057072639465332

Final encoder loss: 0.03824141249060631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25711894035339355 0.05326437950134277

Final encoder loss: 0.03879566490650177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25844478607177734 0.051186561584472656

Final encoder loss: 0.035945501178503036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.256497859954834 0.0534210205078125

Final encoder loss: 0.0343533493578434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2802398204803467 0.05202031135559082

Final encoder loss: 0.033725712448358536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587406635284424 0.052997589111328125

Final encoder loss: 0.03435194864869118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2700216770172119 0.056391000747680664

Final encoder loss: 0.03466866910457611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25693202018737793 0.053197383880615234

Final encoder loss: 0.0343424528837204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2577238082885742 0.054255008697509766

Final encoder loss: 0.03326467424631119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25742602348327637 0.05254483222961426

Final encoder loss: 0.032381631433963776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2599453926086426 0.05392885208129883

Final encoder loss: 0.03231446444988251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25908851623535156 0.05323624610900879

Final encoder loss: 0.03264220058917999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25944066047668457 0.0558469295501709

Final encoder loss: 0.033443648368120193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2758820056915283 0.05328798294067383

Final encoder loss: 0.03279270604252815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25635838508605957 0.05310368537902832

Final encoder loss: 0.03159418702125549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582676410675049 0.05245637893676758

Final encoder loss: 0.03067544475197792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582120895385742 0.05388283729553223

Final encoder loss: 0.030400946736335754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582213878631592 0.05301094055175781

Final encoder loss: 0.030539775267243385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2850966453552246 0.05503082275390625

Final encoder loss: 0.0314367450773716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25852012634277344 0.05224967002868652

Final encoder loss: 0.031051799654960632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25398778915405273 0.05335736274719238

Final encoder loss: 0.03020743653178215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577970027923584 0.05214834213256836

Final encoder loss: 0.029597632586956024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2687969207763672 0.05370330810546875

Final encoder loss: 0.029025299474596977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26844191551208496 0.05232954025268555

Final encoder loss: 0.0292963944375515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2756938934326172 0.05324554443359375

Final encoder loss: 0.03022286854684353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2582995891571045 0.052968740463256836

Final encoder loss: 0.02961069531738758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25356364250183105 0.05425524711608887

Final encoder loss: 0.029314370825886726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2697865962982178 0.05299234390258789

Final encoder loss: 0.028706002980470657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2751920223236084 0.054682254791259766

Final encoder loss: 0.028309058398008347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27105069160461426 0.05353093147277832

Final encoder loss: 0.02849639765918255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2744874954223633 0.05494809150695801

Final encoder loss: 0.029527217149734497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26909685134887695 0.053571462631225586

Final encoder loss: 0.028723781928420067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25852465629577637 0.05480384826660156

Final encoder loss: 0.028662875294685364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26909613609313965 0.05295205116271973

Final encoder loss: 0.028471069410443306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2679588794708252 0.05443120002746582

Final encoder loss: 0.027838263660669327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27480244636535645 0.05192279815673828

Final encoder loss: 0.028077274560928345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26719117164611816 0.052577972412109375

Final encoder loss: 0.029056081548333168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2749488353729248 0.052056312561035156

Final encoder loss: 0.02835882641375065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542407512664795 0.052378177642822266

Final encoder loss: 0.028170129284262657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25990748405456543 0.05355691909790039

Final encoder loss: 0.027812013402581215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573056221008301 0.05264019966125488

Final encoder loss: 0.027484729886054993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25966596603393555 0.05254316329956055

Final encoder loss: 0.02745988965034485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2693796157836914 0.05230593681335449

Final encoder loss: 0.02823992259800434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2698087692260742 0.05342245101928711

Final encoder loss: 0.02782723866403103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537803649902344 0.05193281173706055

Final encoder loss: 0.027876846492290497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.272768497467041 0.0524439811706543

Final encoder loss: 0.02744613215327263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2590205669403076 0.05222439765930176

Final encoder loss: 0.027054525911808014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2731292247772217 0.052602529525756836

Final encoder loss: 0.02709212899208069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568802833557129 0.05324673652648926

Final encoder loss: 0.02817520871758461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2815542221069336 0.05382966995239258

Final encoder loss: 0.027413299307227135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554666996002197 0.051277875900268555

Final encoder loss: 0.02763204835355282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2609426975250244 0.05152153968811035

Final encoder loss: 0.027221962809562683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25907373428344727 0.05269122123718262

Final encoder loss: 0.02673993818461895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26081037521362305 0.05329155921936035

Final encoder loss: 0.026865484192967415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25670528411865234 0.05197405815124512

Final encoder loss: 0.027771800756454468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25925660133361816 0.05488467216491699

Final encoder loss: 0.027117257937788963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25484538078308105 0.05295753479003906

Final encoder loss: 0.027220314368605614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592296600341797 0.05534648895263672

Final encoder loss: 0.02701408788561821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27066516876220703 0.052182912826538086

Final encoder loss: 0.02652794122695923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581770420074463 0.054050445556640625

Final encoder loss: 0.026607120409607887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25908827781677246 0.05194497108459473

Final encoder loss: 0.02759356051683426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2596476078033447 0.05431795120239258

Final encoder loss: 0.026987245306372643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551686763763428 0.05229043960571289

Final encoder loss: 0.02707979828119278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25810933113098145 0.054032325744628906

Final encoder loss: 0.026705631986260414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2754180431365967 0.05214643478393555

Final encoder loss: 0.026339253410696983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2613179683685303 0.054233551025390625

Final encoder loss: 0.026292523369193077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2691366672515869 0.052410125732421875

Final encoder loss: 0.02715756557881832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2699856758117676 0.05446028709411621

Final encoder loss: 0.026638027280569077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562129497528076 0.05247139930725098

Final encoder loss: 0.026927178725600243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26891136169433594 0.0548248291015625

Final encoder loss: 0.026636706665158272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2836000919342041 0.05225706100463867

Final encoder loss: 0.026166819036006927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2579796314239502 0.05337786674499512

Final encoder loss: 0.026232127100229263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2770199775695801 0.052768707275390625

Final encoder loss: 0.027193069458007812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.276522159576416 0.05354714393615723

Final encoder loss: 0.02649250626564026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563130855560303 0.05270957946777344

Final encoder loss: 0.02678287774324417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25748777389526367 0.053650617599487305

Final encoder loss: 0.02639743499457836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2939596176147461 0.053751230239868164

Final encoder loss: 0.025951581075787544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25678539276123047 0.05267977714538574

Final encoder loss: 0.02590627782046795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.268704891204834 0.054442644119262695

Final encoder loss: 0.026870202273130417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2777900695800781 0.05524611473083496

Final encoder loss: 0.026276467368006706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544386386871338 0.05315256118774414

Final encoder loss: 0.026573723182082176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25994348526000977 0.05428934097290039

Final encoder loss: 0.026310350745916367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25760889053344727 0.05263876914978027

Final encoder loss: 0.025842759758234024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27133870124816895 0.05359005928039551

Final encoder loss: 0.02587011642754078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2753732204437256 0.053109169006347656

Final encoder loss: 0.026858117431402206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26727962493896484 0.051856040954589844

Final encoder loss: 0.026284394785761833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2527916431427002 0.05129528045654297

Final encoder loss: 0.026479434221982956
Final encoder loss: 0.0256012175232172
Final encoder loss: 0.024574274197220802
Final encoder loss: 0.023732580244541168
Final encoder loss: 0.02367326430976391
Final encoder loss: 0.022299794480204582

Training emognition model
Final encoder loss: 0.02986574966531158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08133697509765625 0.2294328212738037

Final encoder loss: 0.02989763388608684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08073687553405762 0.22940874099731445

Final encoder loss: 0.029719020788004907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08065557479858398 0.23050594329833984

Final encoder loss: 0.030401227760427512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0818791389465332 0.23040413856506348

Final encoder loss: 0.02805459407427238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08136582374572754 0.2306053638458252

Final encoder loss: 0.027995681388538377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08190369606018066 0.23076725006103516

Final encoder loss: 0.029403322642186085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08133196830749512 0.23101401329040527

Final encoder loss: 0.031493123652105014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08158540725708008 0.23070454597473145

Final encoder loss: 0.02894797508456741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08142948150634766 0.23076820373535156

Final encoder loss: 0.029549563744787267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08163905143737793 0.23104333877563477

Final encoder loss: 0.029523353386236382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0812673568725586 0.23070955276489258

Final encoder loss: 0.030379096584178453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08145737648010254 0.23063921928405762

Final encoder loss: 0.029524859851873926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08077359199523926 0.23064613342285156

Final encoder loss: 0.030119496619124893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08120369911193848 0.2304091453552246

Final encoder loss: 0.03021350367900598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08094406127929688 0.2306678295135498

Final encoder loss: 0.030153979934024935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08015179634094238 0.2300093173980713


Training emognition model
Final encoder loss: 0.19356371462345123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25072479248046875 0.05060863494873047

Final encoder loss: 0.1949557214975357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24713516235351562 0.04792523384094238

Final encoder loss: 0.08559291809797287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24683022499084473 0.04935622215270996

Final encoder loss: 0.08504270017147064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2473771572113037 0.0495607852935791

Final encoder loss: 0.055166490375995636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24730491638183594 0.04763960838317871

Final encoder loss: 0.05382758378982544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471776008605957 0.04970192909240723

Final encoder loss: 0.04209853336215019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24770641326904297 0.04900169372558594

Final encoder loss: 0.04116540774703026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24748468399047852 0.04887866973876953

Final encoder loss: 0.03556736186146736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24761080741882324 0.04781746864318848

Final encoder loss: 0.03491681069135666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474205493927002 0.04986834526062012

Final encoder loss: 0.03201062232255936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24697661399841309 0.0486760139465332

Final encoder loss: 0.03151044622063637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469336986541748 0.04979419708251953

Final encoder loss: 0.0301554873585701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24665021896362305 0.0490570068359375

Final encoder loss: 0.02975434437394142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24773454666137695 0.04881477355957031

Final encoder loss: 0.029389556497335434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24639129638671875 0.04816913604736328

Final encoder loss: 0.02914019674062729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24617385864257812 0.04917025566101074

Final encoder loss: 0.029302645474672318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2465355396270752 0.04781460762023926

Final encoder loss: 0.029087599366903305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2456822395324707 0.05001378059387207

Final encoder loss: 0.029353197664022446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24667620658874512 0.048639774322509766

Final encoder loss: 0.02936350367963314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2451488971710205 0.049361467361450195

Final encoder loss: 0.029014116153120995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24604249000549316 0.04906773567199707

Final encoder loss: 0.029168548062443733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2458500862121582 0.049199581146240234

Final encoder loss: 0.02882271632552147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24639415740966797 0.04779171943664551

Final encoder loss: 0.028997894376516342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24570989608764648 0.04950261116027832

Final encoder loss: 0.028838548809289932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24844741821289062 0.04918408393859863

Final encoder loss: 0.02874957211315632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24762320518493652 0.04888129234313965

Final encoder loss: 0.028939051553606987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24844908714294434 0.04983639717102051

Final encoder loss: 0.028827151283621788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2467951774597168 0.049278974533081055

Final encoder loss: 0.028586506843566895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24841690063476562 0.04791426658630371

Final encoder loss: 0.02864440530538559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24692964553833008 0.05011129379272461

Final encoder loss: 0.028375204652547836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2475285530090332 0.04808664321899414

Final encoder loss: 0.028543716296553612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24923968315124512 0.048334598541259766

Final encoder loss: 0.02803371287882328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24710941314697266 0.0494074821472168

Final encoder loss: 0.028255799785256386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2467787265777588 0.049295663833618164

Final encoder loss: 0.028146948665380478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2480628490447998 0.050133466720581055

Final encoder loss: 0.028133239597082138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2477567195892334 0.0504300594329834

Final encoder loss: 0.02808295376598835
Final encoder loss: 0.026959143579006195

Training empatch model
Final encoder loss: 0.041982096753143766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07120871543884277 0.17404532432556152

Final encoder loss: 0.042095001896330635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07114028930664062 0.17424368858337402

Final encoder loss: 0.041445907986381904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.08572840690612793 0.17383050918579102

Final encoder loss: 0.040131908817569166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07168817520141602 0.1739799976348877

Final encoder loss: 0.040784202965439886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07248282432556152 0.17420029640197754

Final encoder loss: 0.03758633568240234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07165646553039551 0.17400074005126953

Final encoder loss: 0.038334479171394244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07162785530090332 0.17426800727844238

Final encoder loss: 0.036775321099982015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0707554817199707 0.17396187782287598

Final encoder loss: 0.029749446571258744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07128000259399414 0.17485857009887695

Final encoder loss: 0.029920759977046574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07158374786376953 0.17362618446350098

Final encoder loss: 0.02872477482804034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07131385803222656 0.1743910312652588

Final encoder loss: 0.026660365645191857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07202768325805664 0.1741776466369629

Final encoder loss: 0.026776086930801093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07141542434692383 0.17373991012573242

Final encoder loss: 0.027615053411891233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07202005386352539 0.17486929893493652

Final encoder loss: 0.029118618785541856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07095718383789062 0.17345166206359863

Final encoder loss: 0.02950114050958397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07057905197143555 0.17378449440002441


Training empatch model
Final encoder loss: 0.17114832997322083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1774306297302246 0.04393339157104492

Final encoder loss: 0.08000197261571884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17474603652954102 0.04426169395446777

Final encoder loss: 0.05554269626736641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17550110816955566 0.04390668869018555

Final encoder loss: 0.043874144554138184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17640304565429688 0.043375492095947266

Final encoder loss: 0.037237126380205154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17493247985839844 0.0433192253112793

Final encoder loss: 0.03314856067299843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17470312118530273 0.04448699951171875

Final encoder loss: 0.03045680560171604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17580294609069824 0.04335212707519531

Final encoder loss: 0.028648776933550835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17537474632263184 0.04372668266296387

Final encoder loss: 0.027460498735308647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17594528198242188 0.04340195655822754

Final encoder loss: 0.026698177680373192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17579364776611328 0.042649269104003906

Final encoder loss: 0.026250315830111504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1763160228729248 0.043021202087402344

Final encoder loss: 0.025960858911275864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17457008361816406 0.04312467575073242

Final encoder loss: 0.025816315785050392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17595577239990234 0.043366193771362305

Final encoder loss: 0.025549570098519325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761302947998047 0.04277300834655762

Final encoder loss: 0.025379175320267677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17649245262145996 0.0434112548828125

Final encoder loss: 0.025300193578004837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17461109161376953 0.043161630630493164

Final encoder loss: 0.025238240137696266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17696928977966309 0.04439949989318848

Final encoder loss: 0.02520522102713585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17533326148986816 0.04359889030456543

Final encoder loss: 0.025114934891462326

Training wesad model
Final encoder loss: 0.043346825989335445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07114100456237793 0.1740272045135498

Final encoder loss: 0.04163616505683392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07095837593078613 0.17450928688049316

Final encoder loss: 0.04316242411057003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07166004180908203 0.1736602783203125

Final encoder loss: 0.03954885631285161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07110977172851562 0.17376995086669922

Final encoder loss: 0.029963115369782307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07212138175964355 0.17412710189819336

Final encoder loss: 0.02742657580397134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07135820388793945 0.17383933067321777

Final encoder loss: 0.02815406794840482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07106828689575195 0.1745142936706543

Final encoder loss: 0.029147208871117547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07129430770874023 0.1733689308166504

Final encoder loss: 0.02164114696965359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07101774215698242 0.17424964904785156

Final encoder loss: 0.02274059352442957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07226324081420898 0.17345452308654785

Final encoder loss: 0.024099796656700718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0714256763458252 0.17373061180114746

Final encoder loss: 0.023387225057789406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07194781303405762 0.17418789863586426

Final encoder loss: 0.018804667410332706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07120680809020996 0.17420125007629395

Final encoder loss: 0.018650227580765198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07129287719726562 0.17400240898132324

Final encoder loss: 0.01886192684604915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07130885124206543 0.17380380630493164

Final encoder loss: 0.01891645889396127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07112383842468262 0.17394804954528809


Training wesad model
Final encoder loss: 0.21562142670154572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10740375518798828 0.03332781791687012

Final encoder loss: 0.09767378866672516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10522007942199707 0.03274965286254883

Final encoder loss: 0.0632239505648613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10432672500610352 0.03360152244567871

Final encoder loss: 0.04616505652666092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10499119758605957 0.034000396728515625

Final encoder loss: 0.036541927605867386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10498285293579102 0.03276562690734863

Final encoder loss: 0.03076825849711895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10394573211669922 0.03340315818786621

Final encoder loss: 0.027152838185429573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10391736030578613 0.03352665901184082

Final encoder loss: 0.02485789731144905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10553908348083496 0.03382229804992676

Final encoder loss: 0.023335907608270645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10466241836547852 0.033387184143066406

Final encoder loss: 0.02233569137752056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10392165184020996 0.032884836196899414

Final encoder loss: 0.02181422710418701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10431075096130371 0.03352093696594238

Final encoder loss: 0.021577348932623863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1060328483581543 0.03368020057678223

Final encoder loss: 0.021520201116800308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.104034423828125 0.03344249725341797

Final encoder loss: 0.021479910239577293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10345673561096191 0.0333254337310791

Final encoder loss: 0.021480215713381767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10444855690002441 0.034142255783081055

Final encoder loss: 0.021524831652641296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10604143142700195 0.03319191932678223

Final encoder loss: 0.021465450525283813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1036984920501709 0.03345441818237305

Final encoder loss: 0.021522819995880127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10389876365661621 0.03333139419555664

Final encoder loss: 0.021501347422599792

Calculating loss for amigos model
	Full Pass 0.6760411262512207
numFreeParamsPath 18
Reconstruction loss values: 0.031150368973612785 0.0406053401529789

Calculating loss for dapper model
	Full Pass 0.1514449119567871
numFreeParamsPath 18
Reconstruction loss values: 0.025970397517085075 0.029398540034890175

Calculating loss for case model
	Full Pass 0.8614671230316162
numFreeParamsPath 18
Reconstruction loss values: 0.0375351719558239 0.040279317647218704

Calculating loss for emognition model
	Full Pass 0.2913784980773926
numFreeParamsPath 18
Reconstruction loss values: 0.039995305240154266 0.048372574150562286

Calculating loss for empatch model
	Full Pass 0.10560083389282227
numFreeParamsPath 18
Reconstruction loss values: 0.042003434151411057 0.04983016103506088

Calculating loss for wesad model
	Full Pass 0.07718753814697266
numFreeParamsPath 18
Reconstruction loss values: 0.04240963235497475 0.06154590845108032
Total loss calculation time: 3.870549440383911

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.681381702423096
Total epoch time: 183.1527440547943

Epoch: 37

Training amigos model
Final encoder loss: 0.029636129485211125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11734461784362793 0.3917248249053955

Final encoder loss: 0.028732579255192816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10816597938537598 0.389815092086792

Final encoder loss: 0.02689546061859235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10826921463012695 0.39034199714660645

Final encoder loss: 0.02943454096265829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10906720161437988 0.38956379890441895

Final encoder loss: 0.02951618932231292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10894203186035156 0.39005422592163086

Final encoder loss: 0.03055869812086215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10836338996887207 0.38982152938842773

Final encoder loss: 0.03043155255548572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10803413391113281 0.38910865783691406

Final encoder loss: 0.029520803364940458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10845541954040527 0.3883998394012451

Final encoder loss: 0.028674627318697368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10839295387268066 0.3896331787109375

Final encoder loss: 0.031569725818260994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10832500457763672 0.39025235176086426

Final encoder loss: 0.030938536151235513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10914134979248047 0.38994812965393066

Final encoder loss: 0.02753780078121933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10953712463378906 0.3893113136291504

Final encoder loss: 0.02963807606339845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10884690284729004 0.39121508598327637

Final encoder loss: 0.030022353016163385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10824990272521973 0.38953328132629395

Final encoder loss: 0.03220763080445773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.1082925796508789 0.38912105560302734

Final encoder loss: 0.03003483812385855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10352444648742676 0.38572239875793457


Training dapper model
Final encoder loss: 0.026071706998968037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06220555305480957 0.15021538734436035

Final encoder loss: 0.027069926988725516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06166958808898926 0.1503429412841797

Final encoder loss: 0.02313476493368505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06328177452087402 0.15155792236328125

Final encoder loss: 0.023947461734409696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.062363386154174805 0.1507573127746582

Final encoder loss: 0.02462174992907999
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06239604949951172 0.1512455940246582

Final encoder loss: 0.022295731484347988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06280398368835449 0.14942526817321777

Final encoder loss: 0.021075084010432937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062075138092041016 0.15088486671447754

Final encoder loss: 0.0223650016825334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06191372871398926 0.15192484855651855

Final encoder loss: 0.02309015761623452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06160330772399902 0.1506052017211914

Final encoder loss: 0.023848718718815873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.0617985725402832 0.15058660507202148

Final encoder loss: 0.023255225132849073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06349468231201172 0.15109872817993164

Final encoder loss: 0.022969448643141325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06200265884399414 0.15133380889892578

Final encoder loss: 0.020650610075881985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06214165687561035 0.15109968185424805

Final encoder loss: 0.026024267489968656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06331968307495117 0.1510012149810791

Final encoder loss: 0.023404585110120217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06203770637512207 0.1506950855255127

Final encoder loss: 0.024852397270444807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.0616450309753418 0.15126872062683105


Training emognition model
Final encoder loss: 0.038484843084415056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08348870277404785 0.2744722366333008

Final encoder loss: 0.040795389310468395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0834357738494873 0.2758469581604004

Final encoder loss: 0.03811513283230229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08372139930725098 0.27542996406555176

Final encoder loss: 0.038178614319013586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08475780487060547 0.2753772735595703

Final encoder loss: 0.03817116276634372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08304166793823242 0.2750511169433594

Final encoder loss: 0.039353225774778997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08338427543640137 0.2757413387298584

Final encoder loss: 0.03747852418687674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08406782150268555 0.276019811630249

Final encoder loss: 0.03701480880432931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08338212966918945 0.27568912506103516

Final encoder loss: 0.03734062585029124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08437919616699219 0.27536773681640625

Final encoder loss: 0.03630375292805548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08323955535888672 0.27544212341308594

Final encoder loss: 0.03700249220501074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08342790603637695 0.275576114654541

Final encoder loss: 0.03928444399236412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0849912166595459 0.27641892433166504

Final encoder loss: 0.036674211561240434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0834805965423584 0.27539658546447754

Final encoder loss: 0.03572284692063145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08302688598632812 0.2751798629760742

Final encoder loss: 0.03535538658120284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08384871482849121 0.275895357131958

Final encoder loss: 0.03530245476642319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0828251838684082 0.2747507095336914


Training case model
Final encoder loss: 0.03893533823786712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09197115898132324 0.2650763988494873

Final encoder loss: 0.03443225131476434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09149551391601562 0.26510000228881836

Final encoder loss: 0.03323663976975919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09290170669555664 0.2657499313354492

Final encoder loss: 0.03174224692524785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09257268905639648 0.26516127586364746

Final encoder loss: 0.032849162939069906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09160804748535156 0.26549792289733887

Final encoder loss: 0.03232004719050774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09207701683044434 0.265547513961792

Final encoder loss: 0.031463600274169105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.11115550994873047 0.26627516746520996

Final encoder loss: 0.030416776241979106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09163475036621094 0.26513099670410156

Final encoder loss: 0.029838099453144487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09125471115112305 0.2651846408843994

Final encoder loss: 0.030606695980306308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09192752838134766 0.26624631881713867

Final encoder loss: 0.030190435397657086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09147834777832031 0.2655148506164551

Final encoder loss: 0.02972294876460575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09144830703735352 0.2652928829193115

Final encoder loss: 0.030076030737319587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0913076400756836 0.26645755767822266

Final encoder loss: 0.029367606104189127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0916593074798584 0.26548290252685547

Final encoder loss: 0.028882206702309367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09237885475158691 0.265042781829834

Final encoder loss: 0.028983369627177565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08833861351013184 0.26159143447875977


Training amigos model
Final encoder loss: 0.021804222207090142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10628962516784668 0.3416152000427246

Final encoder loss: 0.02243921144676914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10660696029663086 0.34140586853027344

Final encoder loss: 0.02309029062749892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10719895362854004 0.34167981147766113

Final encoder loss: 0.020291223742316733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10606026649475098 0.3416311740875244

Final encoder loss: 0.023567137640260794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10640716552734375 0.34159374237060547

Final encoder loss: 0.022160440855400497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10587310791015625 0.3417227268218994

Final encoder loss: 0.022375979498139763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10746955871582031 0.34142541885375977

Final encoder loss: 0.02197576250460526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1066446304321289 0.3417832851409912

Final encoder loss: 0.02229625448954078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10653567314147949 0.34169769287109375

Final encoder loss: 0.021235040759070473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10717439651489258 0.34158849716186523

Final encoder loss: 0.021929722902235143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10662031173706055 0.34190988540649414

Final encoder loss: 0.022236418653475795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10724592208862305 0.34195685386657715

Final encoder loss: 0.021031818168879703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10673069953918457 0.340853214263916

Final encoder loss: 0.02214799048986927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10560059547424316 0.3409440517425537

Final encoder loss: 0.02136676467926909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10538816452026367 0.3409552574157715

Final encoder loss: 0.020027517852111393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10042214393615723 0.336902379989624


Training amigos model
Final encoder loss: 0.18077479302883148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45785975456237793 0.07483959197998047

Final encoder loss: 0.1878315657377243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45717310905456543 0.07617616653442383

Final encoder loss: 0.18362976610660553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45740532875061035 0.07402706146240234

Final encoder loss: 0.07508810609579086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47162508964538574 0.0767819881439209

Final encoder loss: 0.07710915058851242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46651697158813477 0.07781529426574707

Final encoder loss: 0.07086648792028427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4682955741882324 0.0800633430480957

Final encoder loss: 0.0440249964594841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47005224227905273 0.07420611381530762

Final encoder loss: 0.0447118878364563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46288108825683594 0.07676076889038086

Final encoder loss: 0.04186398163437843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46786022186279297 0.08090710639953613

Final encoder loss: 0.0320933572947979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47052621841430664 0.07776880264282227

Final encoder loss: 0.03293690085411072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46023988723754883 0.07518148422241211

Final encoder loss: 0.03154345229268074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46662402153015137 0.07927227020263672

Final encoder loss: 0.026805467903614044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4691178798675537 0.07690906524658203

Final encoder loss: 0.02775263600051403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4657280445098877 0.0764920711517334

Final encoder loss: 0.026857642456889153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4666275978088379 0.07568812370300293

Final encoder loss: 0.02465890534222126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.469707727432251 0.07453274726867676

Final encoder loss: 0.025479022413492203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46573925018310547 0.07573223114013672

Final encoder loss: 0.024692557752132416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46735382080078125 0.07137155532836914

Final encoder loss: 0.024279039353132248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46790623664855957 0.07624053955078125

Final encoder loss: 0.02471541054546833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4652409553527832 0.07429146766662598

Final encoder loss: 0.02421349473297596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4676039218902588 0.07285666465759277

Final encoder loss: 0.024400660768151283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4682328701019287 0.07837557792663574

Final encoder loss: 0.024502545595169067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46402812004089355 0.07526993751525879

Final encoder loss: 0.0242919921875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46898555755615234 0.07464122772216797

Final encoder loss: 0.02367870695888996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.468524694442749 0.07499003410339355

Final encoder loss: 0.023875700309872627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4679586887359619 0.0766451358795166

Final encoder loss: 0.02398216724395752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.466472864151001 0.0756988525390625

Final encoder loss: 0.02280154637992382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4677426815032959 0.07617568969726562

Final encoder loss: 0.023179836571216583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45375871658325195 0.07691812515258789

Final encoder loss: 0.023273715749382973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4559504985809326 0.0740804672241211

Final encoder loss: 0.022544322535395622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45319628715515137 0.07346677780151367

Final encoder loss: 0.02278076857328415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4544107913970947 0.0738985538482666

Final encoder loss: 0.022740237414836884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4549214839935303 0.07322287559509277

Final encoder loss: 0.02254392020404339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4617645740509033 0.07556962966918945

Final encoder loss: 0.022306149825453758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46582603454589844 0.07415962219238281

Final encoder loss: 0.022611653432250023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.462993860244751 0.07879281044006348

Final encoder loss: 0.022108912467956543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4640965461730957 0.07691836357116699

Final encoder loss: 0.02192641980946064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4664428234100342 0.07667708396911621

Final encoder loss: 0.022381413727998734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46901774406433105 0.07641744613647461

Final encoder loss: 0.02198871783912182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46730542182922363 0.07584977149963379

Final encoder loss: 0.021862050518393517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46474742889404297 0.07647824287414551

Final encoder loss: 0.02224770374596119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46657443046569824 0.0741417407989502

Final encoder loss: 0.02182360365986824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45833444595336914 0.07107806205749512

Final encoder loss: 0.021702561527490616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45322418212890625 0.0744168758392334

Final encoder loss: 0.021794024854898453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4561600685119629 0.07486653327941895

Final encoder loss: 0.021744219586253166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45432376861572266 0.0740654468536377

Final encoder loss: 0.0214526429772377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4535088539123535 0.07556939125061035

Final encoder loss: 0.021792469546198845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4572787284851074 0.0761876106262207

Final encoder loss: 0.02124348096549511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4647951126098633 0.07561850547790527

Final encoder loss: 0.02129562944173813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4645118713378906 0.07497692108154297

Final encoder loss: 0.021668851375579834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.464705228805542 0.07730460166931152

Final encoder loss: 0.02138400264084339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46265101432800293 0.07485651969909668

Final encoder loss: 0.0212918259203434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46335411071777344 0.07513999938964844

Final encoder loss: 0.021647652611136436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4642763137817383 0.0777275562286377

Final encoder loss: 0.02118835598230362
Final encoder loss: 0.020278362557291985
Final encoder loss: 0.01962745562195778

Training dapper model
Final encoder loss: 0.020315642203502356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05968475341796875 0.10770869255065918

Final encoder loss: 0.019312577345442984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05950522422790527 0.10618066787719727

Final encoder loss: 0.02055639501474264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059441328048706055 0.10647797584533691

Final encoder loss: 0.017930563977683733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05927014350891113 0.10676956176757812

Final encoder loss: 0.017255141021079133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05959486961364746 0.10726547241210938

Final encoder loss: 0.01736147052702842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059453487396240234 0.10724472999572754

Final encoder loss: 0.01816936389578109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05964517593383789 0.10659146308898926

Final encoder loss: 0.01815679760400262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.061363935470581055 0.10841226577758789

Final encoder loss: 0.016884718737823247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06087541580200195 0.10875415802001953

Final encoder loss: 0.016781157525965835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.060671091079711914 0.10811161994934082

Final encoder loss: 0.017313962527155436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06059384346008301 0.10734438896179199

Final encoder loss: 0.017449540826848433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05978798866271973 0.10760927200317383

Final encoder loss: 0.01807485720910172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05971336364746094 0.10708189010620117

Final encoder loss: 0.01803687132302972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06107592582702637 0.10806441307067871

Final encoder loss: 0.017618233700641507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06003165245056152 0.10670351982116699

Final encoder loss: 0.01986596142149786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05968642234802246 0.10666608810424805


Training dapper model
Final encoder loss: 0.20241579413414001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1161649227142334 0.033652544021606445

Final encoder loss: 0.20820821821689606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11652803421020508 0.033963680267333984

Final encoder loss: 0.08405623584985733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11269426345825195 0.03381180763244629

Final encoder loss: 0.08590653538703918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11500287055969238 0.03390073776245117

Final encoder loss: 0.04950090870261192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11295795440673828 0.03357815742492676

Final encoder loss: 0.049330275505781174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11566281318664551 0.034365177154541016

Final encoder loss: 0.034083474427461624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1148386001586914 0.03374910354614258

Final encoder loss: 0.03391721099615097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11511588096618652 0.03416919708251953

Final encoder loss: 0.02629292570054531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11218810081481934 0.03425121307373047

Final encoder loss: 0.026292739436030388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152644157409668 0.034166812896728516

Final encoder loss: 0.022051513195037842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11266422271728516 0.03417062759399414

Final encoder loss: 0.022056328132748604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11492466926574707 0.03424715995788574

Final encoder loss: 0.019689541310071945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11256194114685059 0.033400774002075195

Final encoder loss: 0.019642764702439308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11586427688598633 0.03391599655151367

Final encoder loss: 0.018251918256282806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11239337921142578 0.033675432205200195

Final encoder loss: 0.01826613023877144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11411118507385254 0.03399348258972168

Final encoder loss: 0.01747928373515606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11263203620910645 0.033966779708862305

Final encoder loss: 0.017518920823931694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11523199081420898 0.03354215621948242

Final encoder loss: 0.017011960968375206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11229920387268066 0.034154415130615234

Final encoder loss: 0.01702170819044113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150515079498291 0.03417801856994629

Final encoder loss: 0.016888242214918137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1125948429107666 0.034089088439941406

Final encoder loss: 0.01687302626669407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11434531211853027 0.03378868103027344

Final encoder loss: 0.01667800545692444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11270308494567871 0.0335695743560791

Final encoder loss: 0.01667969301342964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11478447914123535 0.03383207321166992

Final encoder loss: 0.016886048018932343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11240577697753906 0.03351163864135742

Final encoder loss: 0.016613882035017014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11508059501647949 0.0344395637512207

Final encoder loss: 0.016718212515115738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11342811584472656 0.03513932228088379

Final encoder loss: 0.01647518016397953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11597847938537598 0.03433823585510254

Final encoder loss: 0.01642172783613205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11306309700012207 0.033570051193237305

Final encoder loss: 0.016379935666918755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11459684371948242 0.03389477729797363

Final encoder loss: 0.016126714646816254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1129004955291748 0.03360629081726074

Final encoder loss: 0.0161104965955019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11513805389404297 0.03328514099121094

Final encoder loss: 0.015616619028151035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11262965202331543 0.03381013870239258

Final encoder loss: 0.015782030299305916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11394476890563965 0.03345632553100586

Final encoder loss: 0.01541076973080635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1126854419708252 0.03423118591308594

Final encoder loss: 0.015331701375544071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11474990844726562 0.03351593017578125

Final encoder loss: 0.015191907994449139
Final encoder loss: 0.01445496641099453

Training case model
Final encoder loss: 0.024841557862462428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08876895904541016 0.2184925079345703

Final encoder loss: 0.025477374978806006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08897852897644043 0.21854829788208008

Final encoder loss: 0.025736518606912784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08888888359069824 0.21879148483276367

Final encoder loss: 0.024831406438682927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08918190002441406 0.21857285499572754

Final encoder loss: 0.025070682764167056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08889365196228027 0.21837425231933594

Final encoder loss: 0.024972550817681148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08895134925842285 0.21871638298034668

Final encoder loss: 0.025363443254681085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08883166313171387 0.21870899200439453

Final encoder loss: 0.02549591383971619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08901858329772949 0.2186291217803955

Final encoder loss: 0.025485329809836088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08889102935791016 0.2186574935913086

Final encoder loss: 0.025237833669858174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08892321586608887 0.21864032745361328

Final encoder loss: 0.024845835800874266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08893895149230957 0.2186570167541504

Final encoder loss: 0.024509094818384085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08874106407165527 0.2186570167541504

Final encoder loss: 0.02521294996698151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08871078491210938 0.21851873397827148

Final encoder loss: 0.024521416765271174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08919191360473633 0.21824312210083008

Final encoder loss: 0.024895837747764903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08872509002685547 0.21870207786560059

Final encoder loss: 0.025426145649435905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08534693717956543 0.2151327133178711


Training case model
Final encoder loss: 0.202964186668396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2618560791015625 0.0521388053894043

Final encoder loss: 0.18890219926834106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2679288387298584 0.050969839096069336

Final encoder loss: 0.1901455968618393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572038173675537 0.052262306213378906

Final encoder loss: 0.19219253957271576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2664616107940674 0.051453351974487305

Final encoder loss: 0.18080610036849976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2658195495605469 0.05206656455993652

Final encoder loss: 0.19193905591964722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549452781677246 0.051245927810668945

Final encoder loss: 0.10174299031496048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2589106559753418 0.05099678039550781

Final encoder loss: 0.09147115796804428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25699543952941895 0.05105757713317871

Final encoder loss: 0.08808569610118866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2560081481933594 0.05234694480895996

Final encoder loss: 0.08686388283967972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583162784576416 0.05195879936218262

Final encoder loss: 0.07911829650402069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2565124034881592 0.05232739448547363

Final encoder loss: 0.08250504732131958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25451087951660156 0.051619768142700195

Final encoder loss: 0.059479501098394394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2575232982635498 0.05325150489807129

Final encoder loss: 0.05386992171406746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2575874328613281 0.05195903778076172

Final encoder loss: 0.05230654031038284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570497989654541 0.052628278732299805

Final encoder loss: 0.05249214544892311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570765018463135 0.05098700523376465

Final encoder loss: 0.04923161864280701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25713038444519043 0.05144834518432617

Final encoder loss: 0.051182664930820465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534780502319336 0.050333499908447266

Final encoder loss: 0.04245831072330475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573695182800293 0.05091452598571777

Final encoder loss: 0.03952533006668091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25762391090393066 0.05334186553955078

Final encoder loss: 0.03839871659874916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257098913192749 0.052106380462646484

Final encoder loss: 0.03907926380634308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573511600494385 0.05103492736816406

Final encoder loss: 0.03775850683450699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2568213939666748 0.05135798454284668

Final encoder loss: 0.038698017597198486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540123462677002 0.05140852928161621

Final encoder loss: 0.035345811396837234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25681090354919434 0.05225229263305664

Final encoder loss: 0.03404739871621132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257474422454834 0.052964210510253906

Final encoder loss: 0.03324770927429199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25641345977783203 0.05183076858520508

Final encoder loss: 0.033885546028614044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25803112983703613 0.05176067352294922

Final encoder loss: 0.03405313938856125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2571849822998047 0.05215644836425781

Final encoder loss: 0.03416917845606804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538478374481201 0.0509181022644043

Final encoder loss: 0.03284694626927376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577674388885498 0.051871299743652344

Final encoder loss: 0.032229792326688766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25635790824890137 0.05135774612426758

Final encoder loss: 0.03216196596622467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2571225166320801 0.051279306411743164

Final encoder loss: 0.03223046660423279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25560641288757324 0.05191159248352051

Final encoder loss: 0.03311261162161827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2577087879180908 0.05173230171203613

Final encoder loss: 0.03264691308140755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2527306079864502 0.05027055740356445

Final encoder loss: 0.031053941696882248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25699663162231445 0.0518336296081543

Final encoder loss: 0.030373265966773033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2562863826751709 0.0513460636138916

Final encoder loss: 0.029999708756804466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570333480834961 0.05145859718322754

Final encoder loss: 0.030272265896201134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2560439109802246 0.051653146743774414

Final encoder loss: 0.030925631523132324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2564988136291504 0.051247596740722656

Final encoder loss: 0.03052997589111328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2525925636291504 0.050908565521240234

Final encoder loss: 0.029552452266216278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25670409202575684 0.05263710021972656

Final encoder loss: 0.029025986790657043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2562694549560547 0.05070018768310547

Final encoder loss: 0.02868027426302433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25758790969848633 0.05261111259460449

Final encoder loss: 0.028992731124162674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2569925785064697 0.05226778984069824

Final encoder loss: 0.029686858877539635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25621557235717773 0.05309653282165527

Final encoder loss: 0.02926294133067131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25461721420288086 0.0499420166015625

Final encoder loss: 0.028479615226387978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578117847442627 0.05260491371154785

Final encoder loss: 0.028281860053539276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566072940826416 0.05106353759765625

Final encoder loss: 0.02770240604877472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25636982917785645 0.051877498626708984

Final encoder loss: 0.027956118807196617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577548027038574 0.052733659744262695

Final encoder loss: 0.02887488715350628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575497627258301 0.05175495147705078

Final encoder loss: 0.028296593576669693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25469064712524414 0.05120086669921875

Final encoder loss: 0.02804681658744812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566225528717041 0.05244946479797363

Final encoder loss: 0.027942288666963577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25925683975219727 0.052880287170410156

Final encoder loss: 0.02749006450176239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25930070877075195 0.0537257194519043

Final encoder loss: 0.027623867616057396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579314708709717 0.05275893211364746

Final encoder loss: 0.028748661279678345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.261519193649292 0.05403637886047363

Final encoder loss: 0.02791532687842846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552907466888428 0.05171561241149902

Final encoder loss: 0.02741333283483982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2609891891479492 0.052863359451293945

Final encoder loss: 0.027168208733201027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26022815704345703 0.05273628234863281

Final encoder loss: 0.02681519091129303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2681159973144531 0.053305625915527344

Final encoder loss: 0.026990393176674843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581472396850586 0.05278635025024414

Final encoder loss: 0.02786221355199814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583339214324951 0.05336356163024902

Final encoder loss: 0.02730359509587288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564964294433594 0.0529782772064209

Final encoder loss: 0.027221977710723877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.259829044342041 0.05421876907348633

Final encoder loss: 0.027023373171687126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26743006706237793 0.0525968074798584

Final encoder loss: 0.026608861982822418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581603527069092 0.05425691604614258

Final encoder loss: 0.026751242578029633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589113712310791 0.052176475524902344

Final encoder loss: 0.027487218379974365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2593841552734375 0.05567812919616699

Final encoder loss: 0.027038156986236572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25595998764038086 0.05143141746520996

Final encoder loss: 0.026704639196395874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576751708984375 0.05394721031188965

Final encoder loss: 0.026575131341814995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25848984718322754 0.052121877670288086

Final encoder loss: 0.026182128116488457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25736570358276367 0.0523219108581543

Final encoder loss: 0.026320993900299072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25928282737731934 0.05157637596130371

Final encoder loss: 0.027094701305031776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25809288024902344 0.05211019515991211

Final encoder loss: 0.02652263268828392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25786495208740234 0.05175638198852539

Final encoder loss: 0.026599405333399773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26006317138671875 0.05163717269897461

Final encoder loss: 0.026569871231913567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2640566825866699 0.052019596099853516

Final encoder loss: 0.02614891156554222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25838422775268555 0.052356719970703125

Final encoder loss: 0.02620846778154373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2617306709289551 0.0517275333404541

Final encoder loss: 0.027025042101740837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257033109664917 0.052773237228393555

Final encoder loss: 0.026467060670256615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2585761547088623 0.05131697654724121

Final encoder loss: 0.02632109634578228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.258089542388916 0.05193662643432617

Final encoder loss: 0.026083843782544136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27072954177856445 0.05456233024597168

Final encoder loss: 0.025761017575860023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2590813636779785 0.05252575874328613

Final encoder loss: 0.02574620395898819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2603292465209961 0.05479001998901367

Final encoder loss: 0.026700705289840698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591664791107178 0.05239987373352051

Final encoder loss: 0.02604064717888832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562844753265381 0.05292057991027832

Final encoder loss: 0.02623794414103031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26038551330566406 0.05257558822631836

Final encoder loss: 0.02608088217675686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2606027126312256 0.05360126495361328

Final encoder loss: 0.025695858523249626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586336135864258 0.05135202407836914

Final encoder loss: 0.025782935321331024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.260669469833374 0.05238080024719238

Final encoder loss: 0.02654935047030449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591409683227539 0.05402374267578125

Final encoder loss: 0.026045797392725945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2566852569580078 0.05394315719604492

Final encoder loss: 0.02599559910595417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25872159004211426 0.05297255516052246

Final encoder loss: 0.025848042219877243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572033405303955 0.05381417274475098

Final encoder loss: 0.025404762476682663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25872087478637695 0.05107522010803223

Final encoder loss: 0.025416819378733635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570004463195801 0.0543820858001709

Final encoder loss: 0.026159321889281273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2599067687988281 0.05383110046386719

Final encoder loss: 0.025646721944212914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.254138708114624 0.052007436752319336

Final encoder loss: 0.0259240809828043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592594623565674 0.052816152572631836

Final encoder loss: 0.025857113301753998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25937509536743164 0.051827430725097656

Final encoder loss: 0.02544870786368847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2596135139465332 0.05188894271850586

Final encoder loss: 0.02544286474585533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25696468353271484 0.05340909957885742

Final encoder loss: 0.02622818760573864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.259641170501709 0.05232071876525879

Final encoder loss: 0.025681033730506897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542562484741211 0.05175185203552246

Final encoder loss: 0.025668909773230553
Final encoder loss: 0.024890640750527382
Final encoder loss: 0.023988408967852592
Final encoder loss: 0.02326260320842266
Final encoder loss: 0.023047611117362976
Final encoder loss: 0.021749800071120262

Training emognition model
Final encoder loss: 0.029964242674648534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.0815582275390625 0.23046231269836426

Final encoder loss: 0.028461283752095303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08093380928039551 0.23064565658569336

Final encoder loss: 0.02946472183742676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08274269104003906 0.23061442375183105

Final encoder loss: 0.028683320564656174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08094072341918945 0.23077774047851562

Final encoder loss: 0.029401882086938568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08121371269226074 0.23104143142700195

Final encoder loss: 0.028898213702507877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08233475685119629 0.23080897331237793

Final encoder loss: 0.027988772876400086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08171939849853516 0.23070836067199707

Final encoder loss: 0.029469261987905213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08145785331726074 0.2310476303100586

Final encoder loss: 0.029318732702089168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08377790451049805 0.23106098175048828

Final encoder loss: 0.029122752724964254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08118200302124023 0.23070645332336426

Final encoder loss: 0.028681153853521064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.09325575828552246 0.2314596176147461

Final encoder loss: 0.029534189404218585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08108687400817871 0.23010540008544922

Final encoder loss: 0.02825988561120341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08148550987243652 0.23095703125

Final encoder loss: 0.030562234351957596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08209729194641113 0.23138689994812012

Final encoder loss: 0.030367091844550925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0819084644317627 0.2304539680480957

Final encoder loss: 0.028731550273070592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0799250602722168 0.23016881942749023


Training emognition model
Final encoder loss: 0.19358038902282715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2512664794921875 0.04804873466491699

Final encoder loss: 0.19497132301330566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24828147888183594 0.04869675636291504

Final encoder loss: 0.08580908924341202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25200891494750977 0.048647403717041016

Final encoder loss: 0.08545360714197159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24770617485046387 0.04986834526062012

Final encoder loss: 0.055289335548877716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502474784851074 0.048430442810058594

Final encoder loss: 0.053891390562057495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24755096435546875 0.04922628402709961

Final encoder loss: 0.042023055255413055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24963736534118652 0.050580739974975586

Final encoder loss: 0.04109051078557968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2481088638305664 0.04925942420959473

Final encoder loss: 0.035385239869356155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24879741668701172 0.05049633979797363

Final encoder loss: 0.034826915711164474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24727892875671387 0.048909902572631836

Final encoder loss: 0.03170400857925415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24814391136169434 0.051064252853393555

Final encoder loss: 0.031439654529094696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24817228317260742 0.05022430419921875

Final encoder loss: 0.029811058193445206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483367919921875 0.04901719093322754

Final encoder loss: 0.029639650136232376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24797821044921875 0.04737424850463867

Final encoder loss: 0.028968628495931625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24861645698547363 0.04906272888183594

Final encoder loss: 0.028924817219376564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2492992877960205 0.04853963851928711

Final encoder loss: 0.02867552638053894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2479848861694336 0.04926562309265137

Final encoder loss: 0.028823813423514366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474665641784668 0.05250358581542969

Final encoder loss: 0.028532862663269043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2464449405670166 0.04915904998779297

Final encoder loss: 0.029070395976305008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24999713897705078 0.05147743225097656

Final encoder loss: 0.028405848890542984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2480318546295166 0.04894399642944336

Final encoder loss: 0.02909720502793789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24713373184204102 0.05075240135192871

Final encoder loss: 0.028448810800909996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24746298789978027 0.049163818359375

Final encoder loss: 0.02901468239724636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2473912239074707 0.05061936378479004

Final encoder loss: 0.028168492019176483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2486574649810791 0.047770023345947266

Final encoder loss: 0.028475342318415642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24959158897399902 0.049668312072753906

Final encoder loss: 0.02813236601650715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2517688274383545 0.04833412170410156

Final encoder loss: 0.02824903465807438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475900650024414 0.04768085479736328

Final encoder loss: 0.028129583224654198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24905943870544434 0.049810171127319336

Final encoder loss: 0.02820480801165104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24806475639343262 0.04820847511291504

Final encoder loss: 0.027974210679531097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24815988540649414 0.04935336112976074

Final encoder loss: 0.028234127908945084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24831151962280273 0.050473690032958984

Final encoder loss: 0.02774455025792122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24827218055725098 0.050589799880981445

Final encoder loss: 0.02803872711956501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24869537353515625 0.04970693588256836

Final encoder loss: 0.027485309168696404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24755167961120605 0.05137372016906738

Final encoder loss: 0.027999425306916237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24884343147277832 0.048603057861328125

Final encoder loss: 0.027308454737067223
Final encoder loss: 0.026672061532735825

Training empatch model
Final encoder loss: 0.0433798255565352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07157278060913086 0.1741313934326172

Final encoder loss: 0.04015872887067022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07120633125305176 0.17373228073120117

Final encoder loss: 0.041722170323784495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07101678848266602 0.17384696006774902

Final encoder loss: 0.03847694865275714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07331228256225586 0.17385244369506836

Final encoder loss: 0.038818440940793404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07113432884216309 0.17365550994873047

Final encoder loss: 0.03706974701669826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07231450080871582 0.1746363639831543

Final encoder loss: 0.03708798939665528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07088184356689453 0.17377758026123047

Final encoder loss: 0.036603608190953096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07066035270690918 0.1735079288482666

Final encoder loss: 0.02639280393595863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0722970962524414 0.1738574504852295

Final encoder loss: 0.028514571248022962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07083821296691895 0.17218327522277832

Final encoder loss: 0.027547735417878513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07045865058898926 0.17280244827270508

Final encoder loss: 0.02782215050407474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07041668891906738 0.17267870903015137

Final encoder loss: 0.029070366808369597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07053995132446289 0.17279720306396484

Final encoder loss: 0.027195297294239425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07023000717163086 0.17279458045959473

Final encoder loss: 0.02810956187733922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07081103324890137 0.17271685600280762

Final encoder loss: 0.026946946845832757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07012081146240234 0.1724071502685547


Training empatch model
Final encoder loss: 0.17117802798748016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17572259902954102 0.04306292533874512

Final encoder loss: 0.08038735389709473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750352382659912 0.04284381866455078

Final encoder loss: 0.05566224828362465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17368626594543457 0.04323554039001465

Final encoder loss: 0.04378947243094444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17481660842895508 0.04255175590515137

Final encoder loss: 0.03700484707951546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17388010025024414 0.04332756996154785

Final encoder loss: 0.03277202323079109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1742103099822998 0.042981624603271484

Final encoder loss: 0.03003540448844433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750640869140625 0.04369759559631348

Final encoder loss: 0.02819712646305561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17487144470214844 0.0429990291595459

Final encoder loss: 0.026982413604855537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17441105842590332 0.043660879135131836

Final encoder loss: 0.026224123314023018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752021312713623 0.04309868812561035

Final encoder loss: 0.025772882625460625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748051643371582 0.04445075988769531

Final encoder loss: 0.025438746437430382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750810146331787 0.04411435127258301

Final encoder loss: 0.025230320170521736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17466402053833008 0.043976783752441406

Final encoder loss: 0.02510504610836506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17545175552368164 0.044342994689941406

Final encoder loss: 0.024956975132226944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17427563667297363 0.043688058853149414

Final encoder loss: 0.024898933246731758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1756455898284912 0.04346299171447754

Final encoder loss: 0.02476463094353676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746809482574463 0.04422259330749512

Final encoder loss: 0.02463601529598236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17578649520874023 0.043763160705566406

Final encoder loss: 0.02443842962384224

Training wesad model
Final encoder loss: 0.04095820428299162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07077908515930176 0.17378902435302734

Final encoder loss: 0.0391266462080992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07081222534179688 0.1732795238494873

Final encoder loss: 0.03952459546521803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07144498825073242 0.17354059219360352

Final encoder loss: 0.040658943109852554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07111549377441406 0.17383813858032227

Final encoder loss: 0.028351257231698557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07130098342895508 0.17322087287902832

Final encoder loss: 0.026067367351198627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07160091400146484 0.17357468605041504

Final encoder loss: 0.026579386624342668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07099199295043945 0.17392325401306152

Final encoder loss: 0.027732868897691675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07092976570129395 0.17347145080566406

Final encoder loss: 0.02156564830990587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07087397575378418 0.17370128631591797

Final encoder loss: 0.02163612780793875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07208657264709473 0.17389631271362305

Final encoder loss: 0.021085825203776665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710904598236084 0.17354106903076172

Final encoder loss: 0.021118663777507176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0713660717010498 0.17373442649841309

Final encoder loss: 0.017518739067074195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0710458755493164 0.17400789260864258

Final encoder loss: 0.016822634540238166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0708010196685791 0.1739790439605713

Final encoder loss: 0.01770094472379924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07099270820617676 0.17406296730041504

Final encoder loss: 0.018047696557137036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07110738754272461 0.17373394966125488


Training wesad model
Final encoder loss: 0.21556785702705383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10671448707580566 0.03305673599243164

Final encoder loss: 0.09815525263547897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10497570037841797 0.0328059196472168

Final encoder loss: 0.06375663727521896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395383834838867 0.03297138214111328

Final encoder loss: 0.04656190425157547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10339021682739258 0.033355712890625

Final encoder loss: 0.036691706627607346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10428524017333984 0.033356666564941406

Final encoder loss: 0.030690105631947517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10378623008728027 0.03307843208312988

Final encoder loss: 0.026895156130194664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10408496856689453 0.033353567123413086

Final encoder loss: 0.024450186640024185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10352277755737305 0.034282684326171875

Final encoder loss: 0.022816495969891548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10372757911682129 0.03267669677734375

Final encoder loss: 0.021697020158171654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365486145019531 0.03279614448547363

Final encoder loss: 0.021052997559309006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10417461395263672 0.03267860412597656

Final encoder loss: 0.020724043250083923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10439729690551758 0.03247570991516113

Final encoder loss: 0.020799536257982254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10374808311462402 0.03254365921020508

Final encoder loss: 0.02085268497467041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10453510284423828 0.03293871879577637

Final encoder loss: 0.020965201780200005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375332832336426 0.03313040733337402

Final encoder loss: 0.020939895883202553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10379981994628906 0.03250575065612793

Final encoder loss: 0.02099938690662384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.103790283203125 0.032982826232910156

Final encoder loss: 0.020840896293520927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10436367988586426 0.03253912925720215

Final encoder loss: 0.020773014053702354

Calculating loss for amigos model
	Full Pass 0.6391632556915283
numFreeParamsPath 18
Reconstruction loss values: 0.03172497823834419 0.040909238159656525

Calculating loss for dapper model
	Full Pass 0.15190529823303223
numFreeParamsPath 18
Reconstruction loss values: 0.025002235546708107 0.027408557012677193

Calculating loss for case model
	Full Pass 0.8561041355133057
numFreeParamsPath 18
Reconstruction loss values: 0.03632073476910591 0.039555154740810394

Calculating loss for emognition model
	Full Pass 0.2796151638031006
numFreeParamsPath 18
Reconstruction loss values: 0.03913085162639618 0.04722237214446068

Calculating loss for empatch model
	Full Pass 0.10412979125976562
numFreeParamsPath 18
Reconstruction loss values: 0.041453998535871506 0.049226220697164536

Calculating loss for wesad model
	Full Pass 0.07719206809997559
numFreeParamsPath 18
Reconstruction loss values: 0.04223417863249779 0.06020114943385124
Total loss calculation time: 3.7559874057769775

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.1261162757873535
Total epoch time: 182.76644897460938

Epoch: 38

Training emognition model
Final encoder loss: 0.03957091954236987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0899348258972168 0.28565454483032227

Final encoder loss: 0.035855131508101816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.0847933292388916 0.27626776695251465

Final encoder loss: 0.037888596352919655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08388018608093262 0.27602386474609375

Final encoder loss: 0.035748344532825785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08430838584899902 0.275815486907959

Final encoder loss: 0.035139099448472086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08400774002075195 0.276705265045166

Final encoder loss: 0.03629438017335167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0835564136505127 0.27541327476501465

Final encoder loss: 0.03777821927890092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08540201187133789 0.2750229835510254

Final encoder loss: 0.03485713467505449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0838465690612793 0.2750849723815918

Final encoder loss: 0.03626811154780583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08383369445800781 0.27582502365112305

Final encoder loss: 0.03744822563722838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08421206474304199 0.2769930362701416

Final encoder loss: 0.03560589874544051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08395504951477051 0.2762153148651123

Final encoder loss: 0.03605541732544163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08481001853942871 0.275118350982666

Final encoder loss: 0.03485209771840676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0848536491394043 0.27576398849487305

Final encoder loss: 0.0355564143206009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08351969718933105 0.275526762008667

Final encoder loss: 0.03464570567311663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08461666107177734 0.2763381004333496

Final encoder loss: 0.036424942215931434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08288264274597168 0.2746291160583496


Training amigos model
Final encoder loss: 0.03219406792203616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1081547737121582 0.39019775390625

Final encoder loss: 0.030684611581339994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10924649238586426 0.38935375213623047

Final encoder loss: 0.029017692349193925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10948491096496582 0.39049243927001953

Final encoder loss: 0.02892326079864004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10904383659362793 0.389847993850708

Final encoder loss: 0.02929683280625286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10856938362121582 0.3901951313018799

Final encoder loss: 0.0313587526641954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1085209846496582 0.3893277645111084

Final encoder loss: 0.02920522269640538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10877442359924316 0.3892860412597656

Final encoder loss: 0.030883983530375404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1084444522857666 0.39087748527526855

Final encoder loss: 0.030236888887599667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10908079147338867 0.38945651054382324

Final encoder loss: 0.028602382361743613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.11029553413391113 0.38943982124328613

Final encoder loss: 0.029266360425693704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10918760299682617 0.3927738666534424

Final encoder loss: 0.03274130326072738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10916900634765625 0.3898313045501709

Final encoder loss: 0.028073426714506307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10856103897094727 0.3891422748565674

Final encoder loss: 0.03104005967344274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10830950736999512 0.38950252532958984

Final encoder loss: 0.030277189319438704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10847735404968262 0.38956332206726074

Final encoder loss: 0.03015542638607366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10334610939025879 0.3850433826446533


Training dapper model
Final encoder loss: 0.024452233840570943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06380200386047363 0.15096116065979004

Final encoder loss: 0.025645333516645382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06220269203186035 0.15093016624450684

Final encoder loss: 0.024256532007274543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06233954429626465 0.15157818794250488

Final encoder loss: 0.02472615964387168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06344270706176758 0.15075254440307617

Final encoder loss: 0.02405222031943174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0625452995300293 0.15105724334716797

Final encoder loss: 0.024272886027314193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06279826164245605 0.15221214294433594

Final encoder loss: 0.024654853889398087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06229519844055176 0.151076078414917

Final encoder loss: 0.023965253815014635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06211519241333008 0.15110301971435547

Final encoder loss: 0.02023586032490375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06277608871459961 0.15214776992797852

Final encoder loss: 0.02363000123580828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06242179870605469 0.15085172653198242

Final encoder loss: 0.023010607469763557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06219887733459473 0.15111994743347168

Final encoder loss: 0.020967354869470635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06319260597229004 0.15103673934936523

Final encoder loss: 0.022543759592402915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0623781681060791 0.15066766738891602

Final encoder loss: 0.020114055525018926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06235527992248535 0.15226268768310547

Final encoder loss: 0.022103008130121907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06236433982849121 0.15010595321655273

Final encoder loss: 0.02566381957193153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.061692237854003906 0.14977121353149414


Training case model
Final encoder loss: 0.037922621042800295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09245085716247559 0.266171932220459

Final encoder loss: 0.03501804293606368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09132003784179688 0.265885591506958

Final encoder loss: 0.03353420019789092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.091400146484375 0.2660562992095947

Final encoder loss: 0.03194568185668787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09200930595397949 0.26606035232543945

Final encoder loss: 0.03242771478214455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09167814254760742 0.2656283378601074

Final encoder loss: 0.03069263239918685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0912930965423584 0.2649362087249756

Final encoder loss: 0.03086884786672151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09168863296508789 0.26683855056762695

Final encoder loss: 0.030077606799720664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09168648719787598 0.26564526557922363

Final encoder loss: 0.029447773160813873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09228038787841797 0.2646760940551758

Final encoder loss: 0.03024834659513944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09158110618591309 0.2653486728668213

Final encoder loss: 0.028887784737592615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09167885780334473 0.2653622627258301

Final encoder loss: 0.028876059527802183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0926980972290039 0.2664458751678467

Final encoder loss: 0.030832544974430016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09159660339355469 0.26559877395629883

Final encoder loss: 0.028657470905554712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.5216538906097412 0.2667522430419922

Final encoder loss: 0.028727720054623873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09155845642089844 0.2654843330383301

Final encoder loss: 0.02765893866098997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08856725692749023 0.26271986961364746


Training amigos model
Final encoder loss: 0.025524502551215578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10744500160217285 0.3417198657989502

Final encoder loss: 0.0225844273245282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10652327537536621 0.34178662300109863

Final encoder loss: 0.02160517522220358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10621070861816406 0.3416328430175781

Final encoder loss: 0.0215964058667568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10618996620178223 0.3419804573059082

Final encoder loss: 0.01972666923331164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10834407806396484 0.3415989875793457

Final encoder loss: 0.023332553167879324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10634946823120117 0.3419985771179199

Final encoder loss: 0.022478812018237055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1060340404510498 0.3418889045715332

Final encoder loss: 0.021259716516801683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10676693916320801 0.34177088737487793

Final encoder loss: 0.021468191606077797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10694718360900879 0.34169435501098633

Final encoder loss: 0.02230508256349599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10727596282958984 0.3417503833770752

Final encoder loss: 0.021724801205201014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10637068748474121 0.3416604995727539

Final encoder loss: 0.021504765050689444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10625648498535156 0.3417181968688965

Final encoder loss: 0.021484889490034298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1164846420288086 0.34177160263061523

Final encoder loss: 0.022744542334067412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10810661315917969 0.34149980545043945

Final encoder loss: 0.022018065388639447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10663938522338867 0.3420388698577881

Final encoder loss: 0.02370789022618875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1014556884765625 0.33972930908203125


Training amigos model
Final encoder loss: 0.18076740205287933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47727370262145996 0.07813429832458496

Final encoder loss: 0.187834694981575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.473116397857666 0.07534122467041016

Final encoder loss: 0.18362906575202942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46686339378356934 0.07971644401550293

Final encoder loss: 0.07516361773014069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4717440605163574 0.0754389762878418

Final encoder loss: 0.07752605527639389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47113990783691406 0.08012866973876953

Final encoder loss: 0.07111851125955582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47109103202819824 0.07423949241638184

Final encoder loss: 0.04417743161320686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47332763671875 0.0805516242980957

Final encoder loss: 0.0452418178319931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47605323791503906 0.07418608665466309

Final encoder loss: 0.04212130233645439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.466188907623291 0.07562923431396484

Final encoder loss: 0.032272230833768845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47138214111328125 0.07552862167358398

Final encoder loss: 0.03330003470182419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47099733352661133 0.07636165618896484

Final encoder loss: 0.03168736398220062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4718897342681885 0.07617068290710449

Final encoder loss: 0.02692265436053276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47020530700683594 0.07514214515686035

Final encoder loss: 0.027919773012399673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4696657657623291 0.07965540885925293

Final encoder loss: 0.026718679815530777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4672105312347412 0.07491302490234375

Final encoder loss: 0.024623405188322067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47269439697265625 0.07582545280456543

Final encoder loss: 0.025338487699627876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4715731143951416 0.07445621490478516

Final encoder loss: 0.02449699118733406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46800827980041504 0.08038067817687988

Final encoder loss: 0.02414199337363243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4738450050354004 0.07431459426879883

Final encoder loss: 0.024405157193541527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46885156631469727 0.07776117324829102

Final encoder loss: 0.024117596447467804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4669835567474365 0.0760042667388916

Final encoder loss: 0.024217618629336357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4707348346710205 0.07750082015991211

Final encoder loss: 0.024268705397844315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4742605686187744 0.0764927864074707

Final encoder loss: 0.02419804222881794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4663732051849365 0.07469606399536133

Final encoder loss: 0.023838292807340622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47199392318725586 0.07866406440734863

Final encoder loss: 0.023763779550790787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4733576774597168 0.07564401626586914

Final encoder loss: 0.02366846241056919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4667990207672119 0.07552099227905273

Final encoder loss: 0.02281024307012558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47039103507995605 0.07578158378601074

Final encoder loss: 0.023307347670197487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4708552360534668 0.07950401306152344

Final encoder loss: 0.023040540516376495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4727921485900879 0.07368803024291992

Final encoder loss: 0.02224908582866192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4676978588104248 0.0755765438079834

Final encoder loss: 0.022402416914701462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47189950942993164 0.07658267021179199

Final encoder loss: 0.02237600088119507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46831798553466797 0.07560396194458008

Final encoder loss: 0.022312935441732407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47139930725097656 0.0810537338256836

Final encoder loss: 0.02194657176733017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4739830493927002 0.07454562187194824

Final encoder loss: 0.02242346853017807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47032880783081055 0.07612037658691406

Final encoder loss: 0.022059062495827675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.459392786026001 0.0740971565246582

Final encoder loss: 0.021783193573355675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46033596992492676 0.07536578178405762

Final encoder loss: 0.02206272818148136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4559168815612793 0.07457304000854492

Final encoder loss: 0.021844081580638885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46207499504089355 0.0756831169128418

Final encoder loss: 0.02192554995417595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4594302177429199 0.07619881629943848

Final encoder loss: 0.021899960935115814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46975088119506836 0.07677197456359863

Final encoder loss: 0.021537043154239655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46828627586364746 0.07752180099487305

Final encoder loss: 0.021330256015062332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4715542793273926 0.07994461059570312

Final encoder loss: 0.02152267098426819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47054386138916016 0.07533907890319824

Final encoder loss: 0.021410318091511726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46897006034851074 0.07506322860717773

Final encoder loss: 0.021455299109220505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4726991653442383 0.07773494720458984

Final encoder loss: 0.021661845967173576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4691352844238281 0.07335114479064941

Final encoder loss: 0.021173078566789627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4691624641418457 0.07738947868347168

Final encoder loss: 0.02112393267452717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4714994430541992 0.08094024658203125

Final encoder loss: 0.021459920331835747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4720602035522461 0.0761566162109375

Final encoder loss: 0.02124328352510929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4720325469970703 0.07533001899719238

Final encoder loss: 0.021082477644085884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4713106155395508 0.08050918579101562

Final encoder loss: 0.021409517154097557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4723014831542969 0.07392072677612305

Final encoder loss: 0.021000077947974205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.465378999710083 0.07650113105773926

Final encoder loss: 0.02063518948853016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47138547897338867 0.08060860633850098

Final encoder loss: 0.02104974538087845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.471189022064209 0.07626938819885254

Final encoder loss: 0.02099858596920967
Final encoder loss: 0.019858747720718384
Final encoder loss: 0.01932159997522831

Training dapper model
Final encoder loss: 0.019459605387121253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.061293601989746094 0.10785818099975586

Final encoder loss: 0.0167117470792374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05988049507141113 0.10732293128967285

Final encoder loss: 0.01870298793494212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06021690368652344 0.10748124122619629

Final encoder loss: 0.017494332982827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06046009063720703 0.10829734802246094

Final encoder loss: 0.01758113375438742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.0601801872253418 0.10774898529052734

Final encoder loss: 0.016697330306843908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060074567794799805 0.10884666442871094

Final encoder loss: 0.01717642633273777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.06012463569641113 0.10765838623046875

Final encoder loss: 0.01828172934063891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.0606989860534668 0.10761547088623047

Final encoder loss: 0.01614151116745323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.060112953186035156 0.10741114616394043

Final encoder loss: 0.015519790467656109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06005406379699707 0.10791850090026855

Final encoder loss: 0.01761608000661251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060817718505859375 0.10706782341003418

Final encoder loss: 0.021274604134668685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06031084060668945 0.10768461227416992

Final encoder loss: 0.01707008443212906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06136894226074219 0.10815954208374023

Final encoder loss: 0.017198260856017847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06074380874633789 0.10806155204772949

Final encoder loss: 0.01731455427299165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06011176109313965 0.10750460624694824

Final encoder loss: 0.015127628887719903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059800148010253906 0.10799336433410645


Training dapper model
Final encoder loss: 0.20245212316513062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11941361427307129 0.03393912315368652

Final encoder loss: 0.20820288360118866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11576080322265625 0.03459000587463379

Final encoder loss: 0.08519551157951355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1164703369140625 0.03444218635559082

Final encoder loss: 0.08757635951042175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11810994148254395 0.035004615783691406

Final encoder loss: 0.049737926572561264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11628556251525879 0.03490138053894043

Final encoder loss: 0.049943871796131134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159982681274414 0.03464174270629883

Final encoder loss: 0.03389386460185051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11844086647033691 0.035314321517944336

Final encoder loss: 0.033967629075050354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11610817909240723 0.03372597694396973

Final encoder loss: 0.02592994086444378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580824851989746 0.03369331359863281

Final encoder loss: 0.02605690248310566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11756348609924316 0.03691363334655762

Final encoder loss: 0.021663937717676163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1172327995300293 0.0349431037902832

Final encoder loss: 0.021749641746282578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1163187026977539 0.034642696380615234

Final encoder loss: 0.019160417839884758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11764812469482422 0.034755706787109375

Final encoder loss: 0.019317010417580605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11615824699401855 0.034081459045410156

Final encoder loss: 0.017741834744811058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11597204208374023 0.0341486930847168

Final encoder loss: 0.017847836017608643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1170966625213623 0.03548288345336914

Final encoder loss: 0.016927238553762436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1171417236328125 0.03396248817443848

Final encoder loss: 0.016972864046692848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11567401885986328 0.034807443618774414

Final encoder loss: 0.016291659325361252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11823463439941406 0.03410935401916504

Final encoder loss: 0.016602618619799614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150519847869873 0.03403615951538086

Final encoder loss: 0.016248634085059166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11520600318908691 0.03409624099731445

Final encoder loss: 0.01643475890159607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11599922180175781 0.0336759090423584

Final encoder loss: 0.01609572395682335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11524009704589844 0.03386211395263672

Final encoder loss: 0.016231007874011993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11515974998474121 0.03558707237243652

Final encoder loss: 0.015836093574762344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11555147171020508 0.03336787223815918

Final encoder loss: 0.01617688313126564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11528420448303223 0.03435540199279785

Final encoder loss: 0.01591460220515728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1151280403137207 0.03429412841796875

Final encoder loss: 0.01580677554011345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11574149131774902 0.0340731143951416

Final encoder loss: 0.015562833286821842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1154778003692627 0.03437042236328125

Final encoder loss: 0.015882616862654686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150810718536377 0.034148216247558594

Final encoder loss: 0.015255585312843323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11602330207824707 0.033332109451293945

Final encoder loss: 0.015680605545639992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1153867244720459 0.03364205360412598

Final encoder loss: 0.01515437476336956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1155238151550293 0.034340858459472656

Final encoder loss: 0.015298202633857727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11560916900634766 0.034287452697753906

Final encoder loss: 0.015023256652057171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11532020568847656 0.03413248062133789

Final encoder loss: 0.015001272782683372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11508798599243164 0.03382420539855957

Final encoder loss: 0.014866688288748264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11575198173522949 0.034099578857421875

Final encoder loss: 0.014919920824468136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11532711982727051 0.03379416465759277

Final encoder loss: 0.01474262960255146
Final encoder loss: 0.01391239371150732

Training case model
Final encoder loss: 0.02579177188477983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08955025672912598 0.21937298774719238

Final encoder loss: 0.024903785989075188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08986854553222656 0.2190697193145752

Final encoder loss: 0.024998261571550863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08951807022094727 0.21965813636779785

Final encoder loss: 0.02401763968808083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.0896914005279541 0.21883726119995117

Final encoder loss: 0.025021762163412738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08992576599121094 0.21930813789367676

Final encoder loss: 0.024445567408157842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.10356974601745605 0.2193300724029541

Final encoder loss: 0.024288768505009724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08994174003601074 0.21906375885009766

Final encoder loss: 0.02463043000921099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08970856666564941 0.2194063663482666

Final encoder loss: 0.024680854626832158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08911490440368652 0.2189934253692627

Final encoder loss: 0.02431306875379981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08929896354675293 0.21916604042053223

Final encoder loss: 0.024808668840417287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09056997299194336 0.2193303108215332

Final encoder loss: 0.02475292681679479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09009504318237305 0.2189621925354004

Final encoder loss: 0.0245846466934454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08983039855957031 0.21939921379089355

Final encoder loss: 0.024536574022399636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08937954902648926 0.21890950202941895

Final encoder loss: 0.024452049186462495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.10391664505004883 0.21919560432434082

Final encoder loss: 0.025432769060617096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08744168281555176 0.21612548828125


Training case model
Final encoder loss: 0.20297901332378387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2636878490447998 0.053560495376586914

Final encoder loss: 0.18890394270420074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27301788330078125 0.05293631553649902

Final encoder loss: 0.1901545375585556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27085328102111816 0.05211949348449707

Final encoder loss: 0.1921863853931427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27062177658081055 0.05277609825134277

Final encoder loss: 0.18081405758857727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25878405570983887 0.05152010917663574

Final encoder loss: 0.19191761314868927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541518211364746 0.05129384994506836

Final encoder loss: 0.10156136751174927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25989270210266113 0.05275869369506836

Final encoder loss: 0.0911875069141388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690155506134033 0.053132057189941406

Final encoder loss: 0.0878906175494194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2710263729095459 0.051189422607421875

Final encoder loss: 0.08668909966945648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25757622718811035 0.052603960037231445

Final encoder loss: 0.07917559146881104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25751566886901855 0.05208230018615723

Final encoder loss: 0.08216993510723114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544524669647217 0.05228257179260254

Final encoder loss: 0.05941248685121536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.256986141204834 0.051334381103515625

Final encoder loss: 0.05367860943078995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588324546813965 0.0515744686126709

Final encoder loss: 0.052095044404268265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27676939964294434 0.053354501724243164

Final encoder loss: 0.05228240042924881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590785026550293 0.053641557693481445

Final encoder loss: 0.04902956634759903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2697005271911621 0.05542564392089844

Final encoder loss: 0.050770167261362076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558884620666504 0.053546905517578125

Final encoder loss: 0.042427908629179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2597932815551758 0.054547786712646484

Final encoder loss: 0.039322011172771454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583503723144531 0.05272936820983887

Final encoder loss: 0.03828256204724312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25814270973205566 0.05324387550354004

Final encoder loss: 0.03882279619574547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2696969509124756 0.05277681350708008

Final encoder loss: 0.03758544102311134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2695498466491699 0.05292344093322754

Final encoder loss: 0.03830116242170334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25680017471313477 0.052361249923706055

Final encoder loss: 0.035311806946992874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2687094211578369 0.05566072463989258

Final encoder loss: 0.03388515114784241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2775421142578125 0.05281782150268555

Final encoder loss: 0.03307454660534859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26764416694641113 0.05647683143615723

Final encoder loss: 0.03367362916469574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27134108543395996 0.05271410942077637

Final encoder loss: 0.033772364258766174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576329708099365 0.05348372459411621

Final encoder loss: 0.03366896137595177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25565314292907715 0.0524601936340332

Final encoder loss: 0.03275344893336296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25697970390319824 0.052748680114746094

Final encoder loss: 0.031991951167583466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27199673652648926 0.05272507667541504

Final encoder loss: 0.03169985115528107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27193164825439453 0.053675174713134766

Final encoder loss: 0.03200199827551842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27574658393859863 0.05224943161010742

Final encoder loss: 0.03271281719207764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2705848217010498 0.0525357723236084

Final encoder loss: 0.03218492493033409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25524044036865234 0.051786184310913086

Final encoder loss: 0.03076569363474846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2587857246398926 0.05206632614135742

Final encoder loss: 0.029939623549580574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2669196128845215 0.05501556396484375

Final encoder loss: 0.029743501916527748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.256974458694458 0.05308938026428223

Final encoder loss: 0.0299915112555027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2665293216705322 0.05300307273864746

Final encoder loss: 0.03050430491566658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26714420318603516 0.05160236358642578

Final encoder loss: 0.02993636392056942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25390052795410156 0.051215410232543945

Final encoder loss: 0.029307447373867035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2572643756866455 0.05179142951965332

Final encoder loss: 0.028726866468787193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2741367816925049 0.0522465705871582

Final encoder loss: 0.028301557525992393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26738977432250977 0.05144762992858887

Final encoder loss: 0.028569670394062996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27379631996154785 0.0515596866607666

Final encoder loss: 0.029511185362935066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26700282096862793 0.05156278610229492

Final encoder loss: 0.028798798099160194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549424171447754 0.05155444145202637

Final encoder loss: 0.028150860220193863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2562112808227539 0.052129268646240234

Final encoder loss: 0.028012337163090706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2725856304168701 0.05361056327819824

Final encoder loss: 0.027400150895118713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2565639019012451 0.052788734436035156

Final encoder loss: 0.027542509138584137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26666975021362305 0.0516049861907959

Final encoder loss: 0.02845391444861889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2740635871887207 0.053025007247924805

Final encoder loss: 0.02779315784573555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25449109077453613 0.051284074783325195

Final encoder loss: 0.027712857350707054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25592851638793945 0.05176353454589844

Final encoder loss: 0.027415169402956963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26755285263061523 0.05277872085571289

Final encoder loss: 0.02725916914641857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25839948654174805 0.05284237861633301

Final encoder loss: 0.0273382980376482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2679898738861084 0.05182385444641113

Final encoder loss: 0.028242817148566246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2561771869659424 0.05146646499633789

Final encoder loss: 0.02763134054839611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25449132919311523 0.051688432693481445

Final encoder loss: 0.02711007557809353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2680044174194336 0.05241084098815918

Final encoder loss: 0.026798013597726822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2741386890411377 0.05248618125915527

Final encoder loss: 0.02651054598391056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2738053798675537 0.052544593811035156

Final encoder loss: 0.026613852009177208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25642991065979004 0.051232337951660156

Final encoder loss: 0.027452241629362106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25671935081481934 0.05225777626037598

Final encoder loss: 0.026747597381472588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25390005111694336 0.05124211311340332

Final encoder loss: 0.026754330843687057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25693726539611816 0.05311250686645508

Final encoder loss: 0.02658718265593052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566041946411133 0.0512995719909668

Final encoder loss: 0.026308448985219002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2567410469055176 0.05229640007019043

Final encoder loss: 0.026332082226872444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2564969062805176 0.05251812934875488

Final encoder loss: 0.027351846918463707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2563645839691162 0.05273747444152832

Final encoder loss: 0.026640651747584343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25491786003112793 0.051392316818237305

Final encoder loss: 0.026373140513896942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25710415840148926 0.05217599868774414

Final encoder loss: 0.026181740686297417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2737259864807129 0.05242037773132324

Final encoder loss: 0.025801246985793114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2564561367034912 0.05301046371459961

Final encoder loss: 0.025760279968380928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635385513305664 0.0523838996887207

Final encoder loss: 0.026771336793899536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2568933963775635 0.05258893966674805

Final encoder loss: 0.02608468569815159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25400257110595703 0.052124977111816406

Final encoder loss: 0.026223041117191315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.259427547454834 0.053383588790893555

Final encoder loss: 0.026020459830760956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25812721252441406 0.05241060256958008

Final encoder loss: 0.025816507637500763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25699567794799805 0.05273246765136719

Final encoder loss: 0.025736968964338303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25716161727905273 0.05297589302062988

Final encoder loss: 0.026642587035894394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575960159301758 0.05278754234313965

Final encoder loss: 0.0261489599943161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25525426864624023 0.051410675048828125

Final encoder loss: 0.02594364993274212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2563173770904541 0.05177950859069824

Final encoder loss: 0.0257561057806015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27553772926330566 0.051985979080200195

Final encoder loss: 0.025451114401221275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26665735244750977 0.05353045463562012

Final encoder loss: 0.02541792392730713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574329376220703 0.0524747371673584

Final encoder loss: 0.026343803852796555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2686338424682617 0.05203557014465332

Final encoder loss: 0.025577859953045845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25368595123291016 0.05181002616882324

Final encoder loss: 0.025791089981794357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25667452812194824 0.05197644233703613

Final encoder loss: 0.025636550039052963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.256725549697876 0.0514070987701416

Final encoder loss: 0.025409884750843048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25750255584716797 0.05290532112121582

Final encoder loss: 0.025387832894921303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26818346977233887 0.051718711853027344

Final encoder loss: 0.026274487376213074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2684061527252197 0.052797555923461914

Final encoder loss: 0.025638863444328308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.253812313079834 0.05156826972961426

Final encoder loss: 0.025622088462114334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2672402858734131 0.05292916297912598

Final encoder loss: 0.025332454591989517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26722168922424316 0.051586151123046875

Final encoder loss: 0.02505812793970108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2741098403930664 0.05244088172912598

Final encoder loss: 0.02497117966413498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568390369415283 0.052530527114868164

Final encoder loss: 0.02591823600232601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2739231586456299 0.05218076705932617

Final encoder loss: 0.025232331827282906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2531430721282959 0.052370309829711914

Final encoder loss: 0.025489022955298424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25684618949890137 0.052902936935424805

Final encoder loss: 0.025326600298285484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2666447162628174 0.05300760269165039

Final encoder loss: 0.025183770805597305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25675320625305176 0.05133247375488281

Final encoder loss: 0.024972189217805862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25622081756591797 0.052056074142456055

Final encoder loss: 0.02591421641409397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2572751045227051 0.05225872993469238

Final encoder loss: 0.025310084223747253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552347183227539 0.051666259765625

Final encoder loss: 0.02535524033010006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568833827972412 0.05218148231506348

Final encoder loss: 0.025145983323454857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2674705982208252 0.05261969566345215

Final encoder loss: 0.024837035685777664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2671177387237549 0.05173158645629883

Final encoder loss: 0.0247942004352808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27393126487731934 0.0517120361328125

Final encoder loss: 0.025715772062540054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25597119331359863 0.05250263214111328

Final encoder loss: 0.025014076381921768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25563621520996094 0.05173087120056152

Final encoder loss: 0.025247253477573395
Final encoder loss: 0.02460067719221115
Final encoder loss: 0.023647595196962357
Final encoder loss: 0.02284599281847477
Final encoder loss: 0.022750217467546463
Final encoder loss: 0.02133982814848423

Training emognition model
Final encoder loss: 0.02996320931079012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.09972000122070312 0.23128437995910645

Final encoder loss: 0.029178942930373594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08263707160949707 0.2306046485900879

Final encoder loss: 0.027815313723980183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08163595199584961 0.23062467575073242

Final encoder loss: 0.029387253544296846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08219766616821289 0.23108339309692383

Final encoder loss: 0.029034532697510145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08135509490966797 0.23092889785766602

Final encoder loss: 0.02839042673712212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08157563209533691 0.23103547096252441

Final encoder loss: 0.029696082344305236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08298015594482422 0.23139333724975586

Final encoder loss: 0.02759262390868099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08130693435668945 0.23127341270446777

Final encoder loss: 0.030516402718806297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08130216598510742 0.23140430450439453

Final encoder loss: 0.028193626528598692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08272767066955566 0.2314622402191162

Final encoder loss: 0.027721571396922988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0819084644317627 0.23097777366638184

Final encoder loss: 0.027670853014676994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08163118362426758 0.23121356964111328

Final encoder loss: 0.027854322808822225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08259916305541992 0.23081088066101074

Final encoder loss: 0.028091367823277282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08171987533569336 0.23076868057250977

Final encoder loss: 0.029639757533174167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08164262771606445 0.23162198066711426

Final encoder loss: 0.028108928785561888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08028531074523926 0.2298750877380371


Training emognition model
Final encoder loss: 0.1935693472623825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2515897750854492 0.05129718780517578

Final encoder loss: 0.19496555626392365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24930644035339355 0.04912161827087402

Final encoder loss: 0.08642637729644775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24911022186279297 0.049672842025756836

Final encoder loss: 0.08583611994981766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24854278564453125 0.048554182052612305

Final encoder loss: 0.05552542582154274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.250380277633667 0.04956698417663574

Final encoder loss: 0.05383257940411568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2504384517669678 0.04916834831237793

Final encoder loss: 0.04203077405691147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485823631286621 0.04785585403442383

Final encoder loss: 0.040835291147232056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24999070167541504 0.051657915115356445

Final encoder loss: 0.03528057783842087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24747180938720703 0.04948878288269043

Final encoder loss: 0.03449426218867302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2500269412994385 0.050588130950927734

Final encoder loss: 0.031618792563676834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24774432182312012 0.0493922233581543

Final encoder loss: 0.031058568507432938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474524974822998 0.0505213737487793

Final encoder loss: 0.02954276092350483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24775028228759766 0.048287391662597656

Final encoder loss: 0.029252516105771065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24858522415161133 0.05011749267578125

Final encoder loss: 0.028697224333882332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24893689155578613 0.04842185974121094

Final encoder loss: 0.028560565784573555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2483673095703125 0.04953360557556152

Final encoder loss: 0.028427744284272194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25289297103881836 0.04799985885620117

Final encoder loss: 0.02846045047044754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24755501747131348 0.04875922203063965

Final encoder loss: 0.028506502509117126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.250154972076416 0.048543453216552734

Final encoder loss: 0.028564725071191788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24738311767578125 0.05011296272277832

Final encoder loss: 0.02824355661869049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24894261360168457 0.05041766166687012

Final encoder loss: 0.02843364141881466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24657416343688965 0.049297332763671875

Final encoder loss: 0.02814684994518757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24796080589294434 0.05033087730407715

Final encoder loss: 0.028357744216918945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2480771541595459 0.0498199462890625

Final encoder loss: 0.027922477573156357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2471907138824463 0.05158877372741699

Final encoder loss: 0.028059659525752068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24900007247924805 0.04894542694091797

Final encoder loss: 0.027888692915439606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472074031829834 0.04904675483703613

Final encoder loss: 0.028111914172768593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24897503852844238 0.04915618896484375

Final encoder loss: 0.027792982757091522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24813151359558105 0.04926133155822754

Final encoder loss: 0.02786301076412201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24895071983337402 0.04877662658691406

Final encoder loss: 0.02762308157980442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24703550338745117 0.04865598678588867

Final encoder loss: 0.027774201706051826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24823212623596191 0.05297112464904785

Final encoder loss: 0.027253489941358566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24834370613098145 0.050217628479003906

Final encoder loss: 0.02749588154256344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24889016151428223 0.05080580711364746

Final encoder loss: 0.027197562158107758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484734058380127 0.049622535705566406

Final encoder loss: 0.02749781496822834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2462303638458252 0.05108380317687988

Final encoder loss: 0.02709224633872509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24826693534851074 0.049805402755737305

Final encoder loss: 0.027383223176002502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24645614624023438 0.050508975982666016

Final encoder loss: 0.027152055874466896
Final encoder loss: 0.026408057659864426

Training empatch model
Final encoder loss: 0.04076247379129146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07137179374694824 0.1741344928741455

Final encoder loss: 0.041556940280290534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07265686988830566 0.1739044189453125

Final encoder loss: 0.03837168268985735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07149505615234375 0.17453289031982422

Final encoder loss: 0.041106985341028426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07137417793273926 0.1736447811126709

Final encoder loss: 0.0398723525519343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07107973098754883 0.17423510551452637

Final encoder loss: 0.035307445750469756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07323646545410156 0.17357873916625977

Final encoder loss: 0.0365239209851039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07196354866027832 0.17386436462402344

Final encoder loss: 0.03596141407926295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07072901725769043 0.17392873764038086

Final encoder loss: 0.026771465778653254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0728306770324707 0.17415285110473633

Final encoder loss: 0.02435746888916504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07136154174804688 0.1740560531616211

Final encoder loss: 0.031242209143071673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07151556015014648 0.17377996444702148

Final encoder loss: 0.028088514803552688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07126331329345703 0.17427492141723633

Final encoder loss: 0.027989398942936557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07328200340270996 0.17392611503601074

Final encoder loss: 0.027777111837608785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0715341567993164 0.17416930198669434

Final encoder loss: 0.02683001474478974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07125735282897949 0.17429041862487793

Final encoder loss: 0.02893011319835488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07245826721191406 0.17380619049072266


Training empatch model
Final encoder loss: 0.1711585968732834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17789983749389648 0.04415559768676758

Final encoder loss: 0.08031719923019409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17676353454589844 0.04497122764587402

Final encoder loss: 0.05579175800085068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17888092994689941 0.04424881935119629

Final encoder loss: 0.04388130456209183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17656779289245605 0.04381728172302246

Final encoder loss: 0.03703450411558151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17526745796203613 0.04348635673522949

Final encoder loss: 0.03273961693048477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17842721939086914 0.04527163505554199

Final encoder loss: 0.029959317296743393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745152473449707 0.04332590103149414

Final encoder loss: 0.02814239077270031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17592144012451172 0.042841434478759766

Final encoder loss: 0.026904400438070297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1774899959564209 0.045196533203125

Final encoder loss: 0.026105759665369987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17500615119934082 0.04480767250061035

Final encoder loss: 0.02557527832686901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1747591495513916 0.04334402084350586

Final encoder loss: 0.02529330365359783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1773669719696045 0.044922590255737305

Final encoder loss: 0.025005709379911423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17530059814453125 0.044260263442993164

Final encoder loss: 0.024840066209435463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17659735679626465 0.04373049736022949

Final encoder loss: 0.02464999444782734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17772531509399414 0.045055389404296875

Final encoder loss: 0.02456570602953434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17727923393249512 0.04377555847167969

Final encoder loss: 0.024495383724570274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17584514617919922 0.043695688247680664

Final encoder loss: 0.02440020814538002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17627716064453125 0.044675588607788086

Final encoder loss: 0.024312665686011314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504334449768066 0.04274773597717285

Final encoder loss: 0.024311427026987076

Training wesad model
Final encoder loss: 0.04146594454467297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07137608528137207 0.1738414764404297

Final encoder loss: 0.041643442573736277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07239842414855957 0.17380356788635254

Final encoder loss: 0.037811437870754534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07171940803527832 0.1739206314086914

Final encoder loss: 0.03858665384347889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07129979133605957 0.1735377311706543

Final encoder loss: 0.026092771586605383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07120513916015625 0.17475152015686035

Final encoder loss: 0.026359217195704134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0730278491973877 0.17370057106018066

Final encoder loss: 0.028429980220951504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07193708419799805 0.17413711547851562

Final encoder loss: 0.027411086216203456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07125377655029297 0.17415261268615723

Final encoder loss: 0.02130246720134176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07285046577453613 0.17434144020080566

Final encoder loss: 0.020405738107083352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07144665718078613 0.17421627044677734

Final encoder loss: 0.02147126855653077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07160329818725586 0.1739823818206787

Final encoder loss: 0.021538138934064834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07106685638427734 0.1743791103363037

Final encoder loss: 0.017007070522942303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07352161407470703 0.17394328117370605

Final encoder loss: 0.01762467995190884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07175374031066895 0.1739492416381836

Final encoder loss: 0.019061172060538794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07167434692382812 0.1744251251220703

Final encoder loss: 0.01646986073108989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07254815101623535 0.17442059516906738


Training wesad model
Final encoder loss: 0.2155979722738266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1059422492980957 0.03327369689941406

Final encoder loss: 0.09836413711309433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10632038116455078 0.033805131912231445

Final encoder loss: 0.06403909623622894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10471510887145996 0.03291177749633789

Final encoder loss: 0.046757329255342484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10427308082580566 0.034273624420166016

Final encoder loss: 0.03683879226446152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10668587684631348 0.03408670425415039

Final encoder loss: 0.030769387260079384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1053767204284668 0.032784461975097656

Final encoder loss: 0.026924902573227882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393047332763672 0.033666133880615234

Final encoder loss: 0.02441464550793171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10478401184082031 0.033165931701660156

Final encoder loss: 0.02271154150366783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10478901863098145 0.03293347358703613

Final encoder loss: 0.021627767011523247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10555243492126465 0.033791542053222656

Final encoder loss: 0.020935211330652237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10625600814819336 0.03412365913391113

Final encoder loss: 0.020631274208426476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10460495948791504 0.03365039825439453

Final encoder loss: 0.020455781370401382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10417580604553223 0.03331899642944336

Final encoder loss: 0.0204370878636837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10436224937438965 0.03371691703796387

Final encoder loss: 0.020361343398690224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442590713500977 0.03354048728942871

Final encoder loss: 0.02049674279987812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10623741149902344 0.03379487991333008

Final encoder loss: 0.02070331573486328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10640215873718262 0.033434391021728516

Final encoder loss: 0.0208054818212986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10465598106384277 0.03341054916381836

Final encoder loss: 0.02081577107310295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10448598861694336 0.03299689292907715

Final encoder loss: 0.020600151270627975

Calculating loss for amigos model
	Full Pass 0.6762087345123291
numFreeParamsPath 18
Reconstruction loss values: 0.0303226076066494 0.03945779800415039

Calculating loss for dapper model
	Full Pass 0.15310263633728027
numFreeParamsPath 18
Reconstruction loss values: 0.02414439059793949 0.026590347290039062

Calculating loss for case model
	Full Pass 0.8607056140899658
numFreeParamsPath 18
Reconstruction loss values: 0.035105131566524506 0.03836818039417267

Calculating loss for emognition model
	Full Pass 0.29309868812561035
numFreeParamsPath 18
Reconstruction loss values: 0.038791753351688385 0.04654327407479286

Calculating loss for empatch model
	Full Pass 0.10441994667053223
numFreeParamsPath 18
Reconstruction loss values: 0.0406225360929966 0.048049576580524445

Calculating loss for wesad model
	Full Pass 0.07830166816711426
numFreeParamsPath 18
Reconstruction loss values: 0.04140609875321388 0.05875493213534355
Total loss calculation time: 3.849698781967163

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.161052942276001
Total epoch time: 191.0397400856018

Epoch: 39

Training dapper model
Final encoder loss: 0.02416565303592031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06630992889404297 0.1566174030303955

Final encoder loss: 0.021983020796304844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06231546401977539 0.14967608451843262

Final encoder loss: 0.02190584282620087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06142377853393555 0.14902520179748535

Final encoder loss: 0.022712501418923486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.061269521713256836 0.1491551399230957

Final encoder loss: 0.020972181006353608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0612642765045166 0.14853477478027344

Final encoder loss: 0.022589039595577183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06152081489562988 0.14855670928955078

Final encoder loss: 0.020595801179910216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06152963638305664 0.1493394374847412

Final encoder loss: 0.02150610030043842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06135272979736328 0.14879107475280762

Final encoder loss: 0.021097423142828697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06069803237915039 0.149444580078125

Final encoder loss: 0.023188459285938698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06149721145629883 0.14894652366638184

Final encoder loss: 0.02106080028242593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061513662338256836 0.1487720012664795

Final encoder loss: 0.022983086901261888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06151533126831055 0.14933538436889648

Final encoder loss: 0.019506002708881053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06115603446960449 0.1486501693725586

Final encoder loss: 0.021505821375881636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06117582321166992 0.14894604682922363

Final encoder loss: 0.02092286219177886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06137442588806152 0.14890313148498535

Final encoder loss: 0.0233332456503174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06098127365112305 0.14931011199951172


Training case model
Final encoder loss: 0.036300580921349745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09125208854675293 0.26543450355529785

Final encoder loss: 0.033236101425497405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09139871597290039 0.2654564380645752

Final encoder loss: 0.03109140336210356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09209775924682617 0.2686288356781006

Final encoder loss: 0.029676819166400564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09184908866882324 0.26482629776000977

Final encoder loss: 0.03068855627397867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09108805656433105 0.2661113739013672

Final encoder loss: 0.030354450108534367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09228801727294922 0.2649397850036621

Final encoder loss: 0.02997749634677871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09276175498962402 0.2656984329223633

Final encoder loss: 0.029276875696688767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09096384048461914 0.26477885246276855

Final encoder loss: 0.028829585702195955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09111857414245605 0.26474833488464355

Final encoder loss: 0.028770820464752742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09155702590942383 0.2654232978820801

Final encoder loss: 0.029416133634683445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09163022041320801 0.26546573638916016

Final encoder loss: 0.029416036126351772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09111285209655762 0.26503992080688477

Final encoder loss: 0.028821023091467415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09136033058166504 0.2654545307159424

Final encoder loss: 0.027965850366687618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09176826477050781 0.26580262184143066

Final encoder loss: 0.02793944447282244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.0917975902557373 0.2647123336791992

Final encoder loss: 0.02733905860769559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08784270286560059 0.26152753829956055


Training amigos model
Final encoder loss: 0.030387627257955456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10844945907592773 0.389606237411499

Final encoder loss: 0.028568375026105975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10846567153930664 0.38942527770996094

Final encoder loss: 0.030470697848364477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10886621475219727 0.3888125419616699

Final encoder loss: 0.03191364170469168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10851931571960449 0.38942718505859375

Final encoder loss: 0.030463475269849924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10819864273071289 0.3901510238647461

Final encoder loss: 0.028999545655510977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10833239555358887 0.38883376121520996

Final encoder loss: 0.02775549367120256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10898017883300781 0.38959717750549316

Final encoder loss: 0.02972987393832359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10823750495910645 0.38999032974243164

Final encoder loss: 0.03117834044129531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10863614082336426 0.3887627124786377

Final encoder loss: 0.028146373277041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1110076904296875 0.3887920379638672

Final encoder loss: 0.029288680370633445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10829663276672363 0.3884906768798828

Final encoder loss: 0.028761921183548873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10809087753295898 0.3887004852294922

Final encoder loss: 0.026853330636210825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10836052894592285 0.3889153003692627

Final encoder loss: 0.025595597175711197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10838937759399414 0.38924646377563477

Final encoder loss: 0.030215322773962317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10797500610351562 0.38893914222717285

Final encoder loss: 0.030071658356233093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10304594039916992 0.38320207595825195


Training emognition model
Final encoder loss: 0.04029339939131922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08341503143310547 0.27367711067199707

Final encoder loss: 0.037008519600973594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08275079727172852 0.2750868797302246

Final encoder loss: 0.039409133285787865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08371305465698242 0.27541375160217285

Final encoder loss: 0.03720078499557335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08393144607543945 0.2753746509552002

Final encoder loss: 0.03840516033119161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08458304405212402 0.276674747467041

Final encoder loss: 0.03660964942274775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08430123329162598 0.27616381645202637

Final encoder loss: 0.037936607973660594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08492898941040039 0.27593564987182617

Final encoder loss: 0.03563606051500048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08409857749938965 0.27611589431762695

Final encoder loss: 0.035785897795747965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08406639099121094 0.2760474681854248

Final encoder loss: 0.037576276154679976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0850827693939209 0.2759556770324707

Final encoder loss: 0.03532292454754905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08373332023620605 0.2749471664428711

Final encoder loss: 0.03561155270817289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08358120918273926 0.27599239349365234

Final encoder loss: 0.03570059249638335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08428955078125 0.27646732330322266

Final encoder loss: 0.03434094071738775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08374142646789551 0.276292085647583

Final encoder loss: 0.03576108237824773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08431243896484375 0.2754063606262207

Final encoder loss: 0.03655799643218012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0829169750213623 0.2748885154724121


Training amigos model
Final encoder loss: 0.019184833184925653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10661029815673828 0.3420710563659668

Final encoder loss: 0.02202104593689808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10686016082763672 0.3416004180908203

Final encoder loss: 0.021891599655904835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10756087303161621 0.34187960624694824

Final encoder loss: 0.021970825014862867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10634183883666992 0.34205198287963867

Final encoder loss: 0.0212272945964753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10631322860717773 0.3419044017791748

Final encoder loss: 0.023597418152412765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10636043548583984 0.34187746047973633

Final encoder loss: 0.021939798456785252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10709381103515625 0.3447387218475342

Final encoder loss: 0.022622969564000967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10707926750183105 0.34188294410705566

Final encoder loss: 0.021829905418277087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.1063394546508789 0.34143590927124023

Final encoder loss: 0.02064053532094015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10672116279602051 0.3417186737060547

Final encoder loss: 0.021829964269418422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10681724548339844 0.3421592712402344

Final encoder loss: 0.022214096882072147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10757803916931152 0.34180283546447754

Final encoder loss: 0.022903828689795642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10655617713928223 0.3417627811431885

Final encoder loss: 0.020315743889896928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10647368431091309 0.34210634231567383

Final encoder loss: 0.021674558498541236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10627102851867676 0.34171152114868164

Final encoder loss: 0.02122046610159883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10251879692077637 0.3379025459289551


Training amigos model
Final encoder loss: 0.18077890574932098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4741232395172119 0.07792901992797852

Final encoder loss: 0.18784542381763458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4720163345336914 0.07372474670410156

Final encoder loss: 0.1836366355419159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4715385437011719 0.07539844512939453

Final encoder loss: 0.07565581053495407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4738190174102783 0.07721495628356934

Final encoder loss: 0.07833819836378098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4705965518951416 0.077606201171875

Final encoder loss: 0.07179302722215652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46840333938598633 0.0724947452545166

Final encoder loss: 0.04427187517285347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47193193435668945 0.07496452331542969

Final encoder loss: 0.04565979540348053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4712541103363037 0.07375907897949219

Final encoder loss: 0.042421821504831314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46887683868408203 0.07897210121154785

Final encoder loss: 0.03223138675093651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4759554862976074 0.0729055404663086

Final encoder loss: 0.03342993184924126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4684431552886963 0.07528901100158691

Final encoder loss: 0.03180442005395889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4692056179046631 0.07366323471069336

Final encoder loss: 0.026893334463238716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.470583438873291 0.07804226875305176

Final encoder loss: 0.027778077870607376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47448158264160156 0.07728314399719238

Final encoder loss: 0.0268532894551754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46697235107421875 0.07654356956481934

Final encoder loss: 0.024655159562826157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47020506858825684 0.07877373695373535

Final encoder loss: 0.02537846378982067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47237324714660645 0.07080316543579102

Final encoder loss: 0.02447577565908432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47029709815979004 0.07551074028015137

Final encoder loss: 0.024049222469329834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46651434898376465 0.07664322853088379

Final encoder loss: 0.024397622793912888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46968889236450195 0.0777435302734375

Final encoder loss: 0.023827940225601196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4709160327911377 0.07356834411621094

Final encoder loss: 0.023917436599731445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47246432304382324 0.07411408424377441

Final encoder loss: 0.024032466113567352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4696321487426758 0.07541608810424805

Final encoder loss: 0.023825673386454582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46571826934814453 0.08119487762451172

Final encoder loss: 0.023383472114801407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47658276557922363 0.07686018943786621

Final encoder loss: 0.023656649515032768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685835838317871 0.07499384880065918

Final encoder loss: 0.0233450997620821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4682002067565918 0.0812835693359375

Final encoder loss: 0.022680193185806274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47137904167175293 0.07327461242675781

Final encoder loss: 0.022738508880138397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47294139862060547 0.07787108421325684

Final encoder loss: 0.022564591839909554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4645845890045166 0.07531118392944336

Final encoder loss: 0.02212628722190857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4720938205718994 0.08056187629699707

Final encoder loss: 0.022083623334765434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4750175476074219 0.07644128799438477

Final encoder loss: 0.022214876487851143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4691920280456543 0.07563138008117676

Final encoder loss: 0.021929128095507622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4735071659088135 0.07513165473937988

Final encoder loss: 0.02164902724325657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46958470344543457 0.08150553703308105

Final encoder loss: 0.021953929215669632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4742262363433838 0.07496857643127441

Final encoder loss: 0.02179243043065071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4690384864807129 0.07715272903442383

Final encoder loss: 0.021641748026013374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4706597328186035 0.08070492744445801

Final encoder loss: 0.02195761539041996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46790361404418945 0.07349109649658203

Final encoder loss: 0.021537046879529953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4751718044281006 0.07622694969177246

Final encoder loss: 0.021423613652586937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46668195724487305 0.07549262046813965

Final encoder loss: 0.0215920377522707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4659252166748047 0.07940220832824707

Final encoder loss: 0.021223166957497597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47425293922424316 0.07345342636108398

Final encoder loss: 0.021067265421152115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47195911407470703 0.07258224487304688

Final encoder loss: 0.021175885573029518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46958327293395996 0.07598996162414551

Final encoder loss: 0.021148422732949257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47066259384155273 0.07804393768310547

Final encoder loss: 0.020806187763810158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4757053852081299 0.07395243644714355

Final encoder loss: 0.021090473979711533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46582937240600586 0.07470464706420898

Final encoder loss: 0.020995205268263817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4710676670074463 0.07874059677124023

Final encoder loss: 0.020821435377001762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47037577629089355 0.0743708610534668

Final encoder loss: 0.021119045093655586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4713630676269531 0.07546305656433105

Final encoder loss: 0.02104363590478897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4722743034362793 0.07324361801147461

Final encoder loss: 0.02063628099858761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47040414810180664 0.08036160469055176

Final encoder loss: 0.020934516564011574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4688234329223633 0.0754239559173584

Final encoder loss: 0.020780405029654503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4724113941192627 0.07534503936767578

Final encoder loss: 0.0206195879727602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47229695320129395 0.07636761665344238

Final encoder loss: 0.02090325765311718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4678623676300049 0.08025240898132324

Final encoder loss: 0.020788289606571198
Final encoder loss: 0.01934671588242054
Final encoder loss: 0.01890900917351246

Training dapper model
Final encoder loss: 0.018937567760235207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06048178672790527 0.10763740539550781

Final encoder loss: 0.01987919925562303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06000638008117676 0.10736441612243652

Final encoder loss: 0.01695282831865368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06140470504760742 0.10812187194824219

Final encoder loss: 0.018151292103722603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0602107048034668 0.1073455810546875

Final encoder loss: 0.016780011675794727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06006002426147461 0.10772299766540527

Final encoder loss: 0.017792784344007596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06005716323852539 0.10792827606201172

Final encoder loss: 0.016527878503780388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.060555219650268555 0.10678625106811523

Final encoder loss: 0.01641425345005746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06035208702087402 0.10760736465454102

Final encoder loss: 0.017160789404425662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05990338325500488 0.10787677764892578

Final encoder loss: 0.017056954753314958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.060861825942993164 0.10792255401611328

Final encoder loss: 0.01650957265908318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06014418601989746 0.10793852806091309

Final encoder loss: 0.017178915301775934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060129642486572266 0.1074368953704834

Final encoder loss: 0.016643785549254334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06027817726135254 0.10840511322021484

Final encoder loss: 0.016070979479499797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06128835678100586 0.10827016830444336

Final encoder loss: 0.016119715839940228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06066155433654785 0.10775041580200195

Final encoder loss: 0.01784037148199949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.06023120880126953 0.10798120498657227


Training dapper model
Final encoder loss: 0.2024695724248886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11999726295471191 0.0338132381439209

Final encoder loss: 0.20818808674812317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11744999885559082 0.03467416763305664

Final encoder loss: 0.0848567932844162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11672449111938477 0.03466176986694336

Final encoder loss: 0.08710239827632904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11937808990478516 0.03437948226928711

Final encoder loss: 0.04960274323821068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11675381660461426 0.0346379280090332

Final encoder loss: 0.049884356558322906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11931610107421875 0.03553915023803711

Final encoder loss: 0.03388235718011856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11726212501525879 0.034508705139160156

Final encoder loss: 0.034012388437986374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11627459526062012 0.03497624397277832

Final encoder loss: 0.025943033397197723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11833643913269043 0.03441429138183594

Final encoder loss: 0.0260502640157938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1168365478515625 0.033722877502441406

Final encoder loss: 0.021627161651849747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11775326728820801 0.0352935791015625

Final encoder loss: 0.021684616804122925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1170496940612793 0.03368544578552246

Final encoder loss: 0.019308093935251236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1159820556640625 0.03444385528564453

Final encoder loss: 0.019178157672286034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11764836311340332 0.034300804138183594

Final encoder loss: 0.017768429592251778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11687731742858887 0.03423428535461426

Final encoder loss: 0.01780494675040245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11834526062011719 0.03401827812194824

Final encoder loss: 0.017011325806379318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1157522201538086 0.034149169921875

Final encoder loss: 0.017024017870426178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1158745288848877 0.03374052047729492

Final encoder loss: 0.01667151413857937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11543989181518555 0.03411746025085449

Final encoder loss: 0.016499029472470284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527466773986816 0.033281803131103516

Final encoder loss: 0.016472909599542618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11566758155822754 0.03400707244873047

Final encoder loss: 0.016278976574540138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1155385971069336 0.03422689437866211

Final encoder loss: 0.0162043534219265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11504173278808594 0.03405141830444336

Final encoder loss: 0.01607796922326088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11565685272216797 0.03405928611755371

Final encoder loss: 0.016371576115489006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11540889739990234 0.03423047065734863

Final encoder loss: 0.01596912182867527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11522889137268066 0.03396964073181152

Final encoder loss: 0.016119355335831642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11568236351013184 0.034200191497802734

Final encoder loss: 0.01603097654879093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11548352241516113 0.03365325927734375

Final encoder loss: 0.01573849655687809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11530804634094238 0.03400135040283203

Final encoder loss: 0.015985198318958282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11578369140625 0.03333401679992676

Final encoder loss: 0.015233645215630531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11518025398254395 0.03385138511657715

Final encoder loss: 0.015387298539280891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150655746459961 0.0335841178894043

Final encoder loss: 0.015219117514789104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11573410034179688 0.03396272659301758

Final encoder loss: 0.015141076408326626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11549258232116699 0.03359341621398926

Final encoder loss: 0.014974701218307018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11556887626647949 0.03365683555603027

Final encoder loss: 0.014849219471216202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11582517623901367 0.03392291069030762

Final encoder loss: 0.014864577911794186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11535978317260742 0.0338442325592041

Final encoder loss: 0.014898772351443768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11478042602539062 0.03406572341918945

Final encoder loss: 0.01488875225186348
Final encoder loss: 0.013897157274186611

Training case model
Final encoder loss: 0.02645816983871942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08984875679016113 0.21939349174499512

Final encoder loss: 0.02578031693062863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08948040008544922 0.21957874298095703

Final encoder loss: 0.025125686669895467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09015178680419922 0.21926569938659668

Final encoder loss: 0.024836543391022373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08936619758605957 0.21897149085998535

Final encoder loss: 0.025213657901307196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08985400199890137 0.21902179718017578

Final encoder loss: 0.025577564566799587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0895686149597168 0.21909189224243164

Final encoder loss: 0.025080201067915506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08990240097045898 0.21922922134399414

Final encoder loss: 0.024805717609627003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08973813056945801 0.21969103813171387

Final encoder loss: 0.0247158052584474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09006190299987793 0.2195887565612793

Final encoder loss: 0.025479147463692158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09084653854370117 0.21933841705322266

Final encoder loss: 0.025027218848970932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08921170234680176 0.21952533721923828

Final encoder loss: 0.024929369264534616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09026670455932617 0.2192516326904297

Final encoder loss: 0.024362134116849517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.09011030197143555 0.21927571296691895

Final encoder loss: 0.024406856061335542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08918213844299316 0.21935486793518066

Final encoder loss: 0.02450348868959042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08990764617919922 0.21970725059509277

Final encoder loss: 0.024702731458265056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08688998222351074 0.21612215042114258


Training case model
Final encoder loss: 0.20296339690685272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2646174430847168 0.05077767372131348

Final encoder loss: 0.18890395760536194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28550004959106445 0.052366018295288086

Final encoder loss: 0.19013959169387817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2590363025665283 0.05170083045959473

Final encoder loss: 0.19219240546226501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27603721618652344 0.05368328094482422

Final encoder loss: 0.18082350492477417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2689025402069092 0.05214548110961914

Final encoder loss: 0.1919247955083847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25653886795043945 0.05329298973083496

Final encoder loss: 0.10260792821645737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27955198287963867 0.052388668060302734

Final encoder loss: 0.09236461669206619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2720367908477783 0.05184769630432129

Final encoder loss: 0.08876451104879379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2784852981567383 0.054320573806762695

Final encoder loss: 0.08751939237117767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2687547206878662 0.05279684066772461

Final encoder loss: 0.07944095134735107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2752718925476074 0.05179762840270996

Final encoder loss: 0.08283331990242004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25722193717956543 0.05160045623779297

Final encoder loss: 0.06003933027386665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27866435050964355 0.05234122276306152

Final encoder loss: 0.05431251600384712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.269134521484375 0.055965423583984375

Final encoder loss: 0.0524430088698864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2884175777435303 0.05287909507751465

Final encoder loss: 0.05278182774782181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26093077659606934 0.052794456481933594

Final encoder loss: 0.04940255731344223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2776811122894287 0.0524747371673584

Final encoder loss: 0.05115899443626404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563443183898926 0.05453038215637207

Final encoder loss: 0.04279181733727455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2771463394165039 0.0530087947845459

Final encoder loss: 0.039458293467760086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694089412689209 0.05173015594482422

Final encoder loss: 0.03831624984741211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28494715690612793 0.05201363563537598

Final encoder loss: 0.03898395970463753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27071523666381836 0.05258321762084961

Final encoder loss: 0.03785884380340576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2876462936401367 0.05503273010253906

Final encoder loss: 0.03827172517776489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549283504486084 0.05321860313415527

Final encoder loss: 0.03551286831498146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27778172492980957 0.052230119705200195

Final encoder loss: 0.03377167880535126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694377899169922 0.051635026931762695

Final encoder loss: 0.03291234374046326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2893385887145996 0.05462288856506348

Final encoder loss: 0.0335824117064476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25782132148742676 0.0543975830078125

Final encoder loss: 0.03378929942846298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2848024368286133 0.05262899398803711

Final encoder loss: 0.033486880362033844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569918632507324 0.05275583267211914

Final encoder loss: 0.032581593841314316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28470730781555176 0.0529630184173584

Final encoder loss: 0.03160719573497772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2791128158569336 0.05253434181213379

Final encoder loss: 0.03117976151406765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2884678840637207 0.05199456214904785

Final encoder loss: 0.03156379610300064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2663452625274658 0.05197000503540039

Final encoder loss: 0.03230239823460579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27913379669189453 0.05120205879211426

Final encoder loss: 0.031719088554382324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25359487533569336 0.0506281852722168

Final encoder loss: 0.030655942857265472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2806074619293213 0.05169105529785156

Final encoder loss: 0.029877079650759697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25714659690856934 0.05175495147705078

Final encoder loss: 0.02928798459470272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27954554557800293 0.051497459411621094

Final encoder loss: 0.029826072975993156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2664322853088379 0.05205273628234863

Final encoder loss: 0.030431723222136497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28044962882995605 0.052144765853881836

Final encoder loss: 0.029862532392144203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25473833084106445 0.05131959915161133

Final encoder loss: 0.028949370607733727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2750532627105713 0.052787065505981445

Final encoder loss: 0.028100594878196716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26931190490722656 0.053148746490478516

Final encoder loss: 0.027906110510230064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28598713874816895 0.052542924880981445

Final encoder loss: 0.028300534933805466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25896525382995605 0.0523073673248291

Final encoder loss: 0.02901952527463436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2879467010498047 0.05291342735290527

Final encoder loss: 0.028334051370620728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569572925567627 0.05255889892578125

Final encoder loss: 0.02789337746798992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27758312225341797 0.052150726318359375

Final encoder loss: 0.027616748586297035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26912879943847656 0.05347609519958496

Final encoder loss: 0.027068929746747017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2786588668823242 0.05149507522583008

Final encoder loss: 0.02755727246403694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2688772678375244 0.0514378547668457

Final encoder loss: 0.02818336896598339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28511953353881836 0.053775787353515625

Final encoder loss: 0.027653533965349197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25368547439575195 0.05430269241333008

Final encoder loss: 0.027414176613092422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2855246067047119 0.05342411994934082

Final encoder loss: 0.027039725333452225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2684321403503418 0.05236101150512695

Final encoder loss: 0.026722194626927376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2878706455230713 0.05349993705749512

Final encoder loss: 0.02697329968214035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26817941665649414 0.051848649978637695

Final encoder loss: 0.0278007909655571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2802126407623291 0.051885128021240234

Final encoder loss: 0.027105318382382393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2532331943511963 0.05164194107055664

Final encoder loss: 0.026857076212763786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2796013355255127 0.051369428634643555

Final encoder loss: 0.02654312737286091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2671957015991211 0.052236080169677734

Final encoder loss: 0.02623021975159645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.279590368270874 0.05230140686035156

Final encoder loss: 0.026532381772994995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.266538143157959 0.05152630805969238

Final encoder loss: 0.02727729268372059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.273179292678833 0.05157041549682617

Final encoder loss: 0.02663438580930233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25308799743652344 0.05185842514038086

Final encoder loss: 0.026431627571582794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28560948371887207 0.052384138107299805

Final encoder loss: 0.02611381746828556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573418617248535 0.05252718925476074

Final encoder loss: 0.025833401829004288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2761507034301758 0.052422523498535156

Final encoder loss: 0.02599138766527176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2683596611022949 0.05123090744018555

Final encoder loss: 0.026975221931934357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27688050270080566 0.05263113975524902

Final encoder loss: 0.026239285245537758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25494956970214844 0.051485300064086914

Final encoder loss: 0.026175621896982193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28455114364624023 0.052716970443725586

Final encoder loss: 0.025834906846284866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26960277557373047 0.052481651306152344

Final encoder loss: 0.025456443428993225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28533339500427246 0.051915645599365234

Final encoder loss: 0.025750961154699326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25724124908447266 0.0511476993560791

Final encoder loss: 0.0264578964561224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2852630615234375 0.05298662185668945

Final encoder loss: 0.025913313031196594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558705806732178 0.05241203308105469

Final encoder loss: 0.025863051414489746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2751307487487793 0.0522618293762207

Final encoder loss: 0.02566353604197502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2849290370941162 0.05283665657043457

Final encoder loss: 0.025455735623836517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27599406242370605 0.052236318588256836

Final encoder loss: 0.02548004500567913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2798929214477539 0.05229449272155762

Final encoder loss: 0.02648075297474861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2813754081726074 0.05152297019958496

Final encoder loss: 0.025830883532762527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541182041168213 0.05279135704040527

Final encoder loss: 0.02580793760716915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27958011627197266 0.05255460739135742

Final encoder loss: 0.025426732376217842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2604844570159912 0.05268049240112305

Final encoder loss: 0.025124065577983856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.272907018661499 0.05344700813293457

Final encoder loss: 0.02531450055539608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28104472160339355 0.05265188217163086

Final encoder loss: 0.026011481881141663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2772500514984131 0.05215287208557129

Final encoder loss: 0.025534220039844513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537376880645752 0.05230593681335449

Final encoder loss: 0.025520870462059975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28121018409729004 0.052321672439575195

Final encoder loss: 0.02516273409128189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26723313331604004 0.05251741409301758

Final encoder loss: 0.02500809356570244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2804422378540039 0.0515751838684082

Final encoder loss: 0.0250344667583704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26694226264953613 0.05130171775817871

Final encoder loss: 0.02583305537700653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28023815155029297 0.05320024490356445

Final encoder loss: 0.025263100862503052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534499168395996 0.05180931091308594

Final encoder loss: 0.02533130906522274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28115129470825195 0.05120730400085449

Final encoder loss: 0.02512834593653679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26721787452697754 0.051516056060791016

Final encoder loss: 0.024808261543512344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28731274604797363 0.051525115966796875

Final encoder loss: 0.024959465488791466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576782703399658 0.052143096923828125

Final encoder loss: 0.02566349506378174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27967357635498047 0.05193614959716797

Final encoder loss: 0.025137683376669884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25424957275390625 0.05259585380554199

Final encoder loss: 0.025225019082427025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.279956579208374 0.05182647705078125

Final encoder loss: 0.02498406544327736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26789116859436035 0.05105018615722656

Final encoder loss: 0.024781810119748116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2806696891784668 0.05148601531982422

Final encoder loss: 0.02469290792942047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26700472831726074 0.05100226402282715

Final encoder loss: 0.025635549798607826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27324461936950684 0.05202984809875488

Final encoder loss: 0.02496720291674137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25340819358825684 0.05215334892272949

Final encoder loss: 0.02517305128276348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28002333641052246 0.05281496047973633

Final encoder loss: 0.02479477785527706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2658803462982178 0.05170583724975586

Final encoder loss: 0.02457469329237938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.279799222946167 0.05242276191711426

Final encoder loss: 0.024686211720108986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26576995849609375 0.051848411560058594

Final encoder loss: 0.02535807527601719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2808070182800293 0.0521090030670166

Final encoder loss: 0.024808602407574654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541036605834961 0.052933454513549805

Final encoder loss: 0.024902313947677612
Final encoder loss: 0.024182181805372238
Final encoder loss: 0.023347439244389534
Final encoder loss: 0.022625038400292397
Final encoder loss: 0.02257697284221649
Final encoder loss: 0.02122272551059723

Training emognition model
Final encoder loss: 0.02764041732623732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08173322677612305 0.22985100746154785

Final encoder loss: 0.028439169609790135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08037662506103516 0.2294142246246338

Final encoder loss: 0.028674037264088927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08047223091125488 0.22991609573364258

Final encoder loss: 0.02968744644219788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08048558235168457 0.2292616367340088

Final encoder loss: 0.02876294932608488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08057904243469238 0.22972869873046875

Final encoder loss: 0.027855403177709535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08048510551452637 0.22964143753051758

Final encoder loss: 0.027550135696357506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08041810989379883 0.230820894241333

Final encoder loss: 0.028502031068762045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08119916915893555 0.2295210361480713

Final encoder loss: 0.02746993264931074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08045792579650879 0.2292804718017578

Final encoder loss: 0.028509056612141565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08045744895935059 0.2294292449951172

Final encoder loss: 0.029340008111555023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08032965660095215 0.2290806770324707

Final encoder loss: 0.02764625686127928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0802621841430664 0.22973346710205078

Final encoder loss: 0.027199113894486777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0804286003112793 0.22930049896240234

Final encoder loss: 0.02775544695107954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08034014701843262 0.22950148582458496

Final encoder loss: 0.02796395946011263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08017826080322266 0.22965049743652344

Final encoder loss: 0.02606743983308298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0800023078918457 0.22868752479553223


Training emognition model
Final encoder loss: 0.19355545938014984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24864959716796875 0.04918384552001953

Final encoder loss: 0.19496981799602509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2448129653930664 0.04826235771179199

Final encoder loss: 0.08656537532806396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483079433441162 0.049152374267578125

Final encoder loss: 0.08602909743785858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24563884735107422 0.04828023910522461

Final encoder loss: 0.055481236428022385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24659276008605957 0.048380136489868164

Final encoder loss: 0.05400340259075165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24628543853759766 0.04844808578491211

Final encoder loss: 0.041931524872779846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24613714218139648 0.04873967170715332

Final encoder loss: 0.04089396446943283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2443993091583252 0.04828596115112305

Final encoder loss: 0.03516469895839691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24762845039367676 0.04959416389465332

Final encoder loss: 0.03446072340011597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2456064224243164 0.04894304275512695

Final encoder loss: 0.03147706761956215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24692940711975098 0.04832053184509277

Final encoder loss: 0.0309727992862463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24512267112731934 0.04984259605407715

Final encoder loss: 0.0293276309967041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24765777587890625 0.04890251159667969

Final encoder loss: 0.029113806784152985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.245957612991333 0.04868197441101074

Final encoder loss: 0.028313331305980682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24679946899414062 0.04944920539855957

Final encoder loss: 0.028391266241669655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24756217002868652 0.051206350326538086

Final encoder loss: 0.028004933148622513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24730682373046875 0.049520015716552734

Final encoder loss: 0.028283804655075073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24569988250732422 0.047858238220214844

Final encoder loss: 0.027869315817952156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24709796905517578 0.049448490142822266

Final encoder loss: 0.028392557054758072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24579644203186035 0.048223018646240234

Final encoder loss: 0.027823179960250854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24832677841186523 0.04810595512390137

Final encoder loss: 0.027885032817721367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24502873420715332 0.049065589904785156

Final encoder loss: 0.02755759283900261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24733924865722656 0.04904770851135254

Final encoder loss: 0.027689151465892792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24707889556884766 0.04992961883544922

Final encoder loss: 0.027264470234513283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24649667739868164 0.04840993881225586

Final encoder loss: 0.027476195245981216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24685907363891602 0.04922342300415039

Final encoder loss: 0.027114415541291237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474501132965088 0.049005985260009766

Final encoder loss: 0.02765030786395073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24527668952941895 0.04973435401916504

Final encoder loss: 0.027214350178837776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24639606475830078 0.04846310615539551

Final encoder loss: 0.027367766946554184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24660611152648926 0.05012345314025879

Final encoder loss: 0.02710978500545025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24697041511535645 0.04971814155578613

Final encoder loss: 0.027370329946279526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24468588829040527 0.048598289489746094

Final encoder loss: 0.026805806905031204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24733853340148926 0.0490107536315918

Final encoder loss: 0.02691533975303173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450425624847412 0.04919743537902832

Final encoder loss: 0.02658984623849392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24608612060546875 0.049308061599731445

Final encoder loss: 0.02697501704096794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24647140502929688 0.048886775970458984

Final encoder loss: 0.026475800201296806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24620604515075684 0.048009395599365234

Final encoder loss: 0.0268271341919899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24554133415222168 0.04922604560852051

Final encoder loss: 0.02646976336836815
Final encoder loss: 0.0259206872433424

Training empatch model
Final encoder loss: 0.03729404559686028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0707552433013916 0.1728808879852295

Final encoder loss: 0.0393924975239169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07097363471984863 0.17254137992858887

Final encoder loss: 0.03828200646216555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07060050964355469 0.1727743148803711

Final encoder loss: 0.036727061066404776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0727689266204834 0.17432832717895508

Final encoder loss: 0.038296082164530676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07165336608886719 0.17404532432556152

Final encoder loss: 0.035267676616910146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07258319854736328 0.17506027221679688

Final encoder loss: 0.039111743093570635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07189750671386719 0.17394185066223145

Final encoder loss: 0.03518326418618961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07115507125854492 0.17350435256958008

Final encoder loss: 0.025245443136902523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0713045597076416 0.17449736595153809

Final encoder loss: 0.027120394746925593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07271099090576172 0.1743772029876709

Final encoder loss: 0.026678764116770847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0716087818145752 0.17380881309509277

Final encoder loss: 0.026691976849632745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07150411605834961 0.17385244369506836

Final encoder loss: 0.028162204473290795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07159924507141113 0.17495489120483398

Final encoder loss: 0.026847591880312053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07261157035827637 0.17403888702392578

Final encoder loss: 0.025548848721528572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07187223434448242 0.1740274429321289

Final encoder loss: 0.02643714851716979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07149219512939453 0.1741650104522705


Training empatch model
Final encoder loss: 0.1711340695619583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1785881519317627 0.043054819107055664

Final encoder loss: 0.08002889156341553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1766495704650879 0.04392194747924805

Final encoder loss: 0.055392298847436905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17666387557983398 0.04584169387817383

Final encoder loss: 0.04353763535618782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17842721939086914 0.04358339309692383

Final encoder loss: 0.03676531836390495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17632555961608887 0.043580055236816406

Final encoder loss: 0.03255167976021767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17458295822143555 0.04422116279602051

Final encoder loss: 0.029791641980409622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1785566806793213 0.042619943618774414

Final encoder loss: 0.02797776460647583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17488455772399902 0.04377126693725586

Final encoder loss: 0.026698708534240723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758720874786377 0.04404330253601074

Final encoder loss: 0.025908639654517174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1781470775604248 0.043526411056518555

Final encoder loss: 0.025379763916134834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17595553398132324 0.043497562408447266

Final encoder loss: 0.025070926174521446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17564773559570312 0.042981863021850586

Final encoder loss: 0.02483043447136879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17808198928833008 0.04439902305603027

Final encoder loss: 0.02467411942780018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17605185508728027 0.043596744537353516

Final encoder loss: 0.024528561159968376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17446637153625488 0.04279494285583496

Final encoder loss: 0.02430538274347782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17791080474853516 0.04467201232910156

Final encoder loss: 0.02422104775905609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17524027824401855 0.043346405029296875

Final encoder loss: 0.02404567040503025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17544341087341309 0.04320573806762695

Final encoder loss: 0.023991715162992477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17827248573303223 0.0446932315826416

Final encoder loss: 0.023845423012971878

Training wesad model
Final encoder loss: 0.041600758603820585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07140445709228516 0.17387866973876953

Final encoder loss: 0.03967621496188323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07106828689575195 0.17428159713745117

Final encoder loss: 0.037948068643615517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07283401489257812 0.17420053482055664

Final encoder loss: 0.038269351213337896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07138609886169434 0.174149751663208

Final encoder loss: 0.025657695854100465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07163858413696289 0.17367863655090332

Final encoder loss: 0.026177273598378452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07148480415344238 0.1750929355621338

Final encoder loss: 0.02697739642865831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07245039939880371 0.17379093170166016

Final encoder loss: 0.02768675432251961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07232856750488281 0.17424297332763672

Final encoder loss: 0.020493912264935826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07129240036010742 0.17380738258361816

Final encoder loss: 0.020983267302287573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07248258590698242 0.17430901527404785

Final encoder loss: 0.021990892311082328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0709996223449707 0.1741480827331543

Final encoder loss: 0.020103262930869288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07116842269897461 0.17374634742736816

Final encoder loss: 0.016692413704322822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07096314430236816 0.1746959686279297

Final encoder loss: 0.01711761377524661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07370591163635254 0.17401337623596191

Final encoder loss: 0.018228767590941834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07156491279602051 0.1740880012512207

Final encoder loss: 0.017196439162960327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07119345664978027 0.17427563667297363


Training wesad model
Final encoder loss: 0.2155936062335968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10740518569946289 0.03391885757446289

Final encoder loss: 0.09838207811117172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10617923736572266 0.033097267150878906

Final encoder loss: 0.06420499086380005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1046895980834961 0.03337216377258301

Final encoder loss: 0.04696052893996239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10413408279418945 0.03327322006225586

Final encoder loss: 0.036989446729421616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10440468788146973 0.032865047454833984

Final encoder loss: 0.030850226059556007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1049039363861084 0.034110307693481445

Final encoder loss: 0.026932617649435997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10628390312194824 0.03483748435974121

Final encoder loss: 0.02436733804643154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10391831398010254 0.03374791145324707

Final encoder loss: 0.022622911259531975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10466551780700684 0.033796072006225586

Final encoder loss: 0.021451951935887337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10410380363464355 0.03318500518798828

Final encoder loss: 0.020722296088933945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10389399528503418 0.03352069854736328

Final encoder loss: 0.020309660583734512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10533881187438965 0.033814430236816406

Final encoder loss: 0.02019166573882103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10706400871276855 0.0332789421081543

Final encoder loss: 0.02033974602818489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042475700378418 0.03335237503051758

Final encoder loss: 0.02064850553870201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10426807403564453 0.03339719772338867

Final encoder loss: 0.02084210142493248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10402202606201172 0.03318476676940918

Final encoder loss: 0.02076592668890953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10401630401611328 0.03306937217712402

Final encoder loss: 0.0205587986856699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1062936782836914 0.033798933029174805

Final encoder loss: 0.02020134963095188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10613894462585449 0.03253793716430664

Final encoder loss: 0.02008224092423916

Calculating loss for amigos model
	Full Pass 0.6772856712341309
numFreeParamsPath 18
Reconstruction loss values: 0.02905590645968914 0.03844035044312477

Calculating loss for dapper model
	Full Pass 0.15337133407592773
numFreeParamsPath 18
Reconstruction loss values: 0.024170059710741043 0.026887867599725723

Calculating loss for case model
	Full Pass 0.8602731227874756
numFreeParamsPath 18
Reconstruction loss values: 0.03535176441073418 0.03840786963701248

Calculating loss for emognition model
	Full Pass 0.2952096462249756
numFreeParamsPath 18
Reconstruction loss values: 0.036972224712371826 0.045569002628326416

Calculating loss for empatch model
	Full Pass 0.10645556449890137
numFreeParamsPath 18
Reconstruction loss values: 0.03908637538552284 0.04741556569933891

Calculating loss for wesad model
	Full Pass 0.07719016075134277
numFreeParamsPath 18
Reconstruction loss values: 0.039215173572301865 0.05762762576341629
Total loss calculation time: 3.857943058013916

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.731159210205078
Total epoch time: 191.0229687690735

Epoch: 40

Training case model
Final encoder loss: 0.034980351388398834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09959030151367188 0.273242712020874

Final encoder loss: 0.03270091522286162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0919647216796875 0.26529860496520996

Final encoder loss: 0.03084693929033787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0926504135131836 0.267153263092041

Final encoder loss: 0.030678720601655624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09201812744140625 0.2660038471221924

Final encoder loss: 0.029756854954003088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09128737449645996 0.2660961151123047

Final encoder loss: 0.029652967269595695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0914313793182373 0.2649815082550049

Final encoder loss: 0.029186060456829845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09142065048217773 0.2659013271331787

Final encoder loss: 0.028464253240818606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09317445755004883 0.26543092727661133

Final encoder loss: 0.028118660501397028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09130525588989258 0.2652313709259033

Final encoder loss: 0.027546177589847957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09282898902893066 0.267181396484375

Final encoder loss: 0.028014692731912195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09169554710388184 0.26576924324035645

Final encoder loss: 0.027964983641570882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09140801429748535 0.2661757469177246

Final encoder loss: 0.0284072243023131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0911860466003418 0.2649414539337158

Final encoder loss: 0.027363725634071055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09180951118469238 0.2657938003540039

Final encoder loss: 0.027729700618413528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09427571296691895 0.2658858299255371

Final encoder loss: 0.027449094364755195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08887147903442383 0.2626957893371582


Training dapper model
Final encoder loss: 0.02257801380321777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06273031234741211 0.15302681922912598

Final encoder loss: 0.024153159278595168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06331872940063477 0.15032267570495605

Final encoder loss: 0.02302576783327408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06254911422729492 0.15156340599060059

Final encoder loss: 0.023259520932457037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06205558776855469 0.15077805519104004

Final encoder loss: 0.022226933218008955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06306910514831543 0.15287446975708008

Final encoder loss: 0.02212975892822023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06372737884521484 0.15085315704345703

Final encoder loss: 0.020044501495729474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0629417896270752 0.15097737312316895

Final encoder loss: 0.020593316641763868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.062276363372802734 0.15066814422607422

Final encoder loss: 0.020777201090705574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06310367584228516 0.15308928489685059

Final encoder loss: 0.022472914852614125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06465387344360352 0.15106511116027832

Final encoder loss: 0.023652220784158684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06280350685119629 0.15088796615600586

Final encoder loss: 0.022641530221380633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06249713897705078 0.15041399002075195

Final encoder loss: 0.02240203714357438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0627899169921875 0.15283608436584473

Final encoder loss: 0.023154967252498455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06507730484008789 0.15006160736083984

Final encoder loss: 0.021825588973010467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.062212228775024414 0.15148019790649414

Final encoder loss: 0.019398175812680878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06228947639465332 0.1503458023071289


Training emognition model
Final encoder loss: 0.037595540306005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08417391777038574 0.27617835998535156

Final encoder loss: 0.03708299175191064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08393740653991699 0.2751796245574951

Final encoder loss: 0.036190542322382876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08370399475097656 0.27573704719543457

Final encoder loss: 0.037214147083384594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08584022521972656 0.27542877197265625

Final encoder loss: 0.0374583467408749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08405613899230957 0.27600550651550293

Final encoder loss: 0.03828864651479615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08576297760009766 0.276761531829834

Final encoder loss: 0.036131008137815925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0838022232055664 0.2756466865539551

Final encoder loss: 0.03559755136214565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08343625068664551 0.2764859199523926

Final encoder loss: 0.035445781595407386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08408331871032715 0.2752528190612793

Final encoder loss: 0.03653365210936907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08348727226257324 0.27555251121520996

Final encoder loss: 0.03463200272585073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0855555534362793 0.27718043327331543

Final encoder loss: 0.036635423557843286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08407878875732422 0.27539920806884766

Final encoder loss: 0.03651837700200735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08385634422302246 0.27877378463745117

Final encoder loss: 0.03554736013890141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.0831136703491211 0.27521634101867676

Final encoder loss: 0.03394716863899409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0835728645324707 0.2755131721496582

Final encoder loss: 0.03467364078205892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08481955528259277 0.27583861351013184


Training amigos model
Final encoder loss: 0.029303628630952958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1084439754486084 0.38900232315063477

Final encoder loss: 0.02869294032267833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.11119651794433594 0.38952040672302246

Final encoder loss: 0.028733000037229853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.1086723804473877 0.38950276374816895

Final encoder loss: 0.02944640304247448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10834193229675293 0.39058375358581543

Final encoder loss: 0.027817623129651303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10830545425415039 0.3908851146697998

Final encoder loss: 0.02951156149610558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10816097259521484 0.38987159729003906

Final encoder loss: 0.02831321269326339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.11039853096008301 0.39306211471557617

Final encoder loss: 0.029637342721846417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10857868194580078 0.3901858329772949

Final encoder loss: 0.029834331433312662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.11024999618530273 0.3890256881713867

Final encoder loss: 0.030052321260003616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10812664031982422 0.38979506492614746

Final encoder loss: 0.031213793124028867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10843133926391602 0.38898491859436035

Final encoder loss: 0.028708144631184294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1083519458770752 0.3908967971801758

Final encoder loss: 0.028908611493630693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10856842994689941 0.3925206661224365

Final encoder loss: 0.028792001093457795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.11004233360290527 0.3900790214538574

Final encoder loss: 0.029235077051362218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10852169990539551 0.3899083137512207

Final encoder loss: 0.023976785984075846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10453534126281738 0.38337039947509766


Training amigos model
Final encoder loss: 0.020076501946982912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10636782646179199 0.34147024154663086

Final encoder loss: 0.02113725254220932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10649251937866211 0.34174585342407227

Final encoder loss: 0.020799547409477714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10743522644042969 0.3420133590698242

Final encoder loss: 0.02216100147284738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10615801811218262 0.341796875

Final encoder loss: 0.02167652619286786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10576343536376953 0.3407433032989502

Final encoder loss: 0.021122486300574537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10576963424682617 0.3406977653503418

Final encoder loss: 0.022766115371142634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10542416572570801 0.3408951759338379

Final encoder loss: 0.020586700934156335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10544180870056152 0.34070348739624023

Final encoder loss: 0.02033067723677257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10581755638122559 0.3407881259918213

Final encoder loss: 0.02040833052011406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.1056816577911377 0.3407909870147705

Final encoder loss: 0.02044367353428906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10536885261535645 0.34078264236450195

Final encoder loss: 0.02155744605385309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10547876358032227 0.34142541885375977

Final encoder loss: 0.02062235826936821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10593485832214355 0.3415088653564453

Final encoder loss: 0.022799165984244477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10582208633422852 0.34175658226013184

Final encoder loss: 0.02247801798588099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10631561279296875 0.34148359298706055

Final encoder loss: 0.021916075557988808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10132789611816406 0.3382408618927002


Training amigos model
Final encoder loss: 0.18078003823757172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.471677303314209 0.07445979118347168

Final encoder loss: 0.18782931566238403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47174763679504395 0.0805809497833252

Final encoder loss: 0.1836346536874771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4671320915222168 0.07428193092346191

Final encoder loss: 0.07525162398815155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.469449520111084 0.07739114761352539

Final encoder loss: 0.07817283272743225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4684460163116455 0.07538366317749023

Final encoder loss: 0.0714506208896637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.463529109954834 0.0777425765991211

Final encoder loss: 0.044140297919511795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4694862365722656 0.0769956111907959

Final encoder loss: 0.04554117098450661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46857571601867676 0.07745885848999023

Final encoder loss: 0.04235486313700676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46584582328796387 0.07486486434936523

Final encoder loss: 0.032133106142282486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4679710865020752 0.07836198806762695

Final encoder loss: 0.033167172223329544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4690725803375244 0.0746614933013916

Final encoder loss: 0.031683966517448425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46515440940856934 0.07561874389648438

Final encoder loss: 0.02670120820403099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46329617500305176 0.07414913177490234

Final encoder loss: 0.027519889175891876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4612619876861572 0.07663512229919434

Final encoder loss: 0.026645584031939507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45554447174072266 0.07372474670410156

Final encoder loss: 0.024286925792694092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4600098133087158 0.07607913017272949

Final encoder loss: 0.025012055411934853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.459780216217041 0.07506585121154785

Final encoder loss: 0.024346597492694855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4548184871673584 0.07981610298156738

Final encoder loss: 0.023671232163906097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47011637687683105 0.07624983787536621

Final encoder loss: 0.02394133247435093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47152233123779297 0.07525157928466797

Final encoder loss: 0.023824432864785194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46680617332458496 0.0768589973449707

Final encoder loss: 0.023532813414931297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47589945793151855 0.07637953758239746

Final encoder loss: 0.0233442522585392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47061848640441895 0.07539558410644531

Final encoder loss: 0.02377871796488762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46861767768859863 0.07930326461791992

Final encoder loss: 0.02299950085580349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47374629974365234 0.07590889930725098

Final encoder loss: 0.022955281659960747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4724116325378418 0.07611775398254395

Final encoder loss: 0.023229161277413368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46489763259887695 0.07563161849975586

Final encoder loss: 0.02242284268140793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47080540657043457 0.07954216003417969

Final encoder loss: 0.02259516343474388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47539758682250977 0.07608819007873535

Final encoder loss: 0.022612353786826134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4640777111053467 0.07766890525817871

Final encoder loss: 0.021838640794157982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4702785015106201 0.07545137405395508

Final encoder loss: 0.021930519491434097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46903347969055176 0.07851839065551758

Final encoder loss: 0.022040555253624916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47029805183410645 0.0749807357788086

Final encoder loss: 0.021735651418566704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4697260856628418 0.07616281509399414

Final encoder loss: 0.021803362295031548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47051477432250977 0.08182191848754883

Final encoder loss: 0.022032056003808975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4662342071533203 0.07220816612243652

Final encoder loss: 0.02145291492342949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47343015670776367 0.07628369331359863

Final encoder loss: 0.021178636699914932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46625399589538574 0.07460165023803711

Final encoder loss: 0.021641630679368973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46825623512268066 0.0800018310546875

Final encoder loss: 0.02117817848920822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4742448329925537 0.07509255409240723

Final encoder loss: 0.020787788555026054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47223758697509766 0.07681012153625488

Final encoder loss: 0.02121688611805439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46527838706970215 0.07398724555969238

Final encoder loss: 0.020845480263233185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4719209671020508 0.08299732208251953

Final encoder loss: 0.020476214587688446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47517871856689453 0.07728981971740723

Final encoder loss: 0.02108893357217312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4671614170074463 0.07565975189208984

Final encoder loss: 0.020707186311483383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47092294692993164 0.08233428001403809

Final encoder loss: 0.020554618909955025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4711487293243408 0.07413363456726074

Final encoder loss: 0.021163078024983406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47092628479003906 0.0726466178894043

Final encoder loss: 0.020675620064139366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4685361385345459 0.07563209533691406

Final encoder loss: 0.02044307067990303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.470261812210083 0.07898402214050293

Final encoder loss: 0.020866863429546356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46834850311279297 0.07330536842346191

Final encoder loss: 0.020619966089725494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47224926948547363 0.07481789588928223

Final encoder loss: 0.020355025306344032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4736638069152832 0.07430386543273926

Final encoder loss: 0.02064143493771553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4666614532470703 0.07981538772583008

Final encoder loss: 0.020374316722154617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47350358963012695 0.07472825050354004

Final encoder loss: 0.020072583109140396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4678525924682617 0.0756981372833252

Final encoder loss: 0.02062033675611019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4700462818145752 0.07505130767822266

Final encoder loss: 0.02023436315357685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4705324172973633 0.07336926460266113

Final encoder loss: 0.020045066252350807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47554874420166016 0.0749058723449707

Final encoder loss: 0.020626362413167953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46527647972106934 0.07697868347167969

Final encoder loss: 0.020121661946177483
Final encoder loss: 0.018970901146531105
Final encoder loss: 0.01867217756807804

Training dapper model
Final encoder loss: 0.019024049880711765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06088685989379883 0.10792922973632812

Final encoder loss: 0.017485668567844136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05976533889770508 0.10884571075439453

Final encoder loss: 0.016941990206704616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06014060974121094 0.10718822479248047

Final encoder loss: 0.01809421544810395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05980730056762695 0.10807418823242188

Final encoder loss: 0.01822867821530636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060613393783569336 0.10743451118469238

Final encoder loss: 0.016236526637227796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060004472732543945 0.10722470283508301

Final encoder loss: 0.017627969565139637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05981945991516113 0.10788631439208984

Final encoder loss: 0.01822384283108874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06107211112976074 0.10761308670043945

Final encoder loss: 0.017729400787471646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06010723114013672 0.10775566101074219

Final encoder loss: 0.01555107908799829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05995297431945801 0.1074380874633789

Final encoder loss: 0.01766647016337478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06046295166015625 0.10823369026184082

Final encoder loss: 0.01698827222793729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.059990882873535156 0.10727739334106445

Final encoder loss: 0.016594726460484967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.059828758239746094 0.10696005821228027

Final encoder loss: 0.01638760187172558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05995440483093262 0.1079263687133789

Final encoder loss: 0.016044598384302947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06143331527709961 0.10733628273010254

Final encoder loss: 0.01691051734861905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059705495834350586 0.10739398002624512


Training dapper model
Final encoder loss: 0.2024325430393219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.118133544921875 0.03425145149230957

Final encoder loss: 0.2081918865442276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11829352378845215 0.0336155891418457

Final encoder loss: 0.08524180203676224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11647415161132812 0.034796714782714844

Final encoder loss: 0.08653575927019119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559629440307617 0.03424811363220215

Final encoder loss: 0.05021693930029869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11791300773620605 0.03548836708068848

Final encoder loss: 0.04947303980588913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11553382873535156 0.034596920013427734

Final encoder loss: 0.03434333577752113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11701226234436035 0.03387951850891113

Final encoder loss: 0.03378811106085777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11609983444213867 0.034583330154418945

Final encoder loss: 0.02625800296664238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11733889579772949 0.03429007530212402

Final encoder loss: 0.025926141068339348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11622810363769531 0.033624887466430664

Final encoder loss: 0.0218899454921484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11610984802246094 0.035327911376953125

Final encoder loss: 0.02157287485897541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11809039115905762 0.03347301483154297

Final encoder loss: 0.019334932789206505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11637020111083984 0.034316062927246094

Final encoder loss: 0.019124489277601242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11577558517456055 0.03360915184020996

Final encoder loss: 0.01786009408533573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11831998825073242 0.03408503532409668

Final encoder loss: 0.017715344205498695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11577820777893066 0.03408408164978027

Final encoder loss: 0.016983099281787872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11680769920349121 0.034444570541381836

Final encoder loss: 0.016981659457087517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11728882789611816 0.03446674346923828

Final encoder loss: 0.0165965985506773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11732816696166992 0.034793853759765625

Final encoder loss: 0.016527453437447548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11591172218322754 0.03433990478515625

Final encoder loss: 0.016331594437360764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11637139320373535 0.03461027145385742

Final encoder loss: 0.01622607372701168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11769604682922363 0.03395533561706543

Final encoder loss: 0.016015494242310524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11653280258178711 0.03430891036987305

Final encoder loss: 0.01606733538210392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595439910888672 0.0344085693359375

Final encoder loss: 0.016158707439899445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11837053298950195 0.03403043746948242

Final encoder loss: 0.016045164316892624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11585092544555664 0.033750295639038086

Final encoder loss: 0.016139833256602287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11622166633605957 0.03505754470825195

Final encoder loss: 0.015709808096289635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1174767017364502 0.03450918197631836

Final encoder loss: 0.015810251235961914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11572265625 0.03451275825500488

Final encoder loss: 0.01569155976176262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557674407958984 0.03383755683898926

Final encoder loss: 0.015313566662371159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11676502227783203 0.035489797592163086

Final encoder loss: 0.015440602786839008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11607861518859863 0.03455781936645508

Final encoder loss: 0.015219206921756268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11678147315979004 0.03425335884094238

Final encoder loss: 0.015285435132682323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11545228958129883 0.0353543758392334

Final encoder loss: 0.01485302671790123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11832213401794434 0.03380465507507324

Final encoder loss: 0.014952092431485653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11635589599609375 0.0338902473449707

Final encoder loss: 0.014691216871142387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11643075942993164 0.03384995460510254

Final encoder loss: 0.014850725419819355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11756515502929688 0.035059452056884766

Final encoder loss: 0.014594626612961292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11608004570007324 0.03462862968444824

Final encoder loss: 0.014665544033050537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.115203857421875 0.03370809555053711

Final encoder loss: 0.014739892445504665
Final encoder loss: 0.013703388161957264

Training case model
Final encoder loss: 0.026500022926638833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08988022804260254 0.2192704677581787

Final encoder loss: 0.025752687665516718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0897519588470459 0.21949553489685059

Final encoder loss: 0.02538698372278738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08969354629516602 0.21922969818115234

Final encoder loss: 0.025157497328806882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09058499336242676 0.2192859649658203

Final encoder loss: 0.02496257825096538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0895240306854248 0.21957087516784668

Final encoder loss: 0.02432376056401574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09015822410583496 0.21916961669921875

Final encoder loss: 0.024689900454476738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08895540237426758 0.2191944122314453

Final encoder loss: 0.024653816366734936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08955740928649902 0.21915102005004883

Final encoder loss: 0.025103717305123723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08901238441467285 0.21910715103149414

Final encoder loss: 0.0244498845322068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08881354331970215 0.21934247016906738

Final encoder loss: 0.024612762789104157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08960294723510742 0.21948814392089844

Final encoder loss: 0.025560249160250934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08965682983398438 0.2186903953552246

Final encoder loss: 0.024614092793972274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08933353424072266 0.21916985511779785

Final encoder loss: 0.02424549403978155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08973193168640137 0.21901440620422363

Final encoder loss: 0.02539950583437936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08986449241638184 0.21950030326843262

Final encoder loss: 0.024227606847460205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08661675453186035 0.21606135368347168


Training case model
Final encoder loss: 0.20296576619148254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26434326171875 0.0522158145904541

Final encoder loss: 0.1889050155878067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2683591842651367 0.05272936820983887

Final encoder loss: 0.19014140963554382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2690999507904053 0.054817914962768555

Final encoder loss: 0.19219298660755157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25829553604125977 0.05233407020568848

Final encoder loss: 0.18081094324588776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257443904876709 0.053034305572509766

Final encoder loss: 0.19192570447921753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563517093658447 0.051527976989746094

Final encoder loss: 0.10301101207733154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26714539527893066 0.0544588565826416

Final encoder loss: 0.09232889115810394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577826976776123 0.0530393123626709

Final encoder loss: 0.089128278195858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593066692352295 0.052010536193847656

Final encoder loss: 0.08755863457918167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584114074707031 0.05198240280151367

Final encoder loss: 0.07971715182065964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26790952682495117 0.0539553165435791

Final encoder loss: 0.08328282833099365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551596164703369 0.05191826820373535

Final encoder loss: 0.06057295948266983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2681388854980469 0.0528411865234375

Final encoder loss: 0.054293494671583176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694516181945801 0.052567481994628906

Final encoder loss: 0.05283147469162941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25884222984313965 0.05243825912475586

Final encoder loss: 0.05287664383649826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27367472648620605 0.0524439811706543

Final encoder loss: 0.04960351064801216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27691197395324707 0.05173778533935547

Final encoder loss: 0.051506705582141876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567780017852783 0.05410265922546387

Final encoder loss: 0.04320539906620979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26702117919921875 0.05219864845275879

Final encoder loss: 0.03944909945130348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2589738368988037 0.05179905891418457

Final encoder loss: 0.03859914094209671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26961421966552734 0.05127263069152832

Final encoder loss: 0.03904180973768234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2672586441040039 0.0532383918762207

Final encoder loss: 0.03796086832880974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25617337226867676 0.051514625549316406

Final encoder loss: 0.03856116533279419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552318572998047 0.05202054977416992

Final encoder loss: 0.035785894840955734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27583980560302734 0.051883697509765625

Final encoder loss: 0.033692460507154465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25785303115844727 0.05361604690551758

Final encoder loss: 0.032981570810079575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2579019069671631 0.05234718322753906

Final encoder loss: 0.03358115628361702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2575571537017822 0.05147910118103027

Final encoder loss: 0.03375351056456566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25926899909973145 0.055901527404785156

Final encoder loss: 0.03352884203195572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25478124618530273 0.05017852783203125

Final encoder loss: 0.03275701403617859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26720285415649414 0.05323433876037598

Final encoder loss: 0.03151419386267662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2687392234802246 0.05151200294494629

Final encoder loss: 0.031095365062355995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25740718841552734 0.05319929122924805

Final encoder loss: 0.03155956044793129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2673511505126953 0.05229043960571289

Final encoder loss: 0.03214997053146362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25876450538635254 0.05235004425048828

Final encoder loss: 0.0318807028234005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555363178253174 0.05138826370239258

Final encoder loss: 0.030677104368805885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27542972564697266 0.052512407302856445

Final encoder loss: 0.029772697016596794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25965213775634766 0.05102419853210449

Final encoder loss: 0.02929779514670372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2599189281463623 0.05189204216003418

Final encoder loss: 0.02973119542002678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26881837844848633 0.05224180221557617

Final encoder loss: 0.03031819500029087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2698326110839844 0.05214810371398926

Final encoder loss: 0.029758749529719353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2525973320007324 0.051151275634765625

Final encoder loss: 0.02892349846661091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2655508518218994 0.05156064033508301

Final encoder loss: 0.028010819107294083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2652571201324463 0.05183863639831543

Final encoder loss: 0.027773482725024223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2725212574005127 0.052145957946777344

Final encoder loss: 0.02818906493484974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2559053897857666 0.05090498924255371

Final encoder loss: 0.02892208844423294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2668576240539551 0.053293466567993164

Final encoder loss: 0.02835378423333168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2521626949310303 0.05200648307800293

Final encoder loss: 0.0280364491045475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25620388984680176 0.051680803298950195

Final encoder loss: 0.027506791055202484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25620484352111816 0.051244497299194336

Final encoder loss: 0.027007700875401497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26838159561157227 0.05433821678161621

Final encoder loss: 0.027400895953178406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27304625511169434 0.05143284797668457

Final encoder loss: 0.02828792855143547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2742269039154053 0.052614450454711914

Final encoder loss: 0.027552030980587006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25652122497558594 0.05233907699584961

Final encoder loss: 0.027447791770100594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26857686042785645 0.053836822509765625

Final encoder loss: 0.026924587786197662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2678487300872803 0.052220821380615234

Final encoder loss: 0.026737522333860397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27751731872558594 0.05230712890625

Final encoder loss: 0.026787148788571358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574729919433594 0.05203104019165039

Final encoder loss: 0.027676906436681747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2690277099609375 0.05135321617126465

Final encoder loss: 0.027138249948620796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25583839416503906 0.052887678146362305

Final encoder loss: 0.026898372918367386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26698994636535645 0.05280160903930664

Final encoder loss: 0.026474297046661377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2770659923553467 0.052773475646972656

Final encoder loss: 0.026172978803515434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25878071784973145 0.052206993103027344

Final encoder loss: 0.02639949508011341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2674124240875244 0.054692745208740234

Final encoder loss: 0.02715139277279377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2567729949951172 0.050960540771484375

Final encoder loss: 0.026390280574560165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25475406646728516 0.05233263969421387

Final encoder loss: 0.026393208652734756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26797008514404297 0.05294322967529297

Final encoder loss: 0.025875132530927658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2587306499481201 0.051932573318481445

Final encoder loss: 0.02569526806473732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2686135768890381 0.054290056228637695

Final encoder loss: 0.025897374376654625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26728391647338867 0.052162885665893555

Final encoder loss: 0.026835959404706955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26898622512817383 0.05264401435852051

Final encoder loss: 0.02618459053337574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25631260871887207 0.05023765563964844

Final encoder loss: 0.026155196130275726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25858211517333984 0.0534360408782959

Final encoder loss: 0.025751786306500435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2676200866699219 0.052500247955322266

Final encoder loss: 0.02539972774684429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26897501945495605 0.051598548889160156

Final encoder loss: 0.02565121278166771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2725226879119873 0.052294015884399414

Final encoder loss: 0.02649274282157421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25871872901916504 0.05194711685180664

Final encoder loss: 0.025780215859413147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25429558753967285 0.05289578437805176

Final encoder loss: 0.02593424916267395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27449774742126465 0.051366329193115234

Final encoder loss: 0.025526579469442368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26891112327575684 0.052324533462524414

Final encoder loss: 0.02530759759247303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2764620780944824 0.05202007293701172

Final encoder loss: 0.025256574153900146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26782989501953125 0.053823232650756836

Final encoder loss: 0.02627943828701973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26715612411499023 0.054947853088378906

Final encoder loss: 0.02567087672650814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25473880767822266 0.05239987373352051

Final encoder loss: 0.02567632682621479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2728245258331299 0.05287742614746094

Final encoder loss: 0.025278441607952118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26995420455932617 0.05144953727722168

Final encoder loss: 0.025010081008076668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26720333099365234 0.053731679916381836

Final encoder loss: 0.02513914369046688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2591567039489746 0.052181243896484375

Final encoder loss: 0.025849517434835434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575571537017822 0.05328226089477539

Final encoder loss: 0.025196876376867294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25693535804748535 0.05140042304992676

Final encoder loss: 0.02550218626856804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25867271423339844 0.05364632606506348

Final encoder loss: 0.025059346109628677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2677929401397705 0.05296206474304199

Final encoder loss: 0.02484215795993805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26740169525146484 0.052667856216430664

Final encoder loss: 0.02495400607585907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2674522399902344 0.052994728088378906

Final encoder loss: 0.0258590467274189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26662302017211914 0.05274820327758789

Final encoder loss: 0.025265827775001526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548079490661621 0.0531003475189209

Final encoder loss: 0.025302357971668243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2672543525695801 0.05215764045715332

Final encoder loss: 0.02498508244752884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2680330276489258 0.052694082260131836

Final encoder loss: 0.024647284299135208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2661902904510498 0.05152583122253418

Final encoder loss: 0.024756575003266335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25500941276550293 0.051573753356933594

Final encoder loss: 0.025588620454072952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26573753356933594 0.05152440071105957

Final encoder loss: 0.024927282705903053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2520618438720703 0.05078554153442383

Final encoder loss: 0.025194838643074036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2648589611053467 0.051749467849731445

Final encoder loss: 0.024817636236548424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26577305793762207 0.052179813385009766

Final encoder loss: 0.024671750143170357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27167296409606934 0.05179929733276367

Final encoder loss: 0.024549923837184906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2656536102294922 0.052651166915893555

Final encoder loss: 0.025359507650136948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2560293674468994 0.052378177642822266

Final encoder loss: 0.024913689121603966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25444889068603516 0.05264616012573242

Final encoder loss: 0.025079993531107903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2681875228881836 0.05376482009887695

Final encoder loss: 0.024649858474731445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577381134033203 0.0526270866394043

Final encoder loss: 0.024452203884720802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2585334777832031 0.05289292335510254

Final encoder loss: 0.02453809790313244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26835060119628906 0.052874088287353516

Final encoder loss: 0.02530834637582302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579667568206787 0.05257248878479004

Final encoder loss: 0.024700498208403587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540922164916992 0.05171036720275879

Final encoder loss: 0.024935703724622726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27369213104248047 0.05192422866821289

Final encoder loss: 0.024496186524629593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26764535903930664 0.053136348724365234

Final encoder loss: 0.024300990626215935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683839797973633 0.05270671844482422

Final encoder loss: 0.024348339065909386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26691484451293945 0.05473613739013672

Final encoder loss: 0.02525128237903118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2725050449371338 0.0529940128326416

Final encoder loss: 0.024627581238746643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555205821990967 0.05136704444885254

Final encoder loss: 0.024780530482530594
Final encoder loss: 0.023952867835760117
Final encoder loss: 0.023143237456679344
Final encoder loss: 0.022428123280405998
Final encoder loss: 0.022244399413466454
Final encoder loss: 0.020999137312173843

Training emognition model
Final encoder loss: 0.0284389093625036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08136773109436035 0.22953319549560547

Final encoder loss: 0.027560022133144634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08066439628601074 0.22924160957336426

Final encoder loss: 0.02725449168204455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08034634590148926 0.22952580451965332

Final encoder loss: 0.02782019419136175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08049464225769043 0.2294614315032959

Final encoder loss: 0.02808953807153343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0805521011352539 0.22928547859191895

Final encoder loss: 0.026862790071439643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08024001121520996 0.22983264923095703

Final encoder loss: 0.02828336072560189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08040142059326172 0.22955727577209473

Final encoder loss: 0.028227265841501665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08060407638549805 0.22899770736694336

Final encoder loss: 0.027523178395589248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.0801856517791748 0.22958946228027344

Final encoder loss: 0.027504667882153163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08027005195617676 0.22901058197021484

Final encoder loss: 0.028581838768080826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08047175407409668 0.229569673538208

Final encoder loss: 0.029815435813721633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08062601089477539 0.23061752319335938

Final encoder loss: 0.028355754373314973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08143353462219238 0.23096799850463867

Final encoder loss: 0.02778758185501041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08162260055541992 0.23083996772766113

Final encoder loss: 0.02809341872211149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08193302154541016 0.2306833267211914

Final encoder loss: 0.029770414903838097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08051276206970215 0.23004388809204102


Training emognition model
Final encoder loss: 0.19355273246765137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24915647506713867 0.04942202568054199

Final encoder loss: 0.19496122002601624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24801135063171387 0.04882550239562988

Final encoder loss: 0.08728191256523132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24753427505493164 0.05007457733154297

Final encoder loss: 0.08693131059408188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24728941917419434 0.04938530921936035

Final encoder loss: 0.055967818945646286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24742889404296875 0.048952341079711914

Final encoder loss: 0.054529208689928055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24640989303588867 0.049193620681762695

Final encoder loss: 0.042266596108675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24684906005859375 0.04872703552246094

Final encoder loss: 0.04119550436735153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463216781616211 0.048247337341308594

Final encoder loss: 0.035277288407087326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467803955078125 0.04912519454956055

Final encoder loss: 0.0346609503030777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24569296836853027 0.04821491241455078

Final encoder loss: 0.031460944563150406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24527430534362793 0.049301862716674805

Final encoder loss: 0.03106674738228321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24811601638793945 0.05046725273132324

Final encoder loss: 0.029355216771364212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24880552291870117 0.04892683029174805

Final encoder loss: 0.0291290245950222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24577665328979492 0.048429250717163086

Final encoder loss: 0.028511496260762215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24726057052612305 0.05017542839050293

Final encoder loss: 0.028238840401172638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450408935546875 0.048628807067871094

Final encoder loss: 0.0281122587621212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24555015563964844 0.04864978790283203

Final encoder loss: 0.028046060353517532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2483992576599121 0.05085134506225586

Final encoder loss: 0.02787250466644764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24825739860534668 0.04774832725524902

Final encoder loss: 0.028046106919646263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24683475494384766 0.04948711395263672

Final encoder loss: 0.0276352372020483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472989559173584 0.048812150955200195

Final encoder loss: 0.027880826964974403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24645662307739258 0.048952579498291016

Final encoder loss: 0.027403555810451508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24601078033447266 0.04865908622741699

Final encoder loss: 0.02782072126865387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24857568740844727 0.0491032600402832

Final encoder loss: 0.02716599963605404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24614381790161133 0.04919266700744629

Final encoder loss: 0.027447840198874474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.245558500289917 0.04803824424743652

Final encoder loss: 0.027256030589342117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24618077278137207 0.04827880859375

Final encoder loss: 0.027248557657003403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24498701095581055 0.04898858070373535

Final encoder loss: 0.02712070383131504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24651837348937988 0.04915666580200195

Final encoder loss: 0.027139831334352493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.245103120803833 0.04947161674499512

Final encoder loss: 0.026857158169150352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2459733486175537 0.04851126670837402

Final encoder loss: 0.02707185223698616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24570202827453613 0.04893136024475098

Final encoder loss: 0.026547718793153763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24550938606262207 0.04893827438354492

Final encoder loss: 0.026973046362400055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2448587417602539 0.04950380325317383

Final encoder loss: 0.02626589685678482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24660658836364746 0.05044388771057129

Final encoder loss: 0.02676401659846306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24463152885437012 0.05048966407775879

Final encoder loss: 0.026396384462714195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24628591537475586 0.04955887794494629

Final encoder loss: 0.026746593415737152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24635815620422363 0.0484316349029541

Final encoder loss: 0.026484740898013115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24590849876403809 0.04850172996520996

Final encoder loss: 0.02665666490793228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450118064880371 0.0489346981048584

Final encoder loss: 0.026308787986636162
Final encoder loss: 0.02559024840593338

Training empatch model
Final encoder loss: 0.03898233252887566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07092809677124023 0.17276287078857422

Final encoder loss: 0.0386274769119429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07054471969604492 0.17287182807922363

Final encoder loss: 0.03611902980173493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07097959518432617 0.17281794548034668

Final encoder loss: 0.035638901562303066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07102799415588379 0.17267251014709473

Final encoder loss: 0.035883463124233274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07069611549377441 0.1730043888092041

Final encoder loss: 0.03473167983119363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07097935676574707 0.17301559448242188

Final encoder loss: 0.03824590848577314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07046675682067871 0.17301106452941895

Final encoder loss: 0.03693507757676848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07039594650268555 0.17221951484680176

Final encoder loss: 0.027391436811770677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07097744941711426 0.17293238639831543

Final encoder loss: 0.027634304712019928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07059574127197266 0.17316603660583496

Final encoder loss: 0.027009955679087642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07119369506835938 0.17301154136657715

Final encoder loss: 0.025807336687719902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07036542892456055 0.17310881614685059

Final encoder loss: 0.02615553617890799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07115888595581055 0.17252755165100098

Final encoder loss: 0.02657248339308151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07090044021606445 0.1735234260559082

Final encoder loss: 0.02596716674351875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07073020935058594 0.17288684844970703

Final encoder loss: 0.028272479665978078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07044148445129395 0.17242002487182617


Training empatch model
Final encoder loss: 0.17115269601345062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1768498420715332 0.04270601272583008

Final encoder loss: 0.0802023783326149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17447400093078613 0.04365968704223633

Final encoder loss: 0.05539173632860184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17440080642700195 0.04362988471984863

Final encoder loss: 0.043420303612947464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17380976676940918 0.04239153861999512

Final encoder loss: 0.03652558848261833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17444658279418945 0.043035268783569336

Final encoder loss: 0.03224814310669899
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17391109466552734 0.04358696937561035

Final encoder loss: 0.029408274218440056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17401647567749023 0.042679548263549805

Final encoder loss: 0.027522113174200058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17527556419372559 0.04360032081604004

Final encoder loss: 0.02624085173010826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17400503158569336 0.043580055236816406

Final encoder loss: 0.025439852848649025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17487239837646484 0.043370962142944336

Final encoder loss: 0.024932444095611572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17355084419250488 0.04332542419433594

Final encoder loss: 0.024617061018943787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17412042617797852 0.04315996170043945

Final encoder loss: 0.024285921826958656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1734907627105713 0.04346632957458496

Final encoder loss: 0.0240203645080328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1741924285888672 0.0430445671081543

Final encoder loss: 0.023843392729759216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17373108863830566 0.04343152046203613

Final encoder loss: 0.02366826869547367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1747596263885498 0.04325604438781738

Final encoder loss: 0.023651132360100746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17352032661437988 0.04264688491821289

Final encoder loss: 0.023611241951584816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17479491233825684 0.04394412040710449

Final encoder loss: 0.023654880002141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744251251220703 0.04349231719970703

Final encoder loss: 0.02343899942934513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1743006706237793 0.04361748695373535

Final encoder loss: 0.02332966960966587

Training wesad model
Final encoder loss: 0.03971624264871392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07086324691772461 0.1730659008026123

Final encoder loss: 0.03848374396141581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07035326957702637 0.1732926368713379

Final encoder loss: 0.03487980111590728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07102322578430176 0.17299246788024902

Final encoder loss: 0.03634428109225489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07093214988708496 0.1729421615600586

Final encoder loss: 0.026820961386020366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07030940055847168 0.17293620109558105

Final encoder loss: 0.02557506566035895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07087469100952148 0.1729128360748291

Final encoder loss: 0.025191314729621266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07068514823913574 0.1730058193206787

Final encoder loss: 0.026592829859845932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07039427757263184 0.1729426383972168

Final encoder loss: 0.02016916229558971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07072997093200684 0.17293596267700195

Final encoder loss: 0.01952415668953153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07078742980957031 0.17267560958862305

Final encoder loss: 0.02106994577763546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07018136978149414 0.1730484962463379

Final encoder loss: 0.01993058219663352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0703434944152832 0.17247343063354492

Final encoder loss: 0.016196870152120072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07047796249389648 0.17229056358337402

Final encoder loss: 0.01655549653171636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07054543495178223 0.17272615432739258

Final encoder loss: 0.016258433269384608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07071971893310547 0.17258787155151367

Final encoder loss: 0.01731757922901782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0703439712524414 0.17267680168151855


Training wesad model
Final encoder loss: 0.2156156450510025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10528755187988281 0.03276395797729492

Final encoder loss: 0.09783292561769485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10407495498657227 0.03301215171813965

Final encoder loss: 0.0631716325879097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10355448722839355 0.033280372619628906

Final encoder loss: 0.04621917009353638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357475280761719 0.03270292282104492

Final encoder loss: 0.03644067049026489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10346865653991699 0.03285670280456543

Final encoder loss: 0.03041667304933071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356712341308594 0.03334856033325195

Final encoder loss: 0.026567135006189346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10393238067626953 0.032605886459350586

Final encoder loss: 0.02398720383644104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10300779342651367 0.03284311294555664

Final encoder loss: 0.02227296121418476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10338997840881348 0.033204078674316406

Final encoder loss: 0.02113805152475834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10368657112121582 0.033220767974853516

Final encoder loss: 0.020455913618206978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10314583778381348 0.032505035400390625

Final encoder loss: 0.020106995478272438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10336470603942871 0.0329136848449707

Final encoder loss: 0.020069314166903496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10354280471801758 0.03319120407104492

Final encoder loss: 0.020037369802594185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10364055633544922 0.032912492752075195

Final encoder loss: 0.020073074847459793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388326644897461 0.0326838493347168

Final encoder loss: 0.019960036501288414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10298371315002441 0.033298492431640625

Final encoder loss: 0.019936034455895424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10311055183410645 0.0332334041595459

Final encoder loss: 0.019876180216670036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10407614707946777 0.03304624557495117

Final encoder loss: 0.01981334574520588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10333514213562012 0.033312320709228516

Final encoder loss: 0.019758621230721474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10317134857177734 0.03305840492248535

Final encoder loss: 0.019773079082369804

Calculating loss for amigos model
	Full Pass 0.6732931137084961
numFreeParamsPath 18
Reconstruction loss values: 0.02814497798681259 0.03811730071902275

Calculating loss for dapper model
	Full Pass 0.15072035789489746
numFreeParamsPath 18
Reconstruction loss values: 0.023286612704396248 0.026155782863497734

Calculating loss for case model
	Full Pass 0.8557929992675781
numFreeParamsPath 18
Reconstruction loss values: 0.03495541960000992 0.03791812062263489

Calculating loss for emognition model
	Full Pass 0.2903149127960205
numFreeParamsPath 18
Reconstruction loss values: 0.03658490255475044 0.04498622193932533

Calculating loss for empatch model
	Full Pass 0.10480523109436035
numFreeParamsPath 18
Reconstruction loss values: 0.03823370113968849 0.04621509090065956

Calculating loss for wesad model
	Full Pass 0.07680606842041016
numFreeParamsPath 18
Reconstruction loss values: 0.038302451372146606 0.05688612535595894
Total loss calculation time: 3.822859287261963

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.128594398498535
Total epoch time: 196.84322714805603

Epoch: 41

Training amigos model
Final encoder loss: 0.02750246262036933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11714863777160645 0.39100098609924316

Final encoder loss: 0.026985330329685156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10738205909729004 0.38881397247314453

Final encoder loss: 0.026896767914501143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10863828659057617 0.3913702964782715

Final encoder loss: 0.028359852071922497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10968017578125 0.38851404190063477

Final encoder loss: 0.028362078837450268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10810399055480957 0.38890814781188965

Final encoder loss: 0.027820816761337283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10803651809692383 0.38888096809387207

Final encoder loss: 0.025766653799944583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1089479923248291 0.39056968688964844

Final encoder loss: 0.02778918771932827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10792875289916992 0.3897390365600586

Final encoder loss: 0.026095000020795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10988378524780273 0.3911468982696533

Final encoder loss: 0.026328603682257392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10828423500061035 0.38976502418518066

Final encoder loss: 0.02754899425483659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10913634300231934 0.3895137310028076

Final encoder loss: 0.0264365913777116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10853409767150879 0.38899874687194824

Final encoder loss: 0.028832021722808854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10837244987487793 0.3895418643951416

Final encoder loss: 0.02566564605093134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10931515693664551 0.390596866607666

Final encoder loss: 0.028749045243364038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10854673385620117 0.38927793502807617

Final encoder loss: 0.02807698084224393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10493850708007812 0.3856327533721924


Training emognition model
Final encoder loss: 0.03645699094870265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08342599868774414 0.2756540775299072

Final encoder loss: 0.035295002351132906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08449625968933105 0.27600812911987305

Final encoder loss: 0.038783701563868796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08365559577941895 0.27581214904785156

Final encoder loss: 0.03604337142830202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08365869522094727 0.27619481086730957

Final encoder loss: 0.03490439974268973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08345603942871094 0.2752959728240967

Final encoder loss: 0.03374840356881887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08346843719482422 0.2754693031311035

Final encoder loss: 0.03545166999538892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08485889434814453 0.277479887008667

Final encoder loss: 0.03543202021941784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0841057300567627 0.27527618408203125

Final encoder loss: 0.03360891396797651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0845491886138916 0.276630163192749

Final encoder loss: 0.03363696189347618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08354306221008301 0.2757141590118408

Final encoder loss: 0.0350128102042509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08383321762084961 0.27577638626098633

Final encoder loss: 0.033413684509202055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08644604682922363 0.27569007873535156

Final encoder loss: 0.03251501095052864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08410930633544922 0.2751171588897705

Final encoder loss: 0.03406731121706158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08477997779846191 0.27704930305480957

Final encoder loss: 0.03347639851919548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0837700366973877 0.27587008476257324

Final encoder loss: 0.03339587326765354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0826714038848877 0.27507638931274414


Training dapper model
Final encoder loss: 0.024942019459436086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06306910514831543 0.15008163452148438

Final encoder loss: 0.023137636205582946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06267404556274414 0.15119481086730957

Final encoder loss: 0.02198234226452799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06252646446228027 0.15082502365112305

Final encoder loss: 0.024179202385662145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06708741188049316 0.1532297134399414

Final encoder loss: 0.022725433839051148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06367039680480957 0.1506505012512207

Final encoder loss: 0.02170076194903204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06269168853759766 0.15033268928527832

Final encoder loss: 0.021081031749901287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062399864196777344 0.15119028091430664

Final encoder loss: 0.0238553540143133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06325435638427734 0.15304827690124512

Final encoder loss: 0.021781582492948107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06437921524047852 0.15109729766845703

Final encoder loss: 0.02254763971079038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06278395652770996 0.15121006965637207

Final encoder loss: 0.021472607752988117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06220245361328125 0.15101909637451172

Final encoder loss: 0.022553672914329602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06341958045959473 0.15327763557434082

Final encoder loss: 0.022071403251700823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06463956832885742 0.1506044864654541

Final encoder loss: 0.020299661295369666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.46479034423828125 0.15260934829711914

Final encoder loss: 0.020825311137717458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06377410888671875 0.15289068222045898

Final encoder loss: 0.02149209663525906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06183671951293945 0.15089631080627441


Training case model
Final encoder loss: 0.03724943239879941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09146523475646973 0.2654879093170166

Final encoder loss: 0.033673993569774324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09306764602661133 0.26665163040161133

Final encoder loss: 0.03170783280791971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0920097827911377 0.26511287689208984

Final encoder loss: 0.030773320955348116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09248018264770508 0.26716065406799316

Final encoder loss: 0.0301709350713993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.10670638084411621 0.2660789489746094

Final encoder loss: 0.030568220214848386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09159064292907715 0.2658982276916504

Final encoder loss: 0.029496515369814284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.0925140380859375 0.264801025390625

Final encoder loss: 0.028010247305265384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09186530113220215 0.26500535011291504

Final encoder loss: 0.027876052085859714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09296035766601562 0.26767849922180176

Final encoder loss: 0.027952380795233638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09178781509399414 0.26593852043151855

Final encoder loss: 0.027728192048477436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09179258346557617 0.26624131202697754

Final encoder loss: 0.027497988132968335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0915524959564209 0.26588940620422363

Final encoder loss: 0.027322729816315172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.0916440486907959 0.26563429832458496

Final encoder loss: 0.027507791546946648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09264707565307617 0.265430212020874

Final encoder loss: 0.027855695169769414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09144234657287598 0.2665417194366455

Final encoder loss: 0.02795009758325407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08982658386230469 0.2693307399749756


Training amigos model
Final encoder loss: 0.02276619410622717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10657644271850586 0.3416292667388916

Final encoder loss: 0.019931729590282225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10801911354064941 0.3418591022491455

Final encoder loss: 0.01875808461650805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10630965232849121 0.341961145401001

Final encoder loss: 0.021397941091219494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10741996765136719 0.3427236080169678

Final encoder loss: 0.02000911642688161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10677194595336914 0.3419027328491211

Final encoder loss: 0.021276597258460518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10778093338012695 0.3418843746185303

Final encoder loss: 0.02203030652762914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1061246395111084 0.34230852127075195

Final encoder loss: 0.020925457001449172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10793852806091309 0.34200167655944824

Final encoder loss: 0.020628260634622742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.1060640811920166 0.3419349193572998

Final encoder loss: 0.021345367796716388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.11020636558532715 0.3420088291168213

Final encoder loss: 0.021052275691988996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10628795623779297 0.3417470455169678

Final encoder loss: 0.021721909747763852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10708856582641602 0.34172868728637695

Final encoder loss: 0.021387648983365145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10608553886413574 0.34189724922180176

Final encoder loss: 0.022060610140963298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10680174827575684 0.3414926528930664

Final encoder loss: 0.02023392510224539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10596847534179688 0.34207630157470703

Final encoder loss: 0.020936410954358657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10215950012207031 0.337813138961792


Training amigos model
Final encoder loss: 0.18075799942016602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4790647029876709 0.0795736312866211

Final encoder loss: 0.18781781196594238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4753403663635254 0.07393217086791992

Final encoder loss: 0.18363220989704132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47438812255859375 0.08116579055786133

Final encoder loss: 0.0758429765701294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4703786373138428 0.0797111988067627

Final encoder loss: 0.07813380658626556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4707009792327881 0.07755756378173828

Final encoder loss: 0.07213309407234192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654388427734375 0.075469970703125

Final encoder loss: 0.0442471019923687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.469602108001709 0.07606315612792969

Final encoder loss: 0.04536212235689163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47989749908447266 0.07106304168701172

Final encoder loss: 0.042584240436553955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4727363586425781 0.07409834861755371

Final encoder loss: 0.03190983831882477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4720423221588135 0.07839441299438477

Final encoder loss: 0.032871313393116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4683859348297119 0.07807803153991699

Final encoder loss: 0.03172141686081886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46581196784973145 0.07394266128540039

Final encoder loss: 0.02636907622218132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.467853307723999 0.0752716064453125

Final encoder loss: 0.02724462002515793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4741222858428955 0.07841897010803223

Final encoder loss: 0.026667127385735512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47150373458862305 0.0730595588684082

Final encoder loss: 0.023930808529257774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4748976230621338 0.08095407485961914

Final encoder loss: 0.02463102713227272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4702026844024658 0.08136701583862305

Final encoder loss: 0.024178477004170418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46302151679992676 0.07542634010314941

Final encoder loss: 0.02323410101234913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4692561626434326 0.0735328197479248

Final encoder loss: 0.02359628491103649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46860504150390625 0.07731819152832031

Final encoder loss: 0.023404786363244057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4738025665283203 0.07517766952514648

Final encoder loss: 0.02336193062365055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4756350517272949 0.0833134651184082

Final encoder loss: 0.023260656744241714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4712815284729004 0.07878398895263672

Final encoder loss: 0.02330928109586239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46680355072021484 0.07559871673583984

Final encoder loss: 0.022806771099567413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4715895652770996 0.08134222030639648

Final encoder loss: 0.022704094648361206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4710729122161865 0.07643675804138184

Final encoder loss: 0.022850431501865387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45674705505371094 0.07437252998352051

Final encoder loss: 0.021899882704019547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46053409576416016 0.07527828216552734

Final encoder loss: 0.021956468001008034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45911121368408203 0.07411575317382812

Final encoder loss: 0.022352086380124092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4570186138153076 0.07349967956542969

Final encoder loss: 0.02133946307003498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4604671001434326 0.07705044746398926

Final encoder loss: 0.021491961553692818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4695148468017578 0.0748140811920166

Final encoder loss: 0.021757658571004868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46592259407043457 0.07526469230651855

Final encoder loss: 0.021363232284784317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46828198432922363 0.07764863967895508

Final encoder loss: 0.02133679762482643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46726012229919434 0.07669711112976074

Final encoder loss: 0.021717006340622902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4659249782562256 0.07651877403259277

Final encoder loss: 0.021281303837895393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47037672996520996 0.07518959045410156

Final encoder loss: 0.02115432731807232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4687838554382324 0.07781386375427246

Final encoder loss: 0.021383441984653473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4647510051727295 0.07530570030212402

Final encoder loss: 0.020883504301309586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.471024751663208 0.07907509803771973

Final encoder loss: 0.020741241052746773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46814799308776855 0.07695960998535156

Final encoder loss: 0.021131252869963646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46579527854919434 0.07679319381713867

Final encoder loss: 0.020451458171010017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4686291217803955 0.07641148567199707

Final encoder loss: 0.020487289875745773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46973562240600586 0.07937169075012207

Final encoder loss: 0.020701643079519272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4677846431732178 0.07589340209960938

Final encoder loss: 0.020284738391637802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4696693420410156 0.07357287406921387

Final encoder loss: 0.020074307918548584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46414732933044434 0.07605767250061035

Final encoder loss: 0.02071535401046276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45862460136413574 0.07210969924926758

Final encoder loss: 0.02033020369708538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46033310890197754 0.07549595832824707

Final encoder loss: 0.020017320290207863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4600379467010498 0.07389307022094727

Final encoder loss: 0.020450999960303307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4561305046081543 0.0744333267211914

Final encoder loss: 0.020300069823861122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4669363498687744 0.07518887519836426

Final encoder loss: 0.020182540640234947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4704418182373047 0.08003425598144531

Final encoder loss: 0.020511427894234657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4702754020690918 0.07445740699768066

Final encoder loss: 0.02018447034060955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46880555152893066 0.07561564445495605

Final encoder loss: 0.019897501915693283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.468475341796875 0.07350730895996094

Final encoder loss: 0.02041773870587349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4656383991241455 0.08040046691894531

Final encoder loss: 0.019851263612508774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4763951301574707 0.07536172866821289

Final encoder loss: 0.019647419452667236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4689962863922119 0.07547712326049805

Final encoder loss: 0.020406300202012062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46878647804260254 0.08074760437011719

Final encoder loss: 0.019780069589614868
Final encoder loss: 0.018812300637364388
Final encoder loss: 0.018361510708928108

Training dapper model
Final encoder loss: 0.016644320473548958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059976816177368164 0.10746121406555176

Final encoder loss: 0.01614333912433634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06008458137512207 0.10702872276306152

Final encoder loss: 0.016416034883985364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059799909591674805 0.10777091979980469

Final encoder loss: 0.0181943307381064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06074118614196777 0.10689973831176758

Final encoder loss: 0.016287591039705153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060227394104003906 0.10762429237365723

Final encoder loss: 0.017842997410872054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05996108055114746 0.10779619216918945

Final encoder loss: 0.018240148849269493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.06127333641052246 0.10813164710998535

Final encoder loss: 0.016214095767048835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059850215911865234 0.10764408111572266

Final encoder loss: 0.016101405918038093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06001687049865723 0.10739016532897949

Final encoder loss: 0.01568497597678572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06005668640136719 0.10793042182922363

Final encoder loss: 0.016574171740728998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06108260154724121 0.10745429992675781

Final encoder loss: 0.01758669218074367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06032395362854004 0.10745859146118164

Final encoder loss: 0.01455278428656642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.060332536697387695 0.10792040824890137

Final encoder loss: 0.015898114769916753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06075572967529297 0.1078195571899414

Final encoder loss: 0.01653700310394229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.060359954833984375 0.10782694816589355

Final encoder loss: 0.01596711740063668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05972743034362793 0.10717391967773438


Training dapper model
Final encoder loss: 0.2024342119693756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11908292770385742 0.03533339500427246

Final encoder loss: 0.2081880271434784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11621665954589844 0.03377127647399902

Final encoder loss: 0.08444902300834656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11622238159179688 0.03458261489868164

Final encoder loss: 0.08597246557474136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1165771484375 0.03493356704711914

Final encoder loss: 0.04968797788023949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11737179756164551 0.03405117988586426

Final encoder loss: 0.048953473567962646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11602163314819336 0.034513235092163086

Final encoder loss: 0.03398866206407547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11649107933044434 0.034880638122558594

Final encoder loss: 0.0332687608897686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11805582046508789 0.03397727012634277

Final encoder loss: 0.025989340618252754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11636710166931152 0.03458523750305176

Final encoder loss: 0.025468595325946808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1163642406463623 0.03380537033081055

Final encoder loss: 0.021521368995308876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1187903881072998 0.03476667404174805

Final encoder loss: 0.02112467959523201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11606311798095703 0.034932851791381836

Final encoder loss: 0.018998300656676292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11658954620361328 0.034375667572021484

Final encoder loss: 0.018713736906647682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1177666187286377 0.035141706466674805

Final encoder loss: 0.01740770973265171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11701631546020508 0.03406190872192383

Final encoder loss: 0.01732526905834675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11671710014343262 0.033795833587646484

Final encoder loss: 0.016628367826342583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11626148223876953 0.03527712821960449

Final encoder loss: 0.016570527106523514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11743521690368652 0.03428816795349121

Final encoder loss: 0.016254810616374016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1158599853515625 0.03463292121887207

Final encoder loss: 0.016185881569981575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1170499324798584 0.0346982479095459

Final encoder loss: 0.01619858667254448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1186361312866211 0.03421163558959961

Final encoder loss: 0.016004091128706932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11566472053527832 0.03397178649902344

Final encoder loss: 0.016190527006983757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1167912483215332 0.034462690353393555

Final encoder loss: 0.015854118391871452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11789703369140625 0.03525137901306152

Final encoder loss: 0.01585550419986248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11574316024780273 0.03481030464172363

Final encoder loss: 0.01577797159552574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11671590805053711 0.03415274620056152

Final encoder loss: 0.015739046037197113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11635708808898926 0.03568220138549805

Final encoder loss: 0.01562943123281002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11680221557617188 0.034165143966674805

Final encoder loss: 0.015255393460392952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11701726913452148 0.03421497344970703

Final encoder loss: 0.015486466698348522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11590147018432617 0.034787654876708984

Final encoder loss: 0.014991549775004387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11814236640930176 0.03357839584350586

Final encoder loss: 0.015126143582165241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11731171607971191 0.03432822227478027

Final encoder loss: 0.014474461786448956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11563491821289062 0.033933162689208984

Final encoder loss: 0.014821396209299564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11796140670776367 0.03496193885803223

Final encoder loss: 0.014535223133862019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11593246459960938 0.03404116630554199

Final encoder loss: 0.01462042797356844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11596560478210449 0.03449440002441406

Final encoder loss: 0.014497016556560993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11682844161987305 0.03458452224731445

Final encoder loss: 0.014456510543823242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11723613739013672 0.034499406814575195

Final encoder loss: 0.014694367535412312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11606812477111816 0.03428173065185547

Final encoder loss: 0.014444383792579174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11553955078125 0.03514599800109863

Final encoder loss: 0.014522512443363667
Final encoder loss: 0.013461310416460037

Training case model
Final encoder loss: 0.02359775093472496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08919501304626465 0.21907782554626465

Final encoder loss: 0.024098802737216842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0903315544128418 0.21915507316589355

Final encoder loss: 0.023637324594875466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08919405937194824 0.21929454803466797

Final encoder loss: 0.023546407264825462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08989810943603516 0.2191610336303711

Final encoder loss: 0.023554586836090203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08972978591918945 0.2192075252532959

Final encoder loss: 0.023037272120802907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08942627906799316 0.219071626663208

Final encoder loss: 0.02377009730310829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08979916572570801 0.2194838523864746

Final encoder loss: 0.023479769613092276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08935332298278809 0.21976232528686523

Final encoder loss: 0.02376532680527977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09012031555175781 0.21924543380737305

Final encoder loss: 0.02327607575178872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.09009218215942383 0.2190418243408203

Final encoder loss: 0.023969294598258983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.090240478515625 0.21920037269592285

Final encoder loss: 0.023526426599775777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09006595611572266 0.2192389965057373

Final encoder loss: 0.023582881993437402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.09129571914672852 0.21922636032104492

Final encoder loss: 0.02332586705519624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08924293518066406 0.2212061882019043

Final encoder loss: 0.02338576333292536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09062695503234863 0.2191450595855713

Final encoder loss: 0.023047807177693624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08665943145751953 0.2160782814025879


Training case model
Final encoder loss: 0.20296363532543182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2637495994567871 0.05214738845825195

Final encoder loss: 0.18890950083732605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27604222297668457 0.053164005279541016

Final encoder loss: 0.19015204906463623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2689650058746338 0.05187106132507324

Final encoder loss: 0.19219554960727692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.270251989364624 0.05370354652404785

Final encoder loss: 0.1808091402053833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2754194736480713 0.05289864540100098

Final encoder loss: 0.19193540513515472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25692129135131836 0.0523068904876709

Final encoder loss: 0.1034700945019722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2707784175872803 0.05222678184509277

Final encoder loss: 0.09284317493438721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27654433250427246 0.05414128303527832

Final encoder loss: 0.0895574688911438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.258617639541626 0.052686214447021484

Final encoder loss: 0.08791927248239517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26833248138427734 0.052742958068847656

Final encoder loss: 0.08025331795215607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28050947189331055 0.051503658294677734

Final encoder loss: 0.08347950130701065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25548219680786133 0.05169034004211426

Final encoder loss: 0.060583967715501785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26924991607666016 0.05241036415100098

Final encoder loss: 0.05445486307144165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27695369720458984 0.052109479904174805

Final encoder loss: 0.05281471461057663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27055954933166504 0.052176713943481445

Final encoder loss: 0.052832912653684616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2870008945465088 0.05309319496154785

Final encoder loss: 0.04968566447496414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.268937349319458 0.05247068405151367

Final encoder loss: 0.05148595944046974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2579643726348877 0.052171945571899414

Final encoder loss: 0.04283662140369415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27041125297546387 0.05217719078063965

Final encoder loss: 0.03921132907271385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27690935134887695 0.053522348403930664

Final encoder loss: 0.03839836269617081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683839797973633 0.05158090591430664

Final encoder loss: 0.03886597231030464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27578091621398926 0.05315351486206055

Final encoder loss: 0.03771822899580002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27873897552490234 0.05339980125427246

Final encoder loss: 0.03830747678875923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571146488189697 0.0524749755859375

Final encoder loss: 0.035080309957265854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26987195014953613 0.05212688446044922

Final encoder loss: 0.032998524606227875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2785341739654541 0.05157303810119629

Final encoder loss: 0.03280427306890488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.278275728225708 0.05209684371948242

Final encoder loss: 0.03309173509478569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2683382034301758 0.05375838279724121

Final encoder loss: 0.03333865851163864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2694549560546875 0.052849769592285156

Final encoder loss: 0.03305278345942497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560396194458008 0.05139422416687012

Final encoder loss: 0.03208858519792557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27842020988464355 0.05558013916015625

Final encoder loss: 0.030826527625322342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26849985122680664 0.05242300033569336

Final encoder loss: 0.030667489394545555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2746000289916992 0.05350232124328613

Final encoder loss: 0.03087850660085678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27011895179748535 0.05279278755187988

Final encoder loss: 0.031923819333314896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2781858444213867 0.055387258529663086

Final encoder loss: 0.03134172409772873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25428199768066406 0.05273914337158203

Final encoder loss: 0.0300202127546072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2748274803161621 0.05371570587158203

Final encoder loss: 0.028958477079868317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28859972953796387 0.05274033546447754

Final encoder loss: 0.028784578666090965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27921605110168457 0.055327653884887695

Final encoder loss: 0.029032455757260323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26799917221069336 0.05255484580993652

Final encoder loss: 0.030044596642255783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27469921112060547 0.05236625671386719

Final encoder loss: 0.029187124222517014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2573373317718506 0.051743268966674805

Final encoder loss: 0.028349149972200394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576477527618408 0.05312800407409668

Final encoder loss: 0.027547091245651245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27629971504211426 0.05373215675354004

Final encoder loss: 0.02730352245271206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2762563228607178 0.05208301544189453

Final encoder loss: 0.027630001306533813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27896642684936523 0.05223989486694336

Final encoder loss: 0.028519615530967712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2682163715362549 0.05466437339782715

Final encoder loss: 0.027861693874001503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548868656158447 0.05169200897216797

Final encoder loss: 0.027277637273073196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2689642906188965 0.05294179916381836

Final encoder loss: 0.026693718507885933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27979326248168945 0.05248737335205078

Final encoder loss: 0.02652529627084732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2685964107513428 0.05251431465148926

Final encoder loss: 0.026646729558706284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26779794692993164 0.05338764190673828

Final encoder loss: 0.027608396485447884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2759978771209717 0.05192279815673828

Final encoder loss: 0.026870479807257652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2568492889404297 0.05112576484680176

Final encoder loss: 0.026658441871404648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2774209976196289 0.05266308784484863

Final encoder loss: 0.026332935318350792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2693753242492676 0.05202794075012207

Final encoder loss: 0.026175295934081078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2756969928741455 0.051470279693603516

Final encoder loss: 0.026200246065855026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2888498306274414 0.052351951599121094

Final encoder loss: 0.027237258851528168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.274263858795166 0.053945302963256836

Final encoder loss: 0.02654888853430748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557985782623291 0.051572322845458984

Final encoder loss: 0.02618074230849743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25922060012817383 0.0537106990814209

Final encoder loss: 0.025755634531378746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2804751396179199 0.05269360542297363

Final encoder loss: 0.025458555668592453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25768136978149414 0.05380129814147949

Final encoder loss: 0.02566332370042801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2697577476501465 0.05232644081115723

Final encoder loss: 0.026731453835964203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2848048210144043 0.053888797760009766

Final encoder loss: 0.02579667419195175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2570934295654297 0.05299115180969238

Final encoder loss: 0.025846529752016068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2679250240325928 0.05145382881164551

Final encoder loss: 0.025440005585551262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28003954887390137 0.05238795280456543

Final encoder loss: 0.025263020768761635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25859785079956055 0.053765058517456055

Final encoder loss: 0.025337999686598778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2770504951477051 0.054319143295288086

Final encoder loss: 0.02633104845881462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2778482437133789 0.05349397659301758

Final encoder loss: 0.025525202974677086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563645839691162 0.051258087158203125

Final encoder loss: 0.02542867138981819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2705681324005127 0.05173134803771973

Final encoder loss: 0.024954352527856827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2756006717681885 0.05402207374572754

Final encoder loss: 0.024850092828273773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593667507171631 0.05185890197753906

Final encoder loss: 0.024932563304901123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2680988311767578 0.052282094955444336

Final encoder loss: 0.02575378678739071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27950286865234375 0.0518643856048584

Final encoder loss: 0.025111069902777672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25533223152160645 0.053113698959350586

Final encoder loss: 0.02523551881313324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27719855308532715 0.05290412902832031

Final encoder loss: 0.0249845739454031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2710232734680176 0.05224323272705078

Final encoder loss: 0.024746229872107506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27117419242858887 0.05206131935119629

Final encoder loss: 0.024714704602956772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25701045989990234 0.0542905330657959

Final encoder loss: 0.02575373277068138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2779858112335205 0.0520176887512207

Final encoder loss: 0.025043191388249397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2572762966156006 0.052599430084228516

Final encoder loss: 0.02490147016942501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27772951126098633 0.052629947662353516

Final encoder loss: 0.024512920528650284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27663660049438477 0.055434465408325195

Final encoder loss: 0.02439275197684765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.268404483795166 0.05248856544494629

Final encoder loss: 0.024363446980714798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28374528884887695 0.05276036262512207

Final encoder loss: 0.025376765057444572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2690403461456299 0.05260825157165527

Final encoder loss: 0.02464197389781475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25633788108825684 0.052974700927734375

Final encoder loss: 0.02481710910797119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26865625381469727 0.05297446250915527

Final encoder loss: 0.02446107380092144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2753891944885254 0.05350685119628906

Final encoder loss: 0.024339132010936737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28877735137939453 0.05307459831237793

Final encoder loss: 0.024404939264059067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25847816467285156 0.05295705795288086

Final encoder loss: 0.02525087632238865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27016186714172363 0.052602529525756836

Final encoder loss: 0.024545155465602875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25932955741882324 0.05284929275512695

Final encoder loss: 0.0245705246925354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2597508430480957 0.05253767967224121

Final encoder loss: 0.02422325871884823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28015708923339844 0.052680253982543945

Final encoder loss: 0.024056842550635338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27981090545654297 0.05143570899963379

Final encoder loss: 0.024055341258645058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2732987403869629 0.05142664909362793

Final encoder loss: 0.02482607774436474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2733805179595947 0.05233454704284668

Final encoder loss: 0.024275388568639755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25396275520324707 0.052274465560913086

Final encoder loss: 0.024516234174370766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2666137218475342 0.05216193199157715

Final encoder loss: 0.024190383031964302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2740764617919922 0.05188250541687012

Final encoder loss: 0.024061882868409157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26651859283447266 0.052160024642944336

Final encoder loss: 0.023922160267829895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2735719680786133 0.052179813385009766

Final encoder loss: 0.024872489273548126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26659607887268066 0.05208945274353027

Final encoder loss: 0.02429615706205368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549128532409668 0.0535585880279541

Final encoder loss: 0.024348249658942223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.28531885147094727 0.05318474769592285

Final encoder loss: 0.023914234712719917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26882147789001465 0.05353999137878418

Final encoder loss: 0.023715978488326073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2693593502044678 0.05307340621948242

Final encoder loss: 0.023744188249111176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2773311138153076 0.051123857498168945

Final encoder loss: 0.024729173630475998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27698802947998047 0.05326271057128906

Final encoder loss: 0.02396332286298275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25532078742980957 0.05216073989868164

Final encoder loss: 0.024225670844316483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.269329309463501 0.053019046783447266

Final encoder loss: 0.023906096816062927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.278886079788208 0.05207419395446777

Final encoder loss: 0.023802589625120163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.259716272354126 0.052243947982788086

Final encoder loss: 0.023766888305544853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27634644508361816 0.05508756637573242

Final encoder loss: 0.024669483304023743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26821398735046387 0.051668405532836914

Final encoder loss: 0.02399631403386593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25573229789733887 0.05208706855773926

Final encoder loss: 0.0241000447422266
Final encoder loss: 0.0232144296169281
Final encoder loss: 0.022509567439556122
Final encoder loss: 0.021695829927921295
Final encoder loss: 0.021724525839090347
Final encoder loss: 0.02031579241156578

Training emognition model
Final encoder loss: 0.02805821787234904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08170843124389648 0.23076844215393066

Final encoder loss: 0.027544859445885448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08221817016601562 0.23118257522583008

Final encoder loss: 0.027686017137939986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08110570907592773 0.2310340404510498

Final encoder loss: 0.0293959855449061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08170151710510254 0.23079180717468262

Final encoder loss: 0.028612682075303406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08143734931945801 0.23083090782165527

Final encoder loss: 0.028756279262712684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0811007022857666 0.23061656951904297

Final encoder loss: 0.0268871373668184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08151888847351074 0.23139023780822754

Final encoder loss: 0.026560376818573924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08146095275878906 0.2311701774597168

Final encoder loss: 0.02781546385090392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.09406733512878418 0.23108887672424316

Final encoder loss: 0.027191225675414314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08083891868591309 0.23116254806518555

Final encoder loss: 0.026334728261238644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08159494400024414 0.23085808753967285

Final encoder loss: 0.02746779991769231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08117818832397461 0.23105072975158691

Final encoder loss: 0.0268085501435084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08147883415222168 0.23095035552978516

Final encoder loss: 0.02713000444909162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08161115646362305 0.23098325729370117

Final encoder loss: 0.028007329567785018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08146810531616211 0.2310624122619629

Final encoder loss: 0.02698793874542492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08139181137084961 0.23080158233642578


Training emognition model
Final encoder loss: 0.19356894493103027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.251178503036499 0.04887843132019043

Final encoder loss: 0.19495685398578644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24782299995422363 0.04897475242614746

Final encoder loss: 0.08797647058963776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24872922897338867 0.048933982849121094

Final encoder loss: 0.08736757934093475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24909567832946777 0.04943418502807617

Final encoder loss: 0.05608157068490982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24858522415161133 0.049458980560302734

Final encoder loss: 0.054549407213926315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2476356029510498 0.04926705360412598

Final encoder loss: 0.04205631464719772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484750747680664 0.048769474029541016

Final encoder loss: 0.04095084220170975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24930047988891602 0.04859280586242676

Final encoder loss: 0.03497068211436272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24791169166564941 0.04994392395019531

Final encoder loss: 0.03425054997205734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2483677864074707 0.04926562309265137

Final encoder loss: 0.0310504287481308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24901151657104492 0.049065589904785156

Final encoder loss: 0.030555039644241333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24778413772583008 0.048874616622924805

Final encoder loss: 0.02884608320891857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24724030494689941 0.05044817924499512

Final encoder loss: 0.02846892736852169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24798226356506348 0.04960799217224121

Final encoder loss: 0.027762003242969513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24872827529907227 0.04825949668884277

Final encoder loss: 0.027483481913805008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24860310554504395 0.049791574478149414

Final encoder loss: 0.027299094945192337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24727344512939453 0.04960274696350098

Final encoder loss: 0.02726532518863678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472701072692871 0.04947972297668457

Final encoder loss: 0.027191687375307083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24965739250183105 0.04860639572143555

Final encoder loss: 0.0273625236004591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24877572059631348 0.04865217208862305

Final encoder loss: 0.026981934905052185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24651479721069336 0.048844337463378906

Final encoder loss: 0.027195338159799576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24626946449279785 0.048577308654785156

Final encoder loss: 0.026821676641702652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24699902534484863 0.04910564422607422

Final encoder loss: 0.02694033272564411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24523544311523438 0.04840254783630371

Final encoder loss: 0.026642579585313797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2462170124053955 0.04867672920227051

Final encoder loss: 0.02654631994664669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24614262580871582 0.04831266403198242

Final encoder loss: 0.026528872549533844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2459254264831543 0.04864501953125

Final encoder loss: 0.026477161794900894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2448713779449463 0.048392534255981445

Final encoder loss: 0.02619304694235325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2462480068206787 0.04922056198120117

Final encoder loss: 0.02647263929247856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24729704856872559 0.0492548942565918

Final encoder loss: 0.02630293369293213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24800920486450195 0.05031156539916992

Final encoder loss: 0.02654901146888733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474055290222168 0.04996323585510254

Final encoder loss: 0.026018738746643066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24729371070861816 0.04851555824279785

Final encoder loss: 0.02625017985701561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24864697456359863 0.05000567436218262

Final encoder loss: 0.025846341624855995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2470250129699707 0.050321340560913086

Final encoder loss: 0.026141390204429626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24688506126403809 0.04951333999633789

Final encoder loss: 0.025661438703536987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24817848205566406 0.05003237724304199

Final encoder loss: 0.025823069736361504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24742722511291504 0.049490928649902344

Final encoder loss: 0.02562824636697769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24750971794128418 0.04993176460266113

Final encoder loss: 0.025861024856567383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2465376853942871 0.049851179122924805

Final encoder loss: 0.025675753131508827
Final encoder loss: 0.02492831088602543

Training empatch model
Final encoder loss: 0.03405186892317082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07167625427246094 0.1742846965789795

Final encoder loss: 0.036695769128255284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07129168510437012 0.17384839057922363

Final encoder loss: 0.03807479960875763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07141876220703125 0.17355632781982422

Final encoder loss: 0.03834358573517287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07112455368041992 0.17409515380859375

Final encoder loss: 0.03537034888539834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.071624755859375 0.17414045333862305

Final encoder loss: 0.03664482961139693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07224154472351074 0.17387080192565918

Final encoder loss: 0.03344038763378524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07093667984008789 0.17364287376403809

Final encoder loss: 0.032369603718709894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07102441787719727 0.17342329025268555

Final encoder loss: 0.026204683778154964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07195353507995605 0.17431259155273438

Final encoder loss: 0.026473571126363647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0713188648223877 0.17421936988830566

Final encoder loss: 0.02664129348209292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07141685485839844 0.17275047302246094

Final encoder loss: 0.025568889803252344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07091975212097168 0.17283940315246582

Final encoder loss: 0.02545735946058842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07063722610473633 0.17267227172851562

Final encoder loss: 0.02487841062849538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07077598571777344 0.17299318313598633

Final encoder loss: 0.024296545887828015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07057404518127441 0.17278432846069336

Final encoder loss: 0.02651778332018756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07039165496826172 0.17234587669372559


Training empatch model
Final encoder loss: 0.17116425931453705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17704176902770996 0.04307723045349121

Final encoder loss: 0.08111793547868729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17411541938781738 0.04335331916809082

Final encoder loss: 0.05608421936631203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17361927032470703 0.042485713958740234

Final encoder loss: 0.043803755193948746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17346930503845215 0.043117523193359375

Final encoder loss: 0.036703187972307205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1739039421081543 0.04275941848754883

Final encoder loss: 0.032304536551237106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17431402206420898 0.04323148727416992

Final encoder loss: 0.029443740844726562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17451691627502441 0.04292178153991699

Final encoder loss: 0.027528047561645508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1735706329345703 0.04274797439575195

Final encoder loss: 0.026197602972388268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17549443244934082 0.04274749755859375

Final encoder loss: 0.025342972949147224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17554473876953125 0.043459415435791016

Final encoder loss: 0.02471698261797428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17609524726867676 0.04382181167602539

Final encoder loss: 0.024307813495397568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17445969581604004 0.04365849494934082

Final encoder loss: 0.023987876251339912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17513036727905273 0.04446077346801758

Final encoder loss: 0.023814115673303604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1741037368774414 0.043936967849731445

Final encoder loss: 0.02358311600983143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17420697212219238 0.043024301528930664

Final encoder loss: 0.023496117442846298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17372632026672363 0.04369306564331055

Final encoder loss: 0.02331382967531681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17636609077453613 0.043907880783081055

Final encoder loss: 0.023219982162117958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17467308044433594 0.0439143180847168

Final encoder loss: 0.023100677877664566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17563557624816895 0.04371166229248047

Final encoder loss: 0.023047855123877525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17478442192077637 0.04376721382141113

Final encoder loss: 0.02297217771410942

Training wesad model
Final encoder loss: 0.039720511859576764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07139325141906738 0.17375802993774414

Final encoder loss: 0.03613562983802786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07133173942565918 0.17350125312805176

Final encoder loss: 0.0366106156660894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07105398178100586 0.17403793334960938

Final encoder loss: 0.03524179623152675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07107305526733398 0.17364764213562012

Final encoder loss: 0.02382948537207497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07123851776123047 0.17383551597595215

Final encoder loss: 0.026910094192992656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07107043266296387 0.17401528358459473

Final encoder loss: 0.02511335349344171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0733652114868164 0.17355012893676758

Final encoder loss: 0.024272009351412025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07071232795715332 0.17343735694885254

Final encoder loss: 0.0194813209260508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07113218307495117 0.17369461059570312

Final encoder loss: 0.019162815407510848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07104301452636719 0.17403483390808105

Final encoder loss: 0.019392902259912193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07214069366455078 0.17481088638305664

Final encoder loss: 0.019860095938855793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0722036361694336 0.17419028282165527

Final encoder loss: 0.015288122542929846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07124996185302734 0.17312359809875488

Final encoder loss: 0.016139855846810967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07171010971069336 0.17376089096069336

Final encoder loss: 0.01658483376229524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07105612754821777 0.17360520362854004

Final encoder loss: 0.015955715232160102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07107043266296387 0.17385601997375488


Training wesad model
Final encoder loss: 0.21562299132347107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1062006950378418 0.03300738334655762

Final encoder loss: 0.09919091314077377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1040656566619873 0.033829689025878906

Final encoder loss: 0.06397213786840439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10374069213867188 0.033128976821899414

Final encoder loss: 0.046404462307691574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10401439666748047 0.033041954040527344

Final encoder loss: 0.03629368543624878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10480570793151855 0.0341799259185791

Final encoder loss: 0.030067095533013344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10552287101745605 0.03347969055175781

Final encoder loss: 0.02610239014029503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10631847381591797 0.034049272537231445

Final encoder loss: 0.023495670408010483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10573434829711914 0.03567862510681152

Final encoder loss: 0.021774549037218094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1055898666381836 0.03321075439453125

Final encoder loss: 0.020638512447476387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037135124206543 0.03327631950378418

Final encoder loss: 0.019889770075678825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395479202270508 0.03344154357910156

Final encoder loss: 0.01945520006120205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1050260066986084 0.03412985801696777

Final encoder loss: 0.019320974126458168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10486721992492676 0.03341817855834961

Final encoder loss: 0.019326644018292427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10427451133728027 0.033949851989746094

Final encoder loss: 0.01932280883193016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10430359840393066 0.033151865005493164

Final encoder loss: 0.0192683394998312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10415029525756836 0.03379511833190918

Final encoder loss: 0.01922941580414772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10484719276428223 0.03274726867675781

Final encoder loss: 0.019091231748461723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10360956192016602 0.03340554237365723

Final encoder loss: 0.019115999341011047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10411953926086426 0.03341364860534668

Final encoder loss: 0.019012445583939552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10384774208068848 0.03350639343261719

Final encoder loss: 0.019064119085669518

Calculating loss for amigos model
	Full Pass 0.6824724674224854
numFreeParamsPath 18
Reconstruction loss values: 0.028625445440411568 0.038043636828660965

Calculating loss for dapper model
	Full Pass 0.1504662036895752
numFreeParamsPath 18
Reconstruction loss values: 0.022462401539087296 0.025173254311084747

Calculating loss for case model
	Full Pass 0.9098625183105469
numFreeParamsPath 18
Reconstruction loss values: 0.0335870198905468 0.03656194731593132

Calculating loss for emognition model
	Full Pass 0.29151487350463867
numFreeParamsPath 18
Reconstruction loss values: 0.03671635687351227 0.04425295814871788

Calculating loss for empatch model
	Full Pass 0.10516715049743652
numFreeParamsPath 18
Reconstruction loss values: 0.03826596587896347 0.04626241326332092

Calculating loss for wesad model
	Full Pass 0.07770109176635742
numFreeParamsPath 18
Reconstruction loss values: 0.03747351095080376 0.05578557029366493
Total loss calculation time: 3.8965792655944824

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.707168102264404
Total epoch time: 197.97245573997498

Epoch: 42

Training emognition model
Final encoder loss: 0.03594521280054405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08944511413574219 0.28641343116760254

Final encoder loss: 0.03571514241027423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08316421508789062 0.2769474983215332

Final encoder loss: 0.03565378734189153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08348202705383301 0.2737436294555664

Final encoder loss: 0.03360411341714748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.0827779769897461 0.2740302085876465

Final encoder loss: 0.03503147020884566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0832357406616211 0.27382373809814453

Final encoder loss: 0.032278577741290906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08286881446838379 0.27387166023254395

Final encoder loss: 0.0357789176000431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08248567581176758 0.27292895317077637

Final encoder loss: 0.033379252012344325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08215856552124023 0.273500919342041

Final encoder loss: 0.03535920208399799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0824282169342041 0.2736854553222656

Final encoder loss: 0.031908740442722586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0823507308959961 0.2736191749572754

Final encoder loss: 0.0332283793600051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08266806602478027 0.2730073928833008

Final encoder loss: 0.034212263133097665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08275389671325684 0.2736067771911621

Final encoder loss: 0.03420923235119604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08269405364990234 0.2738931179046631

Final encoder loss: 0.03457880505552685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.0825645923614502 0.27275967597961426

Final encoder loss: 0.03319767271218475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08279943466186523 0.27379298210144043

Final encoder loss: 0.034177918475260215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08176660537719727 0.2723517417907715


Training amigos model
Final encoder loss: 0.02686171287801187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10819149017333984 0.3877830505371094

Final encoder loss: 0.028974611304012535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10766148567199707 0.38788270950317383

Final encoder loss: 0.02676880278388096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10770773887634277 0.3882930278778076

Final encoder loss: 0.02720236174335074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10751867294311523 0.38788723945617676

Final encoder loss: 0.02730482357902374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.1076362133026123 0.3882942199707031

Final encoder loss: 0.027499797417711684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10793304443359375 0.3880891799926758

Final encoder loss: 0.027617174454701902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10838627815246582 0.3880949020385742

Final encoder loss: 0.032035176861825256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10762929916381836 0.3880128860473633

Final encoder loss: 0.02741186985456224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10840511322021484 0.3881855010986328

Final encoder loss: 0.028943747186259073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10779237747192383 0.3886094093322754

Final encoder loss: 0.029079962168453585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10804986953735352 0.3886990547180176

Final encoder loss: 0.025413692446652814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10847115516662598 0.3884775638580322

Final encoder loss: 0.027819274646600604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10806941986083984 0.3882439136505127

Final encoder loss: 0.02948885987593223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10847330093383789 0.38827061653137207

Final encoder loss: 0.029301208274882726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.1081552505493164 0.3881645202636719

Final encoder loss: 0.028940920218422893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10257124900817871 0.3821260929107666


Training case model
Final encoder loss: 0.03478552481254552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09104013442993164 0.26334261894226074

Final encoder loss: 0.031986210464060756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09088802337646484 0.2639193534851074

Final encoder loss: 0.031237385021836317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09132599830627441 0.26491236686706543

Final encoder loss: 0.029988259088640376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09107780456542969 0.26398181915283203

Final encoder loss: 0.03039054552362533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09135079383850098 0.26462340354919434

Final encoder loss: 0.029102233562578274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09108853340148926 0.2638087272644043

Final encoder loss: 0.02900241643072286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09134054183959961 0.2637636661529541

Final encoder loss: 0.029223869921684326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09134888648986816 0.26473093032836914

Final encoder loss: 0.02867952826291903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09086227416992188 0.2651958465576172

Final encoder loss: 0.02883413792267357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09111952781677246 0.264756441116333

Final encoder loss: 0.027766985067992904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09113955497741699 0.26514387130737305

Final encoder loss: 0.02745707213053712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09121417999267578 0.2651996612548828

Final encoder loss: 0.027661384639821696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09137916564941406 0.26485586166381836

Final encoder loss: 0.027285801592291915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09078216552734375 0.26464414596557617

Final encoder loss: 0.027036676889623294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09123682975769043 0.2642481327056885

Final encoder loss: 0.026278465249897073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08810973167419434 0.2611680030822754


Training dapper model
Final encoder loss: 0.023478216667944272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06167244911193848 0.14873027801513672

Final encoder loss: 0.02123984251468062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061840057373046875 0.14897465705871582

Final encoder loss: 0.021712396187660913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06182146072387695 0.14963030815124512

Final encoder loss: 0.022552037100051145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06174468994140625 0.14950776100158691

Final encoder loss: 0.023823223019128797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06105446815490723 0.14857721328735352

Final encoder loss: 0.021055224622585095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06120657920837402 0.14927172660827637

Final encoder loss: 0.019369548538683685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0621187686920166 0.1494457721710205

Final encoder loss: 0.021598469318122062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06154513359069824 0.14981627464294434

Final encoder loss: 0.020637848630439885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06172633171081543 0.14979791641235352

Final encoder loss: 0.01973042104509567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06190323829650879 0.1499323844909668

Final encoder loss: 0.020701935967278734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06186556816101074 0.14934253692626953

Final encoder loss: 0.022575032688800217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06159067153930664 0.15077686309814453

Final encoder loss: 0.020672817004490772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06140327453613281 0.14845752716064453

Final encoder loss: 0.020084750149593048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.08434438705444336 0.14937782287597656

Final encoder loss: 0.02188985052920538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06154823303222656 0.14950346946716309

Final encoder loss: 0.026012623162782546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.061164140701293945 0.14821577072143555


Training amigos model
Final encoder loss: 0.021619080484393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10595989227294922 0.3410379886627197

Final encoder loss: 0.021931216331099176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10602378845214844 0.341397762298584

Final encoder loss: 0.020613397779169772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10595488548278809 0.3410811424255371

Final encoder loss: 0.02078834474001377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10623311996459961 0.34099459648132324

Final encoder loss: 0.022315473288606537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1057901382446289 0.34171175956726074

Final encoder loss: 0.021007918145704053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10659623146057129 0.34180736541748047

Final encoder loss: 0.022145556675862016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10764813423156738 0.3420135974884033

Final encoder loss: 0.020313949466702265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10640430450439453 0.34288597106933594

Final encoder loss: 0.022260129917793657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10890030860900879 0.34212803840637207

Final encoder loss: 0.02186457449512397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10618424415588379 0.3417508602142334

Final encoder loss: 0.01995399699819534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10891294479370117 0.3419766426086426

Final encoder loss: 0.02075141734538183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10625529289245605 0.3418409824371338

Final encoder loss: 0.020772847596552877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10733795166015625 0.341489315032959

Final encoder loss: 0.020098942400897137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10605835914611816 0.3434128761291504

Final encoder loss: 0.021019155869700594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10682344436645508 0.3415944576263428

Final encoder loss: 0.02241859936435516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10098934173583984 0.3382105827331543


Training amigos model
Final encoder loss: 0.18075664341449738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4741477966308594 0.0766911506652832

Final encoder loss: 0.18782472610473633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46804285049438477 0.07585620880126953

Final encoder loss: 0.18362566828727722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4647219181060791 0.07556962966918945

Final encoder loss: 0.07595589011907578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4739413261413574 0.07649397850036621

Final encoder loss: 0.07834021747112274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47510194778442383 0.07664227485656738

Final encoder loss: 0.07164671272039413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4698667526245117 0.08121943473815918

Final encoder loss: 0.044520117342472076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4687652587890625 0.07921481132507324

Final encoder loss: 0.045422617346048355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4675426483154297 0.07809901237487793

Final encoder loss: 0.04227478429675102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4610459804534912 0.07507634162902832

Final encoder loss: 0.03211910277605057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4703938961029053 0.07674694061279297

Final encoder loss: 0.0330553837120533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47701430320739746 0.0771942138671875

Final encoder loss: 0.03151308745145798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4698307514190674 0.07871341705322266

Final encoder loss: 0.026452938094735146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4718327522277832 0.08046913146972656

Final encoder loss: 0.027300795540213585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4691798686981201 0.07624101638793945

Final encoder loss: 0.026498980820178986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46633100509643555 0.07555937767028809

Final encoder loss: 0.02398022636771202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46965980529785156 0.0754232406616211

Final encoder loss: 0.02448909915983677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47765350341796875 0.07527709007263184

Final encoder loss: 0.024114152416586876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4717423915863037 0.07284259796142578

Final encoder loss: 0.023311175405979156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47036242485046387 0.0809774398803711

Final encoder loss: 0.023421524092555046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47199034690856934 0.08304286003112793

Final encoder loss: 0.023258622735738754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4676196575164795 0.07494163513183594

Final encoder loss: 0.023190690204501152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46759867668151855 0.0753016471862793

Final encoder loss: 0.02323816902935505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46997785568237305 0.07840657234191895

Final encoder loss: 0.023290682584047318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47525691986083984 0.07221722602844238

Final encoder loss: 0.022790703922510147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4744095802307129 0.08293581008911133

Final encoder loss: 0.022420339286327362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47237610816955566 0.0789036750793457

Final encoder loss: 0.02262907288968563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46866488456726074 0.07674193382263184

Final encoder loss: 0.022132260724902153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4723787307739258 0.0754692554473877

Final encoder loss: 0.02226150408387184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680960178375244 0.07755064964294434

Final encoder loss: 0.022140800952911377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4736611843109131 0.07812762260437012

Final encoder loss: 0.021480867639183998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47664403915405273 0.07398414611816406

Final encoder loss: 0.021565895527601242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47383594512939453 0.08331918716430664

Final encoder loss: 0.021480262279510498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46610116958618164 0.08140110969543457

Final encoder loss: 0.02123245969414711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46764469146728516 0.07388710975646973

Final encoder loss: 0.021265339106321335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46605563163757324 0.07770776748657227

Final encoder loss: 0.02140791155397892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46555233001708984 0.07708907127380371

Final encoder loss: 0.021064115688204765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47968244552612305 0.07494831085205078

Final encoder loss: 0.020588254556059837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4727449417114258 0.07929539680480957

Final encoder loss: 0.021201947703957558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46753907203674316 0.07848000526428223

Final encoder loss: 0.020929954946041107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46885228157043457 0.07566452026367188

Final encoder loss: 0.020632805302739143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46973323822021484 0.07753682136535645

Final encoder loss: 0.020926397293806076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4656229019165039 0.07272076606750488

Final encoder loss: 0.020483797416090965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47670960426330566 0.0754554271697998

Final encoder loss: 0.020330389961600304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47780489921569824 0.07661843299865723

Final encoder loss: 0.020540617406368256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4684000015258789 0.08038830757141113

Final encoder loss: 0.020468702539801598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4727909564971924 0.08243775367736816

Final encoder loss: 0.02027478814125061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46936726570129395 0.07412147521972656

Final encoder loss: 0.020510446280241013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4650290012359619 0.07543373107910156

Final encoder loss: 0.02040519192814827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47178196907043457 0.0732278823852539

Final encoder loss: 0.019847052171826363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4777393341064453 0.0763540267944336

Final encoder loss: 0.020310884341597557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.471743106842041 0.08444023132324219

Final encoder loss: 0.020320171490311623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4699227809906006 0.08123302459716797

Final encoder loss: 0.019829420372843742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47258996963500977 0.07695746421813965

Final encoder loss: 0.020282795652747154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46878576278686523 0.07840180397033691

Final encoder loss: 0.0199850182980299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.469921350479126 0.07482481002807617

Final encoder loss: 0.019725317135453224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47716212272644043 0.07744002342224121

Final encoder loss: 0.020196380093693733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4728734493255615 0.07504868507385254

Final encoder loss: 0.01993739977478981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.470257043838501 0.08035683631896973

Final encoder loss: 0.019678493961691856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4714663028717041 0.08364415168762207

Final encoder loss: 0.020131530240178108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46829676628112793 0.0761110782623291

Final encoder loss: 0.01996305212378502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4679744243621826 0.07674813270568848

Final encoder loss: 0.019506551325321198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4695625305175781 0.07654881477355957

Final encoder loss: 0.019905906170606613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4679412841796875 0.07435822486877441

Final encoder loss: 0.019835498183965683
Final encoder loss: 0.018587933853268623
Final encoder loss: 0.018246648833155632

Training dapper model
Final encoder loss: 0.017286398951729016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05996203422546387 0.10641241073608398

Final encoder loss: 0.01981341855614925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059264183044433594 0.10625839233398438

Final encoder loss: 0.018442267350379574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.059398651123046875 0.10654282569885254

Final encoder loss: 0.01677451973440712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0592656135559082 0.10634803771972656

Final encoder loss: 0.01629665073311769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05947613716125488 0.10652804374694824

Final encoder loss: 0.01515335930840311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059214115142822266 0.1060025691986084

Final encoder loss: 0.0182451060937099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059473276138305664 0.10637760162353516

Final encoder loss: 0.01935580345187469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05912923812866211 0.10634779930114746

Final encoder loss: 0.016644106160034318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05927109718322754 0.10608124732971191

Final encoder loss: 0.0154416545229803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05985665321350098 0.1076364517211914

Final encoder loss: 0.014900416172383602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05980730056762695 0.10767054557800293

Final encoder loss: 0.015203617147851244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060480356216430664 0.10691428184509277

Final encoder loss: 0.017665272897473074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06006789207458496 0.10767173767089844

Final encoder loss: 0.01579533567333368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059918880462646484 0.10750532150268555

Final encoder loss: 0.01759424484546196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06019139289855957 0.10725164413452148

Final encoder loss: 0.012568126775848254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05930352210998535 0.10690999031066895


Training dapper model
Final encoder loss: 0.20245599746704102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11766862869262695 0.03396177291870117

Final encoder loss: 0.2081715166568756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1168522834777832 0.03417706489562988

Final encoder loss: 0.08475501090288162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11534309387207031 0.03459310531616211

Final encoder loss: 0.08584460616111755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11654949188232422 0.034093379974365234

Final encoder loss: 0.050283029675483704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11617875099182129 0.03426671028137207

Final encoder loss: 0.04899940267205238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11624431610107422 0.03392791748046875

Final encoder loss: 0.03446309268474579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1167154312133789 0.03470444679260254

Final encoder loss: 0.033466167747974396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157681941986084 0.034493446350097656

Final encoder loss: 0.026291145011782646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1160435676574707 0.03402352333068848

Final encoder loss: 0.02566927671432495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11683440208435059 0.03394031524658203

Final encoder loss: 0.02167470008134842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11609601974487305 0.03419184684753418

Final encoder loss: 0.021342048421502113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11663103103637695 0.034741878509521484

Final encoder loss: 0.019022708758711815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11643409729003906 0.03465390205383301

Final encoder loss: 0.01880636252462864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11672449111938477 0.03352522850036621

Final encoder loss: 0.017537543550133705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11615276336669922 0.0342106819152832

Final encoder loss: 0.017339523881673813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11621928215026855 0.03394293785095215

Final encoder loss: 0.01663638837635517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1165611743927002 0.0348050594329834

Final encoder loss: 0.016525939106941223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11766552925109863 0.034745216369628906

Final encoder loss: 0.01623924821615219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1165618896484375 0.03415870666503906

Final encoder loss: 0.016094256192445755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11588668823242188 0.03423643112182617

Final encoder loss: 0.0162200964987278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11648225784301758 0.033910512924194336

Final encoder loss: 0.0157656017690897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11647582054138184 0.03465414047241211

Final encoder loss: 0.01593015529215336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11586952209472656 0.034061431884765625

Final encoder loss: 0.015633441507816315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1167607307434082 0.03410458564758301

Final encoder loss: 0.015919124707579613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1166689395904541 0.033724069595336914

Final encoder loss: 0.015498210676014423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.12122893333435059 0.034218549728393555

Final encoder loss: 0.015516112558543682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11632251739501953 0.034569740295410156

Final encoder loss: 0.01568596437573433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11590266227722168 0.034538984298706055

Final encoder loss: 0.015325339511036873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11594939231872559 0.034126996994018555

Final encoder loss: 0.015498134307563305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157689094543457 0.03387165069580078

Final encoder loss: 0.014803574420511723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11633706092834473 0.0343780517578125

Final encoder loss: 0.015174539759755135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11615538597106934 0.03397321701049805

Final encoder loss: 0.01468377374112606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11609506607055664 0.03479576110839844

Final encoder loss: 0.014677622355520725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11667847633361816 0.03419780731201172

Final encoder loss: 0.014667782932519913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11595869064331055 0.03405046463012695

Final encoder loss: 0.014523164369165897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11599159240722656 0.034432411193847656

Final encoder loss: 0.014517754316329956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11611008644104004 0.03456616401672363

Final encoder loss: 0.014101742766797543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159977912902832 0.03524470329284668

Final encoder loss: 0.014253622852265835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1161351203918457 0.034070491790771484

Final encoder loss: 0.014151051640510559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11586403846740723 0.03384733200073242

Final encoder loss: 0.014131168834865093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11693835258483887 0.03400468826293945

Final encoder loss: 0.01404106430709362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11609363555908203 0.03462362289428711

Final encoder loss: 0.014027534052729607
Final encoder loss: 0.01314567867666483

Training case model
Final encoder loss: 0.024648142984655133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08927369117736816 0.21885967254638672

Final encoder loss: 0.023970652760949347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08890247344970703 0.21988630294799805

Final encoder loss: 0.024945433575130415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08973193168640137 0.21895766258239746

Final encoder loss: 0.023751588454039258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08899521827697754 0.2191171646118164

Final encoder loss: 0.023871668857533702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0895836353302002 0.21896600723266602

Final encoder loss: 0.023589442981450773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08958983421325684 0.2188396453857422

Final encoder loss: 0.023437518611320835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08967280387878418 0.21897006034851074

Final encoder loss: 0.023891363619839102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08949160575866699 0.21919465065002441

Final encoder loss: 0.02407334118277823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08943700790405273 0.21900081634521484

Final encoder loss: 0.023644608042990234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08866572380065918 0.2185511589050293

Final encoder loss: 0.02356520361442354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08886432647705078 0.21845674514770508

Final encoder loss: 0.02378816236515337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08918905258178711 0.21868324279785156

Final encoder loss: 0.023399102687365638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08932089805603027 0.21837115287780762

Final encoder loss: 0.024244192306163124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08877229690551758 0.21864056587219238

Final encoder loss: 0.023692439728928982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08934426307678223 0.21851682662963867

Final encoder loss: 0.02308989459647479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08562922477722168 0.21526217460632324


Training case model
Final encoder loss: 0.20297278463840485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2624471187591553 0.05217432975769043

Final encoder loss: 0.18890394270420074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26804208755493164 0.05234885215759277

Final encoder loss: 0.19015346467494965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2732529640197754 0.05185294151306152

Final encoder loss: 0.1921849250793457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25672483444213867 0.05290651321411133

Final encoder loss: 0.18081282079219818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26842689514160156 0.052276611328125

Final encoder loss: 0.19192193448543549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2574291229248047 0.05191826820373535

Final encoder loss: 0.10339359194040298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2695648670196533 0.052778005599975586

Final encoder loss: 0.09291790425777435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25881052017211914 0.0527043342590332

Final encoder loss: 0.08962986618280411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26886892318725586 0.05173230171203613

Final encoder loss: 0.08831305056810379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26850318908691406 0.052542924880981445

Final encoder loss: 0.0802750512957573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2609446048736572 0.05295968055725098

Final encoder loss: 0.08384490013122559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548863887786865 0.05350685119628906

Final encoder loss: 0.06052321195602417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26941537857055664 0.05170154571533203

Final encoder loss: 0.054550498723983765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2594869136810303 0.05213117599487305

Final encoder loss: 0.05287739634513855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2782554626464844 0.05590248107910156

Final encoder loss: 0.05320065841078758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2745792865753174 0.05270242691040039

Final encoder loss: 0.04971974343061447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2778170108795166 0.0521388053894043

Final encoder loss: 0.05189472436904907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25816798210144043 0.050678253173828125

Final encoder loss: 0.042855773121118546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27093958854675293 0.05345010757446289

Final encoder loss: 0.039467841386795044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2772819995880127 0.05325436592102051

Final encoder loss: 0.038400813937187195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26877665519714355 0.05296015739440918

Final encoder loss: 0.03912483900785446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25936436653137207 0.05264854431152344

Final encoder loss: 0.03766804561018944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2690086364746094 0.05374455451965332

Final encoder loss: 0.03864671289920807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549898624420166 0.052494049072265625

Final encoder loss: 0.035129331052303314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2690246105194092 0.05168318748474121

Final encoder loss: 0.03338662162423134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2597503662109375 0.05171608924865723

Final encoder loss: 0.032778549939394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26807165145874023 0.05383753776550293

Final encoder loss: 0.033368419855833054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25813746452331543 0.05279541015625

Final encoder loss: 0.033488497138023376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578001022338867 0.052373647689819336

Final encoder loss: 0.033286552876234055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25661754608154297 0.05314350128173828

Final encoder loss: 0.03215523436665535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26886820793151855 0.053305625915527344

Final encoder loss: 0.03108333609998226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2777140140533447 0.05219101905822754

Final encoder loss: 0.03100738860666752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26844120025634766 0.052724599838256836

Final encoder loss: 0.031116653233766556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26873064041137695 0.054433584213256836

Final encoder loss: 0.03194134309887886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27442455291748047 0.05185055732727051

Final encoder loss: 0.03163359686732292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25574517250061035 0.05111432075500488

Final encoder loss: 0.030013926327228546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2697732448577881 0.05260443687438965

Final encoder loss: 0.0291834007948637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2778308391571045 0.05345439910888672

Final encoder loss: 0.0287367794662714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2678225040435791 0.05326390266418457

Final encoder loss: 0.029231658205389977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25901341438293457 0.051868438720703125

Final encoder loss: 0.030079293996095657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2789194583892822 0.05309009552001953

Final encoder loss: 0.02921510487794876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561213970184326 0.05405902862548828

Final encoder loss: 0.028303727507591248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2679464817047119 0.051844120025634766

Final encoder loss: 0.027805494144558907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690441608428955 0.051506757736206055

Final encoder loss: 0.027464086189866066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2793560028076172 0.0511476993560791

Final encoder loss: 0.0276965219527483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25724196434020996 0.05275678634643555

Final encoder loss: 0.02859378792345524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26967835426330566 0.05311179161071777

Final encoder loss: 0.027976173907518387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553384304046631 0.051869869232177734

Final encoder loss: 0.02731539122760296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26992177963256836 0.05170583724975586

Final encoder loss: 0.026803113520145416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26790928840637207 0.05374884605407715

Final encoder loss: 0.026482580229640007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26932454109191895 0.05239295959472656

Final encoder loss: 0.026797184720635414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28429436683654785 0.052934885025024414

Final encoder loss: 0.027811706066131592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2625396251678467 0.05337882041931152

Final encoder loss: 0.02685779146850109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25522470474243164 0.0514979362487793

Final encoder loss: 0.02666785567998886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2691926956176758 0.05460715293884277

Final encoder loss: 0.026377016678452492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2791931629180908 0.05295419692993164

Final encoder loss: 0.026147035881876945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26915836334228516 0.055143117904663086

Final encoder loss: 0.026192834600806236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2685563564300537 0.051766157150268555

Final encoder loss: 0.027139028534293175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2694995403289795 0.052974700927734375

Final encoder loss: 0.026584794744849205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25682783126831055 0.05206441879272461

Final encoder loss: 0.0262506902217865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25778627395629883 0.054601430892944336

Final encoder loss: 0.025772007182240486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27002477645874023 0.05249428749084473

Final encoder loss: 0.025343507528305054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26927971839904785 0.05278301239013672

Final encoder loss: 0.025598643347620964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27927708625793457 0.05186200141906738

Final encoder loss: 0.026643505319952965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27646827697753906 0.054488182067871094

Final encoder loss: 0.025885019451379776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548813819885254 0.05269670486450195

Final encoder loss: 0.02564362809062004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2754664421081543 0.05141091346740723

Final encoder loss: 0.025422265753149986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25992393493652344 0.05157899856567383

Final encoder loss: 0.025231827050447464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27683186531066895 0.05426311492919922

Final encoder loss: 0.025286849588155746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2684931755065918 0.05222892761230469

Final encoder loss: 0.026257360354065895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2592461109161377 0.05051541328430176

Final encoder loss: 0.025492584332823753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25679826736450195 0.05184435844421387

Final encoder loss: 0.025400707498192787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26833605766296387 0.052884817123413086

Final encoder loss: 0.025046037510037422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2703132629394531 0.0521998405456543

Final encoder loss: 0.024850543588399887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683577537536621 0.05304574966430664

Final encoder loss: 0.02493157982826233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2578096389770508 0.053830862045288086

Final encoder loss: 0.02579488977789879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2691495418548584 0.05289173126220703

Final encoder loss: 0.025129670277237892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558586597442627 0.05224156379699707

Final encoder loss: 0.02516367845237255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2704477310180664 0.0532073974609375

Final encoder loss: 0.02492637000977993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572298049926758 0.053333282470703125

Final encoder loss: 0.024622036144137383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26818227767944336 0.052082061767578125

Final encoder loss: 0.024734193459153175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2587566375732422 0.052640676498413086

Final encoder loss: 0.025571567937731743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26991868019104004 0.05097150802612305

Final encoder loss: 0.025087423622608185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25596165657043457 0.054547786712646484

Final encoder loss: 0.024993805214762688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26886916160583496 0.05225038528442383

Final encoder loss: 0.024593761190772057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2692131996154785 0.05231761932373047

Final encoder loss: 0.02435646578669548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27954912185668945 0.052765607833862305

Final encoder loss: 0.02437405474483967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25819897651672363 0.05272960662841797

Final encoder loss: 0.02533838525414467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2705216407775879 0.05224800109863281

Final encoder loss: 0.02463657781481743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255922794342041 0.0518498420715332

Final encoder loss: 0.024694040417671204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26906347274780273 0.05456185340881348

Final encoder loss: 0.024416198953986168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27782201766967773 0.05180168151855469

Final encoder loss: 0.02424345724284649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2752041816711426 0.05117011070251465

Final encoder loss: 0.024151820689439774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27021217346191406 0.05171799659729004

Final encoder loss: 0.02526443637907505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25864386558532715 0.053870439529418945

Final encoder loss: 0.024422122165560722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558138370513916 0.05261516571044922

Final encoder loss: 0.024576794356107712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583122253417969 0.052179574966430664

Final encoder loss: 0.024253731593489647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2600088119506836 0.050594329833984375

Final encoder loss: 0.023991037160158157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27618885040283203 0.053458213806152344

Final encoder loss: 0.024033835157752037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2687368392944336 0.05172276496887207

Final encoder loss: 0.02485175058245659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2687664031982422 0.05344748497009277

Final encoder loss: 0.02422839216887951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569401264190674 0.051511526107788086

Final encoder loss: 0.024387020617723465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26842570304870605 0.05501580238342285

Final encoder loss: 0.024182995781302452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27580904960632324 0.05216789245605469

Final encoder loss: 0.023963788524270058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2743690013885498 0.05237913131713867

Final encoder loss: 0.023937592282891273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2695159912109375 0.052048444747924805

Final encoder loss: 0.02478504739701748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2849736213684082 0.05403256416320801

Final encoder loss: 0.024341104552149773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547032833099365 0.05265212059020996

Final encoder loss: 0.02429763600230217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2760176658630371 0.05384063720703125

Final encoder loss: 0.023988990113139153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28926515579223633 0.05137753486633301

Final encoder loss: 0.0237610824406147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2687063217163086 0.053539276123046875

Final encoder loss: 0.02365659363567829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2679176330566406 0.05163121223449707

Final encoder loss: 0.024617288261651993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25712037086486816 0.052011728286743164

Final encoder loss: 0.02390156500041485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25626564025878906 0.05425095558166504

Final encoder loss: 0.024106593802571297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581138610839844 0.0521998405456543

Final encoder loss: 0.023900995030999184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2678987979888916 0.05158591270446777

Final encoder loss: 0.023694593459367752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27016448974609375 0.05195450782775879

Final encoder loss: 0.023581504821777344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27728772163391113 0.0530848503112793

Final encoder loss: 0.02457321435213089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2587742805480957 0.05230426788330078

Final encoder loss: 0.02400565892457962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25586676597595215 0.053417205810546875

Final encoder loss: 0.024058686569333076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27042365074157715 0.05254030227661133

Final encoder loss: 0.02370905503630638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2764859199523926 0.05332136154174805

Final encoder loss: 0.02351205237209797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26842164993286133 0.053177833557128906

Final encoder loss: 0.023483028635382652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2690849304199219 0.053244829177856445

Final encoder loss: 0.02428087778389454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.279346227645874 0.05208325386047363

Final encoder loss: 0.023671017959713936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.254594087600708 0.053014516830444336

Final encoder loss: 0.023884393274784088
Final encoder loss: 0.02323957346379757
Final encoder loss: 0.02243124134838581
Final encoder loss: 0.021603619679808617
Final encoder loss: 0.02165766805410385
Final encoder loss: 0.020352015271782875

Training emognition model
Final encoder loss: 0.027610750458798565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08163881301879883 0.23119354248046875

Final encoder loss: 0.028438375395288342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08115625381469727 0.23075008392333984

Final encoder loss: 0.027635522354748625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08120584487915039 0.23034429550170898

Final encoder loss: 0.027294418441058246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08128619194030762 0.23100042343139648

Final encoder loss: 0.0275386191568521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08116459846496582 0.23104405403137207

Final encoder loss: 0.026532411851895893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08117485046386719 0.23087072372436523

Final encoder loss: 0.028040377575776553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08212637901306152 0.23149394989013672

Final encoder loss: 0.028523987818951747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08177757263183594 0.2308974266052246

Final encoder loss: 0.026144419115768303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08251500129699707 0.23098111152648926

Final encoder loss: 0.027331533109421286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08114147186279297 0.23132872581481934

Final encoder loss: 0.02726140364754004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0831911563873291 0.23130178451538086

Final encoder loss: 0.028297768004709366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0818643569946289 0.23114943504333496

Final encoder loss: 0.026575935451011345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08258700370788574 0.23063373565673828

Final encoder loss: 0.028229321988605942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08128643035888672 0.23118257522583008

Final encoder loss: 0.027267735876927945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08123660087585449 0.23085308074951172

Final encoder loss: 0.02936050480576631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08033037185668945 0.23027920722961426


Training emognition model
Final encoder loss: 0.1935521811246872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24989891052246094 0.048676252365112305

Final encoder loss: 0.19495895504951477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24757051467895508 0.04829573631286621

Final encoder loss: 0.0871281772851944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24889707565307617 0.04844474792480469

Final encoder loss: 0.08642067015171051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247361421585083 0.049434661865234375

Final encoder loss: 0.055788833647966385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24904870986938477 0.05033230781555176

Final encoder loss: 0.054206427186727524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24855422973632812 0.04893016815185547

Final encoder loss: 0.04196946322917938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24826407432556152 0.050098419189453125

Final encoder loss: 0.04087220877408981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24686264991760254 0.04862236976623535

Final encoder loss: 0.034966904670000076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2507762908935547 0.048345088958740234

Final encoder loss: 0.03422965854406357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24596405029296875 0.05107235908508301

Final encoder loss: 0.03116639144718647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24818015098571777 0.0500941276550293

Final encoder loss: 0.030573830008506775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2479088306427002 0.048329830169677734

Final encoder loss: 0.029072757810354233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24757003784179688 0.05075573921203613

Final encoder loss: 0.02855437807738781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470846176147461 0.05018353462219238

Final encoder loss: 0.02797534503042698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24903392791748047 0.04910111427307129

Final encoder loss: 0.02760663628578186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475593090057373 0.05099678039550781

Final encoder loss: 0.027431102469563484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478179931640625 0.05090188980102539

Final encoder loss: 0.0273459292948246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472078800201416 0.04927992820739746

Final encoder loss: 0.027275437489151955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496800422668457 0.04842185974121094

Final encoder loss: 0.02753710187971592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24834561347961426 0.05006575584411621

Final encoder loss: 0.027085857465863228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24867606163024902 0.0507051944732666

Final encoder loss: 0.027271045371890068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24831724166870117 0.050849199295043945

Final encoder loss: 0.02699975110590458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24871206283569336 0.050055742263793945

Final encoder loss: 0.026914581656455994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24745535850524902 0.05046510696411133

Final encoder loss: 0.0268565621227026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24695086479187012 0.048302412033081055

Final encoder loss: 0.026584086939692497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24619030952453613 0.04876995086669922

Final encoder loss: 0.02656332030892372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24705934524536133 0.04851484298706055

Final encoder loss: 0.026630878448486328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2447216510772705 0.04857373237609863

Final encoder loss: 0.026433434337377548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2466447353363037 0.04965949058532715

Final encoder loss: 0.02643917128443718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2440948486328125 0.04933905601501465

Final encoder loss: 0.026308655738830566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2461378574371338 0.04901242256164551

Final encoder loss: 0.02641269750893116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24506235122680664 0.048673152923583984

Final encoder loss: 0.025994859635829926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2465052604675293 0.04819488525390625

Final encoder loss: 0.02612970769405365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24521279335021973 0.04868316650390625

Final encoder loss: 0.025889543816447258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24789714813232422 0.04962587356567383

Final encoder loss: 0.026263684034347534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24626398086547852 0.04962730407714844

Final encoder loss: 0.02581941708922386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24864482879638672 0.049193382263183594

Final encoder loss: 0.026033220812678337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24819469451904297 0.05052638053894043

Final encoder loss: 0.02586369961500168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24885940551757812 0.04981112480163574

Final encoder loss: 0.025942252948880196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24779462814331055 0.049903154373168945

Final encoder loss: 0.02589837647974491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24992108345031738 0.04951333999633789

Final encoder loss: 0.0256218109279871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2460765838623047 0.050051212310791016

Final encoder loss: 0.02574879117310047
Final encoder loss: 0.024817148223519325

Training empatch model
Final encoder loss: 0.03963278979918618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07113504409790039 0.17380857467651367

Final encoder loss: 0.03597822918451251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07071709632873535 0.17398619651794434

Final encoder loss: 0.03640208045579015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07143044471740723 0.17358040809631348

Final encoder loss: 0.03464119595019923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07147026062011719 0.17403340339660645

Final encoder loss: 0.036550702184019825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07193636894226074 0.17350554466247559

Final encoder loss: 0.03604772980293546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07152390480041504 0.17390012741088867

Final encoder loss: 0.03337381204991123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07230234146118164 0.17415118217468262

Final encoder loss: 0.03464983035689131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07079243659973145 0.173659086227417

Final encoder loss: 0.02508350456626162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07142877578735352 0.17401671409606934

Final encoder loss: 0.025525588620750325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0714254379272461 0.1740431785583496

Final encoder loss: 0.02565315334878614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07160162925720215 0.17401909828186035

Final encoder loss: 0.02530423243496036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0713653564453125 0.17389798164367676

Final encoder loss: 0.02594840345986264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07143235206604004 0.17421174049377441

Final encoder loss: 0.027441422962274236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07216143608093262 0.17406582832336426

Final encoder loss: 0.02481297960126475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07124900817871094 0.1756267547607422

Final encoder loss: 0.0266276791413568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07189011573791504 0.17403268814086914


Training empatch model
Final encoder loss: 0.1711573302745819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17763066291809082 0.044031620025634766

Final encoder loss: 0.0808693990111351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17679929733276367 0.044298410415649414

Final encoder loss: 0.05570634827017784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17557787895202637 0.044492244720458984

Final encoder loss: 0.04347514733672142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17609047889709473 0.043505191802978516

Final encoder loss: 0.03636161610484123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17534780502319336 0.04438352584838867

Final encoder loss: 0.03192925825715065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1771986484527588 0.043947696685791016

Final encoder loss: 0.029039492830634117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17468953132629395 0.04414534568786621

Final encoder loss: 0.027120882645249367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1753826141357422 0.04423880577087402

Final encoder loss: 0.025788454338908195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17455840110778809 0.04425358772277832

Final encoder loss: 0.024919386953115463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1767265796661377 0.044000864028930664

Final encoder loss: 0.0243129450827837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761159896850586 0.04356527328491211

Final encoder loss: 0.0239468514919281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758410930633545 0.04409146308898926

Final encoder loss: 0.023645158857107162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755688190460205 0.04477977752685547

Final encoder loss: 0.02345198579132557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745152473449707 0.043595314025878906

Final encoder loss: 0.023308226838707924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754775047302246 0.04367685317993164

Final encoder loss: 0.023088399320840836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1758136749267578 0.043653249740600586

Final encoder loss: 0.02291019819676876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17692184448242188 0.04431867599487305

Final encoder loss: 0.022793559357523918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17540478706359863 0.04372000694274902

Final encoder loss: 0.0227480698376894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17557263374328613 0.043970346450805664

Final encoder loss: 0.02268277481198311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17573761940002441 0.04345965385437012

Final encoder loss: 0.022595582529902458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17655682563781738 0.04380011558532715

Final encoder loss: 0.022570954635739326

Training wesad model
Final encoder loss: 0.03736404319947056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07121992111206055 0.17403197288513184

Final encoder loss: 0.03610826007512411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0720522403717041 0.1738414764404297

Final encoder loss: 0.03479222866910065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07114648818969727 0.1741952896118164

Final encoder loss: 0.03624505228254038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07208967208862305 0.17406988143920898

Final encoder loss: 0.022666787517865432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07147431373596191 0.17387104034423828

Final encoder loss: 0.02360606467284549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07237362861633301 0.17435669898986816

Final encoder loss: 0.026704109698437943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07183265686035156 0.17373967170715332

Final encoder loss: 0.02513575271002133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07135605812072754 0.1743755340576172

Final encoder loss: 0.01843502872080194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0710442066192627 0.17428898811340332

Final encoder loss: 0.019558526848406914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07128381729125977 0.1744241714477539

Final encoder loss: 0.019999633257361617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07121586799621582 0.17388129234313965

Final encoder loss: 0.019409721248592804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07137203216552734 0.17407584190368652

Final encoder loss: 0.016115953763690648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07224678993225098 0.1741166114807129

Final encoder loss: 0.015263899569162851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07147622108459473 0.1739058494567871

Final encoder loss: 0.016760923656388604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07264590263366699 0.17451858520507812

Final encoder loss: 0.015595077953702892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07160663604736328 0.1739516258239746


Training wesad model
Final encoder loss: 0.21561861038208008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1080780029296875 0.03363370895385742

Final encoder loss: 0.09949283301830292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10432696342468262 0.03326725959777832

Final encoder loss: 0.06412507593631744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10468053817749023 0.03338789939880371

Final encoder loss: 0.04651812091469765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10467314720153809 0.034047603607177734

Final encoder loss: 0.03626057133078575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10588884353637695 0.032906293869018555

Final encoder loss: 0.029995020478963852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357093811035156 0.03400373458862305

Final encoder loss: 0.02604501135647297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042475700378418 0.033828020095825195

Final encoder loss: 0.023483337834477425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442304611206055 0.03363633155822754

Final encoder loss: 0.02174939401447773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10467672348022461 0.03362154960632324

Final encoder loss: 0.020567284896969795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388731956481934 0.03342556953430176

Final encoder loss: 0.019760170951485634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10448288917541504 0.033364295959472656

Final encoder loss: 0.019279124215245247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10561037063598633 0.03430914878845215

Final encoder loss: 0.019092688336968422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042931079864502 0.033600568771362305

Final encoder loss: 0.01910269819200039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10424280166625977 0.03384995460510254

Final encoder loss: 0.019101379439234734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10488605499267578 0.03383898735046387

Final encoder loss: 0.019130287691950798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10550498962402344 0.03316497802734375

Final encoder loss: 0.019195443019270897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10447573661804199 0.033954620361328125

Final encoder loss: 0.01917298138141632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10457253456115723 0.033498525619506836

Final encoder loss: 0.019060982391238213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10510945320129395 0.03380465507507324

Final encoder loss: 0.01889244094491005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10514140129089355 0.033429861068725586

Final encoder loss: 0.018960287794470787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395646095275879 0.03288841247558594

Final encoder loss: 0.018970075994729996

Calculating loss for amigos model
	Full Pass 0.6846184730529785
numFreeParamsPath 18
Reconstruction loss values: 0.028286004438996315 0.03750687465071678

Calculating loss for dapper model
	Full Pass 0.1506335735321045
numFreeParamsPath 18
Reconstruction loss values: 0.021431971341371536 0.02497434988617897

Calculating loss for case model
	Full Pass 0.9079306125640869
numFreeParamsPath 18
Reconstruction loss values: 0.033092256635427475 0.03646598756313324

Calculating loss for emognition model
	Full Pass 0.2912476062774658
numFreeParamsPath 18
Reconstruction loss values: 0.036606013774871826 0.04432176798582077

Calculating loss for empatch model
	Full Pass 0.1044466495513916
numFreeParamsPath 18
Reconstruction loss values: 0.0370221845805645 0.04454817995429039

Calculating loss for wesad model
	Full Pass 0.07674217224121094
numFreeParamsPath 18
Reconstruction loss values: 0.037217386066913605 0.05453014373779297
Total loss calculation time: 3.8986611366271973

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.751567602157593
Total epoch time: 204.15668416023254

Epoch: 43

Training dapper model
Final encoder loss: 0.02067153474255913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06764006614685059 0.15799713134765625

Final encoder loss: 0.01888713253493453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.0613710880279541 0.15178489685058594

Final encoder loss: 0.022279320338534393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06277322769165039 0.1504676342010498

Final encoder loss: 0.01939827729760175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06207776069641113 0.15185284614562988

Final encoder loss: 0.01896160372212695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06217503547668457 0.14996576309204102

Final encoder loss: 0.01978997813427733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06208181381225586 0.15196990966796875

Final encoder loss: 0.02360876056916917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06275629997253418 0.15051913261413574

Final encoder loss: 0.020451548848711608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06257438659667969 0.15162444114685059

Final encoder loss: 0.02223027251603945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06215953826904297 0.1503610610961914

Final encoder loss: 0.020795442111637214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06338667869567871 0.15107989311218262

Final encoder loss: 0.020032979031341623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06189465522766113 0.15320897102355957

Final encoder loss: 0.01930391529404821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06332612037658691 0.15015506744384766

Final encoder loss: 0.020476042167560618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.0619049072265625 0.15073704719543457

Final encoder loss: 0.01986888343362751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06263089179992676 0.150712251663208

Final encoder loss: 0.022413859379895217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06175494194030762 0.1508316993713379

Final encoder loss: 0.02057890925754003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06167197227478027 0.14988040924072266


Training amigos model
Final encoder loss: 0.028478725030906822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1083676815032959 0.3890407085418701

Final encoder loss: 0.026824008808033806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10741686820983887 0.38747119903564453

Final encoder loss: 0.028951492511420242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10733938217163086 0.38732409477233887

Final encoder loss: 0.027597563106863957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.1077585220336914 0.3878757953643799

Final encoder loss: 0.02852505258491704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10783529281616211 0.3876953125

Final encoder loss: 0.026673181504154462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10764479637145996 0.3876230716705322

Final encoder loss: 0.02670763963148259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1076807975769043 0.3877851963043213

Final encoder loss: 0.0271529483117294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10772562026977539 0.38843750953674316

Final encoder loss: 0.028617624823766744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10834860801696777 0.38893914222717285

Final encoder loss: 0.026197942720818403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.1082468032836914 0.38935160636901855

Final encoder loss: 0.02422813013380049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10767054557800293 0.389603853225708

Final encoder loss: 0.028078283115091092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10831284523010254 0.3889641761779785

Final encoder loss: 0.0263964997556365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10870027542114258 0.3895092010498047

Final encoder loss: 0.027111542322660723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10845708847045898 0.3889927864074707

Final encoder loss: 0.028431271151943223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10841822624206543 0.38910508155822754

Final encoder loss: 0.028188336065786693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10346007347106934 0.38421106338500977


Training case model
Final encoder loss: 0.03388072746036372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09130096435546875 0.26519107818603516

Final encoder loss: 0.03065099372014213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09105753898620605 0.2642207145690918

Final encoder loss: 0.030908543280261734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09118390083312988 0.2656996250152588

Final encoder loss: 0.02920275268979984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09145236015319824 0.2670938968658447

Final encoder loss: 0.029090897155249193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09184646606445312 0.26484060287475586

Final encoder loss: 0.027987698864538477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09163284301757812 0.26598453521728516

Final encoder loss: 0.028520101842849012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09382390975952148 0.2669963836669922

Final encoder loss: 0.02801830617013995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09152078628540039 0.26471972465515137

Final encoder loss: 0.02831278639396896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09277009963989258 0.2653539180755615

Final encoder loss: 0.027226422184386978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09096670150756836 0.2646820545196533

Final encoder loss: 0.02706596487081057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09177899360656738 0.2653498649597168

Final encoder loss: 0.026336610802978725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09122300148010254 0.2650144100189209

Final encoder loss: 0.026306722353544948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09226703643798828 0.26720643043518066

Final encoder loss: 0.026793103148913828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09227371215820312 0.2657029628753662

Final encoder loss: 0.025674360183603417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09102296829223633 0.2654721736907959

Final encoder loss: 0.027056730968802557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08785653114318848 0.2626228332519531


Training emognition model
Final encoder loss: 0.03799194442176693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08331608772277832 0.2754096984863281

Final encoder loss: 0.036050411609989265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08300137519836426 0.2738769054412842

Final encoder loss: 0.034849480292867684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0830385684967041 0.2750363349914551

Final encoder loss: 0.0348871015928428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08297204971313477 0.27455830574035645

Final encoder loss: 0.034099270073333736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08312749862670898 0.2747368812561035

Final encoder loss: 0.034130200780711134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08301687240600586 0.27425599098205566

Final encoder loss: 0.03378760594617853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08309650421142578 0.2740657329559326

Final encoder loss: 0.03424314453354609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08313775062561035 0.2739095687866211

Final encoder loss: 0.03282798867315367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08354711532592773 0.2742502689361572

Final encoder loss: 0.03283365890597756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08281493186950684 0.2744889259338379

Final encoder loss: 0.03465894130164399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08294534683227539 0.27489542961120605

Final encoder loss: 0.03593812788728954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08285975456237793 0.2737565040588379

Final encoder loss: 0.034183094560014844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08263921737670898 0.27311158180236816

Final encoder loss: 0.03211553164637695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08263945579528809 0.27329421043395996

Final encoder loss: 0.033411518017920395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0824127197265625 0.27474522590637207

Final encoder loss: 0.03403822845601513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08220338821411133 0.27451300621032715


Training amigos model
Final encoder loss: 0.020696854073517252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10678315162658691 0.3415818214416504

Final encoder loss: 0.021584469323921107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10603451728820801 0.34098196029663086

Final encoder loss: 0.020276398080570746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10580945014953613 0.341627836227417

Final encoder loss: 0.020877670455441817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10594058036804199 0.3407704830169678

Final encoder loss: 0.019829319724489754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10550689697265625 0.34108424186706543

Final encoder loss: 0.019302890851911454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10586047172546387 0.341325044631958

Final encoder loss: 0.020939645685254995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10577726364135742 0.340839147567749

Final encoder loss: 0.02177473233897224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10570883750915527 0.3408174514770508

Final encoder loss: 0.019940086351894704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10566926002502441 0.34115004539489746

Final encoder loss: 0.020734425803906463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10575342178344727 0.34076857566833496

Final encoder loss: 0.020731142692413183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10545849800109863 0.34076976776123047

Final encoder loss: 0.019216683022923335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10590219497680664 0.34151339530944824

Final encoder loss: 0.021252976203968463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10615420341491699 0.3411126136779785

Final encoder loss: 0.023021816244927856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1058046817779541 0.3409450054168701

Final encoder loss: 0.01904925019664681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10560083389282227 0.34124302864074707

Final encoder loss: 0.021139382895824788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10086631774902344 0.33740758895874023


Training amigos model
Final encoder loss: 0.18075385689735413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46353626251220703 0.0728759765625

Final encoder loss: 0.1878196895122528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4616870880126953 0.07390832901000977

Final encoder loss: 0.18364687263965607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45714497566223145 0.0734105110168457

Final encoder loss: 0.0754697397351265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4628899097442627 0.07501864433288574

Final encoder loss: 0.07790596783161163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4626433849334717 0.07428765296936035

Final encoder loss: 0.07131188362836838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4567122459411621 0.07328987121582031

Final encoder loss: 0.04412129148840904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4595623016357422 0.07223987579345703

Final encoder loss: 0.0452546589076519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46115732192993164 0.07521367073059082

Final encoder loss: 0.04216378927230835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45634961128234863 0.07507967948913574

Final encoder loss: 0.03187405318021774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45908379554748535 0.07477521896362305

Final encoder loss: 0.03298052400350571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45944690704345703 0.07364177703857422

Final encoder loss: 0.03133629634976387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45766329765319824 0.0737307071685791

Final encoder loss: 0.026366064324975014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46124887466430664 0.07402586936950684

Final encoder loss: 0.02737218141555786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4600985050201416 0.07467341423034668

Final encoder loss: 0.026285992935299873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45616674423217773 0.07583856582641602

Final encoder loss: 0.023999370634555817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4609503746032715 0.07388997077941895

Final encoder loss: 0.024837244302034378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4596107006072998 0.07351112365722656

Final encoder loss: 0.023930469527840614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.458446741104126 0.07345962524414062

Final encoder loss: 0.023150628432631493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4595637321472168 0.07335686683654785

Final encoder loss: 0.023952167481184006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45890092849731445 0.07592511177062988

Final encoder loss: 0.02324959821999073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4572486877441406 0.07376527786254883

Final encoder loss: 0.023037219420075417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4595482349395752 0.07526731491088867

Final encoder loss: 0.023475660011172295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4609842300415039 0.07316112518310547

Final encoder loss: 0.023066647350788116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4563124179840088 0.07196164131164551

Final encoder loss: 0.022950317710638046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4602468013763428 0.07479047775268555

Final encoder loss: 0.02271602675318718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45854663848876953 0.07449126243591309

Final encoder loss: 0.022518299520015717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45839619636535645 0.07376837730407715

Final encoder loss: 0.022115031257271767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4611167907714844 0.07393312454223633

Final encoder loss: 0.021945128217339516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46172261238098145 0.07371044158935547

Final encoder loss: 0.02203674428164959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4574277400970459 0.07355523109436035

Final encoder loss: 0.021276192739605904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47461605072021484 0.0820000171661377

Final encoder loss: 0.021357517689466476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46884775161743164 0.08131003379821777

Final encoder loss: 0.02145085297524929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46497273445129395 0.07455277442932129

Final encoder loss: 0.021184615790843964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46544337272644043 0.07675886154174805

Final encoder loss: 0.0210738442838192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46913790702819824 0.07693815231323242

Final encoder loss: 0.021333681419491768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47498035430908203 0.07488393783569336

Final encoder loss: 0.020901596173644066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4758586883544922 0.07693600654602051

Final encoder loss: 0.020812248811125755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.472553014755249 0.07811617851257324

Final encoder loss: 0.02093207836151123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46562695503234863 0.07436323165893555

Final encoder loss: 0.020735204219818115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4696063995361328 0.07451772689819336

Final encoder loss: 0.020512966439127922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4662361145019531 0.07503151893615723

Final encoder loss: 0.020597051829099655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47378039360046387 0.07783627510070801

Final encoder loss: 0.020341906696558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4749126434326172 0.07634091377258301

Final encoder loss: 0.02027050405740738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4715702533721924 0.08170819282531738

Final encoder loss: 0.02039296366274357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46433281898498535 0.0784158706665039

Final encoder loss: 0.02024165540933609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46756911277770996 0.07637357711791992

Final encoder loss: 0.02000615745782852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46736741065979004 0.0742638111114502

Final encoder loss: 0.020524505525827408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46451425552368164 0.07591795921325684

Final encoder loss: 0.020097285509109497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4780712127685547 0.07443928718566895

Final encoder loss: 0.019902663305401802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47435522079467773 0.07731866836547852

Final encoder loss: 0.02029230259358883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46769165992736816 0.07707977294921875

Final encoder loss: 0.02012098953127861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4684021472930908 0.07535839080810547

Final encoder loss: 0.01974158175289631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4675278663635254 0.07561349868774414

Final encoder loss: 0.0200310368090868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4624958038330078 0.07523822784423828

Final encoder loss: 0.01986950822174549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4750216007232666 0.07805228233337402

Final encoder loss: 0.019685886800289154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4748351573944092 0.07307243347167969

Final encoder loss: 0.019891731441020966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46866941452026367 0.08049631118774414

Final encoder loss: 0.019833553582429886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4677920341491699 0.07930922508239746

Final encoder loss: 0.01958085410296917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4687952995300293 0.07748055458068848

Final encoder loss: 0.019877944141626358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4645109176635742 0.07436060905456543

Final encoder loss: 0.019752128049731255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46831679344177246 0.07809281349182129

Final encoder loss: 0.019490966573357582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47633862495422363 0.07289314270019531

Final encoder loss: 0.019937308505177498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47141265869140625 0.07269859313964844

Final encoder loss: 0.01963701657950878
Final encoder loss: 0.018469173461198807
Final encoder loss: 0.01785023882985115

Training dapper model
Final encoder loss: 0.01938372409953947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06059837341308594 0.10790896415710449

Final encoder loss: 0.020655798587509915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05983400344848633 0.1068563461303711

Final encoder loss: 0.015783819198123165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0599055290222168 0.10781121253967285

Final encoder loss: 0.01601627410128756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.0615386962890625 0.10842585563659668

Final encoder loss: 0.015479544123776131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06101346015930176 0.10706114768981934

Final encoder loss: 0.01615059115514669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05972003936767578 0.10807657241821289

Final encoder loss: 0.01741486050083251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05989265441894531 0.10707640647888184

Final encoder loss: 0.01613863283795133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060410261154174805 0.10776066780090332

Final encoder loss: 0.014687265998306694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.061460018157958984 0.10829639434814453

Final encoder loss: 0.016005653638542887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.061865806579589844 0.10735797882080078

Final encoder loss: 0.015927612636134625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060109615325927734 0.10771942138671875

Final encoder loss: 0.015612976955960475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05998563766479492 0.10742545127868652

Final encoder loss: 0.014984703247325598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05989408493041992 0.10709810256958008

Final encoder loss: 0.01582787954847819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.060852766036987305 0.10878968238830566

Final encoder loss: 0.016742983764124843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06220507621765137 0.10751175880432129

Final encoder loss: 0.014341011443832666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05912137031555176 0.10739827156066895


Training dapper model
Final encoder loss: 0.202446848154068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11890912055969238 0.03435397148132324

Final encoder loss: 0.20821353793144226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159827709197998 0.034535884857177734

Final encoder loss: 0.08483850210905075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11892580986022949 0.034325599670410156

Final encoder loss: 0.08593873679637909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1160881519317627 0.03381037712097168

Final encoder loss: 0.0504196435213089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11691737174987793 0.03408169746398926

Final encoder loss: 0.04924238473176956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11602449417114258 0.03406214714050293

Final encoder loss: 0.0346980094909668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11588311195373535 0.034792184829711914

Final encoder loss: 0.03352704271674156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11846542358398438 0.034990787506103516

Final encoder loss: 0.026608914136886597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1170189380645752 0.03434133529663086

Final encoder loss: 0.025684243068099022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1161203384399414 0.03428053855895996

Final encoder loss: 0.02206568792462349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11619257926940918 0.03428149223327637

Final encoder loss: 0.021341472864151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11577582359313965 0.03492450714111328

Final encoder loss: 0.019473526626825333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11844182014465332 0.034932851791381836

Final encoder loss: 0.018812080845236778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11625552177429199 0.034394025802612305

Final encoder loss: 0.017824113368988037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11687207221984863 0.03429555892944336

Final encoder loss: 0.017459584400057793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11519122123718262 0.03407430648803711

Final encoder loss: 0.016840806230902672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11566519737243652 0.035219669342041016

Final encoder loss: 0.01664675958454609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11797714233398438 0.035649776458740234

Final encoder loss: 0.016315029934048653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1165926456451416 0.03432583808898926

Final encoder loss: 0.01614951901137829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11530947685241699 0.03446221351623535

Final encoder loss: 0.016170578077435493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11566305160522461 0.03399038314819336

Final encoder loss: 0.01590902917087078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11467432975769043 0.03435826301574707

Final encoder loss: 0.01621118001639843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11959099769592285 0.036293983459472656

Final encoder loss: 0.01588571071624756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11481523513793945 0.033777713775634766

Final encoder loss: 0.016300365328788757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11674213409423828 0.034793853759765625

Final encoder loss: 0.015779241919517517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11581659317016602 0.03380250930786133

Final encoder loss: 0.015944277867674828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11591434478759766 0.03551054000854492

Final encoder loss: 0.015668267384171486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11714434623718262 0.03543829917907715

Final encoder loss: 0.015785569325089455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1154484748840332 0.03477120399475098

Final encoder loss: 0.015237798914313316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11621785163879395 0.03412914276123047

Final encoder loss: 0.0149881262332201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11570334434509277 0.03452277183532715

Final encoder loss: 0.01476890780031681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157841682434082 0.035490989685058594

Final encoder loss: 0.014852258376777172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1184835433959961 0.035784244537353516

Final encoder loss: 0.0144925806671381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11487007141113281 0.03354954719543457

Final encoder loss: 0.014551147818565369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11651372909545898 0.03431344032287598

Final encoder loss: 0.01427831407636404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11581707000732422 0.03467726707458496

Final encoder loss: 0.01427675224840641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11643624305725098 0.03518271446228027

Final encoder loss: 0.014189072884619236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11825180053710938 0.035393476486206055

Final encoder loss: 0.014302399940788746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11571311950683594 0.034249067306518555

Final encoder loss: 0.014364529401063919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557579040527344 0.03397202491760254

Final encoder loss: 0.014444667845964432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11607551574707031 0.03450155258178711

Final encoder loss: 0.014228894375264645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11707878112792969 0.03435325622558594

Final encoder loss: 0.014444228261709213
Final encoder loss: 0.01294379960745573

Training case model
Final encoder loss: 0.02456422983118694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08966064453125 0.21930909156799316

Final encoder loss: 0.023189445180427842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08917450904846191 0.21908116340637207

Final encoder loss: 0.023224127922693405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.09160041809082031 0.21947121620178223

Final encoder loss: 0.023219112950081045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09011363983154297 0.2190093994140625

Final encoder loss: 0.02360106785823136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0891575813293457 0.219343900680542

Final encoder loss: 0.023370468948100034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09053206443786621 0.2190837860107422

Final encoder loss: 0.023056842888184267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08962440490722656 0.21901941299438477

Final encoder loss: 0.02304618286196991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.09025406837463379 0.21915817260742188

Final encoder loss: 0.023324901375573064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08899641036987305 0.21947455406188965

Final encoder loss: 0.023579433376836097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08918023109436035 0.2195730209350586

Final encoder loss: 0.022499532430281084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09049820899963379 0.21931838989257812

Final encoder loss: 0.023457008151907096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08968377113342285 0.21928000450134277

Final encoder loss: 0.023415742452416788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08878707885742188 0.2192378044128418

Final encoder loss: 0.022678323778586192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09039044380187988 0.21932578086853027

Final encoder loss: 0.02374007800209034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08881378173828125 0.21882414817810059

Final encoder loss: 0.023385214936581997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08579802513122559 0.21613621711730957


Training case model
Final encoder loss: 0.2029738575220108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26403379440307617 0.05177760124206543

Final encoder loss: 0.18890780210494995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26117420196533203 0.05367136001586914

Final encoder loss: 0.19014762341976166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25783205032348633 0.05212998390197754

Final encoder loss: 0.19218699634075165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25867581367492676 0.05380892753601074

Final encoder loss: 0.180816188454628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25772619247436523 0.05169796943664551

Final encoder loss: 0.19192151725292206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25694966316223145 0.05337381362915039

Final encoder loss: 0.10349476337432861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257920503616333 0.05185985565185547

Final encoder loss: 0.09326918423175812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2580265998840332 0.05347490310668945

Final encoder loss: 0.08950404077768326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594716548919678 0.05351567268371582

Final encoder loss: 0.08852042257785797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25694894790649414 0.05383729934692383

Final encoder loss: 0.07995966821908951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26113390922546387 0.051758527755737305

Final encoder loss: 0.0838746502995491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25499510765075684 0.053304195404052734

Final encoder loss: 0.06035621464252472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586665153503418 0.05178117752075195

Final encoder loss: 0.05477939173579216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26027512550354004 0.05199742317199707

Final encoder loss: 0.052748117595911026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25997447967529297 0.053737640380859375

Final encoder loss: 0.05319569632411003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576107978820801 0.052521705627441406

Final encoder loss: 0.04926322028040886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2624237537384033 0.05203843116760254

Final encoder loss: 0.05171571299433708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25510311126708984 0.052056074142456055

Final encoder loss: 0.04249433055520058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2618248462677002 0.05195450782775879

Final encoder loss: 0.039525680243968964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25767087936401367 0.05206918716430664

Final encoder loss: 0.03828670457005501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2598569393157959 0.051624298095703125

Final encoder loss: 0.03896096348762512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2562289237976074 0.05351567268371582

Final encoder loss: 0.03726857900619507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591683864593506 0.051774024963378906

Final encoder loss: 0.038412585854530334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2568390369415283 0.051429033279418945

Final encoder loss: 0.03465926647186279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576558589935303 0.0522003173828125

Final encoder loss: 0.033408116549253464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25688695907592773 0.05179429054260254

Final encoder loss: 0.03245268017053604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2557559013366699 0.051238059997558594

Final encoder loss: 0.03311675786972046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25637149810791016 0.051254987716674805

Final encoder loss: 0.03295348212122917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2564353942871094 0.051795005798339844

Final encoder loss: 0.032941702753305435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543818950653076 0.052514076232910156

Final encoder loss: 0.03144894540309906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568378448486328 0.05291008949279785

Final encoder loss: 0.03100648522377014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2576918601989746 0.0525507926940918

Final encoder loss: 0.03034767508506775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25690245628356934 0.05184173583984375

Final encoder loss: 0.0308176651597023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25838541984558105 0.05251598358154297

Final encoder loss: 0.031445425003767014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575714588165283 0.0521543025970459

Final encoder loss: 0.03119877725839615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553715705871582 0.051404714584350586

Final encoder loss: 0.02934897318482399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25756263732910156 0.052607059478759766

Final encoder loss: 0.02906390279531479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579166889190674 0.05215907096862793

Final encoder loss: 0.028510162606835365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570657730102539 0.051552772521972656

Final encoder loss: 0.02892654575407505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580435276031494 0.05188941955566406

Final encoder loss: 0.02950294315814972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256483793258667 0.0532686710357666

Final encoder loss: 0.02917388454079628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538185119628906 0.05365800857543945

Final encoder loss: 0.027781091630458832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586631774902344 0.05388212203979492

Final encoder loss: 0.02747001126408577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2591557502746582 0.052097320556640625

Final encoder loss: 0.027043042704463005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257659912109375 0.05211782455444336

Final encoder loss: 0.027320533990859985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574770450592041 0.05178356170654297

Final encoder loss: 0.028047015890479088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2580900192260742 0.05262112617492676

Final encoder loss: 0.027788223698735237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25435853004455566 0.051399946212768555

Final encoder loss: 0.026650993153452873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577641010284424 0.052817344665527344

Final encoder loss: 0.026623008772730827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582235336303711 0.053389549255371094

Final encoder loss: 0.02618854120373726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576596736907959 0.053030967712402344

Final encoder loss: 0.02641294337809086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573556900024414 0.0519864559173584

Final encoder loss: 0.027213212102651596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574148178100586 0.052457571029663086

Final encoder loss: 0.026584796607494354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25484132766723633 0.052213430404663086

Final encoder loss: 0.026064051315188408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2579333782196045 0.0520319938659668

Final encoder loss: 0.02610107511281967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25771594047546387 0.052967071533203125

Final encoder loss: 0.02568545565009117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2579612731933594 0.05455183982849121

Final encoder loss: 0.02596585638821125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257688045501709 0.052889108657836914

Final encoder loss: 0.02665698155760765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257171630859375 0.051050424575805664

Final encoder loss: 0.026218516752123833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25525689125061035 0.05069231986999512

Final encoder loss: 0.02555069699883461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25691938400268555 0.05262637138366699

Final encoder loss: 0.025505248457193375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25698113441467285 0.05243372917175293

Final encoder loss: 0.025252388790249825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25675535202026367 0.051692962646484375

Final encoder loss: 0.025250477716326714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25659966468811035 0.051522254943847656

Final encoder loss: 0.02623850293457508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25711989402770996 0.050907135009765625

Final encoder loss: 0.02547580748796463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539186477661133 0.051424264907836914

Final encoder loss: 0.02519533969461918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257080078125 0.053740739822387695

Final encoder loss: 0.025204867124557495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25644516944885254 0.0521543025970459

Final encoder loss: 0.024756966158747673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566678524017334 0.05225872993469238

Final encoder loss: 0.0249581728130579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577705383300781 0.054300546646118164

Final encoder loss: 0.0257396399974823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25689053535461426 0.05324292182922363

Final encoder loss: 0.02536693960428238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255051851272583 0.05145883560180664

Final encoder loss: 0.024738119915127754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578601837158203 0.05270814895629883

Final encoder loss: 0.024788502603769302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2597963809967041 0.05055546760559082

Final encoder loss: 0.024567345157265663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2571249008178711 0.053340911865234375

Final encoder loss: 0.02449856325984001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584497928619385 0.051865339279174805

Final encoder loss: 0.025340497493743896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25976037979125977 0.05176496505737305

Final encoder loss: 0.024749990552663803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255277156829834 0.05389237403869629

Final encoder loss: 0.024507101625204086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25842881202697754 0.051973819732666016

Final encoder loss: 0.02459188550710678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258821964263916 0.052469491958618164

Final encoder loss: 0.024276241660118103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2592198848724365 0.052252769470214844

Final encoder loss: 0.02428865246474743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25929951667785645 0.05423569679260254

Final encoder loss: 0.025093138217926025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583122253417969 0.05293869972229004

Final encoder loss: 0.02463301084935665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549302577972412 0.05185246467590332

Final encoder loss: 0.024271056056022644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25965285301208496 0.05424642562866211

Final encoder loss: 0.024403037503361702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25862669944763184 0.05219554901123047

Final encoder loss: 0.024038570001721382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25797390937805176 0.051451921463012695

Final encoder loss: 0.023979661986231804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2594616413116455 0.05313754081726074

Final encoder loss: 0.024941738694906235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25719523429870605 0.05477261543273926

Final encoder loss: 0.024320626631379128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545504570007324 0.0518345832824707

Final encoder loss: 0.02419993281364441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25887298583984375 0.05355024337768555

Final encoder loss: 0.024241652339696884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25965118408203125 0.05270886421203613

Final encoder loss: 0.023839741945266724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577381134033203 0.05215644836425781

Final encoder loss: 0.023857304826378822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25923585891723633 0.05343270301818848

Final encoder loss: 0.02463286556303501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2599015235900879 0.05250406265258789

Final encoder loss: 0.024264011532068253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25383949279785156 0.05420732498168945

Final encoder loss: 0.023870259523391724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25730252265930176 0.052675724029541016

Final encoder loss: 0.023898709565401077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25958991050720215 0.051833152770996094

Final encoder loss: 0.02376927062869072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25821542739868164 0.052443504333496094

Final encoder loss: 0.02368142642080784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25834012031555176 0.05517125129699707

Final encoder loss: 0.0244283564388752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25758910179138184 0.052525997161865234

Final encoder loss: 0.023933395743370056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25521087646484375 0.05228590965270996

Final encoder loss: 0.023785008117556572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25763726234436035 0.05301403999328613

Final encoder loss: 0.023944973945617676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582061290740967 0.05216526985168457

Final encoder loss: 0.023664046078920364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2591722011566162 0.051824331283569336

Final encoder loss: 0.023521652445197105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25930190086364746 0.05148911476135254

Final encoder loss: 0.024295786395668983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25908851623535156 0.05442357063293457

Final encoder loss: 0.02386455237865448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551262378692627 0.05288839340209961

Final encoder loss: 0.02365703508257866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586708068847656 0.05216050148010254

Final encoder loss: 0.02371334470808506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26000380516052246 0.053572654724121094

Final encoder loss: 0.023432916030287743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25829553604125977 0.0521855354309082

Final encoder loss: 0.023358440026640892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25922179222106934 0.052126169204711914

Final encoder loss: 0.024202700704336166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2608013153076172 0.05225729942321777

Final encoder loss: 0.023650217801332474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25452637672424316 0.053350210189819336

Final encoder loss: 0.023630816489458084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25791001319885254 0.05251049995422363

Final encoder loss: 0.023696869611740112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2758212089538574 0.052375078201293945

Final encoder loss: 0.02332405187189579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2699391841888428 0.051361799240112305

Final encoder loss: 0.02326124534010887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2682034969329834 0.05391860008239746

Final encoder loss: 0.02404872700572014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25730133056640625 0.05141115188598633

Final encoder loss: 0.02363455481827259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558135986328125 0.05280470848083496

Final encoder loss: 0.0233932938426733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25970005989074707 0.05214118957519531

Final encoder loss: 0.023440426215529442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577221393585205 0.05178332328796387

Final encoder loss: 0.02325652353465557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588047981262207 0.05135369300842285

Final encoder loss: 0.023100093007087708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25963544845581055 0.05179738998413086

Final encoder loss: 0.023895813152194023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25820088386535645 0.05367922782897949

Final encoder loss: 0.023406941443681717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543454170227051 0.05139756202697754

Final encoder loss: 0.023332200944423676
Final encoder loss: 0.02305537275969982
Final encoder loss: 0.022116074338555336
Final encoder loss: 0.021329309791326523
Final encoder loss: 0.02114887349307537
Final encoder loss: 0.02007276564836502

Training emognition model
Final encoder loss: 0.026194527835140837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08156657218933105 0.23064279556274414

Final encoder loss: 0.026548974261096026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0819406509399414 0.23127245903015137

Final encoder loss: 0.0282141154984538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08117532730102539 0.23103117942810059

Final encoder loss: 0.027033151922417835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08200693130493164 0.231154203414917

Final encoder loss: 0.027157970556027946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08126711845397949 0.23117423057556152

Final encoder loss: 0.025643017495002886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08285808563232422 0.23064160346984863

Final encoder loss: 0.026351423768753175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08202004432678223 0.23077106475830078

Final encoder loss: 0.02623956946543341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08274126052856445 0.23085570335388184

Final encoder loss: 0.025551946076471918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08097457885742188 0.23080968856811523

Final encoder loss: 0.026172000499653503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08095908164978027 0.23091721534729004

Final encoder loss: 0.026953082080647044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08114886283874512 0.2306227684020996

Final encoder loss: 0.02745459208360502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0811460018157959 0.23046302795410156

Final encoder loss: 0.025910915827398767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08111143112182617 0.23109936714172363

Final encoder loss: 0.0256050593462495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08181095123291016 0.23072576522827148

Final encoder loss: 0.026027763803927788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08244967460632324 0.23079252243041992

Final encoder loss: 0.026760682921902684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08062577247619629 0.23119902610778809


Training emognition model
Final encoder loss: 0.193553626537323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2510097026824951 0.0499114990234375

Final encoder loss: 0.1949547678232193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24918341636657715 0.048029184341430664

Final encoder loss: 0.08753407001495361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24794745445251465 0.05054950714111328

Final encoder loss: 0.0873391181230545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24713420867919922 0.04956483840942383

Final encoder loss: 0.055847980082035065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25014781951904297 0.04961514472961426

Final encoder loss: 0.05451586842536926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24619579315185547 0.05004692077636719

Final encoder loss: 0.041828643530607224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24728751182556152 0.048992156982421875

Final encoder loss: 0.0408669114112854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24746012687683105 0.048897504806518555

Final encoder loss: 0.03469157963991165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2471604347229004 0.049231767654418945

Final encoder loss: 0.034107837826013565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2467055320739746 0.0488278865814209

Final encoder loss: 0.03080093488097191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2476656436920166 0.049842119216918945

Final encoder loss: 0.030349960550665855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24757099151611328 0.05040264129638672

Final encoder loss: 0.028637660667300224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478199005126953 0.049027442932128906

Final encoder loss: 0.028281206265091896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472825050354004 0.04870104789733887

Final encoder loss: 0.027462827041745186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24943876266479492 0.049741268157958984

Final encoder loss: 0.027330836281180382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24711251258850098 0.04985642433166504

Final encoder loss: 0.026910360902547836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24878430366516113 0.050229787826538086

Final encoder loss: 0.027112504467368126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24882745742797852 0.04868960380554199

Final encoder loss: 0.026749232783913612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2470095157623291 0.050035953521728516

Final encoder loss: 0.027044735848903656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24840593338012695 0.04796338081359863

Final encoder loss: 0.026579780504107475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24889731407165527 0.0490572452545166

Final encoder loss: 0.026665668934583664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24712038040161133 0.050594329833984375

Final encoder loss: 0.026315588504076004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482314109802246 0.048213958740234375

Final encoder loss: 0.026345644146203995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24740862846374512 0.04911661148071289

Final encoder loss: 0.02617870457470417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24716496467590332 0.05037736892700195

Final encoder loss: 0.02594110369682312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24771451950073242 0.04829096794128418

Final encoder loss: 0.025900963693857193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24736642837524414 0.04995369911193848

Final encoder loss: 0.026014860719442368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24770736694335938 0.04994630813598633

Final encoder loss: 0.025798143818974495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24753403663635254 0.04964494705200195

Final encoder loss: 0.025898871943354607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24828386306762695 0.049738407135009766

Final encoder loss: 0.02563055418431759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24892163276672363 0.05079483985900879

Final encoder loss: 0.025993138551712036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24751496315002441 0.05005502700805664

Final encoder loss: 0.0254667941480875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2475872039794922 0.04912829399108887

Final encoder loss: 0.025589339435100555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24767589569091797 0.04889202117919922

Final encoder loss: 0.02522106282413006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24760127067565918 0.04953718185424805

Final encoder loss: 0.025460420176386833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24702119827270508 0.04834318161010742

Final encoder loss: 0.02510884217917919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24832677841186523 0.048654794692993164

Final encoder loss: 0.025234339758753777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464604377746582 0.051312923431396484

Final encoder loss: 0.025111526250839233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494659423828125 0.04929924011230469

Final encoder loss: 0.025399820879101753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24984431266784668 0.049628496170043945

Final encoder loss: 0.025003915652632713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485356330871582 0.050417423248291016

Final encoder loss: 0.025265155360102654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24736666679382324 0.04915881156921387

Final encoder loss: 0.025151507928967476
Final encoder loss: 0.0243366751819849

Training empatch model
Final encoder loss: 0.03898609050269827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07194113731384277 0.1741619110107422

Final encoder loss: 0.0377740264481044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07110476493835449 0.17406702041625977

Final encoder loss: 0.03616606819633838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07096672058105469 0.17396306991577148

Final encoder loss: 0.03540752348478205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07085943222045898 0.17383050918579102

Final encoder loss: 0.03457010053006881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07110834121704102 0.17415833473205566

Final encoder loss: 0.033839417442148544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07213330268859863 0.17423343658447266

Final encoder loss: 0.03079756524033969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07123398780822754 0.17387604713439941

Final encoder loss: 0.03265814194024042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07132267951965332 0.17409420013427734

Final encoder loss: 0.024142982539198242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07172989845275879 0.1742556095123291

Final encoder loss: 0.026229168899567404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07098531723022461 0.17463469505310059

Final encoder loss: 0.02547252221963753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07113504409790039 0.1737370491027832

Final encoder loss: 0.026244365345544675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07158446311950684 0.17414140701293945

Final encoder loss: 0.025275091670707024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07241463661193848 0.174485445022583

Final encoder loss: 0.025839181516239335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0713660717010498 0.17396044731140137

Final encoder loss: 0.025945336640434472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07182645797729492 0.17436790466308594

Final encoder loss: 0.02574549347365732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07115006446838379 0.17384624481201172


Training empatch model
Final encoder loss: 0.1711752712726593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17712712287902832 0.04475736618041992

Final encoder loss: 0.08093627542257309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17456340789794922 0.043004512786865234

Final encoder loss: 0.05554875358939171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17606258392333984 0.04471945762634277

Final encoder loss: 0.04320758208632469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17470717430114746 0.04414939880371094

Final encoder loss: 0.03615540638566017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.176727294921875 0.04409337043762207

Final encoder loss: 0.03172186017036438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17559099197387695 0.042830467224121094

Final encoder loss: 0.028782213106751442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17602896690368652 0.04423356056213379

Final encoder loss: 0.026858435943722725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17480182647705078 0.04318857192993164

Final encoder loss: 0.025574147701263428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17536568641662598 0.04497361183166504

Final encoder loss: 0.02469978854060173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17501211166381836 0.04373359680175781

Final encoder loss: 0.024050140753388405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17476367950439453 0.04405069351196289

Final encoder loss: 0.02362394519150257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17433834075927734 0.04389762878417969

Final encoder loss: 0.02333240583539009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17433834075927734 0.04453444480895996

Final encoder loss: 0.023190323263406754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17497682571411133 0.043312788009643555

Final encoder loss: 0.02290954254567623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17514467239379883 0.044536590576171875

Final encoder loss: 0.02273796871304512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17502570152282715 0.04371142387390137

Final encoder loss: 0.022561240941286087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17437148094177246 0.04422426223754883

Final encoder loss: 0.022487029433250427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17622900009155273 0.04396414756774902

Final encoder loss: 0.022405138239264488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17589235305786133 0.043927907943725586

Final encoder loss: 0.022327082231640816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17499399185180664 0.043257951736450195

Final encoder loss: 0.02231670916080475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17389464378356934 0.04276275634765625

Final encoder loss: 0.022193999961018562

Training wesad model
Final encoder loss: 0.039990362406443936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0713193416595459 0.17389678955078125

Final encoder loss: 0.03517328619215074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07119631767272949 0.17406654357910156

Final encoder loss: 0.03836942403915163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07101607322692871 0.1739025115966797

Final encoder loss: 0.035892036920721224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07091569900512695 0.1741957664489746

Final encoder loss: 0.023612643684643855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07236242294311523 0.17431092262268066

Final encoder loss: 0.02383416337064222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07139277458190918 0.17371225357055664

Final encoder loss: 0.025142734624039535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07143187522888184 0.17449522018432617

Final encoder loss: 0.02643034072158128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07147073745727539 0.17420530319213867

Final encoder loss: 0.019348367621921804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07105278968811035 0.17438173294067383

Final encoder loss: 0.01921419629048284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07119321823120117 0.17371869087219238

Final encoder loss: 0.01967079888948928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710763931274414 0.17403650283813477

Final encoder loss: 0.019904868480835166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07250499725341797 0.17466235160827637

Final encoder loss: 0.016005812622551904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07098221778869629 0.17410969734191895

Final encoder loss: 0.015816256196177207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07118749618530273 0.1740739345550537

Final encoder loss: 0.01595181246292647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07086396217346191 0.17412614822387695

Final encoder loss: 0.01616491980444568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07088112831115723 0.17411065101623535


Training wesad model
Final encoder loss: 0.21560963988304138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10678815841674805 0.03289318084716797

Final encoder loss: 0.09935668855905533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10380768775939941 0.03336834907531738

Final encoder loss: 0.06382613629102707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10404634475708008 0.0333104133605957

Final encoder loss: 0.04619308188557625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10529589653015137 0.03422260284423828

Final encoder loss: 0.03616278991103172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1041111946105957 0.03375554084777832

Final encoder loss: 0.03000462055206299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042025089263916 0.033556222915649414

Final encoder loss: 0.026059282943606377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037149429321289 0.03318309783935547

Final encoder loss: 0.023452313616871834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1055452823638916 0.033594369888305664

Final encoder loss: 0.02169599011540413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365533828735352 0.033453941345214844

Final encoder loss: 0.0205059926956892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10357165336608887 0.03342843055725098

Final encoder loss: 0.01976386271417141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1035013198852539 0.033822059631347656

Final encoder loss: 0.019313549622893333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10606789588928223 0.03479957580566406

Final encoder loss: 0.01912207342684269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356688499450684 0.03323078155517578

Final encoder loss: 0.019047768786549568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10374903678894043 0.033437252044677734

Final encoder loss: 0.019026394933462143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10517621040344238 0.034151315689086914

Final encoder loss: 0.01887013204395771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10531902313232422 0.03262042999267578

Final encoder loss: 0.018837416544556618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1046452522277832 0.0328214168548584

Final encoder loss: 0.018886681646108627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10367798805236816 0.0334322452545166

Final encoder loss: 0.01898462139070034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1040799617767334 0.034082889556884766

Final encoder loss: 0.018885016441345215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10423946380615234 0.03276515007019043

Final encoder loss: 0.018890593200922012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395240783691406 0.03344988822937012

Final encoder loss: 0.018783167004585266

Calculating loss for amigos model
	Full Pass 0.6868836879730225
numFreeParamsPath 18
Reconstruction loss values: 0.0279499813914299 0.03688126429915428

Calculating loss for dapper model
	Full Pass 0.15201258659362793
numFreeParamsPath 18
Reconstruction loss values: 0.023728128522634506 0.025877946987748146

Calculating loss for case model
	Full Pass 1.393073558807373
numFreeParamsPath 18
Reconstruction loss values: 0.03289633244276047 0.035856470465660095

Calculating loss for emognition model
	Full Pass 0.2912712097167969
numFreeParamsPath 18
Reconstruction loss values: 0.03471260890364647 0.04341244697570801

Calculating loss for empatch model
	Full Pass 0.10432243347167969
numFreeParamsPath 18
Reconstruction loss values: 0.0358610674738884 0.04301544651389122

Calculating loss for wesad model
	Full Pass 0.07666945457458496
numFreeParamsPath 18
Reconstruction loss values: 0.03694285824894905 0.05435764044523239
Total loss calculation time: 4.403433322906494

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.714384317398071
Total epoch time: 202.61372876167297

Epoch: 44

Training emognition model
Final encoder loss: 0.033711758707808945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08995199203491211 0.28456687927246094

Final encoder loss: 0.031825808744260976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08400249481201172 0.2756617069244385

Final encoder loss: 0.033576329610318735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08368539810180664 0.2751758098602295

Final encoder loss: 0.032235938326401974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08308744430541992 0.2755105495452881

Final encoder loss: 0.03356636638811855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08327674865722656 0.27641868591308594

Final encoder loss: 0.03298561717417545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08300638198852539 0.2756490707397461

Final encoder loss: 0.034970508545461444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.5330009460449219 0.2768270969390869

Final encoder loss: 0.033131617864059944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08319878578186035 0.27553892135620117

Final encoder loss: 0.03375500722952429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08300423622131348 0.2753920555114746

Final encoder loss: 0.033194277046892234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.083770751953125 0.2769291400909424

Final encoder loss: 0.03211851146759443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08328914642333984 0.2757411003112793

Final encoder loss: 0.03427828832779351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0832211971282959 0.27527809143066406

Final encoder loss: 0.03331325987658964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08408641815185547 0.27647852897644043

Final encoder loss: 0.03262152627586453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08375430107116699 0.2759871482849121

Final encoder loss: 0.03338656785675784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08388137817382812 0.2754058837890625

Final encoder loss: 0.03272525220508739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08308291435241699 0.2755286693572998


Training amigos model
Final encoder loss: 0.026289128647279683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10848641395568848 0.3885185718536377

Final encoder loss: 0.028415820377105765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10892629623413086 0.3890986442565918

Final encoder loss: 0.027436293641873267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10885477066040039 0.3893311023712158

Final encoder loss: 0.025052314494088627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.11826038360595703 0.38916802406311035

Final encoder loss: 0.02949592868455898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10890579223632812 0.3901042938232422

Final encoder loss: 0.02852837814009482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10931253433227539 0.38932275772094727

Final encoder loss: 0.02447803978682799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10991382598876953 0.3895537853240967

Final encoder loss: 0.029280600395869237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1090548038482666 0.3907020092010498

Final encoder loss: 0.02661955279139336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10921049118041992 0.39008331298828125

Final encoder loss: 0.027260447544656945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10839700698852539 0.39017295837402344

Final encoder loss: 0.02660860695702527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10838055610656738 0.3894228935241699

Final encoder loss: 0.026653094190115082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10850071907043457 0.39017772674560547

Final encoder loss: 0.027568607019953666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.1089487075805664 0.3897583484649658

Final encoder loss: 0.027808422184058688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1088261604309082 0.3893857002258301

Final encoder loss: 0.027136638654652597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10834288597106934 0.3901638984680176

Final encoder loss: 0.029262974010108266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10364818572998047 0.38391876220703125


Training dapper model
Final encoder loss: 0.024629044184912668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06381559371948242 0.1504991054534912

Final encoder loss: 0.027824739267462626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.0625298023223877 0.15123486518859863

Final encoder loss: 0.02489687474220718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06260395050048828 0.15252065658569336

Final encoder loss: 0.021126113073248843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06258487701416016 0.150681734085083

Final encoder loss: 0.02227576203285268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06249284744262695 0.15128827095031738

Final encoder loss: 0.02279080999025337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06342148780822754 0.15229249000549316

Final encoder loss: 0.020460797486315414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0625314712524414 0.1510326862335205

Final encoder loss: 0.024890270109613734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06235456466674805 0.15191030502319336

Final encoder loss: 0.021181876676983524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06222963333129883 0.15075993537902832

Final encoder loss: 0.020428863196756568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06267428398132324 0.15108919143676758

Final encoder loss: 0.02036157286651729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06340479850769043 0.15117764472961426

Final encoder loss: 0.02275006962723874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06298112869262695 0.15094876289367676

Final encoder loss: 0.01885544088841003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06249499320983887 0.1518087387084961

Final encoder loss: 0.019279662970872757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06249594688415527 0.15091228485107422

Final encoder loss: 0.019986795635944313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06224536895751953 0.15024447441101074

Final encoder loss: 0.020835183984919205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06345558166503906 0.1511993408203125


Training case model
Final encoder loss: 0.034535879395585466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09162116050720215 0.26492810249328613

Final encoder loss: 0.031204257972742525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0923316478729248 0.2650418281555176

Final encoder loss: 0.029858262589968727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09210968017578125 0.2656521797180176

Final encoder loss: 0.03018556324302059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09187889099121094 0.266268253326416

Final encoder loss: 0.02919536896464868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09279823303222656 0.2653629779815674

Final encoder loss: 0.0280919520491436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09139776229858398 0.2652111053466797

Final encoder loss: 0.028622609561177986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09149050712585449 0.26525139808654785

Final encoder loss: 0.027848547786553202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09135222434997559 0.26401829719543457

Final encoder loss: 0.026695571361910357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09106779098510742 0.2636682987213135

Final encoder loss: 0.027369147950620936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09124231338500977 0.2641029357910156

Final encoder loss: 0.02734845382740943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09088921546936035 0.263958215713501

Final encoder loss: 0.026562434066115432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09075450897216797 0.2644174098968506

Final encoder loss: 0.027388206025165444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09115743637084961 0.26432132720947266

Final encoder loss: 0.026377210909478695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09104704856872559 0.2639899253845215

Final encoder loss: 0.026653424231800175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09071540832519531 0.26391172409057617

Final encoder loss: 0.026001316652079814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08762836456298828 0.26026272773742676


Training amigos model
Final encoder loss: 0.02089042707883729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10796141624450684 0.3419313430786133

Final encoder loss: 0.019648255340906765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10652279853820801 0.3416709899902344

Final encoder loss: 0.02064109745358058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10681414604187012 0.34162259101867676

Final encoder loss: 0.020250381711247073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10729146003723145 0.3416893482208252

Final encoder loss: 0.019656156135608085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10653042793273926 0.3415992259979248

Final encoder loss: 0.019493027399862025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10629510879516602 0.3417847156524658

Final encoder loss: 0.020566406224074827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10624837875366211 0.341808557510376

Final encoder loss: 0.021108785088729706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10611367225646973 0.34160399436950684

Final encoder loss: 0.020141662820576604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10592913627624512 0.34170031547546387

Final encoder loss: 0.019616127074495057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10703730583190918 0.34177112579345703

Final encoder loss: 0.02065794744200045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1069490909576416 0.3417937755584717

Final encoder loss: 0.018667828795424127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10634565353393555 0.34160566329956055

Final encoder loss: 0.021604830421384266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10591530799865723 0.34174370765686035

Final encoder loss: 0.01951696805107307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10624074935913086 0.3414897918701172

Final encoder loss: 0.02046659965599452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10611796379089355 0.3417172431945801

Final encoder loss: 0.02074570685662017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1011042594909668 0.3383293151855469


Training amigos model
Final encoder loss: 0.18075516819953918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47534751892089844 0.07694578170776367

Final encoder loss: 0.18782168626785278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46568965911865234 0.07451605796813965

Final encoder loss: 0.18364210426807404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45737266540527344 0.07357573509216309

Final encoder loss: 0.07679605484008789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4602322578430176 0.07503676414489746

Final encoder loss: 0.07870713621377945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46071696281433105 0.07425618171691895

Final encoder loss: 0.07232484221458435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.456862211227417 0.07449507713317871

Final encoder loss: 0.044685132801532745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46478724479675293 0.0758206844329834

Final encoder loss: 0.04540088027715683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4679436683654785 0.07628846168518066

Final encoder loss: 0.04231531172990799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46658825874328613 0.07806062698364258

Final encoder loss: 0.03194795176386833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46863627433776855 0.07402181625366211

Final encoder loss: 0.03291599825024605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46898698806762695 0.07594752311706543

Final encoder loss: 0.03124934248626232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46463537216186523 0.07452106475830078

Final encoder loss: 0.026152875274419785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4703648090362549 0.07765960693359375

Final encoder loss: 0.027247052639722824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4687221050262451 0.07587528228759766

Final encoder loss: 0.026014892384409904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45825695991516113 0.07351851463317871

Final encoder loss: 0.023521224036812782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47309255599975586 0.0742805004119873

Final encoder loss: 0.02446252852678299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4634222984313965 0.07343530654907227

Final encoder loss: 0.023572659119963646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4596407413482666 0.07387375831604004

Final encoder loss: 0.022635284811258316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.476076602935791 0.07668137550354004

Final encoder loss: 0.023274147883057594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4646115303039551 0.07503199577331543

Final encoder loss: 0.022840555757284164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46159839630126953 0.07399797439575195

Final encoder loss: 0.02248973771929741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4645373821258545 0.0753943920135498

Final encoder loss: 0.02268357202410698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46210432052612305 0.07448410987854004

Final encoder loss: 0.022716393694281578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45703983306884766 0.07518744468688965

Final encoder loss: 0.021979989483952522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46286869049072266 0.07459092140197754

Final encoder loss: 0.02218765579164028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46120572090148926 0.0767819881439209

Final encoder loss: 0.022105004638433456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4577054977416992 0.07265329360961914

Final encoder loss: 0.021257586777210236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46301960945129395 0.07320761680603027

Final encoder loss: 0.021579759195446968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4671058654785156 0.07630038261413574

Final encoder loss: 0.021370787173509598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45627641677856445 0.0744020938873291

Final encoder loss: 0.02071334980428219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45960378646850586 0.07606959342956543

Final encoder loss: 0.021113673225045204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46080493927001953 0.07683181762695312

Final encoder loss: 0.020768828690052032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4560086727142334 0.07334041595458984

Final encoder loss: 0.020400090143084526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4604222774505615 0.07522201538085938

Final encoder loss: 0.0206694845110178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4602372646331787 0.07517504692077637

Final encoder loss: 0.020659087225794792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4560995101928711 0.07422971725463867

Final encoder loss: 0.020372722297906876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4608297348022461 0.07451462745666504

Final encoder loss: 0.020387915894389153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46090078353881836 0.07459044456481934

Final encoder loss: 0.0205704215914011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45845651626586914 0.07294344902038574

Final encoder loss: 0.020094575360417366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4620800018310547 0.07459235191345215

Final encoder loss: 0.020059101283550262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46140289306640625 0.07445526123046875

Final encoder loss: 0.02025805227458477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4559445381164551 0.07423520088195801

Final encoder loss: 0.01960942894220352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4603238105773926 0.07422208786010742

Final encoder loss: 0.019863370805978775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4613771438598633 0.07452535629272461

Final encoder loss: 0.01996404118835926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4567861557006836 0.07487821578979492

Final encoder loss: 0.019472947344183922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.460237979888916 0.07449221611022949

Final encoder loss: 0.019892901182174683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4618356227874756 0.07419085502624512

Final encoder loss: 0.01969687081873417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4567997455596924 0.07413554191589355

Final encoder loss: 0.01940135285258293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4602503776550293 0.07542991638183594

Final encoder loss: 0.01967119611799717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4620065689086914 0.07691693305969238

Final encoder loss: 0.019710185006260872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4562342166900635 0.07409977912902832

Final encoder loss: 0.01939345709979534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4599325656890869 0.07633781433105469

Final encoder loss: 0.019367137923836708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46100902557373047 0.07465100288391113

Final encoder loss: 0.01954496093094349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.455341100692749 0.07472729682922363

Final encoder loss: 0.019255606457591057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46008968353271484 0.07424187660217285

Final encoder loss: 0.019154725596308708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46193456649780273 0.07568550109863281

Final encoder loss: 0.019570333883166313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4565765857696533 0.07099604606628418

Final encoder loss: 0.019233280792832375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46049070358276367 0.07425141334533691

Final encoder loss: 0.01911541447043419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45982789993286133 0.07470202445983887

Final encoder loss: 0.019542066380381584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45947790145874023 0.07487869262695312

Final encoder loss: 0.018893148750066757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4597964286804199 0.07218217849731445

Final encoder loss: 0.019162923097610474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46088504791259766 0.07644224166870117

Final encoder loss: 0.019462041556835175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4575483798980713 0.07457327842712402

Final encoder loss: 0.018844909965991974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4618110656738281 0.07509684562683105

Final encoder loss: 0.019042542204260826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4611198902130127 0.07608628273010254

Final encoder loss: 0.019048329442739487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4571101665496826 0.07376718521118164

Final encoder loss: 0.01900308020412922
Final encoder loss: 0.01806892827153206
Final encoder loss: 0.017477016896009445

Training dapper model
Final encoder loss: 0.018512962033803676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06021261215209961 0.10735940933227539

Final encoder loss: 0.017608445182440705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.060080766677856445 0.10803341865539551

Final encoder loss: 0.017549223975808662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06124234199523926 0.10824799537658691

Final encoder loss: 0.017425362628567018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06033158302307129 0.10826921463012695

Final encoder loss: 0.0150699228958644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05997896194458008 0.10732579231262207

Final encoder loss: 0.016612543179269173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05992436408996582 0.10738587379455566

Final encoder loss: 0.015217137474014646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05996108055114746 0.10837221145629883

Final encoder loss: 0.015915432985364263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.07534289360046387 0.10810017585754395

Final encoder loss: 0.01601204024093475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05960726737976074 0.10721945762634277

Final encoder loss: 0.014995809635478429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0609431266784668 0.10738849639892578

Final encoder loss: 0.01689774239930171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05999875068664551 0.10721302032470703

Final encoder loss: 0.015089134677592162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.06034088134765625 0.10827136039733887

Final encoder loss: 0.014423610121615369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.061038970947265625 0.10792088508605957

Final encoder loss: 0.01464097270592559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059477806091308594 0.10674262046813965

Final encoder loss: 0.015019543581015732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06029558181762695 0.10838055610656738

Final encoder loss: 0.015446841409671027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.0601963996887207 0.10716080665588379


Training dapper model
Final encoder loss: 0.20244702696800232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11936616897583008 0.03528904914855957

Final encoder loss: 0.20819835364818573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11898159980773926 0.03478264808654785

Final encoder loss: 0.08400650322437286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11633086204528809 0.03443098068237305

Final encoder loss: 0.0859014093875885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11600828170776367 0.034150123596191406

Final encoder loss: 0.04996267333626747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11631274223327637 0.034403324127197266

Final encoder loss: 0.04956788569688797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11793994903564453 0.03556251525878906

Final encoder loss: 0.03441512584686279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11963176727294922 0.03428316116333008

Final encoder loss: 0.03387446701526642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11558222770690918 0.033960819244384766

Final encoder loss: 0.026278354227542877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11731290817260742 0.03438925743103027

Final encoder loss: 0.02592688426375389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11719918251037598 0.03406572341918945

Final encoder loss: 0.02174527384340763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11764979362487793 0.03469562530517578

Final encoder loss: 0.021464938297867775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11963963508605957 0.03415203094482422

Final encoder loss: 0.019151659682393074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11560821533203125 0.03486037254333496

Final encoder loss: 0.018823271617293358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11620068550109863 0.03390145301818848

Final encoder loss: 0.01749442145228386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11716127395629883 0.03531146049499512

Final encoder loss: 0.01730038970708847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11947774887084961 0.03525853157043457

Final encoder loss: 0.016655834391713142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11890935897827148 0.03359270095825195

Final encoder loss: 0.016488749533891678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11648750305175781 0.03427314758300781

Final encoder loss: 0.01632518321275711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1162881851196289 0.034120798110961914

Final encoder loss: 0.01612507365643978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11655116081237793 0.034288883209228516

Final encoder loss: 0.016283394768834114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11863112449645996 0.035398244857788086

Final encoder loss: 0.015719367191195488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11905813217163086 0.03443789482116699

Final encoder loss: 0.01608363166451454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11632490158081055 0.03430676460266113

Final encoder loss: 0.015602701343595982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1165165901184082 0.03386092185974121

Final encoder loss: 0.016246788203716278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11596179008483887 0.034452199935913086

Final encoder loss: 0.01581140235066414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11921358108520508 0.0350191593170166

Final encoder loss: 0.015829168260097504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1181490421295166 0.03401494026184082

Final encoder loss: 0.016072357073426247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11794900894165039 0.03431415557861328

Final encoder loss: 0.015385838225483894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11665630340576172 0.03368186950683594

Final encoder loss: 0.015689559280872345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1165153980255127 0.033774614334106445

Final encoder loss: 0.015120400115847588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11900091171264648 0.034705162048339844

Final encoder loss: 0.015052596107125282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11703753471374512 0.03357815742492676

Final encoder loss: 0.014580458402633667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11640548706054688 0.0346827507019043

Final encoder loss: 0.014351451769471169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11635351181030273 0.034888267517089844

Final encoder loss: 0.014483347535133362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11709094047546387 0.035143136978149414

Final encoder loss: 0.014146747067570686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11939883232116699 0.03513956069946289

Final encoder loss: 0.014502087607979774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11739969253540039 0.03440451622009277

Final encoder loss: 0.014035684987902641
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1163320541381836 0.03477215766906738

Final encoder loss: 0.01443465519696474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11632633209228516 0.0344393253326416

Final encoder loss: 0.014193741604685783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11580419540405273 0.034871578216552734

Final encoder loss: 0.0142845269292593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11948919296264648 0.035599708557128906

Final encoder loss: 0.014188465662300587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1166219711303711 0.03503704071044922

Final encoder loss: 0.01411469653248787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11668896675109863 0.03493165969848633

Final encoder loss: 0.014201479032635689
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11663269996643066 0.034852027893066406

Final encoder loss: 0.01385421585291624
Final encoder loss: 0.012847570702433586

Training case model
Final encoder loss: 0.02404874274377942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.09019732475280762 0.2191462516784668

Final encoder loss: 0.02281876412526624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.0899040699005127 0.21894550323486328

Final encoder loss: 0.022773915582226696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08975958824157715 0.2194981575012207

Final encoder loss: 0.022652823347477483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08954977989196777 0.21873259544372559

Final encoder loss: 0.022924115932165517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08967971801757812 0.2192671298980713

Final encoder loss: 0.022373849655569305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09158635139465332 0.21917009353637695

Final encoder loss: 0.023094294996555437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09000897407531738 0.21901345252990723

Final encoder loss: 0.022775622544135597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08921480178833008 0.2192847728729248

Final encoder loss: 0.022456586466347354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09067320823669434 0.21946263313293457

Final encoder loss: 0.022649418716122728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08960223197937012 0.21916842460632324

Final encoder loss: 0.02303632208406695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08938026428222656 0.21931958198547363

Final encoder loss: 0.022872994904846275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09149289131164551 0.21933794021606445

Final encoder loss: 0.022757449736996137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08945894241333008 0.21922779083251953

Final encoder loss: 0.022866060805628433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08934426307678223 0.21967411041259766

Final encoder loss: 0.02235091390043198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.0899195671081543 0.21894145011901855

Final encoder loss: 0.02286203631834228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08649039268493652 0.2160959243774414


Training case model
Final encoder loss: 0.2029719352722168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26453065872192383 0.05259823799133301

Final encoder loss: 0.18891243636608124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2777702808380127 0.05229496955871582

Final encoder loss: 0.19015686213970184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2716243267059326 0.05102229118347168

Final encoder loss: 0.19218792021274567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2702620029449463 0.053461313247680664

Final encoder loss: 0.18080483376979828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2721593379974365 0.05294322967529297

Final encoder loss: 0.1919303983449936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556757926940918 0.05222010612487793

Final encoder loss: 0.1040182039141655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26304030418395996 0.05181717872619629

Final encoder loss: 0.09364232420921326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2614569664001465 0.05357623100280762

Final encoder loss: 0.08975953608751297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26345181465148926 0.052347421646118164

Final encoder loss: 0.08875080198049545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26798534393310547 0.05281329154968262

Final encoder loss: 0.08040400594472885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27299928665161133 0.05185365676879883

Final encoder loss: 0.08416358381509781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25826239585876465 0.05219674110412598

Final encoder loss: 0.06064675375819206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26031064987182617 0.05171823501586914

Final encoder loss: 0.05484674498438835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2683241367340088 0.053437232971191406

Final encoder loss: 0.05271674320101738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27210068702697754 0.05283331871032715

Final encoder loss: 0.05317048355937004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.268115758895874 0.05283546447753906

Final encoder loss: 0.04942028596997261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2718465328216553 0.051918983459472656

Final encoder loss: 0.05171483755111694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563660144805908 0.0518498420715332

Final encoder loss: 0.042528487741947174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26049160957336426 0.05420374870300293

Final encoder loss: 0.039282456040382385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27582550048828125 0.05244779586791992

Final encoder loss: 0.03792941942811012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2606511116027832 0.05577373504638672

Final encoder loss: 0.03873654454946518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580687999725342 0.05329465866088867

Final encoder loss: 0.03727593272924423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27138543128967285 0.05527615547180176

Final encoder loss: 0.0381445474922657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563931941986084 0.05211520195007324

Final encoder loss: 0.03459639474749565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2600085735321045 0.05341458320617676

Final encoder loss: 0.03287159651517868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25852084159851074 0.05220794677734375

Final encoder loss: 0.031780462712049484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27048635482788086 0.05437302589416504

Final encoder loss: 0.032797671854496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27855730056762695 0.051581621170043945

Final encoder loss: 0.03276621177792549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2604351043701172 0.05364274978637695

Final encoder loss: 0.03262123465538025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25462961196899414 0.05212259292602539

Final encoder loss: 0.031394459307193756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2721271514892578 0.054305076599121094

Final encoder loss: 0.030321219936013222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26798081398010254 0.05183076858520508

Final encoder loss: 0.030063608661293983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.270233154296875 0.05346226692199707

Final encoder loss: 0.0304803978651762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2697122097015381 0.05296921730041504

Final encoder loss: 0.03100551851093769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.277782678604126 0.05296659469604492

Final encoder loss: 0.03065391257405281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571694850921631 0.05169677734375

Final encoder loss: 0.029214181005954742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2594890594482422 0.054216623306274414

Final encoder loss: 0.0285226721316576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25891637802124023 0.0531468391418457

Final encoder loss: 0.028158467262983322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2568023204803467 0.05383181571960449

Final encoder loss: 0.02833663485944271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2806582450866699 0.053591012954711914

Final encoder loss: 0.02915606088936329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2592940330505371 0.053594112396240234

Final encoder loss: 0.028733594343066216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25631237030029297 0.05108833312988281

Final encoder loss: 0.02758612670004368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25679683685302734 0.0557098388671875

Final encoder loss: 0.0269169919192791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25945568084716797 0.053766489028930664

Final encoder loss: 0.02643466740846634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2678065299987793 0.0525667667388916

Final encoder loss: 0.026921790093183517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2586221694946289 0.052889108657836914

Final encoder loss: 0.027628453448414803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26955509185791016 0.05361747741699219

Final encoder loss: 0.027122756466269493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2573127746582031 0.05140495300292969

Final encoder loss: 0.02656571753323078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2686336040496826 0.05312037467956543

Final encoder loss: 0.02612619660794735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25920581817626953 0.053505897521972656

Final encoder loss: 0.025653494521975517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2602970600128174 0.0533299446105957

Final encoder loss: 0.025921761989593506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.270357608795166 0.05234503746032715

Final encoder loss: 0.02690490335226059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27094078063964844 0.05257129669189453

Final encoder loss: 0.02616436593234539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2568542957305908 0.05305147171020508

Final encoder loss: 0.025768784806132317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.267230749130249 0.053528547286987305

Final encoder loss: 0.02549220435321331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2617645263671875 0.052936553955078125

Final encoder loss: 0.025122208520770073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26958799362182617 0.052576303482055664

Final encoder loss: 0.025554632768034935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28165125846862793 0.051903724670410156

Final encoder loss: 0.02635038271546364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25877928733825684 0.05308651924133301

Final encoder loss: 0.025761572644114494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.259352445602417 0.051650285720825195

Final encoder loss: 0.025287006050348282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588493824005127 0.05214500427246094

Final encoder loss: 0.024991832673549652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27066779136657715 0.05237889289855957

Final encoder loss: 0.024711929261684418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2880420684814453 0.05248522758483887

Final encoder loss: 0.024754751473665237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2801661491394043 0.0523684024810791

Final encoder loss: 0.025738956406712532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2597639560699463 0.05280447006225586

Final encoder loss: 0.025047391653060913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564845085144043 0.05187845230102539

Final encoder loss: 0.024752039462327957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586071491241455 0.051889657974243164

Final encoder loss: 0.024658629670739174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26267194747924805 0.05185866355895996

Final encoder loss: 0.0243182722479105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2686455249786377 0.05141019821166992

Final encoder loss: 0.024454614147543907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2702317237854004 0.055174827575683594

Final encoder loss: 0.025323331356048584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26993775367736816 0.05312538146972656

Final encoder loss: 0.02463490329682827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25806689262390137 0.05597519874572754

Final encoder loss: 0.024544132873415947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2683534622192383 0.05403327941894531

Final encoder loss: 0.024272892624139786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2718997001647949 0.055036306381225586

Final encoder loss: 0.023958655074238777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2581000328063965 0.05271291732788086

Final encoder loss: 0.02407282218337059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26064419746398926 0.05322003364562988

Final encoder loss: 0.025168968364596367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26930904388427734 0.05245327949523926

Final encoder loss: 0.024323485791683197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25819969177246094 0.05280661582946777

Final encoder loss: 0.024191899225115776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2677879333496094 0.05223512649536133

Final encoder loss: 0.024055607616901398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2600564956665039 0.05486249923706055

Final encoder loss: 0.023794161155819893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2684667110443115 0.052452802658081055

Final encoder loss: 0.02391306683421135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.270143985748291 0.054225921630859375

Final encoder loss: 0.02483389340341091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27516674995422363 0.052840232849121094

Final encoder loss: 0.02413572184741497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25619053840637207 0.053247928619384766

Final encoder loss: 0.024105042219161987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25832390785217285 0.052764892578125

Final encoder loss: 0.023800961673259735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2696990966796875 0.05453610420227051

Final encoder loss: 0.0235793087631464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576899528503418 0.052178144454956055

Final encoder loss: 0.023458490148186684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2795093059539795 0.05277419090270996

Final encoder loss: 0.024468010291457176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2797551155090332 0.0544435977935791

Final encoder loss: 0.023817947134375572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539055347442627 0.05195474624633789

Final encoder loss: 0.023819945752620697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570621967315674 0.05193829536437988

Final encoder loss: 0.02371280826628208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2556648254394531 0.05274820327758789

Final encoder loss: 0.02338986098766327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2735106945037842 0.05129122734069824

Final encoder loss: 0.023429248481988907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25484347343444824 0.052701711654663086

Final encoder loss: 0.024193083867430687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27405428886413574 0.05187392234802246

Final encoder loss: 0.02368568442761898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.253814697265625 0.050673723220825195

Final encoder loss: 0.023667292669415474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25693488121032715 0.05248117446899414

Final encoder loss: 0.023455709218978882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2683243751525879 0.05215573310852051

Final encoder loss: 0.02321869507431984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26813840866088867 0.052550554275512695

Final encoder loss: 0.0232197605073452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26839470863342285 0.05205798149108887

Final encoder loss: 0.02411741018295288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26804566383361816 0.05169081687927246

Final encoder loss: 0.02349279448390007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255657434463501 0.051549673080444336

Final encoder loss: 0.02352975867688656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25828981399536133 0.05289793014526367

Final encoder loss: 0.02334449626505375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2753932476043701 0.0523676872253418

Final encoder loss: 0.023071376606822014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26870131492614746 0.053193092346191406

Final encoder loss: 0.023137159645557404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567713260650635 0.05281519889831543

Final encoder loss: 0.02390393614768982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25767040252685547 0.05304312705993652

Final encoder loss: 0.023334966972470284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25467562675476074 0.05098915100097656

Final encoder loss: 0.023482464253902435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581596374511719 0.05174064636230469

Final encoder loss: 0.023166129365563393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.276472806930542 0.052246809005737305

Final encoder loss: 0.0228895153850317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2750077247619629 0.052776336669921875

Final encoder loss: 0.022872675210237503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2685811519622803 0.05209088325500488

Final encoder loss: 0.023794997483491898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2680389881134033 0.052503347396850586

Final encoder loss: 0.02314632758498192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25500035285949707 0.05283927917480469

Final encoder loss: 0.023227335885167122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25818657875061035 0.05250811576843262

Final encoder loss: 0.023171186447143555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26714038848876953 0.054369211196899414

Final encoder loss: 0.02281252294778824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2592306137084961 0.05319809913635254

Final encoder loss: 0.022836603224277496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25717878341674805 0.05264997482299805

Final encoder loss: 0.02364378049969673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2689805030822754 0.05294036865234375

Final encoder loss: 0.023081019520759583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556169033050537 0.053819894790649414

Final encoder loss: 0.023197945207357407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25859832763671875 0.05226445198059082

Final encoder loss: 0.022965164855122566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2671632766723633 0.0525050163269043

Final encoder loss: 0.022745294496417046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25838708877563477 0.05260753631591797

Final encoder loss: 0.022682761773467064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2765312194824219 0.05180644989013672

Final encoder loss: 0.02362554334104061
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585182189941406 0.05321788787841797

Final encoder loss: 0.022902874276041985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562065124511719 0.05107545852661133

Final encoder loss: 0.02308041788637638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25733137130737305 0.052278995513916016

Final encoder loss: 0.022935617715120316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26744818687438965 0.05206918716430664

Final encoder loss: 0.022644253447651863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2670283317565918 0.05271744728088379

Final encoder loss: 0.022617604583501816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27269530296325684 0.05151176452636719

Final encoder loss: 0.023356124758720398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256411075592041 0.05226755142211914

Final encoder loss: 0.022953884676098824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538161277770996 0.05115556716918945

Final encoder loss: 0.023043500259518623
Final encoder loss: 0.022280218079686165
Final encoder loss: 0.02147282473742962
Final encoder loss: 0.020720260217785835
Final encoder loss: 0.020734306424856186
Final encoder loss: 0.01942800171673298

Training emognition model
Final encoder loss: 0.02638469651963847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08252692222595215 0.23144030570983887

Final encoder loss: 0.026205615915610603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08132123947143555 0.23086786270141602

Final encoder loss: 0.02692479765226335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08273768424987793 0.23037481307983398

Final encoder loss: 0.027031886624295016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0813758373260498 0.23072576522827148

Final encoder loss: 0.026455616433941287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0821542739868164 0.23061680793762207

Final encoder loss: 0.025462731710180375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08114480972290039 0.23083066940307617

Final encoder loss: 0.025163638840276573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08123278617858887 0.23156523704528809

Final encoder loss: 0.026339821520903087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08192014694213867 0.2309260368347168

Final encoder loss: 0.026912427229557185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.0818033218383789 0.23070979118347168

Final encoder loss: 0.026242776139241985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08202838897705078 0.23090100288391113

Final encoder loss: 0.026329257638889735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08131933212280273 0.23126983642578125

Final encoder loss: 0.025977210664532364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08212041854858398 0.231048583984375

Final encoder loss: 0.027154910829566756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08139824867248535 0.23063969612121582

Final encoder loss: 0.025754269556550257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08240079879760742 0.23100972175598145

Final encoder loss: 0.02737917901040801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08096122741699219 0.23086905479431152

Final encoder loss: 0.027560368127737656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0819234848022461 0.2313241958618164


Training emognition model
Final encoder loss: 0.19356432557106018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2524237632751465 0.049664974212646484

Final encoder loss: 0.19496563076972961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24803996086120605 0.0492095947265625

Final encoder loss: 0.08847198635339737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24825382232666016 0.04833102226257324

Final encoder loss: 0.08823679387569427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24946022033691406 0.04895520210266113

Final encoder loss: 0.05627875402569771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24824881553649902 0.04940533638000488

Final encoder loss: 0.054985735565423965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.248732328414917 0.04896831512451172

Final encoder loss: 0.041998717933893204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2501950263977051 0.049909114837646484

Final encoder loss: 0.04103145748376846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2474229335784912 0.05033087730407715

Final encoder loss: 0.03476707264780998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24916815757751465 0.049669742584228516

Final encoder loss: 0.03413337469100952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24732398986816406 0.04880118370056152

Final encoder loss: 0.03078734315931797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24919390678405762 0.05045008659362793

Final encoder loss: 0.03030511923134327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24687600135803223 0.048230648040771484

Final encoder loss: 0.02860664762556553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483692169189453 0.04811263084411621

Final encoder loss: 0.028068451210856438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2473764419555664 0.05081462860107422

Final encoder loss: 0.02746427431702614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24728918075561523 0.0502316951751709

Final encoder loss: 0.02700674906373024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24892854690551758 0.04896831512451172

Final encoder loss: 0.02682408317923546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24950742721557617 0.051256656646728516

Final encoder loss: 0.02670644223690033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475903034210205 0.04876399040222168

Final encoder loss: 0.026438554748892784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502458095550537 0.048980712890625

Final encoder loss: 0.026583990082144737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2489454746246338 0.04850339889526367

Final encoder loss: 0.026158494874835014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24844646453857422 0.04943108558654785

Final encoder loss: 0.02653779648244381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24799251556396484 0.0489351749420166

Final encoder loss: 0.026009241119027138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24881529808044434 0.04929327964782715

Final encoder loss: 0.02622319757938385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475299835205078 0.051822662353515625

Final encoder loss: 0.025760207325220108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24947547912597656 0.04812741279602051

Final encoder loss: 0.025831541046500206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2498621940612793 0.04963493347167969

Final encoder loss: 0.02567731775343418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474832534790039 0.050194501876831055

Final encoder loss: 0.025638166815042496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2473001480102539 0.04910469055175781

Final encoder loss: 0.025540802627801895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24872589111328125 0.048670053482055664

Final encoder loss: 0.02537548914551735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24643874168395996 0.051628828048706055

Final encoder loss: 0.025492442771792412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484285831451416 0.048627614974975586

Final encoder loss: 0.02534439042210579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24719858169555664 0.049321651458740234

Final encoder loss: 0.02517799474298954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24968266487121582 0.0507814884185791

Final encoder loss: 0.025309987366199493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2480320930480957 0.049848318099975586

Final encoder loss: 0.02521088719367981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24885082244873047 0.049399614334106445

Final encoder loss: 0.02528088353574276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24953007698059082 0.04970884323120117

Final encoder loss: 0.024828936904668808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472989559173584 0.04876589775085449

Final encoder loss: 0.025042787194252014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24799251556396484 0.04871058464050293

Final encoder loss: 0.02493390627205372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24906420707702637 0.04886484146118164

Final encoder loss: 0.024957632645964622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2492508888244629 0.04956841468811035

Final encoder loss: 0.024756506085395813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2497272491455078 0.04823613166809082

Final encoder loss: 0.024864615872502327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24936223030090332 0.04975318908691406

Final encoder loss: 0.024700796231627464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24837255477905273 0.04967546463012695

Final encoder loss: 0.024899378418922424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741387367248535 0.048424720764160156

Final encoder loss: 0.024599414318799973
Final encoder loss: 0.023921575397253036

Training empatch model
Final encoder loss: 0.03497091723883459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07169675827026367 0.17411327362060547

Final encoder loss: 0.035838880664497245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07160115242004395 0.17380928993225098

Final encoder loss: 0.031764171650051035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0714118480682373 0.17443513870239258

Final encoder loss: 0.03519041250038567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07204747200012207 0.17349553108215332

Final encoder loss: 0.031467564107318664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0714876651763916 0.17429900169372559

Final encoder loss: 0.03407556520458001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07268118858337402 0.17490768432617188

Final encoder loss: 0.033007772973144445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07147049903869629 0.1740431785583496

Final encoder loss: 0.032354315463858724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07123970985412598 0.17368412017822266

Final encoder loss: 0.025807409930224935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07102370262145996 0.17432403564453125

Final encoder loss: 0.025115645402263843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07151389122009277 0.1740415096282959

Final encoder loss: 0.02372025084583083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07255291938781738 0.17395973205566406

Final encoder loss: 0.024946387089422668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07150435447692871 0.17409133911132812

Final encoder loss: 0.025863722221930206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0729219913482666 0.17418313026428223

Final encoder loss: 0.02548840355413922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07222771644592285 0.17408013343811035

Final encoder loss: 0.023944311883301265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07152867317199707 0.17441892623901367

Final encoder loss: 0.023514014671415992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07096219062805176 0.173720121383667


Training empatch model
Final encoder loss: 0.1711762398481369
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17747950553894043 0.04419875144958496

Final encoder loss: 0.08136768639087677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17553496360778809 0.04357552528381348

Final encoder loss: 0.05571749806404114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17618608474731445 0.04409456253051758

Final encoder loss: 0.04318605735898018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750166416168213 0.042961835861206055

Final encoder loss: 0.036065101623535156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17570996284484863 0.04435420036315918

Final encoder loss: 0.03160456568002701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17574429512023926 0.0439300537109375

Final encoder loss: 0.028702378273010254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17453956604003906 0.04381418228149414

Final encoder loss: 0.02673298306763172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17485690116882324 0.04356551170349121

Final encoder loss: 0.02532723918557167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17475461959838867 0.044370174407958984

Final encoder loss: 0.024376170709729195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17639565467834473 0.04332995414733887

Final encoder loss: 0.02376682497560978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17489027976989746 0.04414844512939453

Final encoder loss: 0.023425757884979248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17686891555786133 0.04311370849609375

Final encoder loss: 0.023168416693806648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17451739311218262 0.04368019104003906

Final encoder loss: 0.02289448492228985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17654037475585938 0.0437617301940918

Final encoder loss: 0.02267305925488472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1755380630493164 0.0437166690826416

Final encoder loss: 0.022481556981801987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17638325691223145 0.04326891899108887

Final encoder loss: 0.022387603297829628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504572868347168 0.04332685470581055

Final encoder loss: 0.02222295291721821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17789649963378906 0.0435943603515625

Final encoder loss: 0.022120926529169083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744542121887207 0.04306316375732422

Final encoder loss: 0.022046944126486778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17703580856323242 0.043782711029052734

Final encoder loss: 0.021959958598017693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754753589630127 0.04312729835510254

Final encoder loss: 0.021897824481129646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17798757553100586 0.043836355209350586

Final encoder loss: 0.02186286821961403

Training wesad model
Final encoder loss: 0.03474896987339982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07127189636230469 0.17437100410461426

Final encoder loss: 0.03777080422117344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0712134838104248 0.1740877628326416

Final encoder loss: 0.0362651688100747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07110953330993652 0.17397236824035645

Final encoder loss: 0.03351386598877885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07226848602294922 0.17428112030029297

Final encoder loss: 0.023536319821397374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07146811485290527 0.17345714569091797

Final encoder loss: 0.02370792288635429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07153892517089844 0.1743464469909668

Final encoder loss: 0.02352519465597918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07121157646179199 0.17416834831237793

Final encoder loss: 0.025470356325469932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07100415229797363 0.1741809844970703

Final encoder loss: 0.01832284127902506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0717003345489502 0.17383527755737305

Final encoder loss: 0.018166201798409266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07052087783813477 0.1735830307006836

Final encoder loss: 0.018868803052589308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07206273078918457 0.17430925369262695

Final encoder loss: 0.019314526189525845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.071075439453125 0.17466330528259277

Final encoder loss: 0.015034644246868014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07198500633239746 0.174027681350708

Final encoder loss: 0.015698313724208206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07090926170349121 0.17366337776184082

Final encoder loss: 0.01512516332832253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0710296630859375 0.17564010620117188

Final encoder loss: 0.015487663484949546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07146596908569336 0.17383170127868652


Training wesad model
Final encoder loss: 0.2155546098947525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10597968101501465 0.03352522850036621

Final encoder loss: 0.0996413454413414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10470724105834961 0.03406953811645508

Final encoder loss: 0.06377502530813217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10509896278381348 0.03351879119873047

Final encoder loss: 0.0459708608686924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10376977920532227 0.03339242935180664

Final encoder loss: 0.035842422395944595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10416960716247559 0.0334932804107666

Final encoder loss: 0.02967822551727295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10457253456115723 0.033881187438964844

Final encoder loss: 0.02570788934826851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456299781799316 0.033605098724365234

Final encoder loss: 0.023061882704496384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10361719131469727 0.033460140228271484

Final encoder loss: 0.021252702921628952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10427212715148926 0.033902645111083984

Final encoder loss: 0.020037446171045303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10476565361022949 0.03424715995788574

Final encoder loss: 0.01926756463944912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10430216789245605 0.033661842346191406

Final encoder loss: 0.018861515447497368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456466674804688 0.033443450927734375

Final encoder loss: 0.018644902855157852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10426139831542969 0.03392171859741211

Final encoder loss: 0.01858609914779663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10529708862304688 0.034316062927246094

Final encoder loss: 0.01839517615735531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10401701927185059 0.03351020812988281

Final encoder loss: 0.01827801764011383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10439109802246094 0.033529043197631836

Final encoder loss: 0.018278125673532486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10447168350219727 0.03351163864135742

Final encoder loss: 0.01825674995779991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10545206069946289 0.03335213661193848

Final encoder loss: 0.01826530322432518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10451245307922363 0.033490657806396484

Final encoder loss: 0.01833677664399147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456514358520508 0.033203840255737305

Final encoder loss: 0.01833733543753624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385370254516602 0.033606767654418945

Final encoder loss: 0.01835794933140278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10580849647521973 0.03309321403503418

Final encoder loss: 0.018310626968741417

Calculating loss for amigos model
	Full Pass 0.7012994289398193
numFreeParamsPath 18
Reconstruction loss values: 0.02695283852517605 0.03638475388288498

Calculating loss for dapper model
	Full Pass 0.15118670463562012
numFreeParamsPath 18
Reconstruction loss values: 0.022989941760897636 0.02475142665207386

Calculating loss for case model
	Full Pass 0.9095382690429688
numFreeParamsPath 18
Reconstruction loss values: 0.03152758628129959 0.03469187021255493

Calculating loss for emognition model
	Full Pass 0.2925717830657959
numFreeParamsPath 18
Reconstruction loss values: 0.03469781577587128 0.04229985177516937

Calculating loss for empatch model
	Full Pass 0.10491776466369629
numFreeParamsPath 18
Reconstruction loss values: 0.036068566143512726 0.042656395584344864

Calculating loss for wesad model
	Full Pass 0.07854008674621582
numFreeParamsPath 18
Reconstruction loss values: 0.03568800166249275 0.05268977954983711
Total loss calculation time: 3.9488370418548584

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.237850666046143
Total epoch time: 210.4050645828247

Epoch: 45

Training dapper model
Final encoder loss: 0.02090163393969558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06812262535095215 0.15851163864135742

Final encoder loss: 0.02019332441753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06362152099609375 0.15257930755615234

Final encoder loss: 0.0211115363126884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06226205825805664 0.15107345581054688

Final encoder loss: 0.019607823607401778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.0632786750793457 0.15090370178222656

Final encoder loss: 0.02045803611253009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06277728080749512 0.15056848526000977

Final encoder loss: 0.018954874023385537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06283044815063477 0.1520371437072754

Final encoder loss: 0.01947048954761926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.0625462532043457 0.15147757530212402

Final encoder loss: 0.01895450618839083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06287622451782227 0.1513195037841797

Final encoder loss: 0.020228919541660235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06376981735229492 0.1518843173980713

Final encoder loss: 0.021407338374942752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06254720687866211 0.15104269981384277

Final encoder loss: 0.0201636856868355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06257152557373047 0.15043878555297852

Final encoder loss: 0.02122694711692619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06360626220703125 0.15104436874389648

Final encoder loss: 0.018073056343248954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06631636619567871 0.15085124969482422

Final encoder loss: 0.019990902404987538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06236672401428223 0.15253376960754395

Final encoder loss: 0.019952817054373687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06195259094238281 0.15079212188720703

Final encoder loss: 0.019972676609781543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06212019920349121 0.1490645408630371


Training case model
Final encoder loss: 0.03183261208108613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09238791465759277 0.2658224105834961

Final encoder loss: 0.029881044380674127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09154701232910156 0.2649657726287842

Final encoder loss: 0.029159812666948292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09134507179260254 0.26575326919555664

Final encoder loss: 0.028408249166098465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09213900566101074 0.266141414642334

Final encoder loss: 0.028078187106974534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09163308143615723 0.2652316093444824

Final encoder loss: 0.027308479309159344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.0919029712677002 0.265521764755249

Final encoder loss: 0.027151980398747746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09156227111816406 0.2667577266693115

Final encoder loss: 0.026810273879418155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0921027660369873 0.27115917205810547

Final encoder loss: 0.02665721698072662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09265375137329102 0.26465725898742676

Final encoder loss: 0.02556073718110409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09109783172607422 0.2645604610443115

Final encoder loss: 0.025793256089300606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09167933464050293 0.26544857025146484

Final encoder loss: 0.025898513888833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09299707412719727 0.2659752368927002

Final encoder loss: 0.02554385621037154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09200239181518555 0.26610231399536133

Final encoder loss: 0.02518034485609984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09163951873779297 0.2653336524963379

Final encoder loss: 0.02558761247425904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09235715866088867 0.2660660743713379

Final encoder loss: 0.02575212052246011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08873510360717773 0.26273560523986816


Training emognition model
Final encoder loss: 0.03440717522311333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.0840301513671875 0.27643322944641113

Final encoder loss: 0.033562300495470834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08319973945617676 0.2758796215057373

Final encoder loss: 0.034813874610735644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08369684219360352 0.2758028507232666

Final encoder loss: 0.03383063036280103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.10297560691833496 0.2782726287841797

Final encoder loss: 0.03093594498637451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08419394493103027 0.27482008934020996

Final encoder loss: 0.033287871185415374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08330202102661133 0.2752342224121094

Final encoder loss: 0.03322896437366234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08356213569641113 0.2766256332397461

Final encoder loss: 0.0329114925475372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08342862129211426 0.2759547233581543

Final encoder loss: 0.03172371175755217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08395862579345703 0.2754688262939453

Final encoder loss: 0.03282732425445954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08326172828674316 0.27523040771484375

Final encoder loss: 0.03459041795337257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08388090133666992 0.2745547294616699

Final encoder loss: 0.03076512804366702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08501839637756348 0.27661991119384766

Final encoder loss: 0.03220584110914575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0834817886352539 0.2759525775909424

Final encoder loss: 0.03334064338335095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08347678184509277 0.2757267951965332

Final encoder loss: 0.03296299197274116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08331537246704102 0.277099609375

Final encoder loss: 0.031321528922981755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08304643630981445 0.27472877502441406


Training amigos model
Final encoder loss: 0.028811984979912806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10940217971801758 0.3890547752380371

Final encoder loss: 0.025157413971130797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10919189453125 0.3903541564941406

Final encoder loss: 0.026559649487560812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10839605331420898 0.3905372619628906

Final encoder loss: 0.02648384874932382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10796165466308594 0.3887765407562256

Final encoder loss: 0.025118892802189124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10850954055786133 0.3898470401763916

Final encoder loss: 0.027072244370953964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10842609405517578 0.3895742893218994

Final encoder loss: 0.027920157317179266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10792160034179688 0.38989710807800293

Final encoder loss: 0.02788051423994895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10889387130737305 0.3897054195404053

Final encoder loss: 0.027521296277139486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10944318771362305 0.3901546001434326

Final encoder loss: 0.026976607524775206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10863280296325684 0.3895845413208008

Final encoder loss: 0.025715443154856934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10924506187438965 0.3901026248931885

Final encoder loss: 0.02706029597178176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10933756828308105 0.38994622230529785

Final encoder loss: 0.026283146206026706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10876035690307617 0.38931846618652344

Final encoder loss: 0.0249263453669294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10867023468017578 0.3894071578979492

Final encoder loss: 0.024242859345847897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10822439193725586 0.3884758949279785

Final encoder loss: 0.026644627704738924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10268878936767578 0.38289785385131836


Training amigos model
Final encoder loss: 0.019489966166793625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10615968704223633 0.340939998626709

Final encoder loss: 0.020733593821497375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10579442977905273 0.3410170078277588

Final encoder loss: 0.018862272174507082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10581660270690918 0.3424570560455322

Final encoder loss: 0.018489583385892842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10582780838012695 0.34087252616882324

Final encoder loss: 0.018576751971832617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1058497428894043 0.34081292152404785

Final encoder loss: 0.01826457265461084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10598969459533691 0.34077024459838867

Final encoder loss: 0.020142903163001134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10656929016113281 0.34177327156066895

Final encoder loss: 0.018834270805566378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10699248313903809 0.3415193557739258

Final encoder loss: 0.01892490022012793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.107269287109375 0.34206366539001465

Final encoder loss: 0.020235859626938177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10668182373046875 0.34185290336608887

Final encoder loss: 0.020380286729565687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10645461082458496 0.34151458740234375

Final encoder loss: 0.019091735111821037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10680413246154785 0.34172868728637695

Final encoder loss: 0.017284348772870065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10631346702575684 0.3418412208557129

Final encoder loss: 0.020120399260513847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10731744766235352 0.34166884422302246

Final encoder loss: 0.020833642145921576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10675334930419922 0.34157347679138184

Final encoder loss: 0.020216099440954052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10163712501525879 0.3381776809692383


Training amigos model
Final encoder loss: 0.1807660311460495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4697577953338623 0.07693600654602051

Final encoder loss: 0.1878337860107422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4731018543243408 0.08218216896057129

Final encoder loss: 0.1836346834897995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47249484062194824 0.07533931732177734

Final encoder loss: 0.0766335055232048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4684770107269287 0.07439637184143066

Final encoder loss: 0.0791715532541275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47049617767333984 0.08005070686340332

Final encoder loss: 0.07287347316741943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47071170806884766 0.07664346694946289

Final encoder loss: 0.044539790600538254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4702744483947754 0.07597589492797852

Final encoder loss: 0.04563630372285843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47097349166870117 0.08043813705444336

Final encoder loss: 0.04266926273703575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4715113639831543 0.07814288139343262

Final encoder loss: 0.03184083476662636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46950387954711914 0.07415175437927246

Final encoder loss: 0.03284584730863571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47124528884887695 0.08142375946044922

Final encoder loss: 0.03141738846898079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4712395668029785 0.07731485366821289

Final encoder loss: 0.0260961651802063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.465684175491333 0.07541561126708984

Final encoder loss: 0.02696858160197735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4718344211578369 0.0782470703125

Final encoder loss: 0.02614654041826725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4703390598297119 0.07501387596130371

Final encoder loss: 0.023606767877936363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4721853733062744 0.07583451271057129

Final encoder loss: 0.02426823601126671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47274184226989746 0.074066162109375

Final encoder loss: 0.023605795577168465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46907711029052734 0.07412505149841309

Final encoder loss: 0.022763876244425774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4696366786956787 0.07521367073059082

Final encoder loss: 0.023332558572292328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4724156856536865 0.07520580291748047

Final encoder loss: 0.022771593183279037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46800923347473145 0.07474398612976074

Final encoder loss: 0.022401606664061546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46904730796813965 0.07866573333740234

Final encoder loss: 0.022838592529296875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4714772701263428 0.07559370994567871

Final encoder loss: 0.022767383605241776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46860289573669434 0.07522916793823242

Final encoder loss: 0.02171238698065281
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47147488594055176 0.07176089286804199

Final encoder loss: 0.022127743810415268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4604015350341797 0.07675766944885254

Final encoder loss: 0.02217685803771019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4566631317138672 0.07397150993347168

Final encoder loss: 0.02088906429708004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4591495990753174 0.07572150230407715

Final encoder loss: 0.021301206201314926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4593813419342041 0.07241344451904297

Final encoder loss: 0.021389152854681015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45612168312072754 0.07250213623046875

Final encoder loss: 0.02041986957192421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46840667724609375 0.07661628723144531

Final encoder loss: 0.020807819440960884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47089147567749023 0.07563185691833496

Final encoder loss: 0.020687004551291466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4702446460723877 0.07544088363647461

Final encoder loss: 0.020287510007619858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4735076427459717 0.07493233680725098

Final encoder loss: 0.020228039473295212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4716942310333252 0.07678580284118652

Final encoder loss: 0.020633479580283165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46655869483947754 0.07423019409179688

Final encoder loss: 0.020052703097462654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4704132080078125 0.07702112197875977

Final encoder loss: 0.020067285746335983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47152161598205566 0.08017897605895996

Final encoder loss: 0.02018573507666588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46628451347351074 0.08086609840393066

Final encoder loss: 0.019711006432771683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4704103469848633 0.07694816589355469

Final encoder loss: 0.019719205796718597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4724240303039551 0.07599782943725586

Final encoder loss: 0.020110515877604485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45633745193481445 0.07430696487426758

Final encoder loss: 0.019354287534952164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4583148956298828 0.07383322715759277

Final encoder loss: 0.019451258704066277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4592399597167969 0.07414579391479492

Final encoder loss: 0.019749727100133896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45562195777893066 0.07350444793701172

Final encoder loss: 0.019222605973482132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4587283134460449 0.07425618171691895

Final encoder loss: 0.019292255863547325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4650864601135254 0.07677721977233887

Final encoder loss: 0.019689278677105904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4676487445831299 0.07610273361206055

Final encoder loss: 0.01913626678287983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46660757064819336 0.07672524452209473

Final encoder loss: 0.019138174131512642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46971988677978516 0.0726480484008789

Final encoder loss: 0.019450627267360687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4641871452331543 0.07608461380004883

Final encoder loss: 0.019110463559627533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47039151191711426 0.07552576065063477

Final encoder loss: 0.01904718019068241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4678797721862793 0.07636833190917969

Final encoder loss: 0.01952117495238781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46536874771118164 0.07459807395935059

Final encoder loss: 0.018864285200834274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4615943431854248 0.0768136978149414

Final encoder loss: 0.01885577104985714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47509098052978516 0.07517814636230469

Final encoder loss: 0.019219795241951942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46454286575317383 0.07347822189331055

Final encoder loss: 0.018711702898144722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45950746536254883 0.07360148429870605

Final encoder loss: 0.01880776137113571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4765801429748535 0.07839226722717285

Final encoder loss: 0.019277827814221382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45872068405151367 0.0733644962310791

Final encoder loss: 0.018684377893805504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46636271476745605 0.07581090927124023

Final encoder loss: 0.01873520202934742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4619941711425781 0.07497096061706543

Final encoder loss: 0.01925099454820156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4598360061645508 0.07416558265686035

Final encoder loss: 0.018736090511083603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45995450019836426 0.07500505447387695

Final encoder loss: 0.018726129084825516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4630167484283447 0.07436037063598633

Final encoder loss: 0.0192427821457386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4591820240020752 0.07257890701293945

Final encoder loss: 0.01861630752682686
Final encoder loss: 0.017617935314774513
Final encoder loss: 0.017355740070343018

Training dapper model
Final encoder loss: 0.014930153368204298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05989813804626465 0.10658836364746094

Final encoder loss: 0.01769731365218563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05923819541931152 0.10583925247192383

Final encoder loss: 0.016686483576680147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05922508239746094 0.10634112358093262

Final encoder loss: 0.017527852349690357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059305429458618164 0.10646510124206543

Final encoder loss: 0.016329895865279487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059033870697021484 0.10650849342346191

Final encoder loss: 0.017122064599671086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059146881103515625 0.10615158081054688

Final encoder loss: 0.017201080370565374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05893206596374512 0.10619187355041504

Final encoder loss: 0.014012313885624121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.058806657791137695 0.10621476173400879

Final encoder loss: 0.015363401798783694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05919504165649414 0.10632944107055664

Final encoder loss: 0.014458131711028067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05932021141052246 0.10624575614929199

Final encoder loss: 0.015632175695641966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05914664268493652 0.1063084602355957

Final encoder loss: 0.018200194553189553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.059188127517700195 0.10581398010253906

Final encoder loss: 0.014063612094434234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05919837951660156 0.10652828216552734

Final encoder loss: 0.01581346647616991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05915117263793945 0.1064918041229248

Final encoder loss: 0.01620649930026339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05907750129699707 0.1063694953918457

Final encoder loss: 0.016268235275082695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05883502960205078 0.10584878921508789


Training dapper model
Final encoder loss: 0.20243676006793976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11683034896850586 0.03393745422363281

Final encoder loss: 0.2081935703754425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11440753936767578 0.033566951751708984

Final encoder loss: 0.08330199122428894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11571526527404785 0.03444862365722656

Final encoder loss: 0.08535821735858917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11525392532348633 0.03422093391418457

Final encoder loss: 0.049520429223775864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1155097484588623 0.03403282165527344

Final encoder loss: 0.049342311918735504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11486291885375977 0.03369569778442383

Final encoder loss: 0.034110937267541885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11569690704345703 0.033669233322143555

Final encoder loss: 0.033820219337940216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11483550071716309 0.03416943550109863

Final encoder loss: 0.026166917756199837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11517453193664551 0.034363746643066406

Final encoder loss: 0.025960959494113922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11482357978820801 0.0335085391998291

Final encoder loss: 0.02169753424823284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1152808666229248 0.03400468826293945

Final encoder loss: 0.02147088758647442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1149129867553711 0.03341388702392578

Final encoder loss: 0.0191964041441679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11545825004577637 0.03410959243774414

Final encoder loss: 0.01882363110780716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11474871635437012 0.03406810760498047

Final encoder loss: 0.01754910498857498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11515235900878906 0.03365445137023926

Final encoder loss: 0.01738550327718258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11502361297607422 0.03403306007385254

Final encoder loss: 0.016633503139019012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559271812438965 0.033728599548339844

Final encoder loss: 0.016424769535660744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11500430107116699 0.03374814987182617

Final encoder loss: 0.016075212508440018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559700965881348 0.03379011154174805

Final encoder loss: 0.016081778332591057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11495852470397949 0.033416032791137695

Final encoder loss: 0.01638372614979744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11544394493103027 0.03404951095581055

Final encoder loss: 0.015843862667679787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11526966094970703 0.03386330604553223

Final encoder loss: 0.016167711466550827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11504292488098145 0.0339052677154541

Final encoder loss: 0.015883823856711388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11470890045166016 0.03418612480163574

Final encoder loss: 0.016096778213977814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1155996322631836 0.033960580825805664

Final encoder loss: 0.015559123829007149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11536383628845215 0.03355741500854492

Final encoder loss: 0.015679696574807167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11553430557250977 0.033860206604003906

Final encoder loss: 0.015845926478505135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11516618728637695 0.03345608711242676

Final encoder loss: 0.01569601334631443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11576128005981445 0.03391003608703613

Final encoder loss: 0.01568690314888954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11472368240356445 0.03377962112426758

Final encoder loss: 0.014904024079442024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11572742462158203 0.0335540771484375

Final encoder loss: 0.01466777827590704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11476755142211914 0.0334320068359375

Final encoder loss: 0.014504310674965382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11508440971374512 0.03445720672607422

Final encoder loss: 0.014258365146815777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527347564697266 0.03387928009033203

Final encoder loss: 0.014288526028394699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11582231521606445 0.0337982177734375

Final encoder loss: 0.014056296087801456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11515974998474121 0.03392434120178223

Final encoder loss: 0.014246395789086819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580133438110352 0.03338003158569336

Final encoder loss: 0.014091662131249905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11501312255859375 0.03439950942993164

Final encoder loss: 0.014207735657691956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11504745483398438 0.03424572944641113

Final encoder loss: 0.014187256805598736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152653694152832 0.034395694732666016

Final encoder loss: 0.014213399030268192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11505961418151855 0.03393292427062988

Final encoder loss: 0.014174861833453178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11474466323852539 0.03369283676147461

Final encoder loss: 0.014266888611018658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11562800407409668 0.03407788276672363

Final encoder loss: 0.01395467109978199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11444902420043945 0.03382301330566406

Final encoder loss: 0.01398239005357027
Final encoder loss: 0.012927194125950336

Training case model
Final encoder loss: 0.024132076240442488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08900213241577148 0.21872782707214355

Final encoder loss: 0.02353559118400271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08894801139831543 0.21870660781860352

Final encoder loss: 0.023485132669736862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0893106460571289 0.21834635734558105

Final encoder loss: 0.023196637389977733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08934187889099121 0.2183237075805664

Final encoder loss: 0.023238071901131037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0897672176361084 0.2187032699584961

Final encoder loss: 0.02299963456437191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08886194229125977 0.21863961219787598

Final encoder loss: 0.023276213291860928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.088836669921875 0.2183382511138916

Final encoder loss: 0.022465415738488315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.0892174243927002 0.21859288215637207

Final encoder loss: 0.02245917287233196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.0890040397644043 0.2186412811279297

Final encoder loss: 0.022674531134841518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08922839164733887 0.21871638298034668

Final encoder loss: 0.022693559592917786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08929705619812012 0.2186264991760254

Final encoder loss: 0.023085943628578916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08891677856445312 0.218705415725708

Final encoder loss: 0.02299088470481388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08889245986938477 0.21849751472473145

Final encoder loss: 0.02319433783440847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08923864364624023 0.21819067001342773

Final encoder loss: 0.02294085114476615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08891868591308594 0.2185044288635254

Final encoder loss: 0.022341805201139976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08566808700561523 0.2151167392730713


Training case model
Final encoder loss: 0.20295991003513336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26198339462280273 0.05236101150512695

Final encoder loss: 0.18891137838363647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577204704284668 0.051285505294799805

Final encoder loss: 0.190156489610672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2568521499633789 0.05156207084655762

Final encoder loss: 0.1921943575143814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25614356994628906 0.05167388916015625

Final encoder loss: 0.18080948293209076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25711989402770996 0.052733659744262695

Final encoder loss: 0.19191256165504456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544372081756592 0.05181407928466797

Final encoder loss: 0.103959821164608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569284439086914 0.05263257026672363

Final encoder loss: 0.09411486983299255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25751709938049316 0.051705360412597656

Final encoder loss: 0.0898260697722435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2565310001373291 0.0517125129699707

Final encoder loss: 0.08886445313692093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26824212074279785 0.05193901062011719

Final encoder loss: 0.0806494876742363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2665097713470459 0.05220627784729004

Final encoder loss: 0.08430951088666916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551438808441162 0.051897287368774414

Final encoder loss: 0.06072152033448219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582216262817383 0.05317115783691406

Final encoder loss: 0.05512488633394241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25865793228149414 0.05234074592590332

Final encoder loss: 0.05289103090763092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2561187744140625 0.0520780086517334

Final encoder loss: 0.05319703742861748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2752194404602051 0.052060604095458984

Final encoder loss: 0.04956609010696411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25616884231567383 0.05309581756591797

Final encoder loss: 0.05181945115327835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25397586822509766 0.05071616172790527

Final encoder loss: 0.042682111263275146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25700926780700684 0.05259442329406738

Final encoder loss: 0.0394856221973896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25851869583129883 0.05198812484741211

Final encoder loss: 0.0379827618598938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25662803649902344 0.05232095718383789

Final encoder loss: 0.03868220001459122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25681281089782715 0.05116772651672363

Final encoder loss: 0.037136539816856384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25647950172424316 0.05294322967529297

Final encoder loss: 0.03829819709062576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542080879211426 0.052124977111816406

Final encoder loss: 0.03468037024140358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25659751892089844 0.05256390571594238

Final encoder loss: 0.03296535089612007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26808643341064453 0.0523073673248291

Final encoder loss: 0.031834423542022705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566249370574951 0.0534517765045166

Final encoder loss: 0.03272455558180809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567324638366699 0.051648616790771484

Final encoder loss: 0.032736778259277344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25679945945739746 0.05124187469482422

Final encoder loss: 0.03275613859295845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25433969497680664 0.051177024841308594

Final encoder loss: 0.031202537938952446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580454349517822 0.05208134651184082

Final encoder loss: 0.030585432425141335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572910785675049 0.05226635932922363

Final encoder loss: 0.02976861596107483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26892971992492676 0.05353832244873047

Final encoder loss: 0.030332166701555252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26928138732910156 0.05222487449645996

Final encoder loss: 0.031163938343524933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581198215484619 0.05546998977661133

Final encoder loss: 0.03071647323668003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2577536106109619 0.05308723449707031

Final encoder loss: 0.029073398560285568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25774216651916504 0.05418038368225098

Final encoder loss: 0.028543464839458466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26023125648498535 0.053371429443359375

Final encoder loss: 0.02795150689780712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25746798515319824 0.05426335334777832

Final encoder loss: 0.028236612677574158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26000285148620605 0.0530550479888916

Final encoder loss: 0.028987519443035126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2594485282897949 0.05347156524658203

Final encoder loss: 0.028494296595454216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2580890655517578 0.05218005180358887

Final encoder loss: 0.027379104867577553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25969719886779785 0.05212879180908203

Final encoder loss: 0.027020204812288284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2636871337890625 0.05175280570983887

Final encoder loss: 0.02625436522066593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26784324645996094 0.05241584777832031

Final encoder loss: 0.026805033907294273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2645435333251953 0.05337834358215332

Final encoder loss: 0.02775699459016323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25991153717041016 0.05231618881225586

Final encoder loss: 0.02705756016075611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2579164505004883 0.05104517936706543

Final encoder loss: 0.02630797028541565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25934648513793945 0.0526430606842041

Final encoder loss: 0.026002181693911552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2734231948852539 0.05175304412841797

Final encoder loss: 0.025516191497445107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577943801879883 0.05329084396362305

Final encoder loss: 0.025757385417819023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2725820541381836 0.054152488708496094

Final encoder loss: 0.0266988817602396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26050567626953125 0.05275607109069824

Final encoder loss: 0.02622648887336254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.258833646774292 0.055349111557006836

Final encoder loss: 0.025645224377512932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586023807525635 0.05235862731933594

Final encoder loss: 0.025502078235149384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2712845802307129 0.055877685546875

Final encoder loss: 0.025027839466929436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26816296577453613 0.051644086837768555

Final encoder loss: 0.025342978537082672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25981879234313965 0.05463123321533203

Final encoder loss: 0.026310710236430168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590034008026123 0.05257987976074219

Final encoder loss: 0.025659864768385887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2577681541442871 0.053519248962402344

Final encoder loss: 0.02513882704079151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25855255126953125 0.05297493934631348

Final encoder loss: 0.0248999185860157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2590959072113037 0.05269503593444824

Final encoder loss: 0.024435630068182945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25942206382751465 0.053699493408203125

Final encoder loss: 0.02465781755745411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.259765625 0.05518817901611328

Final encoder loss: 0.025682643055915833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25875163078308105 0.05418062210083008

Final encoder loss: 0.02501179836690426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556774616241455 0.053893327713012695

Final encoder loss: 0.024707531556487083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25817203521728516 0.05260586738586426

Final encoder loss: 0.024627018719911575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25796937942504883 0.05458950996398926

Final encoder loss: 0.024140914902091026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588620185852051 0.052987098693847656

Final encoder loss: 0.02441290020942688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26963376998901367 0.05357933044433594

Final encoder loss: 0.02540167234838009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26924896240234375 0.0536191463470459

Final encoder loss: 0.024746447801589966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25473499298095703 0.052066802978515625

Final encoder loss: 0.02432532235980034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2595510482788086 0.052721500396728516

Final encoder loss: 0.024214496836066246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26923084259033203 0.05202674865722656

Final encoder loss: 0.02385430596768856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25896382331848145 0.05329179763793945

Final encoder loss: 0.02391905151307583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2603166103363037 0.05215764045715332

Final encoder loss: 0.02476915530860424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2598154544830322 0.05241584777832031

Final encoder loss: 0.02436804212629795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25595521926879883 0.05265402793884277

Final encoder loss: 0.02410496398806572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26265501976013184 0.052114009857177734

Final encoder loss: 0.02402876690030098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25850749015808105 0.053450584411621094

Final encoder loss: 0.023632565513253212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2606058120727539 0.05180811882019043

Final encoder loss: 0.023728203028440475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26985692977905273 0.05263090133666992

Final encoder loss: 0.024761563166975975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25978755950927734 0.05271172523498535

Final encoder loss: 0.024152683094143867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2576148509979248 0.052350521087646484

Final encoder loss: 0.02386448159813881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26021289825439453 0.05254316329956055

Final encoder loss: 0.023799946531653404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26956868171691895 0.052831411361694336

Final encoder loss: 0.02335643209517002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2600281238555908 0.05596923828125

Final encoder loss: 0.02346559427678585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2587568759918213 0.05242753028869629

Final encoder loss: 0.02420564740896225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2602987289428711 0.055747032165527344

Final encoder loss: 0.023839959874749184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552042007446289 0.05217909812927246

Final encoder loss: 0.02368208020925522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26146817207336426 0.05502462387084961

Final encoder loss: 0.023609107360243797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2592170238494873 0.05275535583496094

Final encoder loss: 0.02319948375225067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2598395347595215 0.05501055717468262

Final encoder loss: 0.02333567850291729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2586822509765625 0.05160951614379883

Final encoder loss: 0.024352239444851875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25879573822021484 0.054283857345581055

Final encoder loss: 0.023752903565764427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25583481788635254 0.05219697952270508

Final encoder loss: 0.02349538914859295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2575266361236572 0.05410337448120117

Final encoder loss: 0.02337108552455902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25911617279052734 0.052132606506347656

Final encoder loss: 0.023032894358038902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594277858734131 0.053053855895996094

Final encoder loss: 0.02307802066206932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2588346004486084 0.05270242691040039

Final encoder loss: 0.023893551900982857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2681553363800049 0.05563163757324219

Final encoder loss: 0.0234249085187912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25666165351867676 0.05292224884033203

Final encoder loss: 0.0233602374792099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25849485397338867 0.05469822883605957

Final encoder loss: 0.023359952494502068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25908374786376953 0.054138898849487305

Final encoder loss: 0.022917715832591057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.268202543258667 0.05252265930175781

Final encoder loss: 0.022947104647755623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25948548316955566 0.05300712585449219

Final encoder loss: 0.023944497108459473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25794124603271484 0.05333900451660156

Final encoder loss: 0.023378293961286545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2574481964111328 0.05183076858520508

Final encoder loss: 0.023284481838345528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586784362792969 0.05218935012817383

Final encoder loss: 0.023124417290091515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26254987716674805 0.05174589157104492

Final encoder loss: 0.02275780215859413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586548328399658 0.05213499069213867

Final encoder loss: 0.02283318340778351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26425766944885254 0.05278277397155762

Final encoder loss: 0.02358321286737919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25678586959838867 0.05311250686645508

Final encoder loss: 0.02314494177699089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25812840461730957 0.050649166107177734

Final encoder loss: 0.02310192584991455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25824689865112305 0.05299496650695801

Final encoder loss: 0.023053258657455444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2615530490875244 0.053928375244140625

Final encoder loss: 0.022696001455187798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576475143432617 0.05272412300109863

Final encoder loss: 0.022700781002640724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26051855087280273 0.054108381271362305

Final encoder loss: 0.02362162061035633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575368881225586 0.05273890495300293

Final encoder loss: 0.02315327152609825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25847554206848145 0.05539059638977051

Final encoder loss: 0.023043978959321976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257366418838501 0.05352139472961426

Final encoder loss: 0.022967297583818436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2598097324371338 0.05507302284240723

Final encoder loss: 0.022564871236681938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26897382736206055 0.052092552185058594

Final encoder loss: 0.022536272183060646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2605912685394287 0.05435061454772949

Final encoder loss: 0.023385409265756607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25809311866760254 0.05286264419555664

Final encoder loss: 0.022904962301254272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25787925720214844 0.05269885063171387

Final encoder loss: 0.022939395159482956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25832676887512207 0.05255842208862305

Final encoder loss: 0.022828809916973114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26999592781066895 0.05283999443054199

Final encoder loss: 0.022417228668928146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2690877914428711 0.05201292037963867

Final encoder loss: 0.022495443001389503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2697157859802246 0.053925275802612305

Final encoder loss: 0.023412786424160004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2690157890319824 0.05197262763977051

Final encoder loss: 0.022853007540106773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558317184448242 0.05360770225524902

Final encoder loss: 0.022867606952786446
Final encoder loss: 0.022279592230916023
Final encoder loss: 0.021386606618762016
Final encoder loss: 0.020645717158913612
Final encoder loss: 0.020586755126714706
Final encoder loss: 0.01947472058236599

Training emognition model
Final encoder loss: 0.027017954413988944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08237051963806152 0.23065400123596191

Final encoder loss: 0.026419172468964663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08108663558959961 0.2312331199645996

Final encoder loss: 0.026952962232913828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08193111419677734 0.23036551475524902

Final encoder loss: 0.026611764961030505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08137702941894531 0.23173046112060547

Final encoder loss: 0.026056611569721333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08149266242980957 0.23111557960510254

Final encoder loss: 0.026329379188788655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08028149604797363 0.23030853271484375

Final encoder loss: 0.02594549288137575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08094501495361328 0.23065900802612305

Final encoder loss: 0.025308383906104976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08307123184204102 0.2312014102935791

Final encoder loss: 0.027608990332805065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08162283897399902 0.23076534271240234

Final encoder loss: 0.02619020266729639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08177852630615234 0.23076844215393066

Final encoder loss: 0.026265694974086477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08294939994812012 0.2310807704925537

Final encoder loss: 0.02700742303266017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08220672607421875 0.2310476303100586

Final encoder loss: 0.025024254403964608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08200740814208984 0.23126745223999023

Final encoder loss: 0.025588200407710007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08401179313659668 0.23096203804016113

Final encoder loss: 0.025796915673272115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0814824104309082 0.23094558715820312

Final encoder loss: 0.02402506078891397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0804743766784668 0.23107075691223145


Training emognition model
Final encoder loss: 0.19356529414653778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25069260597229004 0.050046443939208984

Final encoder loss: 0.1949683278799057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24995970726013184 0.049752235412597656

Final encoder loss: 0.08837926387786865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24981331825256348 0.04989504814147949

Final encoder loss: 0.08846204727888107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741196632385254 0.05073738098144531

Final encoder loss: 0.056413132697343826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2505321502685547 0.05040550231933594

Final encoder loss: 0.05514053627848625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24787640571594238 0.048803091049194336

Final encoder loss: 0.042117711156606674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24844980239868164 0.05010652542114258

Final encoder loss: 0.041123759001493454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24837636947631836 0.049001455307006836

Final encoder loss: 0.03481318801641464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2510554790496826 0.0499119758605957

Final encoder loss: 0.03410948067903519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24775171279907227 0.05093526840209961

Final encoder loss: 0.03071865625679493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24822282791137695 0.049515724182128906

Final encoder loss: 0.030215011909604073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24921226501464844 0.04865121841430664

Final encoder loss: 0.028386708348989487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24665093421936035 0.04889845848083496

Final encoder loss: 0.027972284704446793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24523186683654785 0.04879021644592285

Final encoder loss: 0.02719261310994625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467482089996338 0.049889564514160156

Final encoder loss: 0.026880864053964615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24505996704101562 0.048613548278808594

Final encoder loss: 0.026709794998168945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24602651596069336 0.04835844039916992

Final encoder loss: 0.02649034559726715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.245405912399292 0.04864239692687988

Final encoder loss: 0.02651113085448742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24602007865905762 0.04898428916931152

Final encoder loss: 0.026426611468195915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24521613121032715 0.04943585395812988

Final encoder loss: 0.026186784729361534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24596571922302246 0.04937148094177246

Final encoder loss: 0.026312626898288727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24617600440979004 0.048870086669921875

Final encoder loss: 0.025888852775096893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.247208833694458 0.04947829246520996

Final encoder loss: 0.02583913318812847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24603939056396484 0.049454450607299805

Final encoder loss: 0.025565272197127342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24801135063171387 0.0494074821472168

Final encoder loss: 0.025417285040020943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2459414005279541 0.049088478088378906

Final encoder loss: 0.025436922907829285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472057342529297 0.05000567436218262

Final encoder loss: 0.02526467852294445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464151382446289 0.04995560646057129

Final encoder loss: 0.025357548147439957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2479569911956787 0.048799753189086914

Final encoder loss: 0.02513023279607296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24689507484436035 0.04948902130126953

Final encoder loss: 0.025189902633428574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24855351448059082 0.05025124549865723

Final encoder loss: 0.025069015100598335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2461702823638916 0.04993557929992676

Final encoder loss: 0.024874011054635048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485651969909668 0.048098087310791016

Final encoder loss: 0.024989118799567223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24588513374328613 0.049131155014038086

Final encoder loss: 0.024939078837633133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2476503849029541 0.048268795013427734

Final encoder loss: 0.02497461624443531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.246293306350708 0.048645973205566406

Final encoder loss: 0.024767622351646423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.247114896774292 0.04788017272949219

Final encoder loss: 0.024800609797239304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24645256996154785 0.049185991287231445

Final encoder loss: 0.02473599649965763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24741339683532715 0.0497899055480957

Final encoder loss: 0.024665435776114464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24704694747924805 0.049653053283691406

Final encoder loss: 0.024504996836185455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2474193572998047 0.04915142059326172

Final encoder loss: 0.024482708424329758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24697661399841309 0.0502924919128418

Final encoder loss: 0.024555690586566925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2477555274963379 0.050191402435302734

Final encoder loss: 0.024521298706531525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24692368507385254 0.05027651786804199

Final encoder loss: 0.02440175972878933
Final encoder loss: 0.023558517917990685

Training empatch model
Final encoder loss: 0.03357617353612025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07112431526184082 0.17399311065673828

Final encoder loss: 0.03439389710428972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07151055335998535 0.17345952987670898

Final encoder loss: 0.03404205478244051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07162785530090332 0.17477774620056152

Final encoder loss: 0.03734588891341125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07125186920166016 0.17347383499145508

Final encoder loss: 0.03312495808597157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0710759162902832 0.17346715927124023

Final encoder loss: 0.032942867098063024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07127261161804199 0.17397117614746094

Final encoder loss: 0.035163403250455555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07102298736572266 0.1737229824066162

Final encoder loss: 0.03465474957837847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07107257843017578 0.17329001426696777

Final encoder loss: 0.023560452139373873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07142186164855957 0.17435336112976074

Final encoder loss: 0.025091214206908338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07232260704040527 0.1733396053314209

Final encoder loss: 0.023896471423719364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0707082748413086 0.17311668395996094

Final encoder loss: 0.02491886691691585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07056903839111328 0.1731274127960205

Final encoder loss: 0.024805476066655077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07035207748413086 0.17307186126708984

Final encoder loss: 0.022784459716522033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07032513618469238 0.17317938804626465

Final encoder loss: 0.02554244079074155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07060527801513672 0.17320799827575684

Final encoder loss: 0.02352761725950494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07040953636169434 0.1723344326019287


Training empatch model
Final encoder loss: 0.1711605340242386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17583107948303223 0.04312920570373535

Final encoder loss: 0.08079767972230911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17449450492858887 0.04295945167541504

Final encoder loss: 0.0555126890540123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17334985733032227 0.04258465766906738

Final encoder loss: 0.04314174875617027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17353034019470215 0.042253732681274414

Final encoder loss: 0.036049772053956985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17408084869384766 0.04260659217834473

Final encoder loss: 0.0315915122628212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17250657081604004 0.04251742362976074

Final encoder loss: 0.028620796278119087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1741936206817627 0.043025970458984375

Final encoder loss: 0.026639362797141075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1737198829650879 0.042562246322631836

Final encoder loss: 0.02529945597052574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17515134811401367 0.043105363845825195

Final encoder loss: 0.02441209927201271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746833324432373 0.04395103454589844

Final encoder loss: 0.023746827617287636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17578339576721191 0.04416084289550781

Final encoder loss: 0.023338118568062782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745443344116211 0.043668270111083984

Final encoder loss: 0.02303752861917019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17539215087890625 0.04277205467224121

Final encoder loss: 0.022787733003497124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17388248443603516 0.044002532958984375

Final encoder loss: 0.022527307271957397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17663311958312988 0.04309439659118652

Final encoder loss: 0.022305816411972046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17514848709106445 0.04319167137145996

Final encoder loss: 0.02217547409236431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17620491981506348 0.04376554489135742

Final encoder loss: 0.02208133228123188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1753396987915039 0.043478965759277344

Final encoder loss: 0.022045228630304337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17715191841125488 0.04376864433288574

Final encoder loss: 0.021985964849591255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17475152015686035 0.04355931282043457

Final encoder loss: 0.021839985623955727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17665696144104004 0.043264150619506836

Final encoder loss: 0.021774625405669212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17544221878051758 0.04365205764770508

Final encoder loss: 0.02164400741457939

Training wesad model
Final encoder loss: 0.03592170628395221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07117652893066406 0.17369437217712402

Final encoder loss: 0.036524994094346855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07100486755371094 0.1738133430480957

Final encoder loss: 0.032663833673720624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07200479507446289 0.1735231876373291

Final encoder loss: 0.034591255254062696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07127594947814941 0.17334365844726562

Final encoder loss: 0.02285935513485631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07238388061523438 0.1743924617767334

Final encoder loss: 0.025404538322801398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0712440013885498 0.1740119457244873

Final encoder loss: 0.022690910813301136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07056045532226562 0.17409157752990723

Final encoder loss: 0.023437264670653508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0710752010345459 0.17406201362609863

Final encoder loss: 0.018841829218592343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07135748863220215 0.17411208152770996

Final encoder loss: 0.018035923791732307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07225370407104492 0.1739649772644043

Final encoder loss: 0.017383351761724276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07159423828125 0.1737830638885498

Final encoder loss: 0.018132926070702486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07207870483398438 0.17419958114624023

Final encoder loss: 0.014456415131414371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07130217552185059 0.17415761947631836

Final encoder loss: 0.014796635378553571
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07096409797668457 0.1744251251220703

Final encoder loss: 0.015030843658689924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07118582725524902 0.17408061027526855

Final encoder loss: 0.015497925600889176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07115864753723145 0.17409157752990723


Training wesad model
Final encoder loss: 0.21560798585414886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10883831977844238 0.03300142288208008

Final encoder loss: 0.09918659180402756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038203239440918 0.03263235092163086

Final encoder loss: 0.0637948215007782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10348296165466309 0.033197879791259766

Final encoder loss: 0.04608381167054176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10457611083984375 0.03423762321472168

Final encoder loss: 0.03594152256846428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1055610179901123 0.032628536224365234

Final encoder loss: 0.029716240242123604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10428595542907715 0.032700538635253906

Final encoder loss: 0.025707688182592392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10441064834594727 0.03304457664489746

Final encoder loss: 0.023017216473817825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10484170913696289 0.03339552879333496

Final encoder loss: 0.021197665482759476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10447335243225098 0.033622026443481445

Final encoder loss: 0.020005421712994576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10390329360961914 0.033271074295043945

Final encoder loss: 0.019252652302384377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10480880737304688 0.03301644325256348

Final encoder loss: 0.0187822375446558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10439777374267578 0.03419613838195801

Final encoder loss: 0.018468251451849937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10435342788696289 0.032979488372802734

Final encoder loss: 0.01833636872470379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10361814498901367 0.032843828201293945

Final encoder loss: 0.018315238878130913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10478734970092773 0.0337526798248291

Final encoder loss: 0.018296143040060997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10570096969604492 0.03426361083984375

Final encoder loss: 0.018254507333040237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10379195213317871 0.033371925354003906

Final encoder loss: 0.01816929690539837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10424327850341797 0.03285479545593262

Final encoder loss: 0.018035590648651123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10408663749694824 0.0338139533996582

Final encoder loss: 0.017874985933303833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1055288314819336 0.03323054313659668

Final encoder loss: 0.017860829830169678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10380125045776367 0.033045291900634766

Final encoder loss: 0.017920544371008873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388708114624023 0.0335538387298584

Final encoder loss: 0.01804022304713726

Calculating loss for amigos model
	Full Pass 0.6860172748565674
numFreeParamsPath 18
Reconstruction loss values: 0.026112385094165802 0.035866763442754745

Calculating loss for dapper model
	Full Pass 0.15250587463378906
numFreeParamsPath 18
Reconstruction loss values: 0.022965289652347565 0.02565833367407322

Calculating loss for case model
	Full Pass 0.9144339561462402
numFreeParamsPath 18
Reconstruction loss values: 0.032065942883491516 0.03535477817058563

Calculating loss for emognition model
	Full Pass 0.29212117195129395
numFreeParamsPath 18
Reconstruction loss values: 0.03400395065546036 0.04182802885770798

Calculating loss for empatch model
	Full Pass 0.10505843162536621
numFreeParamsPath 18
Reconstruction loss values: 0.03581684082746506 0.04228488728404045

Calculating loss for wesad model
	Full Pass 0.07776665687561035
numFreeParamsPath 18
Reconstruction loss values: 0.035753991454839706 0.051560334861278534
Total loss calculation time: 3.929121255874634

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.6255409717559814
Total epoch time: 208.99587082862854

Epoch: 46

Training dapper model
Final encoder loss: 0.021116768732326773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06610989570617676 0.1571974754333496

Final encoder loss: 0.020789517651044212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06242942810058594 0.1524646282196045

Final encoder loss: 0.01873839252805758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062096595764160156 0.15073776245117188

Final encoder loss: 0.022453486949465293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06242775917053223 0.1502974033355713

Final encoder loss: 0.01894227174191969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06304121017456055 0.15140509605407715

Final encoder loss: 0.01925387583244482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06234002113342285 0.15085577964782715

Final encoder loss: 0.021392277866145577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06221580505371094 0.15124106407165527

Final encoder loss: 0.018606212176776264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06311607360839844 0.15162253379821777

Final encoder loss: 0.020242609372899692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06294679641723633 0.15027904510498047

Final encoder loss: 0.020808203946967814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06216001510620117 0.15143346786499023

Final encoder loss: 0.019330332231981374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06315302848815918 0.15100932121276855

Final encoder loss: 0.018390608100240464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061948299407958984 0.15008330345153809

Final encoder loss: 0.01960153391582938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06241464614868164 0.15107131004333496

Final encoder loss: 0.0194382278785705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06146502494812012 0.15041661262512207

Final encoder loss: 0.01779484293106552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06203007698059082 0.1506032943725586

Final encoder loss: 0.01635765813926802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06257510185241699 0.15121173858642578


Training emognition model
Final encoder loss: 0.03212061656798878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08296465873718262 0.27626657485961914

Final encoder loss: 0.0342412397695175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08475327491760254 0.27570557594299316

Final encoder loss: 0.03270993965543564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08312845230102539 0.27578210830688477

Final encoder loss: 0.03152106329175221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08312129974365234 0.27486419677734375

Final encoder loss: 0.03202836788785319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0839376449584961 0.275907039642334

Final encoder loss: 0.033233670242412985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08386778831481934 0.2747080326080322

Final encoder loss: 0.03320089598562561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.10232996940612793 0.27449536323547363

Final encoder loss: 0.03319693813227356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08300375938415527 0.2750575542449951

Final encoder loss: 0.03425074307826575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08346080780029297 0.27670979499816895

Final encoder loss: 0.0316399443134611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08513116836547852 0.275540828704834

Final encoder loss: 0.032627636087456724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08324289321899414 0.27562785148620605

Final encoder loss: 0.03182148937653232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.0837712287902832 0.27579760551452637

Final encoder loss: 0.03094230942697351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08370232582092285 0.276721715927124

Final encoder loss: 0.03180187092890059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08347725868225098 0.2756664752960205

Final encoder loss: 0.032070229070322705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08463025093078613 0.2753181457519531

Final encoder loss: 0.03287077894786855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0824577808380127 0.2741203308105469


Training case model
Final encoder loss: 0.03274599921494786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.0912938117980957 0.2650034427642822

Final encoder loss: 0.03039931111752018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0924372673034668 0.2661459445953369

Final encoder loss: 0.029484473842542793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09145474433898926 0.26561403274536133

Final encoder loss: 0.028693459487573226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09146475791931152 0.2653050422668457

Final encoder loss: 0.027776352813410992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09167718887329102 0.26692867279052734

Final encoder loss: 0.027029300389225266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09129619598388672 0.2657508850097656

Final encoder loss: 0.026845028660140102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09197115898132324 0.2650308609008789

Final encoder loss: 0.026798553030291796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09149527549743652 0.2653799057006836

Final encoder loss: 0.026720044366319216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09138917922973633 0.26566100120544434

Final encoder loss: 0.026175334437242934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09239959716796875 0.26491618156433105

Final encoder loss: 0.0260823726587505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09133791923522949 0.26505160331726074

Final encoder loss: 0.025581523572193683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09113693237304688 0.2651095390319824

Final encoder loss: 0.025867777000837627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09224247932434082 0.26613450050354004

Final encoder loss: 0.025484827715388687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09112024307250977 0.2657744884490967

Final encoder loss: 0.02577941485649203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09149456024169922 0.2646141052246094

Final encoder loss: 0.02464097135369607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08836913108825684 0.26355814933776855


Training amigos model
Final encoder loss: 0.026331158494543885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10833072662353516 0.3886680603027344

Final encoder loss: 0.02625665549604572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10838079452514648 0.38950181007385254

Final encoder loss: 0.02568977023486268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10824012756347656 0.38875865936279297

Final encoder loss: 0.02492165428554415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.1095280647277832 0.3887035846710205

Final encoder loss: 0.02472949644914546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10920000076293945 0.39005351066589355

Final encoder loss: 0.027332620376361225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10795927047729492 0.39045095443725586

Final encoder loss: 0.026413915962513405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1087501049041748 0.3893113136291504

Final encoder loss: 0.027008128991544433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10843634605407715 0.38889265060424805

Final encoder loss: 0.026421673316322756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.1083834171295166 0.38988280296325684

Final encoder loss: 0.02609555634542926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10845494270324707 0.38968777656555176

Final encoder loss: 0.02519974211405496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10937690734863281 0.3893899917602539

Final encoder loss: 0.026240707505854877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1092369556427002 0.39018917083740234

Final encoder loss: 0.026635116107228952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10810732841491699 0.3898947238922119

Final encoder loss: 0.025073178281150108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.11140871047973633 0.3894507884979248

Final encoder loss: 0.026949553562717873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10857105255126953 0.3897707462310791

Final encoder loss: 0.026965708077273862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10367822647094727 0.3842899799346924


Training amigos model
Final encoder loss: 0.02047533468895842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1061410903930664 0.3417167663574219

Final encoder loss: 0.01961228790192398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10766196250915527 0.34156250953674316

Final encoder loss: 0.018530819080880997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10651779174804688 0.3419148921966553

Final encoder loss: 0.021899954953514704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10625696182250977 0.3416624069213867

Final encoder loss: 0.01860635951357439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.1062631607055664 0.3419516086578369

Final encoder loss: 0.019289718765103547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1063079833984375 0.3416426181793213

Final encoder loss: 0.019375004711093727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10674667358398438 0.3419039249420166

Final encoder loss: 0.019916552358475153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10608053207397461 0.3418738842010498

Final encoder loss: 0.01939507148133715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10600781440734863 0.3417782783508301

Final encoder loss: 0.0204971165820483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.1058197021484375 0.34195709228515625

Final encoder loss: 0.020159236224022332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10740280151367188 0.34255051612854004

Final encoder loss: 0.020433157239838157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10630011558532715 0.3417346477508545

Final encoder loss: 0.01884040602972116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1062624454498291 0.34161949157714844

Final encoder loss: 0.02075961930463049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1066138744354248 0.34251880645751953

Final encoder loss: 0.018750051925630833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10701227188110352 0.3416016101837158

Final encoder loss: 0.0187878282387071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10184311866760254 0.33896446228027344


Training amigos model
Final encoder loss: 0.18074871599674225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47544121742248535 0.07590723037719727

Final encoder loss: 0.18783611059188843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47383999824523926 0.08031773567199707

Final encoder loss: 0.1836031824350357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4714334011077881 0.0767674446105957

Final encoder loss: 0.07637490332126617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4732840061187744 0.07559680938720703

Final encoder loss: 0.07910920679569244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4726126194000244 0.07563638687133789

Final encoder loss: 0.07212308049201965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4663887023925781 0.08065962791442871

Final encoder loss: 0.04454374313354492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.475186824798584 0.07468843460083008

Final encoder loss: 0.04561540111899376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4696195125579834 0.0758810043334961

Final encoder loss: 0.04211317002773285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.467104434967041 0.07866454124450684

Final encoder loss: 0.031969357281923294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4701073169708252 0.07416391372680664

Final encoder loss: 0.03285502269864082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4748837947845459 0.07523393630981445

Final encoder loss: 0.030877310782670975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654507637023926 0.07674741744995117

Final encoder loss: 0.02620946615934372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4721946716308594 0.08018374443054199

Final encoder loss: 0.02698860503733158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47225427627563477 0.07486557960510254

Final encoder loss: 0.02564968727529049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4687936305999756 0.07566523551940918

Final encoder loss: 0.023572934791445732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47151851654052734 0.07720589637756348

Final encoder loss: 0.024234069511294365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46986937522888184 0.0803537368774414

Final encoder loss: 0.02331097051501274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4698925018310547 0.07546520233154297

Final encoder loss: 0.02275225520133972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4683082103729248 0.07546734809875488

Final encoder loss: 0.022957442328333855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47039055824279785 0.07840800285339355

Final encoder loss: 0.022667519748210907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4662022590637207 0.07724452018737793

Final encoder loss: 0.02242075651884079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4743986129760742 0.07556915283203125

Final encoder loss: 0.02254323475062847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47095441818237305 0.07665324211120605

Final encoder loss: 0.022362226620316505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46648550033569336 0.08030533790588379

Final encoder loss: 0.021949345245957375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47234511375427246 0.07618355751037598

Final encoder loss: 0.02188011072576046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4713778495788574 0.07478117942810059

Final encoder loss: 0.021621258929371834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.469160795211792 0.07513022422790527

Final encoder loss: 0.021202461794018745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47313761711120605 0.0762033462524414

Final encoder loss: 0.02136097475886345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4707620143890381 0.07835602760314941

Final encoder loss: 0.02106509730219841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4683339595794678 0.0812993049621582

Final encoder loss: 0.020592225715517998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4602012634277344 0.07404804229736328

Final encoder loss: 0.020531948655843735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4583921432495117 0.07222342491149902

Final encoder loss: 0.020451998338103294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45586562156677246 0.07618308067321777

Final encoder loss: 0.020194143056869507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45975375175476074 0.07493877410888672

Final encoder loss: 0.020018137991428375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.461651086807251 0.07472038269042969

Final encoder loss: 0.02043180912733078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4609341621398926 0.07569599151611328

Final encoder loss: 0.02004978246986866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4714984893798828 0.07783102989196777

Final encoder loss: 0.019783668220043182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4732832908630371 0.0770411491394043

Final encoder loss: 0.020142195746302605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4673116207122803 0.08033394813537598

Final encoder loss: 0.019796667620539665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4726548194885254 0.07690572738647461

Final encoder loss: 0.019640125334262848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4685027599334717 0.07382321357727051

Final encoder loss: 0.01982230693101883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4688563346862793 0.08060765266418457

Final encoder loss: 0.019523175433278084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4728522300720215 0.0740056037902832

Final encoder loss: 0.019302820786833763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4689972400665283 0.07653093338012695

Final encoder loss: 0.019419768825173378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46819257736206055 0.08015680313110352

Final encoder loss: 0.019329272210597992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4709129333496094 0.0768117904663086

Final encoder loss: 0.019042177125811577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46924829483032227 0.0772697925567627

Final encoder loss: 0.019339490681886673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46576619148254395 0.07902693748474121

Final encoder loss: 0.019308732822537422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47579312324523926 0.0751643180847168

Final encoder loss: 0.018867718055844307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47028160095214844 0.07510828971862793

Final encoder loss: 0.01909361407160759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46567749977111816 0.08135509490966797

Final encoder loss: 0.019118761643767357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47379207611083984 0.0758209228515625

Final encoder loss: 0.018885402008891106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4657423496246338 0.07590913772583008

Final encoder loss: 0.019082576036453247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4658925533294678 0.08205008506774902

Final encoder loss: 0.019034018740057945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47452878952026367 0.07585620880126953

Final encoder loss: 0.018652400001883507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46601390838623047 0.07678842544555664

Final encoder loss: 0.018939204514026642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4668452739715576 0.07822704315185547

Final encoder loss: 0.018883055076003075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47243571281433105 0.07506418228149414

Final encoder loss: 0.01842358522117138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47223711013793945 0.0755147933959961

Final encoder loss: 0.018899748101830482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4652864933013916 0.07414937019348145

Final encoder loss: 0.018726658076047897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4732522964477539 0.07747459411621094

Final encoder loss: 0.01841607876121998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47044920921325684 0.07898497581481934

Final encoder loss: 0.01877767965197563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4663519859313965 0.07405996322631836

Final encoder loss: 0.018586883321404457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4711277484893799 0.07569503784179688

Final encoder loss: 0.01836925372481346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47212910652160645 0.07905268669128418

Final encoder loss: 0.018650345504283905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654405117034912 0.0753488540649414

Final encoder loss: 0.018687311559915543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46407103538513184 0.07594680786132812

Final encoder loss: 0.01833520643413067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4570629596710205 0.07562875747680664

Final encoder loss: 0.018546052277088165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4566628932952881 0.0751657485961914

Final encoder loss: 0.018642252311110497
Final encoder loss: 0.017318662256002426
Final encoder loss: 0.016827302053570747

Training dapper model
Final encoder loss: 0.016604969015562205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05954718589782715 0.10756516456604004

Final encoder loss: 0.016670789851467947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05988001823425293 0.1069650650024414

Final encoder loss: 0.015687233614498548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06069612503051758 0.10810494422912598

Final encoder loss: 0.017402180901924278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05994534492492676 0.10709190368652344

Final encoder loss: 0.015463367554909263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059622764587402344 0.1082923412322998

Final encoder loss: 0.016527639013847087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05989789962768555 0.1080467700958252

Final encoder loss: 0.016413283449407116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05988025665283203 0.10796546936035156

Final encoder loss: 0.016163315643490432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06020164489746094 0.1075890064239502

Final encoder loss: 0.01620258919242193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059946298599243164 0.10760164260864258

Final encoder loss: 0.013382505217204592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0606381893157959 0.10940980911254883

Final encoder loss: 0.015004833684690307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06115841865539551 0.10764336585998535

Final encoder loss: 0.015282517709864829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05992531776428223 0.10768866539001465

Final encoder loss: 0.01586082256329938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06008458137512207 0.10738015174865723

Final encoder loss: 0.014629989610479574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059914350509643555 0.10744833946228027

Final encoder loss: 0.01590235681269906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06028318405151367 0.10728192329406738

Final encoder loss: 0.016129346838789894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059244632720947266 0.10721111297607422


Training dapper model
Final encoder loss: 0.20244592428207397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11840510368347168 0.03441357612609863

Final encoder loss: 0.20819605886936188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11556005477905273 0.03374075889587402

Final encoder loss: 0.08475347608327866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11775636672973633 0.03431057929992676

Final encoder loss: 0.08634595572948456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11645793914794922 0.034520626068115234

Final encoder loss: 0.05020128935575485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11817717552185059 0.03436708450317383

Final encoder loss: 0.04966941103339195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11622786521911621 0.03421378135681152

Final encoder loss: 0.03440980985760689
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1161952018737793 0.03476309776306152

Final encoder loss: 0.033782731741666794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11655783653259277 0.034334659576416016

Final encoder loss: 0.026128210127353668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11680316925048828 0.03461194038391113

Final encoder loss: 0.025690214708447456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11655759811401367 0.03448295593261719

Final encoder loss: 0.021465424448251724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11525964736938477 0.03487133979797363

Final encoder loss: 0.0211439561098814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11803412437438965 0.03532600402832031

Final encoder loss: 0.018860643729567528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11576056480407715 0.03414416313171387

Final encoder loss: 0.018545901402831078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11712336540222168 0.03423762321472168

Final encoder loss: 0.017201675102114677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11643767356872559 0.033931732177734375

Final encoder loss: 0.017064891755580902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11678218841552734 0.034180641174316406

Final encoder loss: 0.01648467220366001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11669802665710449 0.03467512130737305

Final encoder loss: 0.01619882881641388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1183624267578125 0.03553485870361328

Final encoder loss: 0.01611686870455742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11615991592407227 0.03416037559509277

Final encoder loss: 0.015571457333862782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11513876914978027 0.03431582450866699

Final encoder loss: 0.015614689327776432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11678624153137207 0.03460955619812012

Final encoder loss: 0.015301648527383804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595797538757324 0.034734249114990234

Final encoder loss: 0.015514199621975422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11689043045043945 0.034621238708496094

Final encoder loss: 0.015294901095330715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563897132873535 0.03395557403564453

Final encoder loss: 0.015310675837099552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1173396110534668 0.03388333320617676

Final encoder loss: 0.015433951281011105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11727523803710938 0.034018754959106445

Final encoder loss: 0.01572456583380699
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11692070960998535 0.034622907638549805

Final encoder loss: 0.015274341218173504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11620926856994629 0.033689260482788086

Final encoder loss: 0.015189209952950478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11649274826049805 0.034891605377197266

Final encoder loss: 0.015165903605520725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11677861213684082 0.03370499610900879

Final encoder loss: 0.014788208529353142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11546659469604492 0.03472900390625

Final encoder loss: 0.014515329152345657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11563920974731445 0.03433799743652344

Final encoder loss: 0.013887764886021614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11516213417053223 0.03552436828613281

Final encoder loss: 0.014190257526934147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11491894721984863 0.03392601013183594

Final encoder loss: 0.013798288069665432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11462950706481934 0.03415489196777344

Final encoder loss: 0.013875382021069527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11508393287658691 0.0340723991394043

Final encoder loss: 0.013773541897535324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11522793769836426 0.033747196197509766

Final encoder loss: 0.013657422736287117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11497092247009277 0.03408455848693848

Final encoder loss: 0.01401545014232397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1153249740600586 0.033498287200927734

Final encoder loss: 0.01370073202997446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11495065689086914 0.0336000919342041

Final encoder loss: 0.013774555176496506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11487674713134766 0.034258127212524414

Final encoder loss: 0.013589604757726192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11555004119873047 0.03393745422363281

Final encoder loss: 0.0139089934527874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11489462852478027 0.03418684005737305

Final encoder loss: 0.013644818216562271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11517095565795898 0.03400826454162598

Final encoder loss: 0.01343795657157898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11542558670043945 0.03424406051635742

Final encoder loss: 0.013376479037106037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11504268646240234 0.03326749801635742

Final encoder loss: 0.013475250452756882
Final encoder loss: 0.012359664775431156

Training case model
Final encoder loss: 0.02408390712380379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08877444267272949 0.2183990478515625

Final encoder loss: 0.022736489936636513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08891415596008301 0.2185804843902588

Final encoder loss: 0.023091187746776657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08844494819641113 0.2189018726348877

Final encoder loss: 0.02243820705462284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08934783935546875 0.21913957595825195

Final encoder loss: 0.022077758613674715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08968448638916016 0.21901392936706543

Final encoder loss: 0.022774094914181513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08922338485717773 0.21913671493530273

Final encoder loss: 0.022539656561021727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08896017074584961 0.21883797645568848

Final encoder loss: 0.02287346187932821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.0895376205444336 0.21913862228393555

Final encoder loss: 0.022031836466215062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08937716484069824 0.21928739547729492

Final encoder loss: 0.022195379081437483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08904242515563965 0.2192699909210205

Final encoder loss: 0.02160039278461907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08969688415527344 0.21921396255493164

Final encoder loss: 0.022105556394686966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0899043083190918 0.21922969818115234

Final encoder loss: 0.022734048139424166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08940601348876953 0.21933555603027344

Final encoder loss: 0.022021446784843614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08879756927490234 0.21930432319641113

Final encoder loss: 0.022240621166693265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08897209167480469 0.21906042098999023

Final encoder loss: 0.021990066237199412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08616018295288086 0.21605277061462402


Training case model
Final encoder loss: 0.20295917987823486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26151061058044434 0.050972700119018555

Final encoder loss: 0.18890579044818878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25934314727783203 0.05212593078613281

Final encoder loss: 0.19014208018779755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2568776607513428 0.051186323165893555

Final encoder loss: 0.1921876072883606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25729870796203613 0.05155062675476074

Final encoder loss: 0.18081606924533844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2577822208404541 0.05312967300415039

Final encoder loss: 0.19192560017108917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255993127822876 0.05223393440246582

Final encoder loss: 0.10423421859741211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2595639228820801 0.05255699157714844

Final encoder loss: 0.09446336328983307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2592780590057373 0.05104470252990723

Final encoder loss: 0.09023819863796234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25693249702453613 0.051232337951660156

Final encoder loss: 0.08911767601966858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25730133056640625 0.05027413368225098

Final encoder loss: 0.08080099523067474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575411796569824 0.05219125747680664

Final encoder loss: 0.08470437675714493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25647997856140137 0.051401615142822266

Final encoder loss: 0.0607774592936039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257641077041626 0.0514681339263916

Final encoder loss: 0.055373139679431915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25835561752319336 0.052187204360961914

Final encoder loss: 0.053048912435770035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25751805305480957 0.0514373779296875

Final encoder loss: 0.05333796888589859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2578127384185791 0.051718950271606445

Final encoder loss: 0.049471717327833176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2569270133972168 0.052184104919433594

Final encoder loss: 0.05194497108459473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554492950439453 0.05242919921875

Final encoder loss: 0.042572181671857834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573366165161133 0.0520632266998291

Final encoder loss: 0.03959400951862335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25714850425720215 0.051405906677246094

Final encoder loss: 0.0380847193300724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577695846557617 0.05131387710571289

Final encoder loss: 0.03874567896127701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25734853744506836 0.05103588104248047

Final encoder loss: 0.036868028342723846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2570922374725342 0.05181694030761719

Final encoder loss: 0.03830759599804878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25496983528137207 0.05159473419189453

Final encoder loss: 0.034502074122428894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25742411613464355 0.051863908767700195

Final encoder loss: 0.03299332037568092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579522132873535 0.05129575729370117

Final encoder loss: 0.031890690326690674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25696420669555664 0.051698923110961914

Final encoder loss: 0.03252774849534035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576124668121338 0.05230402946472168

Final encoder loss: 0.03218762204051018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25793957710266113 0.052104949951171875

Final encoder loss: 0.03268952667713165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25530147552490234 0.051372528076171875

Final encoder loss: 0.031016238033771515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25765252113342285 0.05147957801818848

Final encoder loss: 0.030247218906879425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257474422454834 0.0520632266998291

Final encoder loss: 0.029696809127926826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2567563056945801 0.05127692222595215

Final encoder loss: 0.0300814900547266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571070194244385 0.05137157440185547

Final encoder loss: 0.03063310869038105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25653910636901855 0.05176734924316406

Final encoder loss: 0.030396202579140663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546095848083496 0.05172133445739746

Final encoder loss: 0.028749922290444374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25819969177246094 0.05179929733276367

Final encoder loss: 0.028241952881217003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573094367980957 0.05114459991455078

Final encoder loss: 0.027864951640367508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25721287727355957 0.05045294761657715

Final encoder loss: 0.02798178233206272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257521390914917 0.05199456214904785

Final encoder loss: 0.028690651059150696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2569103240966797 0.05198240280151367

Final encoder loss: 0.02833467349410057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552971839904785 0.05150127410888672

Final encoder loss: 0.026995263993740082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569851875305176 0.05187368392944336

Final encoder loss: 0.02660868503153324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25719285011291504 0.052429914474487305

Final encoder loss: 0.02631434239447117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25782227516174316 0.05262279510498047

Final encoder loss: 0.026460036635398865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572059631347656 0.05126595497131348

Final encoder loss: 0.027104245498776436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25724005699157715 0.051938533782958984

Final encoder loss: 0.026855429634451866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548534870147705 0.05077958106994629

Final encoder loss: 0.025925928726792336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25728869438171387 0.051940202713012695

Final encoder loss: 0.025746650993824005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25647401809692383 0.05098843574523926

Final encoder loss: 0.025334257632493973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584264278411865 0.05101943016052246

Final encoder loss: 0.025481849908828735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25627970695495605 0.05166220664978027

Final encoder loss: 0.026440098881721497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25652456283569336 0.0514066219329834

Final encoder loss: 0.025777718052268028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543468475341797 0.05047249794006348

Final encoder loss: 0.025483515113592148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578551769256592 0.05147838592529297

Final encoder loss: 0.0252024345099926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25753235816955566 0.051697731018066406

Final encoder loss: 0.024913838133215904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25789737701416016 0.05144071578979492

Final encoder loss: 0.02498544007539749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25677013397216797 0.0517122745513916

Final encoder loss: 0.025863224640488625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25692272186279297 0.052063703536987305

Final encoder loss: 0.025342363864183426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550187110900879 0.05136299133300781

Final encoder loss: 0.024761036038398743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25710439682006836 0.052106380462646484

Final encoder loss: 0.024600418284535408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2560727596282959 0.05110812187194824

Final encoder loss: 0.024270733818411827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2589597702026367 0.05059456825256348

Final encoder loss: 0.024318989366292953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25672221183776855 0.051962852478027344

Final encoder loss: 0.02527405507862568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25624895095825195 0.05267667770385742

Final encoder loss: 0.024705789983272552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25475525856018066 0.05019664764404297

Final encoder loss: 0.024467550218105316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25775694847106934 0.052034854888916016

Final encoder loss: 0.02424410544335842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25660109519958496 0.05128312110900879

Final encoder loss: 0.024043278768658638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2569446563720703 0.05164170265197754

Final encoder loss: 0.023878201842308044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25655698776245117 0.0517888069152832

Final encoder loss: 0.024749066680669785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256284236907959 0.05126023292541504

Final encoder loss: 0.02437230572104454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565419673919678 0.05016922950744629

Final encoder loss: 0.02405938133597374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25687313079833984 0.05292224884033203

Final encoder loss: 0.023839488625526428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25666356086730957 0.05226707458496094

Final encoder loss: 0.0235635694116354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25771045684814453 0.05158710479736328

Final encoder loss: 0.023640312254428864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25814223289489746 0.051634788513183594

Final encoder loss: 0.024542899802327156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25655627250671387 0.05106806755065918

Final encoder loss: 0.02396988309919834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25433993339538574 0.050859928131103516

Final encoder loss: 0.023833012208342552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580246925354004 0.05141496658325195

Final encoder loss: 0.02373889833688736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25664758682250977 0.05304217338562012

Final encoder loss: 0.023388801142573357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25598573684692383 0.051522016525268555

Final encoder loss: 0.02339187264442444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571430206298828 0.052675485610961914

Final encoder loss: 0.024215303361415863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25679492950439453 0.05057644844055176

Final encoder loss: 0.023770388215780258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544584274291992 0.05042600631713867

Final encoder loss: 0.02351265214383602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2568819522857666 0.052741289138793945

Final encoder loss: 0.02345285937190056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25853562355041504 0.05194091796875

Final encoder loss: 0.023153085261583328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2663533687591553 0.052021026611328125

Final encoder loss: 0.0230253916233778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25673627853393555 0.05138278007507324

Final encoder loss: 0.02407422661781311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2569570541381836 0.05215883255004883

Final encoder loss: 0.023469988256692886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25436830520629883 0.05128955841064453

Final encoder loss: 0.023458918556571007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577507495880127 0.052184343338012695

Final encoder loss: 0.023223519325256348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566256523132324 0.051073312759399414

Final encoder loss: 0.02309187687933445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2567601203918457 0.05220675468444824

Final encoder loss: 0.02292194962501526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25688791275024414 0.05216836929321289

Final encoder loss: 0.023694731295108795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2567741870880127 0.0516972541809082

Final encoder loss: 0.023348895832896233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537729740142822 0.05096077919006348

Final encoder loss: 0.023136477917432785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25733017921447754 0.05217146873474121

Final encoder loss: 0.023036181926727295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25702714920043945 0.05224132537841797

Final encoder loss: 0.02268890291452408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25650715827941895 0.05255770683288574

Final encoder loss: 0.02270137518644333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2619590759277344 0.051529645919799805

Final encoder loss: 0.023595772683620453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575080394744873 0.05352330207824707

Final encoder loss: 0.023098763078451157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2576937675476074 0.0510716438293457

Final encoder loss: 0.023142918944358826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2594623565673828 0.051732540130615234

Final encoder loss: 0.023062866181135178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2607433795928955 0.05182170867919922

Final encoder loss: 0.022738348692655563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2579944133758545 0.053025007247924805

Final encoder loss: 0.022598715499043465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2586197853088379 0.05491948127746582

Final encoder loss: 0.02342645265161991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25736451148986816 0.051783084869384766

Final encoder loss: 0.022998526692390442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25733423233032227 0.052657365798950195

Final encoder loss: 0.02294272929430008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573275566101074 0.052651405334472656

Final encoder loss: 0.02270270325243473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26079678535461426 0.0527348518371582

Final encoder loss: 0.022499268874526024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25745439529418945 0.05288219451904297

Final encoder loss: 0.02238980308175087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26051998138427734 0.05335378646850586

Final encoder loss: 0.0233328677713871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578589916229248 0.05267667770385742

Final encoder loss: 0.022832345217466354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25646400451660156 0.05268573760986328

Final encoder loss: 0.02284371294081211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2596707344055176 0.05292844772338867

Final encoder loss: 0.022748054936528206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25694918632507324 0.05286097526550293

Final encoder loss: 0.02250920608639717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25774121284484863 0.05080890655517578

Final encoder loss: 0.022310009226202965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25662970542907715 0.05544900894165039

Final encoder loss: 0.023089703172445297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26009178161621094 0.05221152305603027

Final encoder loss: 0.02270946279168129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544736862182617 0.052658796310424805

Final encoder loss: 0.022695941850543022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25942540168762207 0.05164027214050293

Final encoder loss: 0.022560017183423042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2576169967651367 0.05275869369506836

Final encoder loss: 0.02223467081785202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2594566345214844 0.05166339874267578

Final encoder loss: 0.022144168615341187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590019702911377 0.051700592041015625

Final encoder loss: 0.02301626093685627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2594418525695801 0.051860809326171875

Final encoder loss: 0.02262031100690365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25639772415161133 0.05175328254699707

Final encoder loss: 0.022657783702015877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2627274990081787 0.05130743980407715

Final encoder loss: 0.022562919184565544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588465213775635 0.05142831802368164

Final encoder loss: 0.022276952862739563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26036953926086426 0.051206111907958984

Final encoder loss: 0.022107215598225594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2565653324127197 0.05290699005126953

Final encoder loss: 0.022947857156395912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2607860565185547 0.05230212211608887

Final encoder loss: 0.022482285276055336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25493645668029785 0.050951480865478516

Final encoder loss: 0.022555692121386528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25997090339660645 0.05563807487487793

Final encoder loss: 0.022327259182929993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25838327407836914 0.051851511001586914

Final encoder loss: 0.022149469703435898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25863122940063477 0.05413246154785156

Final encoder loss: 0.021967243403196335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25713062286376953 0.05067324638366699

Final encoder loss: 0.022893015295267105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2597506046295166 0.05413699150085449

Final encoder loss: 0.022412579506635666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2575078010559082 0.05068635940551758

Final encoder loss: 0.022472137585282326
Final encoder loss: 0.021945081651210785
Final encoder loss: 0.021135522052645683
Final encoder loss: 0.020257899537682533
Final encoder loss: 0.02017386630177498
Final encoder loss: 0.019150199368596077

Training emognition model
Final encoder loss: 0.026057235018946565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08257937431335449 0.23137640953063965

Final encoder loss: 0.026678038775768215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08158421516418457 0.23103618621826172

Final encoder loss: 0.026244383514050285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0809791088104248 0.23175048828125

Final encoder loss: 0.026058220802611377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08253717422485352 0.23150110244750977

Final encoder loss: 0.025858281476892477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08179116249084473 0.23073220252990723

Final encoder loss: 0.026121467849893793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08113217353820801 0.23109197616577148

Final encoder loss: 0.026764065656457506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08397507667541504 0.23090124130249023

Final encoder loss: 0.025807811336362094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08166098594665527 0.23056292533874512

Final encoder loss: 0.024860263577075437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08248090744018555 0.231367826461792

Final encoder loss: 0.02539236857181555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08121037483215332 0.23036432266235352

Final encoder loss: 0.025808497586000467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08108353614807129 0.23079609870910645

Final encoder loss: 0.024711237114727055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08280754089355469 0.2306835651397705

Final encoder loss: 0.026558688016176262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0809485912322998 0.23044395446777344

Final encoder loss: 0.026856768599019403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08156609535217285 0.2313988208770752

Final encoder loss: 0.02567112610788957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08283758163452148 0.23119711875915527

Final encoder loss: 0.026545504766844585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08103609085083008 0.23003840446472168


Training emognition model
Final encoder loss: 0.19356831908226013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25324034690856934 0.05061626434326172

Final encoder loss: 0.19496287405490875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24816203117370605 0.04818224906921387

Final encoder loss: 0.08832314610481262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25133752822875977 0.050252676010131836

Final encoder loss: 0.08809603005647659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24744892120361328 0.04903554916381836

Final encoder loss: 0.0562579520046711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.248765230178833 0.05022716522216797

Final encoder loss: 0.05502759665250778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24936938285827637 0.04883837699890137

Final encoder loss: 0.04187775403261185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2491140365600586 0.049369096755981445

Final encoder loss: 0.041013844311237335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.248917818069458 0.049466848373413086

Final encoder loss: 0.03455064073204994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24984169006347656 0.04962015151977539

Final encoder loss: 0.03402479365468025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24977636337280273 0.04798555374145508

Final encoder loss: 0.030486809089779854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24964356422424316 0.04901909828186035

Final encoder loss: 0.030128277838230133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24854683876037598 0.05155444145202637

Final encoder loss: 0.028160762041807175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24982333183288574 0.04863619804382324

Final encoder loss: 0.027921652421355247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24917197227478027 0.04988288879394531

Final encoder loss: 0.026914220303297043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24814724922180176 0.048491716384887695

Final encoder loss: 0.026863880455493927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24902677536010742 0.05154228210449219

Final encoder loss: 0.02627730742096901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2488563060760498 0.049353599548339844

Final encoder loss: 0.02646058425307274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24886345863342285 0.05056452751159668

Final encoder loss: 0.026177087798714638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24898386001586914 0.04904913902282715

Final encoder loss: 0.02630453184247017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24732756614685059 0.049813032150268555

Final encoder loss: 0.025816382840275764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25124311447143555 0.048452138900756836

Final encoder loss: 0.026068301871418953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24882006645202637 0.0489497184753418

Final encoder loss: 0.025608591735363007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25020432472229004 0.04889273643493652

Final encoder loss: 0.025762204080820084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2478194236755371 0.04915046691894531

Final encoder loss: 0.025198405608534813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24902129173278809 0.050873756408691406

Final encoder loss: 0.025429051369428635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24831581115722656 0.050490617752075195

Final encoder loss: 0.02506391890347004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2503533363342285 0.0507814884185791

Final encoder loss: 0.025317443534731865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24850106239318848 0.049922943115234375

Final encoder loss: 0.02495322749018669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24881553649902344 0.051454782485961914

Final encoder loss: 0.025160377845168114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24896812438964844 0.0500025749206543

Final encoder loss: 0.024959081783890724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2503180503845215 0.04955768585205078

Final encoder loss: 0.025055570527911186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24791359901428223 0.04933619499206543

Final encoder loss: 0.02471727505326271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24931001663208008 0.04929471015930176

Final encoder loss: 0.024983461946249008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24962329864501953 0.0494227409362793

Final encoder loss: 0.02461753785610199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24889087677001953 0.04923605918884277

Final encoder loss: 0.024764973670244217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24955272674560547 0.052310943603515625

Final encoder loss: 0.024399718269705772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24671053886413574 0.049385786056518555

Final encoder loss: 0.02458580769598484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25036048889160156 0.052052974700927734

Final encoder loss: 0.02440194971859455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24884343147277832 0.04941511154174805

Final encoder loss: 0.024643616750836372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24885797500610352 0.049445152282714844

Final encoder loss: 0.02431831881403923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24778509140014648 0.048981666564941406

Final encoder loss: 0.02447747066617012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24869728088378906 0.05002284049987793

Final encoder loss: 0.0242167841643095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502124309539795 0.04903817176818848

Final encoder loss: 0.02444642409682274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24829840660095215 0.04982781410217285

Final encoder loss: 0.02407117560505867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25196194648742676 0.04909324645996094

Final encoder loss: 0.024375954642891884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24843311309814453 0.04986906051635742

Final encoder loss: 0.02405729703605175
Final encoder loss: 0.023465504869818687

Training empatch model
Final encoder loss: 0.03627489259504111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07103919982910156 0.1739213466644287

Final encoder loss: 0.03587565842362461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07159805297851562 0.1744232177734375

Final encoder loss: 0.034195304102795525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07359099388122559 0.17374205589294434

Final encoder loss: 0.0335880373182716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07159018516540527 0.17416810989379883

Final encoder loss: 0.03249421346979106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.071624755859375 0.17408180236816406

Final encoder loss: 0.03171669588107707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07271575927734375 0.17413854598999023

Final encoder loss: 0.03547520756981017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07104992866516113 0.17585062980651855

Final encoder loss: 0.030931305332595478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07115411758422852 0.17342567443847656

Final encoder loss: 0.02393758866507621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07101106643676758 0.17394661903381348

Final encoder loss: 0.02452663426121427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07361268997192383 0.1739344596862793

Final encoder loss: 0.024728489944635612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07129812240600586 0.17461133003234863

Final encoder loss: 0.023350063467100463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0713651180267334 0.1741957664489746

Final encoder loss: 0.023968259226267715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07282495498657227 0.17394566535949707

Final encoder loss: 0.026229899156016447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07082939147949219 0.17344069480895996

Final encoder loss: 0.02505684090969307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0714864730834961 0.17383933067321777

Final encoder loss: 0.02367447962272735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0706930160522461 0.17416954040527344


Training empatch model
Final encoder loss: 0.17115089297294617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17772412300109863 0.04227781295776367

Final encoder loss: 0.08053173869848251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17644882202148438 0.04353165626525879

Final encoder loss: 0.055250346660614014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17754530906677246 0.04443788528442383

Final encoder loss: 0.04292171820998192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17552852630615234 0.04369497299194336

Final encoder loss: 0.03581186383962631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1743783950805664 0.04389381408691406

Final encoder loss: 0.031340718269348145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17563748359680176 0.04491925239562988

Final encoder loss: 0.028387291356921196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17734718322753906 0.04310107231140137

Final encoder loss: 0.02643759176135063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17529892921447754 0.04352068901062012

Final encoder loss: 0.02502819336950779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746068000793457 0.044544219970703125

Final encoder loss: 0.02414019964635372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17661428451538086 0.043242454528808594

Final encoder loss: 0.023481100797653198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17543601989746094 0.04330015182495117

Final encoder loss: 0.023083340376615524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17486953735351562 0.044853925704956055

Final encoder loss: 0.02274034544825554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.178544282913208 0.04269981384277344

Final encoder loss: 0.022530987858772278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.174102783203125 0.04429483413696289

Final encoder loss: 0.022285794839262962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1754610538482666 0.04401135444641113

Final encoder loss: 0.02205662801861763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17667198181152344 0.043357133865356445

Final encoder loss: 0.021846245974302292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17528867721557617 0.0435793399810791

Final encoder loss: 0.021694378927350044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17411541938781738 0.04293370246887207

Final encoder loss: 0.02156057022511959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1772305965423584 0.04356718063354492

Final encoder loss: 0.02152770757675171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17481660842895508 0.04312920570373535

Final encoder loss: 0.02145470678806305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17515063285827637 0.04253363609313965

Final encoder loss: 0.021417153999209404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17727875709533691 0.04538154602050781

Final encoder loss: 0.021268850192427635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744384765625 0.043701171875

Final encoder loss: 0.02128298208117485

Training wesad model
Final encoder loss: 0.033512202835545526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.08319568634033203 0.17421746253967285

Final encoder loss: 0.038161611330336506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07274889945983887 0.17378497123718262

Final encoder loss: 0.036491425029677074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07146406173706055 0.17374610900878906

Final encoder loss: 0.03414340789796098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07080316543579102 0.17423295974731445

Final encoder loss: 0.024713012795303507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0724329948425293 0.17385649681091309

Final encoder loss: 0.022350613823148704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07050395011901855 0.17366313934326172

Final encoder loss: 0.022964463485286114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07113051414489746 0.17374658584594727

Final encoder loss: 0.02336256159702142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07079458236694336 0.17407798767089844

Final encoder loss: 0.018109780268165628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07220268249511719 0.17359709739685059

Final encoder loss: 0.018378545345685914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07119989395141602 0.17388439178466797

Final encoder loss: 0.018312404950124633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07304573059082031 0.17449402809143066

Final encoder loss: 0.018284544791794165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0711672306060791 0.1736161708831787

Final encoder loss: 0.01510871533512269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0709371566772461 0.17434191703796387

Final encoder loss: 0.014595133573219484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07229948043823242 0.17357540130615234

Final encoder loss: 0.014730195497911306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07021903991699219 0.17255878448486328

Final encoder loss: 0.01574514319127484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07005047798156738 0.17237472534179688


Training wesad model
Final encoder loss: 0.21559366583824158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1052241325378418 0.0331578254699707

Final encoder loss: 0.0992271900177002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10430288314819336 0.03303670883178711

Final encoder loss: 0.06375965476036072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10380935668945312 0.032819509506225586

Final encoder loss: 0.04617168381810188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10343074798583984 0.0325927734375

Final encoder loss: 0.03602982312440872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10396790504455566 0.03458690643310547

Final encoder loss: 0.029771696776151657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10326886177062988 0.032971858978271484

Final encoder loss: 0.025748834013938904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10401797294616699 0.03305768966674805

Final encoder loss: 0.023059990257024765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10322070121765137 0.033203125

Final encoder loss: 0.02124219946563244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388588905334473 0.03329968452453613

Final encoder loss: 0.02000543661415577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10322999954223633 0.03304100036621094

Final encoder loss: 0.019175482913851738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10327005386352539 0.03269505500793457

Final encoder loss: 0.018605904653668404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10355186462402344 0.03326559066772461

Final encoder loss: 0.018233710899949074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10396814346313477 0.03306984901428223

Final encoder loss: 0.018060971051454544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1033332347869873 0.033364295959472656

Final encoder loss: 0.018065722659230232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10326647758483887 0.032482147216796875

Final encoder loss: 0.018138261511921883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10381269454956055 0.033205509185791016

Final encoder loss: 0.01811622641980648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10381007194519043 0.03324723243713379

Final encoder loss: 0.01799754612147808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10325860977172852 0.03312826156616211

Final encoder loss: 0.017873313277959824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375404357910156 0.03291130065917969

Final encoder loss: 0.01786416582763195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10436391830444336 0.03332638740539551

Final encoder loss: 0.017852121964097023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10349845886230469 0.03355717658996582

Final encoder loss: 0.017843615263700485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1040337085723877 0.033487558364868164

Final encoder loss: 0.017855960875749588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10446715354919434 0.03336334228515625

Final encoder loss: 0.01791469380259514

Calculating loss for amigos model
	Full Pass 0.6996877193450928
numFreeParamsPath 18
Reconstruction loss values: 0.025956926867365837 0.035660214722156525

Calculating loss for dapper model
	Full Pass 0.1524953842163086
numFreeParamsPath 18
Reconstruction loss values: 0.022698579356074333 0.024979008361697197

Calculating loss for case model
	Full Pass 0.9097120761871338
numFreeParamsPath 18
Reconstruction loss values: 0.03136199340224266 0.03483383730053902

Calculating loss for emognition model
	Full Pass 0.2912280559539795
numFreeParamsPath 18
Reconstruction loss values: 0.03395570442080498 0.04152834415435791

Calculating loss for empatch model
	Full Pass 0.10498523712158203
numFreeParamsPath 18
Reconstruction loss values: 0.035108767449855804 0.04198407754302025

Calculating loss for wesad model
	Full Pass 0.07746481895446777
numFreeParamsPath 18
Reconstruction loss values: 0.03424622863531113 0.051715187728405
Total loss calculation time: 4.462434530258179

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.105783462524414
Total epoch time: 216.36177778244019

Epoch: 47

Training case model
Final encoder loss: 0.030486350040761594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09888052940368652 0.2748277187347412

Final encoder loss: 0.02825568183510023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09133267402648926 0.2652268409729004

Final encoder loss: 0.027706001121264184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09183287620544434 0.2646782398223877

Final encoder loss: 0.026632391711297828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09153056144714355 0.26517343521118164

Final encoder loss: 0.02636719446787039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09177231788635254 0.2650895118713379

Final encoder loss: 0.02629986559905212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09127664566040039 0.26568007469177246

Final encoder loss: 0.026338714292600512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09152412414550781 0.26515936851501465

Final encoder loss: 0.025127183020714945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09176754951477051 0.26578736305236816

Final encoder loss: 0.025920412618283657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09142327308654785 0.2640964984893799

Final encoder loss: 0.02527045073357029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09083223342895508 0.2642965316772461

Final encoder loss: 0.02527700174698998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09176397323608398 0.2637367248535156

Final encoder loss: 0.02533507608687715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09329009056091309 0.2658371925354004

Final encoder loss: 0.024905169653166786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09212255477905273 0.26405930519104004

Final encoder loss: 0.02484307648110551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09109306335449219 0.26412463188171387

Final encoder loss: 0.025235089522219912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09151387214660645 0.26369595527648926

Final encoder loss: 0.024728094377805744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08796000480651855 0.26175403594970703


Training amigos model
Final encoder loss: 0.026662230192036694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10805630683898926 0.38829922676086426

Final encoder loss: 0.025822276002449743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10750699043273926 0.3892195224761963

Final encoder loss: 0.024451122439697277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10815215110778809 0.389085054397583

Final encoder loss: 0.025954086195289792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10849714279174805 0.38900184631347656

Final encoder loss: 0.025683552162840458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10852289199829102 0.38967370986938477

Final encoder loss: 0.025596100641316254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10900425910949707 0.38975095748901367

Final encoder loss: 0.02529462319479022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.11014914512634277 0.3906848430633545

Final encoder loss: 0.02612902288303732
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10914993286132812 0.39037346839904785

Final encoder loss: 0.025707251330125924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10809636116027832 0.3898634910583496

Final encoder loss: 0.024663403060836808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10814905166625977 0.38939809799194336

Final encoder loss: 0.02472501977462821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10810160636901855 0.3893723487854004

Final encoder loss: 0.02660770686141154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10868597030639648 0.39022231101989746

Final encoder loss: 0.025533229009357238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10874676704406738 0.38968372344970703

Final encoder loss: 0.021886092660226134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10969781875610352 0.38928818702697754

Final encoder loss: 0.025825888777393002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10919690132141113 0.39043283462524414

Final encoder loss: 0.025749200440216682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10335516929626465 0.3846096992492676


Training emognition model
Final encoder loss: 0.034460080524488214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08362197875976562 0.27501964569091797

Final encoder loss: 0.034017598549050676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08309793472290039 0.2755708694458008

Final encoder loss: 0.0328295009929519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08389949798583984 0.2763369083404541

Final encoder loss: 0.03276592350899087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08343815803527832 0.2755873203277588

Final encoder loss: 0.03502038893441946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08494234085083008 0.2754685878753662

Final encoder loss: 0.032752852042264746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0838005542755127 0.27504873275756836

Final encoder loss: 0.03167121143457685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08378481864929199 0.27587056159973145

Final encoder loss: 0.032526906783330145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0843510627746582 0.2766385078430176

Final encoder loss: 0.03199583415828274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08378767967224121 0.27570652961730957

Final encoder loss: 0.032784084699170185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08452343940734863 0.2752723693847656

Final encoder loss: 0.03155939647103339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0836634635925293 0.2758011817932129

Final encoder loss: 0.03181739850834383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08400607109069824 0.2762448787689209

Final encoder loss: 0.03272943897456446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08509540557861328 0.27591896057128906

Final encoder loss: 0.032394054616727196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08324766159057617 0.275862455368042

Final encoder loss: 0.030865138466609727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0833275318145752 0.27574682235717773

Final encoder loss: 0.03139529516414533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08345484733581543 0.2752351760864258


Training dapper model
Final encoder loss: 0.024308211621715838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061922311782836914 0.15013551712036133

Final encoder loss: 0.02177371214760309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.062288761138916016 0.15226411819458008

Final encoder loss: 0.02260316796433348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06255030632019043 0.15076422691345215

Final encoder loss: 0.022527727734935728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06223869323730469 0.15093040466308594

Final encoder loss: 0.019694926626271984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0639491081237793 0.15120744705200195

Final encoder loss: 0.01881193732970336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.062380313873291016 0.15063810348510742

Final encoder loss: 0.021534128685325213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062445640563964844 0.15132546424865723

Final encoder loss: 0.019445087838816447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06357002258300781 0.15076899528503418

Final encoder loss: 0.02007236197618343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06247758865356445 0.15079855918884277

Final encoder loss: 0.01837656878813526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.062184810638427734 0.15252232551574707

Final encoder loss: 0.02050540723185181
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06261897087097168 0.15075182914733887

Final encoder loss: 0.022220927087792625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06252574920654297 0.1502845287322998

Final encoder loss: 0.02098941983978079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06278300285339355 0.15168380737304688

Final encoder loss: 0.02074210208876879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06222176551818848 0.15182256698608398

Final encoder loss: 0.01934978559532247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06218528747558594 0.15064239501953125

Final encoder loss: 0.016932399516158304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06277680397033691 0.15079307556152344


Training amigos model
Final encoder loss: 0.019319824206212716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10669469833374023 0.34171509742736816

Final encoder loss: 0.02047072794601856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1060025691986084 0.34194326400756836

Final encoder loss: 0.02016599028174093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.1075754165649414 0.3417623043060303

Final encoder loss: 0.01885192600725225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10593080520629883 0.3417623043060303

Final encoder loss: 0.021017458793403342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10615682601928711 0.3429884910583496

Final encoder loss: 0.019117741578546264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10660123825073242 0.34171533584594727

Final encoder loss: 0.019067430282668656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.12495827674865723 0.34140467643737793

Final encoder loss: 0.019961923302200705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10682821273803711 0.3437230587005615

Final encoder loss: 0.019013987722216397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.1063375473022461 0.3415660858154297

Final encoder loss: 0.018856206063570136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10672998428344727 0.34173083305358887

Final encoder loss: 0.020132226625952664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10651564598083496 0.3422384262084961

Final encoder loss: 0.019686205273709467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.1075291633605957 0.34154248237609863

Final encoder loss: 0.021168040395724397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.106414794921875 0.34176039695739746

Final encoder loss: 0.02040237964819432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10709953308105469 0.3419768810272217

Final encoder loss: 0.020124218984112405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10687375068664551 0.34184718132019043

Final encoder loss: 0.020387399352978054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10183858871459961 0.33780574798583984


Training amigos model
Final encoder loss: 0.18075230717658997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47012877464294434 0.08238482475280762

Final encoder loss: 0.1878325194120407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4668772220611572 0.07629704475402832

Final encoder loss: 0.1836315542459488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4718320369720459 0.07541036605834961

Final encoder loss: 0.07649126648902893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46422243118286133 0.07660460472106934

Final encoder loss: 0.07884114235639572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4647643566131592 0.07998299598693848

Final encoder loss: 0.07240685820579529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4688105583190918 0.0737152099609375

Final encoder loss: 0.04462164267897606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46608710289001465 0.07657694816589355

Final encoder loss: 0.04561476409435272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4625856876373291 0.07562994956970215

Final encoder loss: 0.04230760782957077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4674415588378906 0.0812077522277832

Final encoder loss: 0.03202059119939804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4685699939727783 0.07555675506591797

Final encoder loss: 0.03286384418606758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4659583568572998 0.07686042785644531

Final encoder loss: 0.03111099638044834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4672386646270752 0.07662200927734375

Final encoder loss: 0.02620002068579197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4643716812133789 0.08020544052124023

Final encoder loss: 0.026991980150341988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4705805778503418 0.07748126983642578

Final encoder loss: 0.02595561183989048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4654698371887207 0.07378530502319336

Final encoder loss: 0.023555656895041466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4656360149383545 0.0760343074798584

Final encoder loss: 0.024313492700457573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46451878547668457 0.07831811904907227

Final encoder loss: 0.02341497130692005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4712646007537842 0.07701468467712402

Final encoder loss: 0.022654671221971512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4630777835845947 0.07559990882873535

Final encoder loss: 0.02325388416647911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4663965702056885 0.08268499374389648

Final encoder loss: 0.02269945666193962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46791553497314453 0.07528543472290039

Final encoder loss: 0.022567545995116234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46819567680358887 0.07591652870178223

Final encoder loss: 0.02284969761967659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46353840827941895 0.07608580589294434

Final encoder loss: 0.0224150400608778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4670596122741699 0.07990550994873047

Final encoder loss: 0.021871624514460564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46680283546447754 0.07382059097290039

Final encoder loss: 0.021970394998788834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4665110111236572 0.07675647735595703

Final encoder loss: 0.02181321196258068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46483325958251953 0.07584786415100098

Final encoder loss: 0.02093660645186901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46412134170532227 0.07948184013366699

Final encoder loss: 0.021199891343712807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46961307525634766 0.07696747779846191

Final encoder loss: 0.021166320890188217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46852922439575195 0.07530546188354492

Final encoder loss: 0.020533928647637367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4667625427246094 0.0770406723022461

Final encoder loss: 0.020683689042925835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4645869731903076 0.0793607234954834

Final encoder loss: 0.0206292811781168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4732675552368164 0.07650995254516602

Final encoder loss: 0.020345930010080338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4650452136993408 0.07461833953857422

Final encoder loss: 0.020278804004192352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4651179313659668 0.07660126686096191

Final encoder loss: 0.02032781019806862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4672863483428955 0.07732772827148438

Final encoder loss: 0.02007070928812027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4715843200683594 0.0767672061920166

Final encoder loss: 0.019858578220009804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.463146448135376 0.07638430595397949

Final encoder loss: 0.020027058199048042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4674370288848877 0.07999277114868164

Final encoder loss: 0.01975352317094803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4653356075286865 0.07475686073303223

Final encoder loss: 0.019517676904797554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4676704406738281 0.07656359672546387

Final encoder loss: 0.019931690767407417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4660987854003906 0.07500004768371582

Final encoder loss: 0.019542180001735687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4667787551879883 0.07980847358703613

Final encoder loss: 0.019282430410385132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46695661544799805 0.07719087600708008

Final encoder loss: 0.019658494740724564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4698328971862793 0.07550716400146484

Final encoder loss: 0.01939166523516178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46103715896606445 0.07538771629333496

Final encoder loss: 0.019139062613248825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46529722213745117 0.08037042617797852

Final encoder loss: 0.01941562257707119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47136449813842773 0.07536196708679199

Final encoder loss: 0.019207125529646873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4655263423919678 0.07682180404663086

Final encoder loss: 0.018949050456285477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4678218364715576 0.07608842849731445

Final encoder loss: 0.019372208043932915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46613144874572754 0.08124732971191406

Final encoder loss: 0.018908221274614334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46909499168395996 0.07225322723388672

Final encoder loss: 0.018764089792966843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4622926712036133 0.07713603973388672

Final encoder loss: 0.01918628253042698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4694027900695801 0.07550406455993652

Final encoder loss: 0.018887866288423538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46373724937438965 0.08102989196777344

Final encoder loss: 0.018521541729569435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46923065185546875 0.07763862609863281

Final encoder loss: 0.018989969044923782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4661405086517334 0.0750584602355957

Final encoder loss: 0.018751565366983414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.464158296585083 0.07666301727294922

Final encoder loss: 0.018475472927093506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46488428115844727 0.07533526420593262

Final encoder loss: 0.01900060661137104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47027134895324707 0.07579255104064941

Final encoder loss: 0.018766386434435844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4647841453552246 0.07488346099853516

Final encoder loss: 0.01852482743561268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4645359516143799 0.08045744895935059

Final encoder loss: 0.018798375502228737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46845316886901855 0.0751655101776123

Final encoder loss: 0.018560705706477165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4677159786224365 0.07520246505737305

Final encoder loss: 0.01847761496901512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4615969657897949 0.0745851993560791

Final encoder loss: 0.01878487691283226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46938037872314453 0.08200311660766602

Final encoder loss: 0.01862076111137867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46790289878845215 0.0758814811706543

Final encoder loss: 0.018148956820368767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4669651985168457 0.07682490348815918

Final encoder loss: 0.018753832206130028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.467052698135376 0.08045816421508789

Final encoder loss: 0.018559813499450684
Final encoder loss: 0.01739496737718582
Final encoder loss: 0.01697634533047676

Training dapper model
Final encoder loss: 0.01715110902332083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.05963730812072754 0.10637855529785156

Final encoder loss: 0.015202176066329943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.05911111831665039 0.10608315467834473

Final encoder loss: 0.017131981481861666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05920577049255371 0.10596847534179688

Final encoder loss: 0.01661872500076826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06043839454650879 0.10677075386047363

Final encoder loss: 0.01523072482044926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.058678627014160156 0.10590362548828125

Final encoder loss: 0.015200127172473802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059203386306762695 0.10633063316345215

Final encoder loss: 0.015957308297060612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0590665340423584 0.10655689239501953

Final encoder loss: 0.014999408361117818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05908560752868652 0.10597634315490723

Final encoder loss: 0.015168574296225586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05911850929260254 0.10618233680725098

Final encoder loss: 0.01539898220116413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05912375450134277 0.10593914985656738

Final encoder loss: 0.016108851309475068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05919909477233887 0.10630416870117188

Final encoder loss: 0.01628190179457006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05891990661621094 0.10588479042053223

Final encoder loss: 0.015016009289001844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05859494209289551 0.10646939277648926

Final encoder loss: 0.01663516248285439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.059198856353759766 0.10629010200500488

Final encoder loss: 0.014764200793986092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05923032760620117 0.10669589042663574

Final encoder loss: 0.013526417186746511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05912899971008301 0.10564041137695312


Training dapper model
Final encoder loss: 0.20244601368904114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11761927604675293 0.033859968185424805

Final encoder loss: 0.20822373032569885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11553525924682617 0.03329634666442871

Final encoder loss: 0.08462464809417725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11536669731140137 0.03419637680053711

Final encoder loss: 0.08662760257720947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11487007141113281 0.03574037551879883

Final encoder loss: 0.05036032199859619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11556744575500488 0.03325486183166504

Final encoder loss: 0.05004354193806648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11511707305908203 0.03377556800842285

Final encoder loss: 0.034628916531801224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11683201789855957 0.03397655487060547

Final encoder loss: 0.033998142927885056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11590266227722168 0.03441119194030762

Final encoder loss: 0.02642320655286312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.118804931640625 0.0340886116027832

Final encoder loss: 0.02589404210448265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11544990539550781 0.033911705017089844

Final encoder loss: 0.021791674196720123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11582636833190918 0.03438997268676758

Final encoder loss: 0.021309131756424904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1169579029083252 0.03432726860046387

Final encoder loss: 0.01911734789609909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11565709114074707 0.03417849540710449

Final encoder loss: 0.018625009804964066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11818242073059082 0.03433370590209961

Final encoder loss: 0.017473220825195312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11814522743225098 0.0352632999420166

Final encoder loss: 0.01706717722117901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11506485939025879 0.03381466865539551

Final encoder loss: 0.016430923715233803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11652541160583496 0.03461599349975586

Final encoder loss: 0.016136866062879562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11653327941894531 0.03430676460266113

Final encoder loss: 0.016149599105119705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11564469337463379 0.03459763526916504

Final encoder loss: 0.015553474426269531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11540818214416504 0.034203290939331055

Final encoder loss: 0.015904396772384644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11638522148132324 0.03478074073791504

Final encoder loss: 0.015346040017902851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1164247989654541 0.03461813926696777

Final encoder loss: 0.015725241973996162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11613583564758301 0.03379702568054199

Final encoder loss: 0.015433256514370441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1163027286529541 0.03535127639770508

Final encoder loss: 0.015844963490962982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1162879467010498 0.03467416763305664

Final encoder loss: 0.015691688284277916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11569070816040039 0.0335545539855957

Final encoder loss: 0.015617692843079567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11561083793640137 0.03492331504821777

Final encoder loss: 0.015392899513244629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1166534423828125 0.03417158126831055

Final encoder loss: 0.014725307002663612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11698460578918457 0.0343015193939209

Final encoder loss: 0.014797395095229149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11543750762939453 0.03539133071899414

Final encoder loss: 0.014141452498733997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11692953109741211 0.034192562103271484

Final encoder loss: 0.014218163676559925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11619067192077637 0.03462028503417969

Final encoder loss: 0.014072035439312458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1154019832611084 0.03462719917297363

Final encoder loss: 0.013745677657425404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11706233024597168 0.03318142890930176

Final encoder loss: 0.014292728155851364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1169123649597168 0.03389549255371094

Final encoder loss: 0.013824010267853737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11533427238464355 0.03474855422973633

Final encoder loss: 0.014083745889365673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11840295791625977 0.0335996150970459

Final encoder loss: 0.013953047804534435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11598539352416992 0.03336310386657715

Final encoder loss: 0.014077574945986271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11548662185668945 0.03368330001831055

Final encoder loss: 0.013812859542667866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11737585067749023 0.03361177444458008

Final encoder loss: 0.013839715160429478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11569690704345703 0.03416633605957031

Final encoder loss: 0.013667196035385132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1150972843170166 0.03329753875732422

Final encoder loss: 0.01360316015779972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11764192581176758 0.03533530235290527

Final encoder loss: 0.01332315243780613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11583828926086426 0.03460288047790527

Final encoder loss: 0.013361450284719467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11639833450317383 0.03411293029785156

Final encoder loss: 0.013064535334706306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11671185493469238 0.035338401794433594

Final encoder loss: 0.013270730152726173
Final encoder loss: 0.01215954776853323

Training case model
Final encoder loss: 0.024345991148217568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08947491645812988 0.2191011905670166

Final encoder loss: 0.023465796199967314
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09042978286743164 0.21902108192443848

Final encoder loss: 0.0232075760092259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08911991119384766 0.21930480003356934

Final encoder loss: 0.02255742607117216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.0898582935333252 0.21909785270690918

Final encoder loss: 0.022551848420676306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08951687812805176 0.21920228004455566

Final encoder loss: 0.022690355990041582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08995676040649414 0.21935653686523438

Final encoder loss: 0.023129343279881517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.09035825729370117 0.21935009956359863

Final encoder loss: 0.022096335190955944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08944487571716309 0.21951961517333984

Final encoder loss: 0.021671313810583032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09047102928161621 0.21889114379882812

Final encoder loss: 0.021961570206421328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08919978141784668 0.21920323371887207

Final encoder loss: 0.021788048424061415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.0998382568359375 0.21907854080200195

Final encoder loss: 0.022579268844548504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.09000468254089355 0.21950316429138184

Final encoder loss: 0.022069782713215438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08960366249084473 0.21894407272338867

Final encoder loss: 0.02243795404569872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09022760391235352 0.2193911075592041

Final encoder loss: 0.022470383444927897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08941459655761719 0.21930313110351562

Final encoder loss: 0.022760799627871114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08713269233703613 0.21596407890319824


Training case model
Final encoder loss: 0.20296083390712738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26537084579467773 0.05234837532043457

Final encoder loss: 0.1889110654592514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2771413326263428 0.05256056785583496

Final encoder loss: 0.1901470273733139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2682962417602539 0.05563712120056152

Final encoder loss: 0.1921778917312622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2672915458679199 0.052068233489990234

Final encoder loss: 0.1807992160320282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2691662311553955 0.05197334289550781

Final encoder loss: 0.19192875921726227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25701236724853516 0.05109548568725586

Final encoder loss: 0.10484255850315094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2679941654205322 0.054167985916137695

Final encoder loss: 0.09470733255147934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2680013179779053 0.05240488052368164

Final encoder loss: 0.09069875627756119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2732045650482178 0.051342010498046875

Final encoder loss: 0.08937910944223404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26803112030029297 0.05227851867675781

Final encoder loss: 0.08132924139499664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2691202163696289 0.0520014762878418

Final encoder loss: 0.08493681252002716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25523972511291504 0.053499460220336914

Final encoder loss: 0.061102669686079025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26813817024230957 0.052442073822021484

Final encoder loss: 0.05537787079811096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2822761535644531 0.05189704895019531

Final encoder loss: 0.05337360501289368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27670931816101074 0.05231356620788574

Final encoder loss: 0.05350158363580704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2846813201904297 0.05131244659423828

Final encoder loss: 0.04996350035071373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27525949478149414 0.053778648376464844

Final encoder loss: 0.052095092833042145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556450366973877 0.051859140396118164

Final encoder loss: 0.04278402402997017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2688143253326416 0.05251955986022949

Final encoder loss: 0.03948107734322548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2888638973236084 0.052034616470336914

Final encoder loss: 0.03834177553653717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26781225204467773 0.05447244644165039

Final encoder loss: 0.03884876146912575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26830124855041504 0.05219125747680664

Final encoder loss: 0.03729068860411644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26848888397216797 0.05150485038757324

Final encoder loss: 0.03837628662586212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25557947158813477 0.05217552185058594

Final encoder loss: 0.034589141607284546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26073288917541504 0.05178403854370117

Final encoder loss: 0.03284188359975815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2685208320617676 0.05202603340148926

Final encoder loss: 0.03212863206863403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26654624938964844 0.05162334442138672

Final encoder loss: 0.03257938101887703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2655808925628662 0.05218935012817383

Final encoder loss: 0.03249232470989227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2674832344055176 0.05167961120605469

Final encoder loss: 0.032498471438884735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25342726707458496 0.050859928131103516

Final encoder loss: 0.031063327565789223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26734352111816406 0.051361083984375

Final encoder loss: 0.030052416026592255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27138519287109375 0.0525360107421875

Final encoder loss: 0.029718831181526184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2665400505065918 0.05153369903564453

Final encoder loss: 0.030021745711565018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27339696884155273 0.050699472427368164

Final encoder loss: 0.030747951939702034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576282024383545 0.05168962478637695

Final encoder loss: 0.03032846190035343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569723129272461 0.05218505859375

Final encoder loss: 0.028899915516376495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2680518627166748 0.05269575119018555

Final encoder loss: 0.028093988075852394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2685363292694092 0.05197954177856445

Final encoder loss: 0.027832120656967163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26987648010253906 0.054285526275634766

Final encoder loss: 0.02783738262951374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2741074562072754 0.053240299224853516

Final encoder loss: 0.028690924867987633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574136257171631 0.053963422775268555

Final encoder loss: 0.028103956952691078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569456100463867 0.05269145965576172

Final encoder loss: 0.026970287784934044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.269711971282959 0.05189776420593262

Final encoder loss: 0.026402859017252922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26879215240478516 0.053075551986694336

Final encoder loss: 0.02609493024647236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26822638511657715 0.05205798149108887

Final encoder loss: 0.026332566514611244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26804375648498535 0.05231189727783203

Final encoder loss: 0.027115032076835632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26821088790893555 0.052849531173706055

Final encoder loss: 0.02679380029439926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25676417350769043 0.05266880989074707

Final encoder loss: 0.025869550183415413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2680234909057617 0.0534665584564209

Final encoder loss: 0.025470882654190063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2830185890197754 0.05267786979675293

Final encoder loss: 0.025336630642414093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2668423652648926 0.053995370864868164

Final encoder loss: 0.025352438911795616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2674105167388916 0.05350947380065918

Final encoder loss: 0.026439061388373375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26692652702331543 0.05098915100097656

Final encoder loss: 0.025732669979333878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25823497772216797 0.05112266540527344

Final encoder loss: 0.025204509496688843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.269176721572876 0.052745819091796875

Final encoder loss: 0.02501087449491024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2655150890350342 0.05141496658325195

Final encoder loss: 0.024872222915291786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.266467809677124 0.05205368995666504

Final encoder loss: 0.02479005977511406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26534605026245117 0.05133676528930664

Final encoder loss: 0.02571125514805317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2656424045562744 0.052106380462646484

Final encoder loss: 0.025244781747460365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2532484531402588 0.052186012268066406

Final encoder loss: 0.02477044239640236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2659432888031006 0.052210092544555664

Final encoder loss: 0.024530835449695587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27138614654541016 0.0514683723449707

Final encoder loss: 0.02434779517352581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.273357629776001 0.05169105529785156

Final encoder loss: 0.02424531616270542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2661616802215576 0.05286908149719238

Final encoder loss: 0.02516130544245243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26804089546203613 0.05248236656188965

Final encoder loss: 0.024521462619304657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25672078132629395 0.051599740982055664

Final encoder loss: 0.024203700944781303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26750946044921875 0.05174565315246582

Final encoder loss: 0.023965241387486458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27396273612976074 0.052161455154418945

Final encoder loss: 0.023821890354156494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27733588218688965 0.0517420768737793

Final encoder loss: 0.02377081662416458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2678804397583008 0.05245637893676758

Final encoder loss: 0.024823417887091637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26781773567199707 0.05118441581726074

Final encoder loss: 0.02423836849629879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25431394577026367 0.05195307731628418

Final encoder loss: 0.02392495423555374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26754140853881836 0.05271601676940918

Final encoder loss: 0.023805471137166023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.282886266708374 0.05260872840881348

Final encoder loss: 0.02362922579050064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26684069633483887 0.051885128021240234

Final encoder loss: 0.02349219098687172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27432918548583984 0.05266213417053223

Final encoder loss: 0.024547122418880463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26761889457702637 0.05282115936279297

Final encoder loss: 0.02384473942220211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25523829460144043 0.051172733306884766

Final encoder loss: 0.023620285093784332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26526904106140137 0.0528101921081543

Final encoder loss: 0.023537857457995415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27936387062072754 0.05359244346618652

Final encoder loss: 0.023377245292067528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26711368560791016 0.051309823989868164

Final encoder loss: 0.02319333329796791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27323412895202637 0.05225396156311035

Final encoder loss: 0.023989085108041763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26627349853515625 0.05219388008117676

Final encoder loss: 0.02366112545132637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25385236740112305 0.05102682113647461

Final encoder loss: 0.023492274805903435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26819801330566406 0.05304241180419922

Final encoder loss: 0.02329203486442566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2783513069152832 0.05168509483337402

Final encoder loss: 0.023195110261440277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2715644836425781 0.05288553237915039

Final encoder loss: 0.022874055430293083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2678804397583008 0.052167415618896484

Final encoder loss: 0.02403058111667633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2680473327636719 0.053800106048583984

Final encoder loss: 0.02329271472990513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543210983276367 0.051915884017944336

Final encoder loss: 0.023177264258265495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25774240493774414 0.052584171295166016

Final encoder loss: 0.023059776052832603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2717468738555908 0.05159449577331543

Final encoder loss: 0.0228867307305336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.266742467880249 0.051021575927734375

Final encoder loss: 0.022762596607208252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2663106918334961 0.05108928680419922

Final encoder loss: 0.02376135066151619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26674389839172363 0.05178356170654297

Final encoder loss: 0.023267725482583046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.252885103225708 0.051238059997558594

Final encoder loss: 0.02305305190384388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26758337020874023 0.05137944221496582

Final encoder loss: 0.022877132520079613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27789926528930664 0.05138683319091797

Final encoder loss: 0.022836951538920403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26882481575012207 0.05269193649291992

Final encoder loss: 0.02251433953642845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27149534225463867 0.05324912071228027

Final encoder loss: 0.023599471896886826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.266904354095459 0.052579402923583984

Final encoder loss: 0.022946372628211975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25417447090148926 0.051314592361450195

Final encoder loss: 0.02293185144662857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2663695812225342 0.05156373977661133

Final encoder loss: 0.022803138941526413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2712736129760742 0.05267453193664551

Final encoder loss: 0.022575702518224716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26630496978759766 0.05144929885864258

Final encoder loss: 0.022455142810940742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2664008140563965 0.0520939826965332

Final encoder loss: 0.023310977965593338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26569604873657227 0.05159497261047363

Final encoder loss: 0.022927161306142807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538423538208008 0.051512956619262695

Final encoder loss: 0.022797783836722374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2718477249145508 0.052114248275756836

Final encoder loss: 0.022667789831757545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2721872329711914 0.0519413948059082

Final encoder loss: 0.022529784590005875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26558732986450195 0.05240297317504883

Final encoder loss: 0.02224605344235897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26616716384887695 0.05184292793273926

Final encoder loss: 0.023273330181837082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26639699935913086 0.05128669738769531

Final encoder loss: 0.02264588512480259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534644603729248 0.05087018013000488

Final encoder loss: 0.022652991116046906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2664525508880615 0.05204319953918457

Final encoder loss: 0.022532284259796143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27289748191833496 0.05176901817321777

Final encoder loss: 0.02236783504486084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26636409759521484 0.05156302452087402

Final encoder loss: 0.022131262347102165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2667686939239502 0.052216529846191406

Final encoder loss: 0.023037811741232872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27180957794189453 0.05134105682373047

Final encoder loss: 0.02265203557908535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25427961349487305 0.052771568298339844

Final encoder loss: 0.02255886048078537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2664525508880615 0.05128598213195801

Final encoder loss: 0.022370781749486923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27208924293518066 0.05156707763671875

Final encoder loss: 0.022374287247657776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26631903648376465 0.05210113525390625

Final encoder loss: 0.022017285227775574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2656412124633789 0.05131340026855469

Final encoder loss: 0.023045510053634644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2664954662322998 0.05276298522949219

Final encoder loss: 0.022416742518544197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25391459465026855 0.05106019973754883

Final encoder loss: 0.022469080984592438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.266937255859375 0.05175280570983887

Final encoder loss: 0.0223702285438776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2711608409881592 0.05184364318847656

Final encoder loss: 0.022160053253173828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2666287422180176 0.05162310600280762

Final encoder loss: 0.02192620560526848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2660658359527588 0.05167555809020996

Final encoder loss: 0.022860966622829437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2674534320831299 0.0511479377746582

Final encoder loss: 0.022432774305343628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541370391845703 0.05080890655517578

Final encoder loss: 0.02237805165350437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27304577827453613 0.053217172622680664

Final encoder loss: 0.022241992875933647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2725367546081543 0.052036285400390625

Final encoder loss: 0.022098049521446228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2659773826599121 0.051773786544799805

Final encoder loss: 0.021792778745293617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2722949981689453 0.05209517478942871

Final encoder loss: 0.022863835096359253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2711150646209717 0.051605224609375

Final encoder loss: 0.022241046652197838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2535252571105957 0.05149078369140625

Final encoder loss: 0.022263016551733017
Final encoder loss: 0.02174570970237255
Final encoder loss: 0.02098635956645012
Final encoder loss: 0.020108139142394066
Final encoder loss: 0.02013012208044529
Final encoder loss: 0.019025199115276337

Training emognition model
Final encoder loss: 0.026453497606360735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08152174949645996 0.2298603057861328

Final encoder loss: 0.026515920552269312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08041548728942871 0.2295525074005127

Final encoder loss: 0.026224647648854675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08060216903686523 0.23041772842407227

Final encoder loss: 0.026558417837230907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08072924613952637 0.22916316986083984

Final encoder loss: 0.026649625022383827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0810856819152832 0.22971200942993164

Final encoder loss: 0.024741260713296128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08129620552062988 0.22999334335327148

Final encoder loss: 0.027596970556137405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08131074905395508 0.23005127906799316

Final encoder loss: 0.024516550685393653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08134770393371582 0.2299659252166748

Final encoder loss: 0.025499372405051332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08061051368713379 0.229811429977417

Final encoder loss: 0.02610922899379051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08110547065734863 0.229400634765625

Final encoder loss: 0.024838285114174824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08031845092773438 0.22997450828552246

Final encoder loss: 0.024612372237412097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08056354522705078 0.22979044914245605

Final encoder loss: 0.025383646822882373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08083176612854004 0.22983932495117188

Final encoder loss: 0.02486694860577371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08136200904846191 0.22942733764648438

Final encoder loss: 0.026264322902089205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08147215843200684 0.22974634170532227

Final encoder loss: 0.02577825719447893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07987308502197266 0.22922587394714355


Training emognition model
Final encoder loss: 0.19356076419353485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25011730194091797 0.04888296127319336

Final encoder loss: 0.1949588656425476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24823641777038574 0.0480039119720459

Final encoder loss: 0.08836857974529266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24799275398254395 0.049347877502441406

Final encoder loss: 0.08838101476430893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24652481079101562 0.04985952377319336

Final encoder loss: 0.05631302669644356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24793744087219238 0.0501859188079834

Final encoder loss: 0.055118903517723083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472374439239502 0.04914426803588867

Final encoder loss: 0.0418948270380497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24730730056762695 0.04873299598693848

Final encoder loss: 0.04099765047430992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24681377410888672 0.04862403869628906

Final encoder loss: 0.03448047861456871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24650883674621582 0.04894709587097168

Final encoder loss: 0.033925194293260574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24644947052001953 0.04962158203125

Final encoder loss: 0.0303451307117939
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24751520156860352 0.04918050765991211

Final encoder loss: 0.029976559802889824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24723076820373535 0.048562049865722656

Final encoder loss: 0.028033079579472542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24753165245056152 0.048651933670043945

Final encoder loss: 0.0277253445237875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24529457092285156 0.04891085624694824

Final encoder loss: 0.02676810696721077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24683308601379395 0.04889416694641113

Final encoder loss: 0.026628950610756874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24692654609680176 0.04963111877441406

Final encoder loss: 0.026252102106809616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2507743835449219 0.04863381385803223

Final encoder loss: 0.0261862650513649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2492508888244629 0.04996824264526367

Final encoder loss: 0.025940123945474625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24885201454162598 0.04913067817687988

Final encoder loss: 0.02603793516755104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24756908416748047 0.050501108169555664

Final encoder loss: 0.025627311319112778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24959230422973633 0.05058121681213379

Final encoder loss: 0.025756925344467163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24715089797973633 0.04782366752624512

Final encoder loss: 0.025396589189767838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24905061721801758 0.04954409599304199

Final encoder loss: 0.0254166591912508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24581170082092285 0.049803972244262695

Final encoder loss: 0.02501586824655533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2530019283294678 0.0484766960144043

Final encoder loss: 0.025019140914082527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24713516235351562 0.0497593879699707

Final encoder loss: 0.024889223277568817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2500033378601074 0.052431344985961914

Final encoder loss: 0.025000140070915222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24817848205566406 0.049802303314208984

Final encoder loss: 0.024643391370773315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2580294609069824 0.05103707313537598

Final encoder loss: 0.024886654689908028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469160556793213 0.04938387870788574

Final encoder loss: 0.02448786236345768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24865317344665527 0.05107855796813965

Final encoder loss: 0.024802956730127335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24808096885681152 0.04910397529602051

Final encoder loss: 0.024255016818642616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24761652946472168 0.05168485641479492

Final encoder loss: 0.024540403857827187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24820542335510254 0.04944014549255371

Final encoder loss: 0.024268537759780884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484455108642578 0.04941558837890625

Final encoder loss: 0.024504126980900764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24982142448425293 0.04955554008483887

Final encoder loss: 0.02416045032441616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24815940856933594 0.049082040786743164

Final encoder loss: 0.02430739253759384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24913477897644043 0.048592567443847656

Final encoder loss: 0.02415299601852894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24959707260131836 0.048487186431884766

Final encoder loss: 0.024307943880558014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24933886528015137 0.05104994773864746

Final encoder loss: 0.023953145369887352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24811625480651855 0.04886317253112793

Final encoder loss: 0.024180255830287933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24908804893493652 0.04893851280212402

Final encoder loss: 0.02389664761722088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24892401695251465 0.04942798614501953

Final encoder loss: 0.024106092751026154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24841952323913574 0.050028085708618164

Final encoder loss: 0.023742204532027245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24919581413269043 0.05011129379272461

Final encoder loss: 0.0240788497030735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24826455116271973 0.0492558479309082

Final encoder loss: 0.023790419101715088
Final encoder loss: 0.023180648684501648

Training empatch model
Final encoder loss: 0.03528667078469658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07128548622131348 0.17409539222717285

Final encoder loss: 0.037404951875100165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07297587394714355 0.17428994178771973

Final encoder loss: 0.03581751035592167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07114720344543457 0.17466402053833008

Final encoder loss: 0.03278344102420298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07148551940917969 0.17381811141967773

Final encoder loss: 0.03359104051401282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0717306137084961 0.17419648170471191

Final encoder loss: 0.031115526329413286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07393574714660645 0.17409825325012207

Final encoder loss: 0.030670315837719512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07224202156066895 0.1739671230316162

Final encoder loss: 0.033170350808081644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0708925724029541 0.17379069328308105

Final encoder loss: 0.025338532953025794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07337498664855957 0.17392945289611816

Final encoder loss: 0.023452798309111664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07131409645080566 0.17407846450805664

Final encoder loss: 0.025831580313883194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07195258140563965 0.17420101165771484

Final encoder loss: 0.023789417520494772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0713043212890625 0.1743602752685547

Final encoder loss: 0.023169077950831923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07385873794555664 0.1747732162475586

Final encoder loss: 0.02397728876813266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07186341285705566 0.17425823211669922

Final encoder loss: 0.023384287462921347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07168745994567871 0.17426085472106934

Final encoder loss: 0.022643686320145913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07318353652954102 0.17444252967834473


Training empatch model
Final encoder loss: 0.171162411570549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17807841300964355 0.043703317642211914

Final encoder loss: 0.08064279705286026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17586016654968262 0.043543338775634766

Final encoder loss: 0.05524999275803566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1779463291168213 0.045732975006103516

Final encoder loss: 0.042872220277786255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17543482780456543 0.04300665855407715

Final encoder loss: 0.03575221449136734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1763744354248047 0.04303550720214844

Final encoder loss: 0.03127146512269974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17749834060668945 0.04535841941833496

Final encoder loss: 0.02829997055232525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17543721199035645 0.043892860412597656

Final encoder loss: 0.026299702003598213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17554283142089844 0.043828487396240234

Final encoder loss: 0.024968009442090988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17818832397460938 0.04518628120422363

Final encoder loss: 0.024102991446852684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1763324737548828 0.04443621635437012

Final encoder loss: 0.02343965880572796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17865562438964844 0.04339289665222168

Final encoder loss: 0.02310403808951378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17735505104064941 0.043972015380859375

Final encoder loss: 0.022850776091217995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17690181732177734 0.04396772384643555

Final encoder loss: 0.022493209689855576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1760575771331787 0.04288196563720703

Final encoder loss: 0.022268300876021385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17610573768615723 0.043609619140625

Final encoder loss: 0.022041669115424156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17693114280700684 0.04520845413208008

Final encoder loss: 0.02195216529071331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17581963539123535 0.04418158531188965

Final encoder loss: 0.02177787944674492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17654657363891602 0.04448366165161133

Final encoder loss: 0.021660415455698967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17702555656433105 0.04358410835266113

Final encoder loss: 0.021492281928658485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17641043663024902 0.04371070861816406

Final encoder loss: 0.02144484408199787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17685747146606445 0.04507327079772949

Final encoder loss: 0.021431710571050644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17726731300354004 0.0434877872467041

Final encoder loss: 0.021438390016555786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17553496360778809 0.043715715408325195

Final encoder loss: 0.02128908969461918

Training wesad model
Final encoder loss: 0.03619987141344442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07238388061523438 0.17434072494506836

Final encoder loss: 0.03490650865018121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07133197784423828 0.1737978458404541

Final encoder loss: 0.032800502241438226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07125496864318848 0.17400836944580078

Final encoder loss: 0.03318786384636003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07073163986206055 0.17389774322509766

Final encoder loss: 0.021998688004104833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07376408576965332 0.17370390892028809

Final encoder loss: 0.021807975559373645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07165741920471191 0.17356634140014648

Final encoder loss: 0.022857370056565247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07090115547180176 0.17413854598999023

Final encoder loss: 0.0226899220605205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07259011268615723 0.17434310913085938

Final encoder loss: 0.016937548987414796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07096600532531738 0.17405247688293457

Final encoder loss: 0.017081303515424594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0713660717010498 0.17350482940673828

Final encoder loss: 0.017897586567420107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07149600982666016 0.17416691780090332

Final encoder loss: 0.018468330015096894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07357382774353027 0.1739788055419922

Final encoder loss: 0.013453357696548636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07150602340698242 0.17363691329956055

Final encoder loss: 0.014937864294889452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0713653564453125 0.17390012741088867

Final encoder loss: 0.01473165429770285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07278823852539062 0.17409491539001465

Final encoder loss: 0.014758020782016613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07122683525085449 0.17466449737548828


Training wesad model
Final encoder loss: 0.21559587121009827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1069486141204834 0.0332493782043457

Final encoder loss: 0.09986091405153275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10360383987426758 0.03373885154724121

Final encoder loss: 0.064338818192482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10597515106201172 0.03444361686706543

Final encoder loss: 0.046498142182826996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10613012313842773 0.034209251403808594

Final encoder loss: 0.03623330965638161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10383820533752441 0.03341960906982422

Final encoder loss: 0.02992238663136959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1044304370880127 0.033681392669677734

Final encoder loss: 0.025812625885009766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10457992553710938 0.03336668014526367

Final encoder loss: 0.023047825321555138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10346436500549316 0.033797264099121094

Final encoder loss: 0.021137382835149765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.106658935546875 0.03409576416015625

Final encoder loss: 0.019860878586769104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10685086250305176 0.033179521560668945

Final encoder loss: 0.019027328118681908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1036834716796875 0.03350090980529785

Final encoder loss: 0.01852717064321041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10449576377868652 0.03379201889038086

Final encoder loss: 0.01822221837937832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10371065139770508 0.03306078910827637

Final encoder loss: 0.01806231401860714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10481810569763184 0.03386330604553223

Final encoder loss: 0.01799134723842144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10599923133850098 0.034110069274902344

Final encoder loss: 0.017956899479031563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10558605194091797 0.03307986259460449

Final encoder loss: 0.01798282377421856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10500717163085938 0.033451080322265625

Final encoder loss: 0.017890477553009987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10416674613952637 0.03368878364562988

Final encoder loss: 0.017661482095718384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385584831237793 0.03324460983276367

Final encoder loss: 0.01760704629123211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10466313362121582 0.034364938735961914

Final encoder loss: 0.017607683315873146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10653066635131836 0.03494381904602051

Final encoder loss: 0.01759999617934227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10433363914489746 0.03301596641540527

Final encoder loss: 0.017623210325837135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10434794425964355 0.03400158882141113

Final encoder loss: 0.017635907977819443

Calculating loss for amigos model
	Full Pass 0.6425468921661377
numFreeParamsPath 18
Reconstruction loss values: 0.025717608630657196 0.03505905345082283

Calculating loss for dapper model
	Full Pass 0.15313434600830078
numFreeParamsPath 18
Reconstruction loss values: 0.020768992602825165 0.02415015734732151

Calculating loss for case model
	Full Pass 0.9167270660400391
numFreeParamsPath 18
Reconstruction loss values: 0.03153981640934944 0.034840404987335205

Calculating loss for emognition model
	Full Pass 0.28099560737609863
numFreeParamsPath 18
Reconstruction loss values: 0.03280363231897354 0.0411999486386776

Calculating loss for empatch model
	Full Pass 0.10611462593078613
numFreeParamsPath 18
Reconstruction loss values: 0.03457745537161827 0.041694145649671555

Calculating loss for wesad model
	Full Pass 0.07877492904663086
numFreeParamsPath 18
Reconstruction loss values: 0.034283347427845 0.05165240168571472
Total loss calculation time: 4.089674234390259

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.649270057678223
Total epoch time: 217.1872365474701

Epoch: 48

Training amigos model
Final encoder loss: 0.02620081620318772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1163794994354248 0.39822912216186523

Final encoder loss: 0.02493258010814405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10915470123291016 0.38896918296813965

Final encoder loss: 0.02495144853457948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10834145545959473 0.38913774490356445

Final encoder loss: 0.025934507475332852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10991477966308594 0.38909149169921875

Final encoder loss: 0.02468519126587168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10842227935791016 0.39050841331481934

Final encoder loss: 0.028281776606273935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10808920860290527 0.39046478271484375

Final encoder loss: 0.025695171957927632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10983419418334961 0.3905031681060791

Final encoder loss: 0.023300520674511778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1084756851196289 0.389279842376709

Final encoder loss: 0.023575079790639566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10900998115539551 0.3883652687072754

Final encoder loss: 0.02678195585086182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10828423500061035 0.38886022567749023

Final encoder loss: 0.02486451416984969
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10823822021484375 0.39075565338134766

Final encoder loss: 0.023577528979641615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10997462272644043 0.3900933265686035

Final encoder loss: 0.026243112000879993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10865664482116699 0.38953232765197754

Final encoder loss: 0.027391157818634396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.1101832389831543 0.3914210796356201

Final encoder loss: 0.02382844507049096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10822296142578125 0.38910365104675293

Final encoder loss: 0.026198385371929044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10404729843139648 0.3834879398345947


Training dapper model
Final encoder loss: 0.01961800823762447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06170988082885742 0.1505112648010254

Final encoder loss: 0.018813511649569452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06358051300048828 0.15216565132141113

Final encoder loss: 0.02082561624122909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06215023994445801 0.1495370864868164

Final encoder loss: 0.02058438308495739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06257963180541992 0.15105867385864258

Final encoder loss: 0.020610094674750792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.062090158462524414 0.15019702911376953

Final encoder loss: 0.02078670905902139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06327390670776367 0.15283560752868652

Final encoder loss: 0.019099626702932127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06282281875610352 0.15012621879577637

Final encoder loss: 0.02006591050408547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06244993209838867 0.1507396697998047

Final encoder loss: 0.020630145990700084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06208372116088867 0.15033245086669922

Final encoder loss: 0.01882177398056597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06310629844665527 0.15281939506530762

Final encoder loss: 0.020186405764117586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06373047828674316 0.15005135536193848

Final encoder loss: 0.01787336934371803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.062259674072265625 0.15103840827941895

Final encoder loss: 0.01843128433588149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06233978271484375 0.15010905265808105

Final encoder loss: 0.020040874503380338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06283855438232422 0.15227413177490234

Final encoder loss: 0.01919400630861512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06394267082214355 0.15025830268859863

Final encoder loss: 0.01717400712768165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.062027931213378906 0.15015316009521484


Training case model
Final encoder loss: 0.031895134567149576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09125781059265137 0.2653768062591553

Final encoder loss: 0.030159505524751978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09395575523376465 0.26532554626464844

Final encoder loss: 0.02885148810306774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09173965454101562 0.26544809341430664

Final encoder loss: 0.028092036905399912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09219217300415039 0.2659318447113037

Final encoder loss: 0.02736960470256873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09189558029174805 0.2658419609069824

Final encoder loss: 0.027198719801815402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09107589721679688 0.2666964530944824

Final encoder loss: 0.02704885585638134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09139585494995117 0.2649812698364258

Final encoder loss: 0.02610588867725725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09132862091064453 0.2659170627593994

Final encoder loss: 0.02563074307947034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09295868873596191 0.2662169933319092

Final encoder loss: 0.026102031802776666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09141993522644043 0.26442980766296387

Final encoder loss: 0.025273625060094007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.0921025276184082 0.26644444465637207

Final encoder loss: 0.025100707894053713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09147405624389648 0.2649726867675781

Final encoder loss: 0.025120421080790416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09135723114013672 0.2658195495605469

Final encoder loss: 0.02553029564941767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09148025512695312 0.26584386825561523

Final encoder loss: 0.02482880265385112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.092376708984375 0.266040563583374

Final encoder loss: 0.02490025528287627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08842229843139648 0.2630727291107178


Training emognition model
Final encoder loss: 0.03404555724631385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08535885810852051 0.27494144439697266

Final encoder loss: 0.03433624199812919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08287668228149414 0.2733776569366455

Final encoder loss: 0.03335130048570922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0826120376586914 0.27308130264282227

Final encoder loss: 0.031833057793682534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08222603797912598 0.27289867401123047

Final encoder loss: 0.03085750602487253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08227753639221191 0.2741234302520752

Final encoder loss: 0.031010414883470904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0828554630279541 0.2742891311645508

Final encoder loss: 0.032145402456830945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08267569541931152 0.27330732345581055

Final encoder loss: 0.03407917417962255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0828554630279541 0.27396178245544434

Final encoder loss: 0.030861883643033824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08245992660522461 0.2737300395965576

Final encoder loss: 0.02989959825519007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08250832557678223 0.27332210540771484

Final encoder loss: 0.030567777194273053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0826272964477539 0.2751610279083252

Final encoder loss: 0.03158011479222701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08352971076965332 0.2755119800567627

Final encoder loss: 0.032427784703617735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08331608772277832 0.275449275970459

Final encoder loss: 0.03003252586812002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08319687843322754 0.2753336429595947

Final encoder loss: 0.03075834254655918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08385372161865234 0.27489590644836426

Final encoder loss: 0.030704809491993754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0829005241394043 0.2739279270172119


Training amigos model
Final encoder loss: 0.019945084527141905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10610485076904297 0.3414192199707031

Final encoder loss: 0.020100156264035454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10652685165405273 0.3414607048034668

Final encoder loss: 0.01936800005715622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10587239265441895 0.34159159660339355

Final encoder loss: 0.020650186471532713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10642838478088379 0.34172558784484863

Final encoder loss: 0.019715492911465817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10605406761169434 0.34183168411254883

Final encoder loss: 0.01797385788563778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10636711120605469 0.3416476249694824

Final encoder loss: 0.01734137606987125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10584759712219238 0.3414733409881592

Final encoder loss: 0.019049825332828518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10645198822021484 0.342282772064209

Final encoder loss: 0.01874498377966156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10582709312438965 0.34154176712036133

Final encoder loss: 0.019946429415973167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10628437995910645 0.34152817726135254

Final encoder loss: 0.018580636209790384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.1060333251953125 0.34209370613098145

Final encoder loss: 0.019028510234662566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10622215270996094 0.341414213180542

Final encoder loss: 0.019285380671896123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10586810111999512 0.34146785736083984

Final encoder loss: 0.018701070705162286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1067056655883789 0.34178853034973145

Final encoder loss: 0.019953108947462814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10604429244995117 0.3413228988647461

Final encoder loss: 0.020212571113338285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10134100914001465 0.3389861583709717


Training amigos model
Final encoder loss: 0.18080826103687286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4591636657714844 0.07932353019714355

Final encoder loss: 0.18780268728733063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4553530216217041 0.07650446891784668

Final encoder loss: 0.18362967669963837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4527144432067871 0.07436752319335938

Final encoder loss: 0.07683203369379044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4683876037597656 0.07645440101623535

Final encoder loss: 0.0786406546831131
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4535045623779297 0.07920598983764648

Final encoder loss: 0.07221253216266632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.444378137588501 0.07452607154846191

Final encoder loss: 0.04462934285402298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4590737819671631 0.07506251335144043

Final encoder loss: 0.045318808406591415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4479825496673584 0.07564258575439453

Final encoder loss: 0.042035043239593506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4460291862487793 0.07310914993286133

Final encoder loss: 0.031832657754421234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44690704345703125 0.07475137710571289

Final encoder loss: 0.032613497227430344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.451291561126709 0.07631897926330566

Final encoder loss: 0.030831247568130493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4552168846130371 0.07693624496459961

Final encoder loss: 0.026059569790959358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45680928230285645 0.08111977577209473

Final encoder loss: 0.026807751506567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47373032569885254 0.07388949394226074

Final encoder loss: 0.02561517246067524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4409658908843994 0.07695245742797852

Final encoder loss: 0.023293012753129005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45760583877563477 0.07729482650756836

Final encoder loss: 0.023945016786456108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46970534324645996 0.07894253730773926

Final encoder loss: 0.023138342425227165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4554109573364258 0.0732414722442627

Final encoder loss: 0.02226741425693035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4155595302581787 0.07630133628845215

Final encoder loss: 0.022700753062963486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4562654495239258 0.07530570030212402

Final encoder loss: 0.022282589226961136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43640732765197754 0.08144974708557129

Final encoder loss: 0.02237570844590664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45720434188842773 0.07562899589538574

Final encoder loss: 0.022375859320163727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.475170373916626 0.07705855369567871

Final encoder loss: 0.022186744958162308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45322585105895996 0.07701826095581055

Final encoder loss: 0.02189159020781517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4705498218536377 0.08033561706542969

Final encoder loss: 0.02185814268887043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45621180534362793 0.0729067325592041

Final encoder loss: 0.021527132019400597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4573783874511719 0.07563900947570801

Final encoder loss: 0.02074667252600193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4541504383087158 0.0753164291381836

Final encoder loss: 0.02101587876677513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45773863792419434 0.08251380920410156

Final encoder loss: 0.02069414034485817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43897128105163574 0.07581591606140137

Final encoder loss: 0.020145731046795845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4603545665740967 0.07731819152832031

Final encoder loss: 0.020448578521609306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45446300506591797 0.07720637321472168

Final encoder loss: 0.020399009808897972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4534749984741211 0.07584381103515625

Final encoder loss: 0.02004164829850197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44019365310668945 0.08113908767700195

Final encoder loss: 0.020015593618154526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4595377445220947 0.07601618766784668

Final encoder loss: 0.020164718851447105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43941640853881836 0.07731413841247559

Final encoder loss: 0.0199062991887331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4566066265106201 0.07663345336914062

Final encoder loss: 0.019630204886198044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4709315299987793 0.08155274391174316

Final encoder loss: 0.01989215426146984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4560127258300781 0.07392215728759766

Final encoder loss: 0.01950497180223465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4689648151397705 0.07701826095581055

Final encoder loss: 0.019372377544641495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4572176933288574 0.07799458503723145

Final encoder loss: 0.01954777166247368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45250558853149414 0.07920956611633301

Final encoder loss: 0.019284222275018692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45899271965026855 0.07329702377319336

Final encoder loss: 0.019032446667551994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46958184242248535 0.07831001281738281

Final encoder loss: 0.019143788143992424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45336246490478516 0.07519912719726562

Final encoder loss: 0.019003836438059807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4706857204437256 0.08238029479980469

Final encoder loss: 0.01903066784143448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4584481716156006 0.07592010498046875

Final encoder loss: 0.019048338755965233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45139622688293457 0.07447290420532227

Final encoder loss: 0.01879880391061306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4696776866912842 0.07697176933288574

Final encoder loss: 0.01872146874666214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45616769790649414 0.08042025566101074

Final encoder loss: 0.018983831629157066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45543861389160156 0.07169699668884277

Final encoder loss: 0.018679387867450714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4686439037322998 0.07561421394348145

Final encoder loss: 0.018512390553951263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4575488567352295 0.07643485069274902

Final encoder loss: 0.01883016712963581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.452878475189209 0.078338623046875

Final encoder loss: 0.018781907856464386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4741942882537842 0.07200956344604492

Final encoder loss: 0.018572073429822922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45529675483703613 0.07538342475891113

Final encoder loss: 0.01874023862183094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4539756774902344 0.07519221305847168

Final encoder loss: 0.018515191972255707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45527029037475586 0.07973957061767578

Final encoder loss: 0.01855888031423092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4585704803466797 0.07510018348693848

Final encoder loss: 0.018707027658820152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4549126625061035 0.07773756980895996

Final encoder loss: 0.01841566152870655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4354410171508789 0.07585382461547852

Final encoder loss: 0.01829131506383419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4555063247680664 0.07850193977355957

Final encoder loss: 0.018586475402116776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4554567337036133 0.07297682762145996

Final encoder loss: 0.018401630222797394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4717991352081299 0.07668328285217285

Final encoder loss: 0.01813499815762043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4514961242675781 0.07621264457702637

Final encoder loss: 0.018399106338620186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43913769721984863 0.07901668548583984

Final encoder loss: 0.018442697823047638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4559144973754883 0.0727696418762207

Final encoder loss: 0.01805357076227665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47474098205566406 0.07688331604003906

Final encoder loss: 0.018446844071149826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4499046802520752 0.07393765449523926

Final encoder loss: 0.018234074115753174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.470409631729126 0.08014345169067383

Final encoder loss: 0.01809215545654297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45688509941101074 0.07348823547363281

Final encoder loss: 0.018444828689098358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45917582511901855 0.07570099830627441

Final encoder loss: 0.01819644309580326
Final encoder loss: 0.0170217864215374
Final encoder loss: 0.01660620979964733

Training dapper model
Final encoder loss: 0.018154289476020092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06006813049316406 0.10858464241027832

Final encoder loss: 0.015666387995653216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06089329719543457 0.10804581642150879

Final encoder loss: 0.01677609002742324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06016874313354492 0.10695528984069824

Final encoder loss: 0.01622586329798599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05980873107910156 0.1072995662689209

Final encoder loss: 0.014753626688594717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05975675582885742 0.10867857933044434

Final encoder loss: 0.01563065942153582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06096625328063965 0.10771989822387695

Final encoder loss: 0.015516019796861418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059816598892211914 0.10739970207214355

Final encoder loss: 0.01573159983259057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05982208251953125 0.10914921760559082

Final encoder loss: 0.014408713381376956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06151008605957031 0.10786104202270508

Final encoder loss: 0.015325759057882814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05998969078063965 0.10763406753540039

Final encoder loss: 0.01690104034033247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06026458740234375 0.10763788223266602

Final encoder loss: 0.01398735537363743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060273170471191406 0.10861444473266602

Final encoder loss: 0.015368077971738581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05985879898071289 0.10759353637695312

Final encoder loss: 0.01483247216871075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06022477149963379 0.10769176483154297

Final encoder loss: 0.015999519443475316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06004953384399414 0.10756969451904297

Final encoder loss: 0.015801971519520567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.06079840660095215 0.1074831485748291


Training dapper model
Final encoder loss: 0.20243951678276062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11739039421081543 0.0347135066986084

Final encoder loss: 0.20822642743587494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11567020416259766 0.03440499305725098

Final encoder loss: 0.08519764989614487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11816692352294922 0.034219980239868164

Final encoder loss: 0.08737530559301376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11639857292175293 0.03428792953491211

Final encoder loss: 0.050255101174116135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11586713790893555 0.03417706489562988

Final encoder loss: 0.05004942789673805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11749839782714844 0.03540325164794922

Final encoder loss: 0.03428907319903374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11635637283325195 0.03422188758850098

Final encoder loss: 0.03388576582074165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11581039428710938 0.034353017807006836

Final encoder loss: 0.02604096755385399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11647748947143555 0.034574031829833984

Final encoder loss: 0.025768183171749115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11709237098693848 0.033991336822509766

Final encoder loss: 0.021475674584507942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11614227294921875 0.03399062156677246

Final encoder loss: 0.021246973425149918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1162714958190918 0.034740447998046875

Final encoder loss: 0.018749039620161057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11826086044311523 0.03357696533203125

Final encoder loss: 0.018535980954766273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11627006530761719 0.03462696075439453

Final encoder loss: 0.017112238332629204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11617898941040039 0.03385519981384277

Final encoder loss: 0.016860665753483772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11837363243103027 0.03486204147338867

Final encoder loss: 0.016111616045236588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11694812774658203 0.03434467315673828

Final encoder loss: 0.01593083143234253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1169586181640625 0.03382086753845215

Final encoder loss: 0.01559266448020935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1172323226928711 0.034538984298706055

Final encoder loss: 0.015368090011179447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11662793159484863 0.034563302993774414

Final encoder loss: 0.015508140437304974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11570572853088379 0.03427410125732422

Final encoder loss: 0.01498251873999834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11719608306884766 0.03497171401977539

Final encoder loss: 0.015188888646662235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11745476722717285 0.03381705284118652

Final encoder loss: 0.014814834110438824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11567234992980957 0.034256696701049805

Final encoder loss: 0.015014326199889183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11615896224975586 0.034522056579589844

Final encoder loss: 0.014832377433776855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11826610565185547 0.03400850296020508

Final encoder loss: 0.014995737932622433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559247970581055 0.03448987007141113

Final encoder loss: 0.014964044094085693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11590027809143066 0.034116506576538086

Final encoder loss: 0.014511572197079659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11717009544372559 0.03517508506774902

Final encoder loss: 0.014317604713141918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11604094505310059 0.034336090087890625

Final encoder loss: 0.014218454249203205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11608505249023438 0.03425908088684082

Final encoder loss: 0.014088381081819534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11665964126586914 0.03538775444030762

Final encoder loss: 0.013853847049176693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11667346954345703 0.03444933891296387

Final encoder loss: 0.013752362690865993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1165308952331543 0.03367972373962402

Final encoder loss: 0.013643370009958744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559581756591797 0.03499770164489746

Final encoder loss: 0.013639392331242561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11826562881469727 0.033985137939453125

Final encoder loss: 0.013510939665138721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11640143394470215 0.03476834297180176

Final encoder loss: 0.013250130228698254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595821380615234 0.03440666198730469

Final encoder loss: 0.013470378704369068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11788773536682129 0.03520369529724121

Final encoder loss: 0.013180791400372982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11623358726501465 0.034047603607177734

Final encoder loss: 0.013377477414906025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11568021774291992 0.03442549705505371

Final encoder loss: 0.013086478225886822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11728334426879883 0.034483909606933594

Final encoder loss: 0.013244994916021824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1169278621673584 0.03377819061279297

Final encoder loss: 0.013092165812849998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595964431762695 0.033933401107788086

Final encoder loss: 0.013042056933045387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11547040939331055 0.03476142883300781

Final encoder loss: 0.012853910215198994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11794066429138184 0.033371686935424805

Final encoder loss: 0.012959831394255161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11603307723999023 0.03463864326477051

Final encoder loss: 0.012887735851109028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11666655540466309 0.0347895622253418

Final encoder loss: 0.01301155611872673
Final encoder loss: 0.011963468044996262

Training case model
Final encoder loss: 0.02214513829990665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08933377265930176 0.21919655799865723

Final encoder loss: 0.022642131142999126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08939933776855469 0.2193286418914795

Final encoder loss: 0.021997847517025843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0894770622253418 0.21953034400939941

Final encoder loss: 0.022306550537915953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08951258659362793 0.21918797492980957

Final encoder loss: 0.022312605165359226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08949470520019531 0.21916580200195312

Final encoder loss: 0.02246403622958058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09018492698669434 0.2193756103515625

Final encoder loss: 0.022255637587804016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08974480628967285 0.21936917304992676

Final encoder loss: 0.021934672287776628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.09078741073608398 0.21950531005859375

Final encoder loss: 0.022036622812941283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08915352821350098 0.21907877922058105

Final encoder loss: 0.02173308405207009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0906069278717041 0.21904301643371582

Final encoder loss: 0.021853013972071685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08925819396972656 0.2191300392150879

Final encoder loss: 0.021830795962610283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08979535102844238 0.2194526195526123

Final encoder loss: 0.02187764820362212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.088958740234375 0.21919918060302734

Final encoder loss: 0.022123640872428567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08946895599365234 0.2190406322479248

Final encoder loss: 0.021578895290812722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08918571472167969 0.21935272216796875

Final encoder loss: 0.021911028700979718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08641219139099121 0.21599268913269043


Training case model
Final encoder loss: 0.20296165347099304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2651519775390625 0.052599430084228516

Final encoder loss: 0.18890494108200073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2798788547515869 0.05299067497253418

Final encoder loss: 0.19015605747699738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2574281692504883 0.05349159240722656

Final encoder loss: 0.19217760860919952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2683682441711426 0.05231928825378418

Final encoder loss: 0.1808250993490219
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2687845230102539 0.05400705337524414

Final encoder loss: 0.19191747903823853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558159828186035 0.05168557167053223

Final encoder loss: 0.10514522343873978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27649354934692383 0.05524945259094238

Final encoder loss: 0.09464027732610703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2764403820037842 0.052535295486450195

Final encoder loss: 0.09078964591026306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587404251098633 0.05247187614440918

Final encoder loss: 0.08974090218544006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27789950370788574 0.05125141143798828

Final encoder loss: 0.08110158145427704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2690610885620117 0.05317401885986328

Final encoder loss: 0.08502139896154404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.256009578704834 0.050980567932128906

Final encoder loss: 0.06123663857579231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2600221633911133 0.05280661582946777

Final encoder loss: 0.05530310794711113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28958940505981445 0.05456686019897461

Final encoder loss: 0.053265463560819626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683377265930176 0.053369760513305664

Final encoder loss: 0.05351344868540764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27806878089904785 0.05207490921020508

Final encoder loss: 0.049668774008750916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2686460018157959 0.05281543731689453

Final encoder loss: 0.05199901759624481
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2568209171295166 0.05371856689453125

Final encoder loss: 0.04281354695558548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2689223289489746 0.052779197692871094

Final encoder loss: 0.03925333172082901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26868271827697754 0.05177569389343262

Final encoder loss: 0.038124874234199524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2786290645599365 0.053255319595336914

Final encoder loss: 0.03867320343852043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28481030464172363 0.05262398719787598

Final encoder loss: 0.03708774968981743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.276792049407959 0.05333209037780762

Final encoder loss: 0.03819587081670761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26113319396972656 0.05208444595336914

Final encoder loss: 0.03448327258229256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27007365226745605 0.05277419090270996

Final encoder loss: 0.03264723718166351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2700345516204834 0.05263853073120117

Final encoder loss: 0.031867995858192444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.28957319259643555 0.05270242691040039

Final encoder loss: 0.032411396503448486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2882683277130127 0.053076982498168945

Final encoder loss: 0.03220444172620773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2772397994995117 0.05177044868469238

Final encoder loss: 0.032431602478027344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557487487792969 0.05196881294250488

Final encoder loss: 0.03086964786052704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2730827331542969 0.05255913734436035

Final encoder loss: 0.02986687608063221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26756930351257324 0.05253434181213379

Final encoder loss: 0.029500136151909828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26770472526550293 0.052726030349731445

Final encoder loss: 0.02981802448630333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27327466011047363 0.05140042304992676

Final encoder loss: 0.0304509736597538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2563800811767578 0.05165863037109375

Final encoder loss: 0.029980216175317764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2536492347717285 0.05228400230407715

Final encoder loss: 0.02858836017549038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569282054901123 0.051824331283569336

Final encoder loss: 0.027736058458685875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2673323154449463 0.053548574447631836

Final encoder loss: 0.02740965224802494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27364134788513184 0.05364561080932617

Final encoder loss: 0.02761092782020569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2696683406829834 0.05105900764465332

Final encoder loss: 0.02808612398803234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26885008811950684 0.05452752113342285

Final encoder loss: 0.027918264269828796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545433044433594 0.05206871032714844

Final encoder loss: 0.026755521073937416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2689023017883301 0.05296611785888672

Final encoder loss: 0.02607342228293419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27033162117004395 0.053208351135253906

Final encoder loss: 0.025862859562039375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2790813446044922 0.05154156684875488

Final encoder loss: 0.02610461227595806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27592015266418457 0.05448508262634277

Final encoder loss: 0.026784678921103477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2577826976776123 0.05251669883728027

Final encoder loss: 0.026384782046079636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564542293548584 0.05122542381286621

Final encoder loss: 0.025719348341226578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25992703437805176 0.05295610427856445

Final encoder loss: 0.02519228495657444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27535319328308105 0.052396535873413086

Final encoder loss: 0.024928811937570572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25847721099853516 0.052989959716796875

Final encoder loss: 0.025042010471224785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26859593391418457 0.051490068435668945

Final encoder loss: 0.02605694904923439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2776672840118408 0.052103281021118164

Final encoder loss: 0.02540626749396324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25647902488708496 0.05232954025268555

Final encoder loss: 0.02501164935529232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2681007385253906 0.05390572547912598

Final encoder loss: 0.024717876687645912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2795224189758301 0.052306175231933594

Final encoder loss: 0.024592461064457893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26967597007751465 0.052706003189086914

Final encoder loss: 0.02454923652112484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2776782512664795 0.05309939384460449

Final encoder loss: 0.025286071002483368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2719290256500244 0.051798105239868164

Final encoder loss: 0.024924438446760178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552225589752197 0.0519251823425293

Final encoder loss: 0.0244140662252903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25824475288391113 0.05203819274902344

Final encoder loss: 0.024118056520819664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28472423553466797 0.052590131759643555

Final encoder loss: 0.023799192160367966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2797880172729492 0.05216860771179199

Final encoder loss: 0.02393347956240177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2779996395111084 0.05377340316772461

Final encoder loss: 0.024672171100974083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27644920349121094 0.05253744125366211

Final encoder loss: 0.024348147213459015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549598217010498 0.0505983829498291

Final encoder loss: 0.024044068530201912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2591896057128906 0.05078768730163574

Final encoder loss: 0.023648325353860855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2806096076965332 0.05281686782836914

Final encoder loss: 0.023603038862347603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2585747241973877 0.054003000259399414

Final encoder loss: 0.023396560922265053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26770997047424316 0.052935123443603516

Final encoder loss: 0.02446647733449936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2790191173553467 0.05319333076477051

Final encoder loss: 0.023869436234235764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.256669282913208 0.05293893814086914

Final encoder loss: 0.02365097403526306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25879859924316406 0.05360102653503418

Final encoder loss: 0.023377353325486183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26879048347473145 0.052352190017700195

Final encoder loss: 0.023258747532963753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26914143562316895 0.054694414138793945

Final encoder loss: 0.02309536002576351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28539037704467773 0.053395986557006836

Final encoder loss: 0.02394280582666397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2694730758666992 0.05161571502685547

Final encoder loss: 0.023549223318696022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548863887786865 0.05208110809326172

Final encoder loss: 0.02342214621603489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2679879665374756 0.05149579048156738

Final encoder loss: 0.02318989299237728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584381103515625 0.05301547050476074

Final encoder loss: 0.02306561917066574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26003170013427734 0.05184531211853027

Final encoder loss: 0.022938957437872887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2689533233642578 0.055626869201660156

Final encoder loss: 0.02372240088880062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2680013179779053 0.05354714393615723

Final encoder loss: 0.0232919342815876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560551166534424 0.05232977867126465

Final encoder loss: 0.023134008049964905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25779199600219727 0.05239725112915039

Final encoder loss: 0.022979389876127243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2707252502441406 0.05231952667236328

Final encoder loss: 0.02270834520459175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25765085220336914 0.05474090576171875

Final encoder loss: 0.022660667076706886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2748091220855713 0.0520479679107666

Final encoder loss: 0.02345038391649723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25849223136901855 0.052440643310546875

Final encoder loss: 0.023007061332464218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25643301010131836 0.05193758010864258

Final encoder loss: 0.02301023155450821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2692887783050537 0.05380988121032715

Final encoder loss: 0.022718047723174095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2759368419647217 0.05221152305603027

Final encoder loss: 0.022653108462691307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2690262794494629 0.05324244499206543

Final encoder loss: 0.022336073219776154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2680346965789795 0.05184173583984375

Final encoder loss: 0.02339276671409607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2821345329284668 0.050977230072021484

Final encoder loss: 0.0228722020983696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25368380546569824 0.05142378807067871

Final encoder loss: 0.022788573056459427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2562081813812256 0.052110910415649414

Final encoder loss: 0.022565776482224464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27353405952453613 0.052229881286621094

Final encoder loss: 0.022425925359129906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2664456367492676 0.0523991584777832

Final encoder loss: 0.02226707525551319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26665425300598145 0.05172896385192871

Final encoder loss: 0.022981397807598114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28125739097595215 0.051274776458740234

Final encoder loss: 0.02263733744621277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540864944458008 0.05127573013305664

Final encoder loss: 0.022700926288962364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25647974014282227 0.05143022537231445

Final encoder loss: 0.02238498628139496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2674713134765625 0.05502176284790039

Final encoder loss: 0.02235696092247963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2747368812561035 0.05342888832092285

Final encoder loss: 0.022180698812007904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26845812797546387 0.0515744686126709

Final encoder loss: 0.02299097366631031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2698826789855957 0.052104949951171875

Final encoder loss: 0.022465405985713005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255495548248291 0.05272388458251953

Final encoder loss: 0.022457638755440712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25850844383239746 0.05365610122680664

Final encoder loss: 0.022366106510162354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2787437438964844 0.05234241485595703

Final encoder loss: 0.022128691896796227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25798511505126953 0.052834510803222656

Final encoder loss: 0.02190612256526947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2684955596923828 0.05338168144226074

Final encoder loss: 0.022701634094119072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.298175573348999 0.05234026908874512

Final encoder loss: 0.022364826872944832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550981044769287 0.05187177658081055

Final encoder loss: 0.022413544356822968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2688431739807129 0.05420970916748047

Final encoder loss: 0.022168446332216263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26879024505615234 0.052519798278808594

Final encoder loss: 0.022072775289416313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2590796947479248 0.054306983947753906

Final encoder loss: 0.021797288209199905
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2759866714477539 0.051323890686035156

Final encoder loss: 0.022729938849806786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581191062927246 0.05266761779785156

Final encoder loss: 0.022246867418289185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25647878646850586 0.05196666717529297

Final encoder loss: 0.02229982241988182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2692439556121826 0.052100181579589844

Final encoder loss: 0.022083522751927376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27657556533813477 0.05231332778930664

Final encoder loss: 0.021934520453214645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683901786804199 0.05301070213317871

Final encoder loss: 0.021676888689398766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.280820369720459 0.05164527893066406

Final encoder loss: 0.022407755255699158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2671070098876953 0.052304983139038086

Final encoder loss: 0.022093266248703003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2533078193664551 0.05157613754272461

Final encoder loss: 0.02219352126121521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2673459053039551 0.052437782287597656

Final encoder loss: 0.022009000182151794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28082776069641113 0.051647186279296875

Final encoder loss: 0.021900860592722893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25615572929382324 0.05236077308654785

Final encoder loss: 0.021693620830774307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2797524929046631 0.052336931228637695

Final encoder loss: 0.02247699722647667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26671934127807617 0.05291295051574707

Final encoder loss: 0.02196696028113365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25330209732055664 0.0515134334564209

Final encoder loss: 0.022071421146392822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576103210449219 0.053086280822753906

Final encoder loss: 0.021927587687969208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2574155330657959 0.051183223724365234

Final encoder loss: 0.021769726648926735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26888513565063477 0.0526430606842041

Final encoder loss: 0.021432066336274147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28459811210632324 0.052149295806884766

Final encoder loss: 0.022313952445983887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2684946060180664 0.052216529846191406

Final encoder loss: 0.021919801831245422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545504570007324 0.052256107330322266

Final encoder loss: 0.022064881399273872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27590441703796387 0.05222201347351074

Final encoder loss: 0.02181142382323742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2679131031036377 0.05245232582092285

Final encoder loss: 0.021721825003623962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2766687870025635 0.05234837532043457

Final encoder loss: 0.02138459123671055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257641077041626 0.052591800689697266

Final encoder loss: 0.022203980013728142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2682454586029053 0.05204916000366211

Final encoder loss: 0.021784136071801186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556118965148926 0.05213022232055664

Final encoder loss: 0.021942874416708946
Final encoder loss: 0.021317031234502792
Final encoder loss: 0.02062702365219593
Final encoder loss: 0.01972312480211258
Final encoder loss: 0.019696500152349472
Final encoder loss: 0.018673231825232506

Training emognition model
Final encoder loss: 0.023851202999516718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08261728286743164 0.23059320449829102

Final encoder loss: 0.02509686403927889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.0808725357055664 0.2306523323059082

Final encoder loss: 0.024797089108390347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08083128929138184 0.23070001602172852

Final encoder loss: 0.02360935877545345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08088183403015137 0.23060274124145508

Final encoder loss: 0.02452297049643259
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08103203773498535 0.23058795928955078

Final encoder loss: 0.024464455763067716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08086299896240234 0.231428861618042

Final encoder loss: 0.025257736336922937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08258628845214844 0.23156189918518066

Final encoder loss: 0.025064926653469377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08257317543029785 0.23252201080322266

Final encoder loss: 0.024284223612604044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08107161521911621 0.23050761222839355

Final encoder loss: 0.024213367957477673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08150482177734375 0.23111724853515625

Final encoder loss: 0.024557258227406505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0815885066986084 0.23097896575927734

Final encoder loss: 0.024922212054262937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08186602592468262 0.2307271957397461

Final encoder loss: 0.02578330778463607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08140730857849121 0.23065876960754395

Final encoder loss: 0.025950886708569005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08135151863098145 0.23096585273742676

Final encoder loss: 0.02537116103468546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.0823218822479248 0.22971057891845703

Final encoder loss: 0.023173680056466573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.0801382064819336 0.22969484329223633


Training emognition model
Final encoder loss: 0.19357016682624817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25013232231140137 0.04830145835876465

Final encoder loss: 0.19496624171733856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.248488187789917 0.04863739013671875

Final encoder loss: 0.08878722041845322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24698162078857422 0.04896068572998047

Final encoder loss: 0.08850859850645065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2465829849243164 0.04821515083312988

Final encoder loss: 0.05637305974960327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478313446044922 0.0491788387298584

Final encoder loss: 0.05490097030997276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24624204635620117 0.049408912658691406

Final encoder loss: 0.041811686009168625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24729299545288086 0.04926633834838867

Final encoder loss: 0.04069443792104721
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24586939811706543 0.04911971092224121

Final encoder loss: 0.03433068096637726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24689316749572754 0.04915213584899902

Final encoder loss: 0.03358999267220497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24788188934326172 0.04961061477661133

Final encoder loss: 0.030146226286888123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24724483489990234 0.0497889518737793

Final encoder loss: 0.029623666778206825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2459421157836914 0.049344778060913086

Final encoder loss: 0.027760658413171768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24738764762878418 0.04904603958129883

Final encoder loss: 0.027369137853384018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24577569961547852 0.04912614822387695

Final encoder loss: 0.026498470455408096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2473430633544922 0.04861116409301758

Final encoder loss: 0.026276694610714912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24574875831604004 0.048586368560791016

Final encoder loss: 0.025882286950945854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24633312225341797 0.048815011978149414

Final encoder loss: 0.025818852707743645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24543309211730957 0.04863715171813965

Final encoder loss: 0.02545539289712906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2468869686126709 0.04983258247375488

Final encoder loss: 0.025552526116371155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2454392910003662 0.04858684539794922

Final encoder loss: 0.025054937228560448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24680662155151367 0.05006551742553711

Final encoder loss: 0.02519199810922146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2456200122833252 0.04962515830993652

Final encoder loss: 0.024574674665927887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24775934219360352 0.04889512062072754

Final encoder loss: 0.024804482236504555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450568675994873 0.048332929611206055

Final encoder loss: 0.02431989647448063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24729371070861816 0.04985666275024414

Final encoder loss: 0.024521926417946815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24520277976989746 0.049118995666503906

Final encoder loss: 0.02441543899476528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24703311920166016 0.04742741584777832

Final encoder loss: 0.024409160017967224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24594950675964355 0.04865431785583496

Final encoder loss: 0.024200430139899254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24704527854919434 0.049170732498168945

Final encoder loss: 0.024339798837900162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2452538013458252 0.04920506477355957

Final encoder loss: 0.023994579911231995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24840736389160156 0.049424171447753906

Final encoder loss: 0.024151477962732315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24590063095092773 0.04882359504699707

Final encoder loss: 0.02380656823515892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2471625804901123 0.04906010627746582

Final encoder loss: 0.023987896740436554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24568819999694824 0.04943084716796875

Final encoder loss: 0.02371448278427124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24700307846069336 0.04952263832092285

Final encoder loss: 0.02381441369652748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2456531524658203 0.04887676239013672

Final encoder loss: 0.023605765774846077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24658918380737305 0.0494687557220459

Final encoder loss: 0.023678341880440712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24720191955566406 0.04959464073181152

Final encoder loss: 0.023481950163841248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472856044769287 0.049681901931762695

Final encoder loss: 0.023663390427827835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24590229988098145 0.04896807670593262

Final encoder loss: 0.0233747698366642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24688959121704102 0.05006265640258789

Final encoder loss: 0.023630768060684204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24523448944091797 0.04889369010925293

Final encoder loss: 0.023329585790634155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24760842323303223 0.05016970634460449

Final encoder loss: 0.02355142869055271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24567818641662598 0.04928135871887207

Final encoder loss: 0.023356517776846886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2471637725830078 0.0491175651550293

Final encoder loss: 0.023530401289463043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24535608291625977 0.04894661903381348

Final encoder loss: 0.023315077647566795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2639603614807129 0.0489659309387207

Final encoder loss: 0.023496555164456367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.29570674896240234 0.04826521873474121

Final encoder loss: 0.023291990160942078
Final encoder loss: 0.022533951327204704

Training empatch model
Final encoder loss: 0.03715551273012517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07118535041809082 0.17331957817077637

Final encoder loss: 0.03265090089557422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07080769538879395 0.1730043888092041

Final encoder loss: 0.03405755721388958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07030010223388672 0.17332148551940918

Final encoder loss: 0.03296652044497455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07090115547180176 0.1726527214050293

Final encoder loss: 0.033205752297261644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07086992263793945 0.17290687561035156

Final encoder loss: 0.03253810811816611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07041430473327637 0.17278099060058594

Final encoder loss: 0.031298874832717705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07097411155700684 0.1729443073272705

Final encoder loss: 0.029200750550793325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07033181190490723 0.17265820503234863

Final encoder loss: 0.024877873550166907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0706477165222168 0.17267084121704102

Final encoder loss: 0.02229314108046066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07062339782714844 0.17267513275146484

Final encoder loss: 0.023442929208888852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07073378562927246 0.1737511157989502

Final encoder loss: 0.02462920968318402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07092142105102539 0.1733245849609375

Final encoder loss: 0.023737804234055682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0708003044128418 0.1729567050933838

Final encoder loss: 0.023119255324760685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07059264183044434 0.1730177402496338

Final encoder loss: 0.022912103916510707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07091784477233887 0.1726081371307373

Final encoder loss: 0.022980389984653515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07117724418640137 0.17270374298095703


Training empatch model
Final encoder loss: 0.1711716651916504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17519140243530273 0.043707847595214844

Final encoder loss: 0.08069854229688644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750164031982422 0.0433955192565918

Final encoder loss: 0.054925817996263504
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1749417781829834 0.0428471565246582

Final encoder loss: 0.04245276004076004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1743454933166504 0.0428922176361084

Final encoder loss: 0.035325758159160614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17383813858032227 0.043328285217285156

Final encoder loss: 0.03085314854979515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17448997497558594 0.04301905632019043

Final encoder loss: 0.02791384980082512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17497801780700684 0.04324746131896973

Final encoder loss: 0.02590964548289776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.174058198928833 0.04288601875305176

Final encoder loss: 0.024548694491386414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1737837791442871 0.043886423110961914

Final encoder loss: 0.023609774187207222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17421388626098633 0.043241024017333984

Final encoder loss: 0.02295815944671631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17391633987426758 0.04326510429382324

Final encoder loss: 0.02245837077498436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17368316650390625 0.04284310340881348

Final encoder loss: 0.022175272926688194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17407846450805664 0.043187856674194336

Final encoder loss: 0.02190844528377056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17383408546447754 0.043019771575927734

Final encoder loss: 0.02170211635529995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17462968826293945 0.04287290573120117

Final encoder loss: 0.02142881043255329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17353510856628418 0.04382920265197754

Final encoder loss: 0.021280910819768906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17339706420898438 0.04275631904602051

Final encoder loss: 0.02118573896586895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17313861846923828 0.04214000701904297

Final encoder loss: 0.02114744484424591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.173600435256958 0.042967796325683594

Final encoder loss: 0.02107388898730278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1731247901916504 0.04306936264038086

Final encoder loss: 0.021008025854825974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21248674392700195 0.04271411895751953

Final encoder loss: 0.02083735354244709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2205641269683838 0.04268360137939453

Final encoder loss: 0.020738612860441208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21570706367492676 0.0434267520904541

Final encoder loss: 0.020660605281591415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.22171735763549805 0.04358315467834473

Final encoder loss: 0.020635131746530533

Training wesad model
Final encoder loss: 0.03368644970198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07112240791320801 0.1728830337524414

Final encoder loss: 0.03391811024815229
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07079172134399414 0.1729447841644287

Final encoder loss: 0.031936584744169834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07079195976257324 0.17307710647583008

Final encoder loss: 0.034354530003600496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07074475288391113 0.17309999465942383

Final encoder loss: 0.02141632254210238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0710000991821289 0.1724834442138672

Final encoder loss: 0.022023713270095318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07111811637878418 0.17302966117858887

Final encoder loss: 0.022893715204580882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07084465026855469 0.17297911643981934

Final encoder loss: 0.023362470332801928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07076668739318848 0.173095703125

Final encoder loss: 0.016504837131134115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07029199600219727 0.17246007919311523

Final encoder loss: 0.01725085301695683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07035708427429199 0.17361855506896973

Final encoder loss: 0.01805625273907421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07108616828918457 0.17377090454101562

Final encoder loss: 0.018261345190754952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07108879089355469 0.17427992820739746

Final encoder loss: 0.014137225378227671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07257795333862305 0.17443418502807617

Final encoder loss: 0.013940785587460426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07163357734680176 0.17403292655944824

Final encoder loss: 0.014366130649656904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07113504409790039 0.1740431785583496

Final encoder loss: 0.014147237928517404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07261419296264648 0.1745467185974121


Training wesad model
Final encoder loss: 0.21557344496250153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10532736778259277 0.032929420471191406

Final encoder loss: 0.09939039498567581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409069061279297 0.03369402885437012

Final encoder loss: 0.06386064738035202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1044304370880127 0.03323984146118164

Final encoder loss: 0.04621478542685509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1033473014831543 0.033490896224975586

Final encoder loss: 0.0359715037047863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1061406135559082 0.034043312072753906

Final encoder loss: 0.029663005843758583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10682225227355957 0.03340506553649902

Final encoder loss: 0.02556527964770794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10364127159118652 0.03389286994934082

Final encoder loss: 0.022805068641901016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037139892578125 0.03369307518005371

Final encoder loss: 0.02089935913681984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10409832000732422 0.033055782318115234

Final encoder loss: 0.019615458324551582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456204414367676 0.033782958984375

Final encoder loss: 0.018779410049319267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10594749450683594 0.03375697135925293

Final encoder loss: 0.018299495801329613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10493731498718262 0.03325057029724121

Final encoder loss: 0.017971375957131386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10403084754943848 0.03331136703491211

Final encoder loss: 0.017787057906389236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10373735427856445 0.03358292579650879

Final encoder loss: 0.0176254715770483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375380516052246 0.0330808162689209

Final encoder loss: 0.01749446988105774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10456109046936035 0.03411555290222168

Final encoder loss: 0.017277982085943222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10618877410888672 0.03498959541320801

Final encoder loss: 0.017335370182991028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10381674766540527 0.033649444580078125

Final encoder loss: 0.017445381730794907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10453462600708008 0.03379082679748535

Final encoder loss: 0.01754586771130562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385394096374512 0.033705949783325195

Final encoder loss: 0.017495794221758842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10396981239318848 0.03350186347961426

Final encoder loss: 0.017400896176695824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10532999038696289 0.03448796272277832

Final encoder loss: 0.017329052090644836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10649800300598145 0.03305792808532715

Final encoder loss: 0.01730022393167019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10370373725891113 0.03336501121520996

Final encoder loss: 0.017186759039759636

Calculating loss for amigos model
	Full Pass 0.7084598541259766
numFreeParamsPath 18
Reconstruction loss values: 0.02595026232302189 0.03457067161798477

Calculating loss for dapper model
	Full Pass 0.15162944793701172
numFreeParamsPath 18
Reconstruction loss values: 0.021119745448231697 0.024647172540426254

Calculating loss for case model
	Full Pass 0.8618223667144775
numFreeParamsPath 18
Reconstruction loss values: 0.03079415298998356 0.03389177843928337

Calculating loss for emognition model
	Full Pass 0.28444552421569824
numFreeParamsPath 18
Reconstruction loss values: 0.032072342932224274 0.039784543216228485

Calculating loss for empatch model
	Full Pass 0.10446405410766602
numFreeParamsPath 18
Reconstruction loss values: 0.03402858227491379 0.041001252830028534

Calculating loss for wesad model
	Full Pass 0.07725119590759277
numFreeParamsPath 18
Reconstruction loss values: 0.03409598767757416 0.050762441009283066
Total loss calculation time: 4.6558825969696045

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.644640922546387
Total epoch time: 223.85988116264343

Epoch: 49

Training case model
Final encoder loss: 0.029817589782534844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.10106778144836426 0.27480506896972656

Final encoder loss: 0.027765289500669622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.0915842056274414 0.2647740840911865

Final encoder loss: 0.027947112183743367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09276580810546875 0.2661147117614746

Final encoder loss: 0.026919410363175605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09167790412902832 0.26578688621520996

Final encoder loss: 0.025733105954458033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09165024757385254 0.2660238742828369

Final encoder loss: 0.02579793696104451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09128022193908691 0.2651336193084717

Final encoder loss: 0.025069824929620038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09185290336608887 0.2650570869445801

Final encoder loss: 0.025152427291611212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09241938591003418 0.2671654224395752

Final encoder loss: 0.02479754039329383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09193897247314453 0.2652275562286377

Final encoder loss: 0.025146752296487485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09202718734741211 0.2664225101470947

Final encoder loss: 0.024784316831655554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09139776229858398 0.2659120559692383

Final encoder loss: 0.024582462671778833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0914454460144043 0.2659189701080322

Final encoder loss: 0.02400286551512178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09247446060180664 0.26518678665161133

Final encoder loss: 0.024497017264626074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09163212776184082 0.2657496929168701

Final encoder loss: 0.024050310525510963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09329366683959961 0.26700782775878906

Final encoder loss: 0.023722125760181224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08905029296875 0.262270450592041


Training amigos model
Final encoder loss: 0.027389149221540048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10884666442871094 0.3905203342437744

Final encoder loss: 0.026272058423610082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10871601104736328 0.38917040824890137

Final encoder loss: 0.02427896561843988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10993409156799316 0.39158010482788086

Final encoder loss: 0.023963065060975872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10822534561157227 0.3895077705383301

Final encoder loss: 0.02575424223476156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10939383506774902 0.38929176330566406

Final encoder loss: 0.023171823933359162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10833930969238281 0.3891160488128662

Final encoder loss: 0.025577840770415167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1083669662475586 0.39002275466918945

Final encoder loss: 0.02473147321477266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10976910591125488 0.3904378414154053

Final encoder loss: 0.02637457926892054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10844850540161133 0.389202356338501

Final encoder loss: 0.02467043422800103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.11028742790222168 0.3908064365386963

Final encoder loss: 0.026738801193068942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10821080207824707 0.38959574699401855

Final encoder loss: 0.025927016018095942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1087644100189209 0.38977742195129395

Final encoder loss: 0.02562235679372165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10855412483215332 0.39008164405822754

Final encoder loss: 0.025904803645927257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10911154747009277 0.3899838924407959

Final encoder loss: 0.025192532759126793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10917234420776367 0.39073848724365234

Final encoder loss: 0.023778588366127146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.1038048267364502 0.38460612297058105


Training dapper model
Final encoder loss: 0.020585090828694818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06324267387390137 0.15202546119689941

Final encoder loss: 0.01941524235404777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.062077999114990234 0.15050148963928223

Final encoder loss: 0.019653775253601773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062172889709472656 0.15072202682495117

Final encoder loss: 0.019568764269560012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06214618682861328 0.15150213241577148

Final encoder loss: 0.02003768651065429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06384730339050293 0.1527554988861084

Final encoder loss: 0.021251226113782884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.061850547790527344 0.1503288745880127

Final encoder loss: 0.019718862288719625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06242036819458008 0.15048837661743164

Final encoder loss: 0.019572872472613745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06218886375427246 0.15137577056884766

Final encoder loss: 0.01882320099835094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06374478340148926 0.15164566040039062

Final encoder loss: 0.018776479186843456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06176447868347168 0.15129899978637695

Final encoder loss: 0.01946174054086138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.0622859001159668 0.1507859230041504

Final encoder loss: 0.0188683557883381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06199336051940918 0.15105748176574707

Final encoder loss: 0.021958603898995628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06389856338500977 0.15202665328979492

Final encoder loss: 0.020302800940422177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.50636887550354 0.15201067924499512

Final encoder loss: 0.0222274614302472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06399917602539062 0.15164518356323242

Final encoder loss: 0.02075675940952327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06162524223327637 0.1497788429260254


Training emognition model
Final encoder loss: 0.034449316249185964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08431839942932129 0.2744290828704834

Final encoder loss: 0.03345682609481168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08394598960876465 0.2767207622528076

Final encoder loss: 0.033158909009267136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08395552635192871 0.275862455368042

Final encoder loss: 0.03264908551493382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08404946327209473 0.27590036392211914

Final encoder loss: 0.03169589452198176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08509588241577148 0.27487659454345703

Final encoder loss: 0.03248215473519491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0836641788482666 0.27608656883239746

Final encoder loss: 0.031129240168401958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08533954620361328 0.2771646976470947

Final encoder loss: 0.03139640885780961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08351302146911621 0.2812161445617676

Final encoder loss: 0.030754674525342713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.0835714340209961 0.2765161991119385

Final encoder loss: 0.031163134704192484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08321213722229004 0.2755095958709717

Final encoder loss: 0.031081552915914037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08356952667236328 0.2754504680633545

Final encoder loss: 0.030933985728388704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08500385284423828 0.27953267097473145

Final encoder loss: 0.030075826828590405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08383679389953613 0.2753617763519287

Final encoder loss: 0.0304141609893263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08478832244873047 0.2767307758331299

Final encoder loss: 0.031227355992538158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08341145515441895 0.2765016555786133

Final encoder loss: 0.031506836796028184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08279705047607422 0.2745392322540283


Training amigos model
Final encoder loss: 0.020721789438503932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10686230659484863 0.34143567085266113

Final encoder loss: 0.018658247253553365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10640406608581543 0.34300804138183594

Final encoder loss: 0.021718327518563067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10678482055664062 0.34166526794433594

Final encoder loss: 0.01806823976192169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10621428489685059 0.34143567085266113

Final encoder loss: 0.0181950607698908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10602521896362305 0.34174275398254395

Final encoder loss: 0.018239405050482153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10624933242797852 0.34152674674987793

Final encoder loss: 0.018842287505187447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10619211196899414 0.3415098190307617

Final encoder loss: 0.018181321771586684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1062326431274414 0.3417234420776367

Final encoder loss: 0.019133304743047547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.1060783863067627 0.3415699005126953

Final encoder loss: 0.019172963090543824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10591650009155273 0.3418097496032715

Final encoder loss: 0.01714730228288409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10671567916870117 0.3416252136230469

Final encoder loss: 0.019160617997582393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10693216323852539 0.34168457984924316

Final encoder loss: 0.019238956727631786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10656523704528809 0.34160327911376953

Final encoder loss: 0.018939456562393257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10759234428405762 0.3420267105102539

Final encoder loss: 0.01995012823919004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10621285438537598 0.34180450439453125

Final encoder loss: 0.019900824544313564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10236811637878418 0.33840370178222656


Training amigos model
Final encoder loss: 0.18074378371238708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44501709938049316 0.08246326446533203

Final encoder loss: 0.18783381581306458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47198033332824707 0.07885456085205078

Final encoder loss: 0.1836395561695099
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43835902214050293 0.07493448257446289

Final encoder loss: 0.07663874328136444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4361295700073242 0.07745051383972168

Final encoder loss: 0.07835149765014648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45981812477111816 0.07821893692016602

Final encoder loss: 0.07213368266820908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44389939308166504 0.07335519790649414

Final encoder loss: 0.044521264731884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4594554901123047 0.08307003974914551

Final encoder loss: 0.04522731527686119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45671510696411133 0.08127045631408691

Final encoder loss: 0.042194727808237076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4429612159729004 0.07602906227111816

Final encoder loss: 0.03185587748885155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4522981643676758 0.07493948936462402

Final encoder loss: 0.032618600875139236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4631173610687256 0.07329893112182617

Final encoder loss: 0.03110348992049694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44019079208374023 0.0727088451385498

Final encoder loss: 0.026036115363240242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4370534420013428 0.08260369300842285

Final encoder loss: 0.026779120787978172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45656561851501465 0.07479691505432129

Final encoder loss: 0.02579997107386589
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43518710136413574 0.07447576522827148

Final encoder loss: 0.0233332309871912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45448875427246094 0.07887840270996094

Final encoder loss: 0.02414879947900772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4474761486053467 0.07698488235473633

Final encoder loss: 0.023246949538588524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4395627975463867 0.082061767578125

Final encoder loss: 0.022612733766436577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44555068016052246 0.07756328582763672

Final encoder loss: 0.0230232086032629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4733610153198242 0.07394695281982422

Final encoder loss: 0.022440019994974136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43033266067504883 0.07318687438964844

Final encoder loss: 0.02246912755072117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44594740867614746 0.0741415023803711

Final encoder loss: 0.0225357823073864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43328142166137695 0.0749046802520752

Final encoder loss: 0.02223316766321659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42900896072387695 0.07364249229431152

Final encoder loss: 0.02190806157886982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.448535680770874 0.07755208015441895

Final encoder loss: 0.021962758153676987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4513411521911621 0.07899808883666992

Final encoder loss: 0.021759631112217903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43674540519714355 0.07573175430297852

Final encoder loss: 0.02089482545852661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45528221130371094 0.07697796821594238

Final encoder loss: 0.021256910637021065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43631434440612793 0.07493305206298828

Final encoder loss: 0.020988665521144867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4363126754760742 0.07622504234313965

Final encoder loss: 0.0200387854129076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45305633544921875 0.07711458206176758

Final encoder loss: 0.020353827625513077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4541816711425781 0.07436275482177734

Final encoder loss: 0.020300114527344704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43581700325012207 0.07524394989013672

Final encoder loss: 0.019952882081270218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43700742721557617 0.07384991645812988

Final encoder loss: 0.020220216363668442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43786001205444336 0.07732605934143066

Final encoder loss: 0.020101429894566536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43459057807922363 0.07722926139831543

Final encoder loss: 0.019921323284506798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4552605152130127 0.07483625411987305

Final encoder loss: 0.019825462251901627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4537618160247803 0.07590746879577637

Final encoder loss: 0.019916418939828873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4342312812805176 0.07682609558105469

Final encoder loss: 0.019523583352565765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45429277420043945 0.07628273963928223

Final encoder loss: 0.01956309750676155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4374687671661377 0.07427406311035156

Final encoder loss: 0.019657427445054054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43619489669799805 0.07518410682678223

Final encoder loss: 0.019228972494602203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4541940689086914 0.07831549644470215

Final encoder loss: 0.01906578056514263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45448899269104004 0.07400870323181152

Final encoder loss: 0.019288863986730576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43079257011413574 0.0769186019897461

Final encoder loss: 0.019293753430247307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4542503356933594 0.07449746131896973

Final encoder loss: 0.018980510532855988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4359300136566162 0.07526445388793945

Final encoder loss: 0.019134456291794777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4292149543762207 0.0740666389465332

Final encoder loss: 0.019019434228539467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4327101707458496 0.07616591453552246

Final encoder loss: 0.01859571784734726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45540761947631836 0.07795429229736328

Final encoder loss: 0.018963420763611794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4365546703338623 0.08081436157226562

Final encoder loss: 0.018869752064347267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4571723937988281 0.07430696487426758

Final encoder loss: 0.018665364012122154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4438290596008301 0.07511258125305176

Final encoder loss: 0.018933551385998726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4351310729980469 0.07475972175598145

Final encoder loss: 0.018672196194529533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4396631717681885 0.07851624488830566

Final encoder loss: 0.01850433088839054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45487093925476074 0.07823991775512695

Final encoder loss: 0.018662549555301666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44109487533569336 0.07707500457763672

Final encoder loss: 0.018351951614022255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45330238342285156 0.07462739944458008

Final encoder loss: 0.018675127997994423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47005701065063477 0.07611489295959473

Final encoder loss: 0.018728282302618027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43628692626953125 0.0816493034362793

Final encoder loss: 0.018258249387145042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4417450428009033 0.07633471488952637

Final encoder loss: 0.018257539719343185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45747923851013184 0.07682037353515625

Final encoder loss: 0.01839575730264187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4323890209197998 0.0741879940032959

Final encoder loss: 0.01848490908741951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4555957317352295 0.08024477958679199

Final encoder loss: 0.01818169839680195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4399702548980713 0.07628560066223145

Final encoder loss: 0.018468935042619705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4417893886566162 0.07702517509460449

Final encoder loss: 0.01853787526488304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.43818163871765137 0.0775148868560791

Final encoder loss: 0.017993411049246788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45642638206481934 0.0770721435546875

Final encoder loss: 0.01845976710319519
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43611574172973633 0.08370518684387207

Final encoder loss: 0.01826328970491886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4605703353881836 0.07442045211791992

Final encoder loss: 0.018103839829564095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45319032669067383 0.07575273513793945

Final encoder loss: 0.01841897703707218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4356513023376465 0.07594799995422363

Final encoder loss: 0.017966847866773605
Final encoder loss: 0.017058951780200005
Final encoder loss: 0.016540387645363808

Training dapper model
Final encoder loss: 0.015262278897896374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06119132041931152 0.10927128791809082

Final encoder loss: 0.01821124021375322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.060029029846191406 0.10762405395507812

Final encoder loss: 0.01816177898841631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.0597074031829834 0.10753417015075684

Final encoder loss: 0.015145459138890205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05995607376098633 0.1080782413482666

Final encoder loss: 0.015851680151458686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06043267250061035 0.10768365859985352

Final encoder loss: 0.0170774912932377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05971860885620117 0.10773777961730957

Final encoder loss: 0.014092074451222927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05973076820373535 0.10751056671142578

Final encoder loss: 0.013865970102833688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060802459716796875 0.10765910148620605

Final encoder loss: 0.015029204518744654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059624671936035156 0.10733413696289062

Final encoder loss: 0.01682754104970218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05969071388244629 0.10930752754211426

Final encoder loss: 0.014612482747962252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.060913801193237305 0.10825657844543457

Final encoder loss: 0.016112576359829132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.060150861740112305 0.10696220397949219

Final encoder loss: 0.014931917153649199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.06051802635192871 0.10741329193115234

Final encoder loss: 0.013363500612643438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05989503860473633 0.10770058631896973

Final encoder loss: 0.014829642241607755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06060528755187988 0.10759425163269043

Final encoder loss: 0.015192708093355116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.0596308708190918 0.10709357261657715


Training dapper model
Final encoder loss: 0.20244188606739044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11362028121948242 0.034194231033325195

Final encoder loss: 0.2082163542509079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11596035957336426 0.03406643867492676

Final encoder loss: 0.08524452149868011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11307024955749512 0.034894466400146484

Final encoder loss: 0.08680614084005356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11306428909301758 0.033814191818237305

Final encoder loss: 0.050155289471149445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11370635032653809 0.035408973693847656

Final encoder loss: 0.049583934247493744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11327099800109863 0.034436702728271484

Final encoder loss: 0.03400397300720215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11257171630859375 0.034342288970947266

Final encoder loss: 0.03355474770069122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11295247077941895 0.03493547439575195

Final encoder loss: 0.02572820708155632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11468696594238281 0.03403425216674805

Final encoder loss: 0.02545410953462124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1133875846862793 0.03463387489318848

Final encoder loss: 0.0211947001516819
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11287403106689453 0.03365612030029297

Final encoder loss: 0.020919358357787132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11458849906921387 0.03469991683959961

Final encoder loss: 0.018496913835406303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11228132247924805 0.03462648391723633

Final encoder loss: 0.0182363148778677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11258578300476074 0.034583330154418945

Final encoder loss: 0.016858471557497978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11399674415588379 0.03497195243835449

Final encoder loss: 0.016668718308210373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11314916610717773 0.03416728973388672

Final encoder loss: 0.015787919983267784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11406254768371582 0.03406476974487305

Final encoder loss: 0.01583612710237503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11275696754455566 0.03488445281982422

Final encoder loss: 0.015349134802818298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11464858055114746 0.03416705131530762

Final encoder loss: 0.015248710289597511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11339902877807617 0.03497791290283203

Final encoder loss: 0.01491291169077158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11294889450073242 0.03378176689147949

Final encoder loss: 0.014846196398139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11408638954162598 0.03478097915649414

Final encoder loss: 0.014935706742107868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11258053779602051 0.03466653823852539

Final encoder loss: 0.014639973640441895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11327290534973145 0.03405356407165527

Final encoder loss: 0.015148810110986233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1134481430053711 0.03522300720214844

Final encoder loss: 0.014621807262301445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11394906044006348 0.034401655197143555

Final encoder loss: 0.01502129528671503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11290097236633301 0.03417563438415527

Final encoder loss: 0.01432841643691063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11370635032653809 0.03484988212585449

Final encoder loss: 0.014176181517541409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11510324478149414 0.03343081474304199

Final encoder loss: 0.014238481409847736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11296749114990234 0.03407120704650879

Final encoder loss: 0.013702054508030415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11245584487915039 0.0343012809753418

Final encoder loss: 0.014162766747176647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11442065238952637 0.03510880470275879

Final encoder loss: 0.013529512099921703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11297869682312012 0.03401303291320801

Final encoder loss: 0.013724062591791153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11270856857299805 0.033945322036743164

Final encoder loss: 0.013613545335829258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11240077018737793 0.03451657295227051

Final encoder loss: 0.013546374626457691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11444520950317383 0.03425717353820801

Final encoder loss: 0.013414711691439152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11355233192443848 0.033885955810546875

Final encoder loss: 0.01319975033402443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11285901069641113 0.03464388847351074

Final encoder loss: 0.013411887921392918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1149747371673584 0.03478813171386719

Final encoder loss: 0.013119304552674294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1130366325378418 0.03415203094482422

Final encoder loss: 0.013124631717801094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11356735229492188 0.034386396408081055

Final encoder loss: 0.013033237308263779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11362409591674805 0.03516340255737305

Final encoder loss: 0.012791413813829422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11350059509277344 0.034499406814575195

Final encoder loss: 0.012976878322660923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11311054229736328 0.03423428535461426

Final encoder loss: 0.012778243981301785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11257767677307129 0.03513789176940918

Final encoder loss: 0.01288636028766632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1143956184387207 0.03376007080078125

Final encoder loss: 0.012699274346232414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1136465072631836 0.03390908241271973

Final encoder loss: 0.012799199670553207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11293816566467285 0.03423190116882324

Final encoder loss: 0.012678752653300762
Final encoder loss: 0.011790091171860695

Training case model
Final encoder loss: 0.02311855963392007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08962512016296387 0.21903443336486816

Final encoder loss: 0.02234675911980984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09056448936462402 0.21944522857666016

Final encoder loss: 0.02223971229543683
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08951568603515625 0.2191452980041504

Final encoder loss: 0.02225220635951913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09116053581237793 0.21907687187194824

Final encoder loss: 0.022059624249189357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08956575393676758 0.21947884559631348

Final encoder loss: 0.021534588814522873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09052634239196777 0.21935391426086426

Final encoder loss: 0.021659590781742295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08964896202087402 0.2193288803100586

Final encoder loss: 0.0217205802557614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08953595161437988 0.21916747093200684

Final encoder loss: 0.02137990623941623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08927679061889648 0.21920251846313477

Final encoder loss: 0.021114619011528887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0890495777130127 0.2196335792541504

Final encoder loss: 0.022221756954617332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08980178833007812 0.2193286418914795

Final encoder loss: 0.02124633718988399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.0897979736328125 0.2189466953277588

Final encoder loss: 0.021600118890859672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08984565734863281 0.2192974090576172

Final encoder loss: 0.021032989066341896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08965373039245605 0.2193739414215088

Final encoder loss: 0.021690121563151747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09031558036804199 0.21930885314941406

Final encoder loss: 0.021338435980864403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08606314659118652 0.21590971946716309


Training case model
Final encoder loss: 0.20295751094818115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2684621810913086 0.05302596092224121

Final encoder loss: 0.1889151781797409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26145505905151367 0.0522007942199707

Final encoder loss: 0.19014020264148712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25978779792785645 0.05292701721191406

Final encoder loss: 0.19218704104423523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.257781982421875 0.05263113975524902

Final encoder loss: 0.1808164119720459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25864124298095703 0.05079984664916992

Final encoder loss: 0.1919270008802414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571132183074951 0.05093669891357422

Final encoder loss: 0.10525195300579071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588217258453369 0.053986549377441406

Final encoder loss: 0.09482076019048691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25775575637817383 0.053270816802978516

Final encoder loss: 0.09092523157596588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25966453552246094 0.05269169807434082

Final encoder loss: 0.0900217667222023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25911498069763184 0.05120205879211426

Final encoder loss: 0.08143924921751022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581160068511963 0.05778074264526367

Final encoder loss: 0.08505029231309891
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567133903503418 0.05261635780334473

Final encoder loss: 0.06140350550413132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.258777379989624 0.054656028747558594

Final encoder loss: 0.05539093539118767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25986528396606445 0.054997920989990234

Final encoder loss: 0.05330721288919449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577526569366455 0.05235028266906738

Final encoder loss: 0.0536520890891552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25969648361206055 0.05440163612365723

Final encoder loss: 0.05000840872526169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2599916458129883 0.05147910118103027

Final encoder loss: 0.05200870707631111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559854984283447 0.05348396301269531

Final encoder loss: 0.04297706112265587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25847744941711426 0.052114009857177734

Final encoder loss: 0.03944895789027214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26934218406677246 0.05343317985534668

Final encoder loss: 0.03813498094677925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2702350616455078 0.05238509178161621

Final encoder loss: 0.03875492140650749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2686901092529297 0.05616950988769531

Final encoder loss: 0.03745584934949875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25790977478027344 0.05303239822387695

Final encoder loss: 0.0383169986307621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567567825317383 0.05323457717895508

Final encoder loss: 0.03463272750377655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26883387565612793 0.05457186698913574

Final encoder loss: 0.032740239053964615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25905275344848633 0.052399635314941406

Final encoder loss: 0.03175157681107521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2683279514312744 0.05226254463195801

Final encoder loss: 0.032358162105083466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2602567672729492 0.052393436431884766

Final encoder loss: 0.03246237337589264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2572360038757324 0.0528411865234375

Final encoder loss: 0.032395172864198685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553532123565674 0.05258917808532715

Final encoder loss: 0.031001202762126923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2594270706176758 0.05514645576477051

Final encoder loss: 0.03008342534303665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588794231414795 0.052828311920166016

Final encoder loss: 0.029395662248134613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587015628814697 0.05455613136291504

Final encoder loss: 0.029744135215878487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589092254638672 0.05350494384765625

Final encoder loss: 0.03059958666563034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591538429260254 0.0529327392578125

Final encoder loss: 0.030150726437568665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.256894588470459 0.053375959396362305

Final encoder loss: 0.028560372069478035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27541446685791016 0.052693843841552734

Final encoder loss: 0.02778775617480278
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583150863647461 0.05274152755737305

Final encoder loss: 0.02734846994280815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2701544761657715 0.05256199836730957

Final encoder loss: 0.02748277597129345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25803470611572266 0.05479693412780762

Final encoder loss: 0.028308426961302757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26848268508911133 0.05208563804626465

Final encoder loss: 0.027951443567872047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25704503059387207 0.05281949043273926

Final encoder loss: 0.026833821088075638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2700459957122803 0.05060553550720215

Final encoder loss: 0.026382459327578545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25824618339538574 0.0540316104888916

Final encoder loss: 0.02580251917243004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2679445743560791 0.05162215232849121

Final encoder loss: 0.025995483621954918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584950923919678 0.05398249626159668

Final encoder loss: 0.026898397132754326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2699763774871826 0.05164623260498047

Final encoder loss: 0.026490118354558945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2564356327056885 0.0529482364654541

Final encoder loss: 0.025666559115052223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26913976669311523 0.052584171295166016

Final encoder loss: 0.025152577087283134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694563865661621 0.05378127098083496

Final encoder loss: 0.0248678307980299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2580881118774414 0.053490400314331055

Final encoder loss: 0.024988355115056038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26810240745544434 0.05385732650756836

Final encoder loss: 0.026037925854325294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2686035633087158 0.05287742614746094

Final encoder loss: 0.025353308767080307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2585735321044922 0.05283832550048828

Final encoder loss: 0.024955542758107185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25803613662719727 0.05456209182739258

Final encoder loss: 0.024747781455516815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588961124420166 0.0529942512512207

Final encoder loss: 0.02442890591919422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26811814308166504 0.05220842361450195

Final encoder loss: 0.02438516728579998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2769646644592285 0.05333662033081055

Final encoder loss: 0.025484273210167885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27330660820007324 0.053415536880493164

Final encoder loss: 0.02497805282473564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565345764160156 0.05174899101257324

Final encoder loss: 0.02433500438928604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26891255378723145 0.05307769775390625

Final encoder loss: 0.024091901257634163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.261030912399292 0.05190253257751465

Final encoder loss: 0.02380932681262493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2682077884674072 0.0529019832611084

Final encoder loss: 0.023823123425245285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26828432083129883 0.053006649017333984

Final encoder loss: 0.024703850969672203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2697713375091553 0.05265545845031738

Final encoder loss: 0.024103764444589615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25717592239379883 0.05593276023864746

Final encoder loss: 0.023904727771878242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25821352005004883 0.05257463455200195

Final encoder loss: 0.0237845778465271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26968884468078613 0.05412745475769043

Final encoder loss: 0.023349881172180176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26006221771240234 0.05319046974182129

Final encoder loss: 0.023324178531765938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25783681869506836 0.05500316619873047

Final encoder loss: 0.024530790746212006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25890398025512695 0.05290937423706055

Final encoder loss: 0.02391907386481762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2596259117126465 0.053880929946899414

Final encoder loss: 0.0235852412879467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26956701278686523 0.052750349044799805

Final encoder loss: 0.02332046441733837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584347724914551 0.05510115623474121

Final encoder loss: 0.023031238466501236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25832176208496094 0.053234100341796875

Final encoder loss: 0.023025767877697945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581605911254883 0.05268430709838867

Final encoder loss: 0.023984579369425774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2692897319793701 0.05292940139770508

Final encoder loss: 0.02343905344605446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2563343048095703 0.052809953689575195

Final encoder loss: 0.02324914187192917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25728440284729004 0.05275464057922363

Final encoder loss: 0.02327374741435051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2602550983428955 0.05380082130432129

Final encoder loss: 0.022901594638824463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2667832374572754 0.053160667419433594

Final encoder loss: 0.02272176370024681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25859713554382324 0.053133249282836914

Final encoder loss: 0.02381863072514534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27964162826538086 0.05297493934631348

Final encoder loss: 0.023280834779143333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2579052448272705 0.05190563201904297

Final encoder loss: 0.0229392871260643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2679939270019531 0.055127620697021484

Final encoder loss: 0.022862806916236877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26840996742248535 0.05282735824584961

Final encoder loss: 0.022580157965421677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584102153778076 0.05191469192504883

Final encoder loss: 0.02247980795800686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2785313129425049 0.05173611640930176

Final encoder loss: 0.023539764806628227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26848769187927246 0.05439400672912598

Final encoder loss: 0.02284126915037632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25573015213012695 0.052855730056762695

Final encoder loss: 0.022866057232022285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2689249515533447 0.05295395851135254

Final encoder loss: 0.022765785455703735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2710733413696289 0.052271366119384766

Final encoder loss: 0.02240685187280178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26801133155822754 0.05255270004272461

Final encoder loss: 0.022297879680991173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584056854248047 0.0529322624206543

Final encoder loss: 0.023298656567931175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.269838809967041 0.052004098892211914

Final encoder loss: 0.02287266030907631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25583338737487793 0.05329084396362305

Final encoder loss: 0.02263607271015644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26880979537963867 0.052368879318237305

Final encoder loss: 0.022529564797878265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25922060012817383 0.0522618293762207

Final encoder loss: 0.022269688546657562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2698392868041992 0.05238080024719238

Final encoder loss: 0.022050853818655014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2760660648345947 0.05380415916442871

Final encoder loss: 0.023062489926815033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26701998710632324 0.052494049072265625

Final encoder loss: 0.022535400465130806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2566258907318115 0.05160045623779297

Final encoder loss: 0.022445807233452797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27039504051208496 0.053037405014038086

Final encoder loss: 0.022469311952590942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2774083614349365 0.053167104721069336

Final encoder loss: 0.02213260717689991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2759416103363037 0.05277729034423828

Final encoder loss: 0.02192300371825695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590036392211914 0.05248236656188965

Final encoder loss: 0.02305786870419979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2682492733001709 0.05222439765930176

Final encoder loss: 0.022431127727031708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534763813018799 0.05150938034057617

Final encoder loss: 0.02233300171792507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2668740749359131 0.05271577835083008

Final encoder loss: 0.022195087745785713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564697265625 0.052206993103027344

Final encoder loss: 0.021974068135023117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2654531002044678 0.051439523696899414

Final encoder loss: 0.021822931244969368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26663899421691895 0.05242609977722168

Final encoder loss: 0.02278384007513523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25567078590393066 0.051194190979003906

Final encoder loss: 0.022146334871649742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25442957878112793 0.05156135559082031

Final encoder loss: 0.022270865738391876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26616525650024414 0.05152297019958496

Final encoder loss: 0.022261878475546837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2669641971588135 0.05237865447998047

Final encoder loss: 0.021892962977290154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593343257904053 0.05174446105957031

Final encoder loss: 0.021685874089598656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2681999206542969 0.05471920967102051

Final encoder loss: 0.022700946778059006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2757694721221924 0.05230832099914551

Final encoder loss: 0.022235682234168053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565956115722656 0.0530545711517334

Final encoder loss: 0.022131027653813362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26871156692504883 0.05249834060668945

Final encoder loss: 0.022003619000315666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.27819275856018066 0.0525507926940918

Final encoder loss: 0.021753428503870964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2673826217651367 0.054053306579589844

Final encoder loss: 0.021510878577828407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2749607563018799 0.052694082260131836

Final encoder loss: 0.022473078221082687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.267927885055542 0.052449941635131836

Final encoder loss: 0.022012382745742798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2570991516113281 0.051367998123168945

Final encoder loss: 0.02204563468694687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26868534088134766 0.05467939376831055

Final encoder loss: 0.022015148773789406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2685122489929199 0.0529026985168457

Final encoder loss: 0.021723469719290733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582094669342041 0.053192138671875

Final encoder loss: 0.021430451422929764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26741576194763184 0.0532381534576416

Final encoder loss: 0.02252964861690998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26940226554870605 0.0527036190032959

Final encoder loss: 0.022000456228852272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25446462631225586 0.05385708808898926

Final encoder loss: 0.021971940994262695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26889538764953613 0.05428290367126465

Final encoder loss: 0.021808065474033356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2597923278808594 0.05353379249572754

Final encoder loss: 0.021511616185307503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26944589614868164 0.05196022987365723

Final encoder loss: 0.021378714591264725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26861119270324707 0.0539858341217041

Final encoder loss: 0.022272834554314613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25756001472473145 0.05416464805603027

Final encoder loss: 0.02172013558447361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567596435546875 0.05206441879272461

Final encoder loss: 0.02186455950140953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26854825019836426 0.051888227462768555

Final encoder loss: 0.021875301375985146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26931333541870117 0.05358600616455078

Final encoder loss: 0.021498626098036766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26848626136779785 0.05340242385864258

Final encoder loss: 0.021280942484736443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580299377441406 0.05266141891479492

Final encoder loss: 0.022238271310925484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26876115798950195 0.05201005935668945

Final encoder loss: 0.02180694043636322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2572355270385742 0.051973581314086914

Final encoder loss: 0.021792415529489517
Final encoder loss: 0.021204670891165733
Final encoder loss: 0.020437991246581078
Final encoder loss: 0.01953900419175625
Final encoder loss: 0.01969064399600029
Final encoder loss: 0.018506325781345367

Training emognition model
Final encoder loss: 0.02508775982758197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08148193359375 0.23057317733764648

Final encoder loss: 0.02422765625036114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08099484443664551 0.2304835319519043

Final encoder loss: 0.024160964422579072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08118224143981934 0.2304854393005371

Final encoder loss: 0.024386491718080005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08149242401123047 0.2308809757232666

Final encoder loss: 0.024325003098199516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08101963996887207 0.2307746410369873

Final encoder loss: 0.02361801665898496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08259272575378418 0.23107290267944336

Final encoder loss: 0.025490633322339286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08162736892700195 0.23139286041259766

Final encoder loss: 0.02383786160627062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08143734931945801 0.23061299324035645

Final encoder loss: 0.024092671524084276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08096981048583984 0.23044872283935547

Final encoder loss: 0.02364680360926687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.0811777114868164 0.23094749450683594

Final encoder loss: 0.02513490663398078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.0815284252166748 0.23084449768066406

Final encoder loss: 0.025212877197988443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08126521110534668 0.2307267189025879

Final encoder loss: 0.023931002359558473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08287405967712402 0.23215317726135254

Final encoder loss: 0.024180109456769416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08152365684509277 0.23084640502929688

Final encoder loss: 0.02402750292171836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08196091651916504 0.23057317733764648

Final encoder loss: 0.02454527172366643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08049988746643066 0.22993850708007812


Training emognition model
Final encoder loss: 0.19355744123458862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.251816987991333 0.04969191551208496

Final encoder loss: 0.19494986534118652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24888181686401367 0.04914379119873047

Final encoder loss: 0.08907157927751541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24808049201965332 0.04956173896789551

Final encoder loss: 0.08860937505960464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24735593795776367 0.05079340934753418

Final encoder loss: 0.05645556375384331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24981188774108887 0.049687862396240234

Final encoder loss: 0.05481230467557907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24685263633728027 0.047361135482788086

Final encoder loss: 0.04179404303431511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24960923194885254 0.0488591194152832

Final encoder loss: 0.040526703000068665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24789214134216309 0.049730539321899414

Final encoder loss: 0.03428514674305916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482457160949707 0.04825949668884277

Final encoder loss: 0.03339482098817825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25193095207214355 0.049521684646606445

Final encoder loss: 0.03010367415845394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24848580360412598 0.04893302917480469

Final encoder loss: 0.029441267251968384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24802494049072266 0.049909353256225586

Final encoder loss: 0.027764787897467613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24882817268371582 0.0489506721496582

Final encoder loss: 0.02723202295601368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24803757667541504 0.04932999610900879

Final encoder loss: 0.026597438380122185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482917308807373 0.04931187629699707

Final encoder loss: 0.02605641447007656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2460789680480957 0.04841256141662598

Final encoder loss: 0.02586354874074459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24648809432983398 0.04833054542541504

Final encoder loss: 0.025553995743393898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24618053436279297 0.04848790168762207

Final encoder loss: 0.025387577712535858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24663734436035156 0.04923224449157715

Final encoder loss: 0.025256814435124397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24578142166137695 0.04791998863220215

Final encoder loss: 0.02486218698322773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467937469482422 0.049013376235961914

Final encoder loss: 0.024991847574710846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24497723579406738 0.04794454574584961

Final encoder loss: 0.02467147447168827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2464611530303955 0.048879384994506836

Final encoder loss: 0.024637505412101746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24549341201782227 0.04805302619934082

Final encoder loss: 0.02430226467549801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24806761741638184 0.05005836486816406

Final encoder loss: 0.02435963973402977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24737143516540527 0.050431013107299805

Final encoder loss: 0.024261675775051117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2480931282043457 0.048477888107299805

Final encoder loss: 0.024170193821191788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24790048599243164 0.04860806465148926

Final encoder loss: 0.024093447253108025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24765658378601074 0.04886317253112793

Final encoder loss: 0.0240239929407835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469651699066162 0.048299551010131836

Final encoder loss: 0.023882972076535225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24695897102355957 0.0484468936920166

Final encoder loss: 0.024023322388529778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24811291694641113 0.04918169975280762

Final encoder loss: 0.023667365312576294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24910378456115723 0.04876518249511719

Final encoder loss: 0.023767661303281784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24749994277954102 0.04907989501953125

Final encoder loss: 0.023519130423665047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24959087371826172 0.049361228942871094

Final encoder loss: 0.023535024374723434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468879222869873 0.04865908622741699

Final encoder loss: 0.023409146815538406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24899530410766602 0.048490285873413086

Final encoder loss: 0.0234693493694067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475881576538086 0.049233436584472656

Final encoder loss: 0.023346569389104843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24886345863342285 0.04907846450805664

Final encoder loss: 0.023408465087413788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24747490882873535 0.0486142635345459

Final encoder loss: 0.023202868178486824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24801993370056152 0.049485206604003906

Final encoder loss: 0.023236846551299095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24776935577392578 0.050145626068115234

Final encoder loss: 0.02332594431936741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24861478805541992 0.04941391944885254

Final encoder loss: 0.02323220670223236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24724197387695312 0.049370765686035156

Final encoder loss: 0.02319774217903614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24841094017028809 0.04951620101928711

Final encoder loss: 0.023259242996573448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24779105186462402 0.04995894432067871

Final encoder loss: 0.02318994142115116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2481834888458252 0.04729962348937988

Final encoder loss: 0.0232313834130764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24474716186523438 0.048935890197753906

Final encoder loss: 0.022946078330278397
Final encoder loss: 0.02228572964668274

Training empatch model
Final encoder loss: 0.03480454000200155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07084131240844727 0.17278146743774414

Final encoder loss: 0.031829967904820296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07187247276306152 0.1731553077697754

Final encoder loss: 0.03176912592327018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07064342498779297 0.1725907325744629

Final encoder loss: 0.03272854028063479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.0707552433013916 0.1724085807800293

Final encoder loss: 0.03207612522950567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07079076766967773 0.172882080078125

Final encoder loss: 0.03076810272974508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07070732116699219 0.17265772819519043

Final encoder loss: 0.02998056519682523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07072329521179199 0.172532320022583

Final encoder loss: 0.030426566704481767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0701594352722168 0.17202973365783691

Final encoder loss: 0.02409676003207631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07060956954956055 0.1729874610900879

Final encoder loss: 0.02241056537651273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07031822204589844 0.17229843139648438

Final encoder loss: 0.025414224736386302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07050013542175293 0.17388582229614258

Final encoder loss: 0.022172309483165963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07150673866271973 0.17406845092773438

Final encoder loss: 0.023911951759471677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07082939147949219 0.17432928085327148

Final encoder loss: 0.02364766248052655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0716257095336914 0.1739482879638672

Final encoder loss: 0.024053049646228675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07117962837219238 0.17400312423706055

Final encoder loss: 0.0233604089306361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07053923606872559 0.17388272285461426


Training empatch model
Final encoder loss: 0.1711738407611847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17805957794189453 0.04381608963012695

Final encoder loss: 0.08019576966762543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17606735229492188 0.04324626922607422

Final encoder loss: 0.054572295397520065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17525267601013184 0.04440140724182129

Final encoder loss: 0.042243581265211105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17560815811157227 0.04343676567077637

Final encoder loss: 0.035207170993089676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1749567985534668 0.04324984550476074

Final encoder loss: 0.030784767121076584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1756441593170166 0.04359793663024902

Final encoder loss: 0.02783193811774254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17460966110229492 0.04311990737915039

Final encoder loss: 0.025809798389673233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752033233642578 0.04346966743469238

Final encoder loss: 0.02440693974494934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17542409896850586 0.043886423110961914

Final encoder loss: 0.02351519837975502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1759650707244873 0.043892621994018555

Final encoder loss: 0.022930651903152466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17473363876342773 0.04360818862915039

Final encoder loss: 0.022537631914019585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17519354820251465 0.04347062110900879

Final encoder loss: 0.022096415981650352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17490339279174805 0.04301309585571289

Final encoder loss: 0.021901395171880722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17627787590026855 0.04261279106140137

Final encoder loss: 0.021627966314554214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17507362365722656 0.04329800605773926

Final encoder loss: 0.021483715623617172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17642450332641602 0.04352211952209473

Final encoder loss: 0.02130376361310482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745460033416748 0.043259382247924805

Final encoder loss: 0.02110580913722515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17483854293823242 0.04290962219238281

Final encoder loss: 0.020905809476971626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17486310005187988 0.04232621192932129

Final encoder loss: 0.020778682082891464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17504262924194336 0.04319000244140625

Final encoder loss: 0.020717134699225426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17783713340759277 0.04453086853027344

Final encoder loss: 0.020711835473775864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1780388355255127 0.04262042045593262

Final encoder loss: 0.02063075266778469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17441558837890625 0.04322361946105957

Final encoder loss: 0.020551782101392746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17621064186096191 0.04363059997558594

Final encoder loss: 0.020543204620480537

Training wesad model
Final encoder loss: 0.03349285392461643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07124519348144531 0.17312908172607422

Final encoder loss: 0.03257784167372173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07089877128601074 0.17319798469543457

Final encoder loss: 0.03263637209635221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07055020332336426 0.1733226776123047

Final encoder loss: 0.03551860014303122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07091999053955078 0.1753535270690918

Final encoder loss: 0.02087339868656764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07178521156311035 0.1744675636291504

Final encoder loss: 0.022081874809093658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0721290111541748 0.17435932159423828

Final encoder loss: 0.02242427418498402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07154655456542969 0.17291736602783203

Final encoder loss: 0.021971886872883654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07157230377197266 0.17412590980529785

Final encoder loss: 0.01716391345835126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07207632064819336 0.17399334907531738

Final encoder loss: 0.016153666079219527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07157206535339355 0.1736605167388916

Final encoder loss: 0.017437690522724432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07081341743469238 0.1740858554840088

Final encoder loss: 0.017170895843987077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07114434242248535 0.17278695106506348

Final encoder loss: 0.013271436276512918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07126164436340332 0.17344927787780762

Final encoder loss: 0.014491005121602129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07091236114501953 0.17350101470947266

Final encoder loss: 0.013977342264588835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07169246673583984 0.17340731620788574

Final encoder loss: 0.013491982752745836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07071328163146973 0.17359638214111328


Training wesad model
Final encoder loss: 0.21560697257518768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10503458976745605 0.03322243690490723

Final encoder loss: 0.09838984906673431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10468173027038574 0.03312039375305176

Final encoder loss: 0.06381871551275253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356903076171875 0.032561540603637695

Final encoder loss: 0.04646160081028938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365486145019531 0.03312206268310547

Final encoder loss: 0.036300480365753174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10390233993530273 0.03321695327758789

Final encoder loss: 0.03001667931675911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10327005386352539 0.03337717056274414

Final encoder loss: 0.025854216888546944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10367083549499512 0.03339648246765137

Final encoder loss: 0.023043546825647354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10406160354614258 0.03267025947570801

Final encoder loss: 0.021102910861372948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10358119010925293 0.03260469436645508

Final encoder loss: 0.01972131058573723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10305929183959961 0.033202409744262695

Final encoder loss: 0.018758440390229225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10448741912841797 0.032755374908447266

Final encoder loss: 0.01819416880607605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10347151756286621 0.03325486183166504

Final encoder loss: 0.017969291657209396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10363268852233887 0.03306841850280762

Final encoder loss: 0.017929423600435257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10345721244812012 0.03325843811035156

Final encoder loss: 0.017779793590307236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1041715145111084 0.032982826232910156

Final encoder loss: 0.01759038306772709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10335540771484375 0.033233642578125

Final encoder loss: 0.017609018832445145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365080833435059 0.03255605697631836

Final encoder loss: 0.017745643854141235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385823249816895 0.03310704231262207

Final encoder loss: 0.017481693997979164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038365364074707 0.03272557258605957

Final encoder loss: 0.017377423122525215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10372781753540039 0.03319549560546875

Final encoder loss: 0.017264729365706444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10363459587097168 0.032653093338012695

Final encoder loss: 0.01724736951291561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10370850563049316 0.03339719772338867

Final encoder loss: 0.017144909128546715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10322928428649902 0.0334627628326416

Final encoder loss: 0.017174765467643738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1035001277923584 0.03334665298461914

Final encoder loss: 0.017120007425546646

Calculating loss for amigos model
	Full Pass 0.6840555667877197
numFreeParamsPath 18
Reconstruction loss values: 0.02568911947309971 0.0346822552382946

Calculating loss for dapper model
	Full Pass 0.15118050575256348
numFreeParamsPath 18
Reconstruction loss values: 0.020167004317045212 0.02319096401333809

Calculating loss for case model
	Full Pass 0.9275171756744385
numFreeParamsPath 18
Reconstruction loss values: 0.031232833862304688 0.03438015654683113

Calculating loss for emognition model
	Full Pass 0.291515588760376
numFreeParamsPath 18
Reconstruction loss values: 0.03191588446497917 0.04023401066660881

Calculating loss for empatch model
	Full Pass 0.10467052459716797
numFreeParamsPath 18
Reconstruction loss values: 0.033219028264284134 0.04048972576856613

Calculating loss for wesad model
	Full Pass 0.07723402976989746
numFreeParamsPath 18
Reconstruction loss values: 0.03453856334090233 0.0504140742123127
Total loss calculation time: 3.9603664875030518

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 4.651550769805908
Total epoch time: 221.49515986442566

Epoch: 50

Training emognition model
Final encoder loss: 0.03260977083143093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08906340599060059 0.2829160690307617

Final encoder loss: 0.03026800666450402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08336567878723145 0.2732574939727783

Final encoder loss: 0.03120963940371695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08318066596984863 0.2739126682281494

Final encoder loss: 0.02983672077101233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08321189880371094 0.2736043930053711

Final encoder loss: 0.03025863037051904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08319854736328125 0.2735421657562256

Final encoder loss: 0.028989665975681467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08324193954467773 0.27375125885009766

Final encoder loss: 0.029479292761023172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0831441879272461 0.2742643356323242

Final encoder loss: 0.031123411062890908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08282947540283203 0.2738790512084961

Final encoder loss: 0.030057203774580533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08439064025878906 0.27352094650268555

Final encoder loss: 0.030145193840787586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0826723575592041 0.2738215923309326

Final encoder loss: 0.0295395753707041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08263969421386719 0.27423787117004395

Final encoder loss: 0.030231568554271487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08291983604431152 0.27387452125549316

Final encoder loss: 0.02979651693815626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08290457725524902 0.2740902900695801

Final encoder loss: 0.028755619340126125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08263492584228516 0.27367687225341797

Final encoder loss: 0.029455482255930098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08283734321594238 0.2737085819244385

Final encoder loss: 0.029272138031705468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08179640769958496 0.2728605270385742


Training case model
Final encoder loss: 0.032043110875474304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09121513366699219 0.2644195556640625

Final encoder loss: 0.029847573739236084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09056305885314941 0.26441121101379395

Final encoder loss: 0.028161779067829244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09256482124328613 0.2646329402923584

Final encoder loss: 0.026733371821452023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09110474586486816 0.2635936737060547

Final encoder loss: 0.02688389679569215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09077811241149902 0.26384830474853516

Final encoder loss: 0.025610106898053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09142613410949707 0.26438260078430176

Final encoder loss: 0.02549999612533241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09169840812683105 0.2638072967529297

Final encoder loss: 0.025520283068465405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09139657020568848 0.2634873390197754

Final encoder loss: 0.024932025215515897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09081268310546875 0.26376938819885254

Final encoder loss: 0.025359362390553447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09063839912414551 0.2638850212097168

Final encoder loss: 0.02441768595051609
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09137296676635742 0.2635469436645508

Final encoder loss: 0.02432041304964407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.0913846492767334 0.26392292976379395

Final encoder loss: 0.02455816587614506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09135198593139648 0.26439690589904785

Final encoder loss: 0.02486353271443247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09098029136657715 0.2637147903442383

Final encoder loss: 0.024609820204971263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09061884880065918 0.26375818252563477

Final encoder loss: 0.024518420260773853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.0873861312866211 0.2615513801574707


Training amigos model
Final encoder loss: 0.025137988159233196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10781192779541016 0.3884317874908447

Final encoder loss: 0.02701297992924602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10798430442810059 0.38870978355407715

Final encoder loss: 0.025725671719331045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10786628723144531 0.3882601261138916

Final encoder loss: 0.024551405398553112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10754752159118652 0.388247013092041

Final encoder loss: 0.024887650865253252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10758566856384277 0.38892149925231934

Final encoder loss: 0.02589564600244898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10777592658996582 0.3891582489013672

Final encoder loss: 0.02613844465946648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10787081718444824 0.3889029026031494

Final encoder loss: 0.026725714142029035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.1076970100402832 0.3887951374053955

Final encoder loss: 0.023789270470120926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10807967185974121 0.3884284496307373

Final encoder loss: 0.02680100036382406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10774755477905273 0.3885314464569092

Final encoder loss: 0.025313403994280892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10820651054382324 0.3887314796447754

Final encoder loss: 0.02337316650167769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10793781280517578 0.38890957832336426

Final encoder loss: 0.027450385334537336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10820722579956055 0.38930439949035645

Final encoder loss: 0.026954950183227903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10764884948730469 0.3882784843444824

Final encoder loss: 0.025237805621050543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10783815383911133 0.3881409168243408

Final encoder loss: 0.025844223878195332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10286307334899902 0.3827528953552246


Training dapper model
Final encoder loss: 0.022229522151816943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.061540842056274414 0.14984679222106934

Final encoder loss: 0.02345506808420651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06149435043334961 0.14927101135253906

Final encoder loss: 0.02043757606321012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.061872243881225586 0.150040864944458

Final encoder loss: 0.01980272343465573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06191658973693848 0.1494886875152588

Final encoder loss: 0.023553590831541862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0614626407623291 0.1499004364013672

Final encoder loss: 0.018485572161716764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06136035919189453 0.15003204345703125

Final encoder loss: 0.01835082179277122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06127357482910156 0.15156316757202148

Final encoder loss: 0.017161051323154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06327533721923828 0.1509397029876709

Final encoder loss: 0.02070921751394343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.062317609786987305 0.15021038055419922

Final encoder loss: 0.021379530156352338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06351208686828613 0.1528313159942627

Final encoder loss: 0.01851112620013112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06335234642028809 0.15033364295959473

Final encoder loss: 0.01833896015121336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06254243850708008 0.15137934684753418

Final encoder loss: 0.018915464488718704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06236100196838379 0.15109467506408691

Final encoder loss: 0.017335905213692885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.0628499984741211 0.15231633186340332

Final encoder loss: 0.019017439492256878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06423354148864746 0.1510157585144043

Final encoder loss: 0.02196039692405356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06205439567565918 0.15075945854187012


Training amigos model
Final encoder loss: 0.020156145991163988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10648179054260254 0.34217023849487305

Final encoder loss: 0.019061665179563515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10685110092163086 0.34123849868774414

Final encoder loss: 0.017669910885823602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10657262802124023 0.34165120124816895

Final encoder loss: 0.018933226715574313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10620880126953125 0.34189367294311523

Final encoder loss: 0.01797327229704495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10573601722717285 0.3418693542480469

Final encoder loss: 0.018660032269410428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10643243789672852 0.34172487258911133

Final encoder loss: 0.017867923432089042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10596346855163574 0.3416557312011719

Final encoder loss: 0.017756440243479242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10626745223999023 0.3417634963989258

Final encoder loss: 0.018821983678638524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10609102249145508 0.3417625427246094

Final encoder loss: 0.019353721356312067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10625195503234863 0.3417491912841797

Final encoder loss: 0.019088806328599335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10676288604736328 0.3423159122467041

Final encoder loss: 0.019810564165259382
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10667800903320312 0.34152793884277344

Final encoder loss: 0.02055661356711622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10714983940124512 0.3418002128601074

Final encoder loss: 0.01782316234251767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10634636878967285 0.3422396183013916

Final encoder loss: 0.020656689581172682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10768699645996094 0.34162116050720215

Final encoder loss: 0.01898181610325746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10200762748718262 0.3384063243865967


Training amigos model
Final encoder loss: 0.1807638555765152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45047974586486816 0.07550930976867676

Final encoder loss: 0.18782471120357513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44223737716674805 0.07723069190979004

Final encoder loss: 0.18362098932266235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45413684844970703 0.07436943054199219

Final encoder loss: 0.07655780762434006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.456157922744751 0.08008170127868652

Final encoder loss: 0.07817358523607254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45470213890075684 0.08164119720458984

Final encoder loss: 0.07248597592115402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45381927490234375 0.07712101936340332

Final encoder loss: 0.04439913481473923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46601033210754395 0.07439970970153809

Final encoder loss: 0.04519566521048546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46924400329589844 0.07615160942077637

Final encoder loss: 0.042304880917072296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.438723087310791 0.07440972328186035

Final encoder loss: 0.03166188299655914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45612621307373047 0.07832765579223633

Final encoder loss: 0.03270997479557991
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46648716926574707 0.08225798606872559

Final encoder loss: 0.030972732231020927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45315003395080566 0.07556915283203125

Final encoder loss: 0.02586546167731285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44928789138793945 0.07603812217712402

Final encoder loss: 0.026846162974834442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4716923236846924 0.07641816139221191

Final encoder loss: 0.025716209784150124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.438800573348999 0.0736243724822998

Final encoder loss: 0.023300599306821823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4528543949127197 0.08044815063476562

Final encoder loss: 0.024021459743380547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46994757652282715 0.08142232894897461

Final encoder loss: 0.023081373423337936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43401288986206055 0.07434248924255371

Final encoder loss: 0.022525472566485405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4571232795715332 0.07685995101928711

Final encoder loss: 0.02279352769255638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.458568811416626 0.07915663719177246

Final encoder loss: 0.022216327488422394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45520544052124023 0.07409954071044922

Final encoder loss: 0.02219872735440731
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4657762050628662 0.08125662803649902

Final encoder loss: 0.022481517866253853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45209193229675293 0.08008360862731934

Final encoder loss: 0.02214992418885231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4451773166656494 0.07644033432006836

Final encoder loss: 0.02131056971848011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4632441997528076 0.0741569995880127

Final encoder loss: 0.021574242040514946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44531726837158203 0.07585549354553223

Final encoder loss: 0.021446045488119125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4520390033721924 0.07517862319946289

Final encoder loss: 0.020584361627697945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45246434211730957 0.08223891258239746

Final encoder loss: 0.021125221624970436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43823885917663574 0.07880783081054688

Final encoder loss: 0.020716680213809013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.447324275970459 0.07541751861572266

Final encoder loss: 0.01993664912879467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45005178451538086 0.0740811824798584

Final encoder loss: 0.0205059964209795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4448556900024414 0.07474374771118164

Final encoder loss: 0.020134294405579567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43475770950317383 0.08188605308532715

Final encoder loss: 0.01987917535007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4544382095336914 0.08209729194641113

Final encoder loss: 0.01995822787284851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4405686855316162 0.07857823371887207

Final encoder loss: 0.019945140928030014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42949938774108887 0.0757451057434082

Final encoder loss: 0.01958606019616127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45874929428100586 0.07639122009277344

Final encoder loss: 0.01963484100997448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.473358154296875 0.0764312744140625

Final encoder loss: 0.019392412155866623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44880080223083496 0.07860612869262695

Final encoder loss: 0.01933187246322632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45557212829589844 0.0807042121887207

Final encoder loss: 0.019464127719402313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4488682746887207 0.07653570175170898

Final encoder loss: 0.01923813670873642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44472241401672363 0.07536029815673828

Final encoder loss: 0.018936920911073685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4731621742248535 0.07707738876342773

Final encoder loss: 0.018838921561837196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4580423831939697 0.07447052001953125

Final encoder loss: 0.018934791907668114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4491233825683594 0.08189749717712402

Final encoder loss: 0.018859010189771652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4552462100982666 0.07845139503479004

Final encoder loss: 0.01882750540971756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44984984397888184 0.0770719051361084

Final encoder loss: 0.018867822363972664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4474155902862549 0.07244062423706055

Final encoder loss: 0.018574437126517296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45806145668029785 0.07628560066223145

Final encoder loss: 0.018692005425691605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.471752405166626 0.07412219047546387

Final encoder loss: 0.018598398193717003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43564844131469727 0.08028483390808105

Final encoder loss: 0.018545588478446007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4546945095062256 0.07761883735656738

Final encoder loss: 0.018594466149806976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45356130599975586 0.0763099193572998

Final encoder loss: 0.01868543215095997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4467592239379883 0.07762813568115234

Final encoder loss: 0.01835700310766697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4468221664428711 0.0769801139831543

Final encoder loss: 0.018207471817731857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44167542457580566 0.0853872299194336

Final encoder loss: 0.018504351377487183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4409513473510742 0.07953023910522461

Final encoder loss: 0.018390977755188942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4549989700317383 0.07622432708740234

Final encoder loss: 0.018211212009191513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.463057279586792 0.07740354537963867

Final encoder loss: 0.018328316509723663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4501521587371826 0.07346987724304199

Final encoder loss: 0.018228160217404366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4559328556060791 0.07547831535339355

Final encoder loss: 0.018038924783468246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45569348335266113 0.08262205123901367

Final encoder loss: 0.018093163147568703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4326958656311035 0.08176016807556152

Final encoder loss: 0.018038660287857056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4267406463623047 0.07688379287719727

Final encoder loss: 0.018000345677137375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45189619064331055 0.07807612419128418

Final encoder loss: 0.01834825612604618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4547688961029053 0.07284212112426758

Final encoder loss: 0.017948798835277557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4713134765625 0.07536625862121582

Final encoder loss: 0.017908265814185143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45297765731811523 0.08066678047180176

Final encoder loss: 0.018077107146382332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4525637626647949 0.07886052131652832

Final encoder loss: 0.01800999790430069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4724414348602295 0.07371401786804199

Final encoder loss: 0.017848705872893333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4236111640930176 0.07590675354003906

Final encoder loss: 0.0179806686937809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4402906894683838 0.07486557960510254

Final encoder loss: 0.017880145460367203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4446601867675781 0.0765390396118164

Final encoder loss: 0.017763277515769005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4558720588684082 0.07434988021850586

Final encoder loss: 0.0177913811057806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4405479431152344 0.07452511787414551

Final encoder loss: 0.017705552279949188
Final encoder loss: 0.016857599839568138
Final encoder loss: 0.016292322427034378

Training dapper model
Final encoder loss: 0.015800415551537942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06003832817077637 0.10762548446655273

Final encoder loss: 0.016293501168099873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.0596766471862793 0.10658979415893555

Final encoder loss: 0.01511962801360494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06017899513244629 0.10699582099914551

Final encoder loss: 0.015060207345972047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05973052978515625 0.10721921920776367

Final encoder loss: 0.014167761422169037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.0598294734954834 0.10682153701782227

Final encoder loss: 0.01541800534021053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0597231388092041 0.10757255554199219

Final encoder loss: 0.014403306660598096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05991649627685547 0.10719633102416992

Final encoder loss: 0.015187769270954133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05970430374145508 0.10785055160522461

Final encoder loss: 0.015023575628201296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059853553771972656 0.10640192031860352

Final encoder loss: 0.013285148765692202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0599362850189209 0.10758209228515625

Final encoder loss: 0.014122870476789817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059851884841918945 0.10754609107971191

Final encoder loss: 0.013443856490010734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05986213684082031 0.10749983787536621

Final encoder loss: 0.014134116692828842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05950164794921875 0.10728716850280762

Final encoder loss: 0.014530341060725102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06000924110412598 0.10705232620239258

Final encoder loss: 0.01413181727480442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05986595153808594 0.10775041580200195

Final encoder loss: 0.01631973803525812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059433937072753906 0.10633134841918945


Training dapper model
Final encoder loss: 0.2024354189634323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11661601066589355 0.035248756408691406

Final encoder loss: 0.2082308828830719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11643671989440918 0.03386712074279785

Final encoder loss: 0.08498714119195938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1120309829711914 0.03446650505065918

Final encoder loss: 0.08694099634885788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11664962768554688 0.03408336639404297

Final encoder loss: 0.05023188889026642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11310434341430664 0.03413891792297363

Final encoder loss: 0.049971163272857666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11622500419616699 0.03371930122375488

Final encoder loss: 0.034141670912504196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11278724670410156 0.03414630889892578

Final encoder loss: 0.03384942188858986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11558246612548828 0.03358030319213867

Final encoder loss: 0.025707460939884186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1137545108795166 0.03416085243225098

Final encoder loss: 0.02568942680954933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11608433723449707 0.033615827560424805

Final encoder loss: 0.021098265424370766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11298131942749023 0.03397727012634277

Final encoder loss: 0.021047182381153107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11542415618896484 0.0350339412689209

Final encoder loss: 0.018468981608748436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11262869834899902 0.03361678123474121

Final encoder loss: 0.018328361213207245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11568593978881836 0.03416895866394043

Final encoder loss: 0.0167332012206316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11276555061340332 0.03388500213623047

Final encoder loss: 0.01668936014175415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11613225936889648 0.03376340866088867

Final encoder loss: 0.015762347728013992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11230063438415527 0.034239768981933594

Final encoder loss: 0.01576506718993187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1155385971069336 0.03367733955383301

Final encoder loss: 0.015131885185837746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11226487159729004 0.034148454666137695

Final encoder loss: 0.015124390833079815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11520123481750488 0.03410053253173828

Final encoder loss: 0.015056363306939602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11335206031799316 0.03564596176147461

Final encoder loss: 0.014820227399468422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11656880378723145 0.03399658203125

Final encoder loss: 0.014826762489974499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11307835578918457 0.03441119194030762

Final encoder loss: 0.014705469831824303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11526226997375488 0.03392624855041504

Final encoder loss: 0.015214546583592892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11301994323730469 0.03412461280822754

Final encoder loss: 0.014725182205438614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11575484275817871 0.034140825271606445

Final encoder loss: 0.015257013030350208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11170387268066406 0.03452467918395996

Final encoder loss: 0.014659139327704906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11677765846252441 0.03413105010986328

Final encoder loss: 0.014354722574353218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1119832992553711 0.03495192527770996

Final encoder loss: 0.014238203875720501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11557149887084961 0.03416919708251953

Final encoder loss: 0.013799281790852547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11298942565917969 0.0338742733001709

Final encoder loss: 0.01383708231151104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11517572402954102 0.03451657295227051

Final encoder loss: 0.013449468649923801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11251044273376465 0.033769845962524414

Final encoder loss: 0.013150994665920734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11590743064880371 0.03451347351074219

Final encoder loss: 0.013223587535321712
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1121530532836914 0.03431558609008789

Final encoder loss: 0.013040292076766491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11562037467956543 0.033402442932128906

Final encoder loss: 0.013049721717834473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11227178573608398 0.034090280532836914

Final encoder loss: 0.01291933935135603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11584353446960449 0.03440403938293457

Final encoder loss: 0.013382813893258572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11215806007385254 0.03455805778503418

Final encoder loss: 0.01304219663143158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11667633056640625 0.034267425537109375

Final encoder loss: 0.013373245485126972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11290812492370605 0.034393310546875

Final encoder loss: 0.012765806168317795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152045726776123 0.034726619720458984

Final encoder loss: 0.013248634524643421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11312055587768555 0.034008026123046875

Final encoder loss: 0.01272121537476778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11571598052978516 0.03418612480163574

Final encoder loss: 0.01282061729580164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11243271827697754 0.03342390060424805

Final encoder loss: 0.012478185817599297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11543011665344238 0.034415245056152344

Final encoder loss: 0.012539919465780258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11267471313476562 0.03391885757446289

Final encoder loss: 0.012724368833005428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11595320701599121 0.03345012664794922

Final encoder loss: 0.012537992559373379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11240887641906738 0.034154415130615234

Final encoder loss: 0.012556822970509529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11639523506164551 0.034245967864990234

Final encoder loss: 0.012633932754397392
Final encoder loss: 0.011660227552056313

Training case model
Final encoder loss: 0.022735292025653983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.0888826847076416 0.21859335899353027

Final encoder loss: 0.022423832081703426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08893585205078125 0.21864581108093262

Final encoder loss: 0.02201921520787393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08890366554260254 0.21857118606567383

Final encoder loss: 0.021977908330018305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08956480026245117 0.21887993812561035

Final encoder loss: 0.022130875346336145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08832359313964844 0.21855664253234863

Final encoder loss: 0.021078190245674148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08898258209228516 0.21875524520874023

Final encoder loss: 0.022292106230908458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08955907821655273 0.21869111061096191

Final encoder loss: 0.02161026301485617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08944153785705566 0.21866893768310547

Final encoder loss: 0.02153319716238205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08909416198730469 0.21857643127441406

Final encoder loss: 0.022163205349882417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08865213394165039 0.21841001510620117

Final encoder loss: 0.02151133832932042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08866357803344727 0.2182157039642334

Final encoder loss: 0.02123771288588375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08995556831359863 0.21940374374389648

Final encoder loss: 0.021296591960750646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08911299705505371 0.21936249732971191

Final encoder loss: 0.021779981823155492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.0895223617553711 0.21912622451782227

Final encoder loss: 0.021436528134284982
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08934211730957031 0.21926355361938477

Final encoder loss: 0.021491338440574204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.0865781307220459 0.21590518951416016


Training case model
Final encoder loss: 0.20296816527843475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2646923065185547 0.05333518981933594

Final encoder loss: 0.18890908360481262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2603461742401123 0.05265522003173828

Final encoder loss: 0.1901482492685318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2700214385986328 0.053820133209228516

Final encoder loss: 0.19219189882278442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571988105773926 0.052408456802368164

Final encoder loss: 0.180818110704422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25878190994262695 0.05262136459350586

Final encoder loss: 0.1919221729040146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25656771659851074 0.05120444297790527

Final encoder loss: 0.10545313358306885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25801873207092285 0.05413055419921875

Final encoder loss: 0.09519062936306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2586331367492676 0.052752017974853516

Final encoder loss: 0.09109000116586685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584822177886963 0.05277705192565918

Final encoder loss: 0.08998444676399231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.279634952545166 0.052109479904174805

Final encoder loss: 0.08156202733516693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579464912414551 0.05238056182861328

Final encoder loss: 0.08538421988487244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25556135177612305 0.05060148239135742

Final encoder loss: 0.06154157593846321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26076602935791016 0.052678823471069336

Final encoder loss: 0.05570955574512482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582402229309082 0.05446648597717285

Final encoder loss: 0.05343363806605339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2700464725494385 0.05250668525695801

Final encoder loss: 0.05367900803685188
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583777904510498 0.052253007888793945

Final encoder loss: 0.049806028604507446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2702910900115967 0.05212283134460449

Final encoder loss: 0.0521608404815197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556488513946533 0.053449392318725586

Final encoder loss: 0.04293736815452576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25869178771972656 0.05106806755065918

Final encoder loss: 0.03967208042740822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26961731910705566 0.0544741153717041

Final encoder loss: 0.03816532343626022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27000856399536133 0.05096316337585449

Final encoder loss: 0.03880374878644943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581632137298584 0.055205583572387695

Final encoder loss: 0.03713924437761307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583122253417969 0.05238032341003418

Final encoder loss: 0.03817756846547127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562844753265381 0.05225253105163574

Final encoder loss: 0.03451027721166611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25838208198547363 0.053850412368774414

Final encoder loss: 0.03280460089445114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25859975814819336 0.05305743217468262

Final encoder loss: 0.03166456148028374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25832366943359375 0.05303955078125

Final encoder loss: 0.0324845127761364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26059985160827637 0.05285143852233887

Final encoder loss: 0.03221018984913826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574777603149414 0.05343151092529297

Final encoder loss: 0.03220506012439728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558891773223877 0.052210330963134766

Final encoder loss: 0.03072812221944332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25893545150756836 0.05292916297912598

Final encoder loss: 0.029943987727165222
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2701559066772461 0.05164623260498047

Final encoder loss: 0.02916826866567135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26906681060791016 0.05395936965942383

Final encoder loss: 0.02966970019042492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26864194869995117 0.05308794975280762

Final encoder loss: 0.03052380494773388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25858640670776367 0.05273699760437012

Final encoder loss: 0.030038224533200264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25516700744628906 0.05534768104553223

Final encoder loss: 0.02833244390785694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25821900367736816 0.05264425277709961

Final encoder loss: 0.027721555903553963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2752861976623535 0.052678585052490234

Final encoder loss: 0.02716498263180256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2602996826171875 0.052559614181518555

Final encoder loss: 0.02752775512635708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2683892250061035 0.05327343940734863

Final encoder loss: 0.028119433671236038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25833916664123535 0.053198814392089844

Final encoder loss: 0.027749402448534966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556159496307373 0.0520777702331543

Final encoder loss: 0.026464372873306274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2593238353729248 0.05146169662475586

Final encoder loss: 0.026150362566113472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26795148849487305 0.05382227897644043

Final encoder loss: 0.02559053525328636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582426071166992 0.051265716552734375

Final encoder loss: 0.025997424498200417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576789855957031 0.052599430084228516

Final encoder loss: 0.02679537981748581
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27051711082458496 0.05373096466064453

Final encoder loss: 0.026288433000445366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25588107109069824 0.05209183692932129

Final encoder loss: 0.025375572964549065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2590503692626953 0.051743268966674805

Final encoder loss: 0.025104453787207603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2610785961151123 0.05251598358154297

Final encoder loss: 0.024789394810795784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257610559463501 0.055370330810546875

Final encoder loss: 0.02495214343070984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26868724822998047 0.05395770072937012

Final encoder loss: 0.025830619037151337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27045321464538574 0.0532536506652832

Final encoder loss: 0.025286173447966576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567124366760254 0.05271649360656738

Final encoder loss: 0.024705100804567337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578897476196289 0.053788185119628906

Final encoder loss: 0.024545731022953987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26963019371032715 0.0527193546295166

Final encoder loss: 0.02423146739602089
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25885844230651855 0.052459001541137695

Final encoder loss: 0.024352721869945526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25892019271850586 0.05130815505981445

Final encoder loss: 0.025514479726552963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.258983850479126 0.05285501480102539

Final encoder loss: 0.02471238188445568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25540971755981445 0.053807973861694336

Final encoder loss: 0.02406645193696022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25967884063720703 0.05161619186401367

Final encoder loss: 0.023957207798957825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2701447010040283 0.053838253021240234

Final encoder loss: 0.023588553071022034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575829029083252 0.053385257720947266

Final encoder loss: 0.023652685806155205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589607238769531 0.053696393966674805

Final encoder loss: 0.024481505155563354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2793703079223633 0.05210447311401367

Final encoder loss: 0.024040956050157547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25513458251953125 0.053730010986328125

Final encoder loss: 0.02361895516514778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578768730163574 0.05279421806335449

Final encoder loss: 0.023681968450546265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2763090133666992 0.052509307861328125

Final encoder loss: 0.023152174428105354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27005672454833984 0.05181884765625

Final encoder loss: 0.023315967991948128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2686324119567871 0.05308103561401367

Final encoder loss: 0.024264998733997345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25925636291503906 0.05228114128112793

Final encoder loss: 0.023829950019717216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547426223754883 0.051972389221191406

Final encoder loss: 0.023294424638152122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2611262798309326 0.05307579040527344

Final encoder loss: 0.02322198823094368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2697293758392334 0.053080081939697266

Final encoder loss: 0.022997744381427765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588367462158203 0.05218839645385742

Final encoder loss: 0.022913126274943352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26295924186706543 0.052683353424072266

Final encoder loss: 0.023810068145394325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2589750289916992 0.05275917053222656

Final encoder loss: 0.023287387564778328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25420594215393066 0.05248665809631348

Final encoder loss: 0.022963786497712135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25925660133361816 0.052224159240722656

Final encoder loss: 0.0229976624250412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2595484256744385 0.052393198013305664

Final encoder loss: 0.022781852632761
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2577986717224121 0.05334734916687012

Final encoder loss: 0.022669995203614235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579059600830078 0.05175495147705078

Final encoder loss: 0.02366470918059349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26829028129577637 0.05263781547546387

Final encoder loss: 0.023149145767092705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2569301128387451 0.05292153358459473

Final encoder loss: 0.022769765928387642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25794315338134766 0.05326557159423828

Final encoder loss: 0.02275245264172554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25855469703674316 0.05275917053222656

Final encoder loss: 0.022424520924687386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2702467441558838 0.052449703216552734

Final encoder loss: 0.022393446415662766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25864553451538086 0.0546872615814209

Final encoder loss: 0.023166993632912636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.28726696968078613 0.052490234375

Final encoder loss: 0.022750914096832275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25531911849975586 0.05295252799987793

Final encoder loss: 0.022550126537680626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2591240406036377 0.05303215980529785

Final encoder loss: 0.022615136578679085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26914525032043457 0.05417346954345703

Final encoder loss: 0.022260410711169243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26759815216064453 0.052580833435058594

Final encoder loss: 0.02226942777633667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26935243606567383 0.053466081619262695

Final encoder loss: 0.02319294586777687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25929808616638184 0.052764177322387695

Final encoder loss: 0.02268698439002037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556915283203125 0.05333876609802246

Final encoder loss: 0.022322477772831917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576746940612793 0.05230116844177246

Final encoder loss: 0.022341277450323105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26854753494262695 0.05243086814880371

Final encoder loss: 0.02210807055234909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2597310543060303 0.05236339569091797

Final encoder loss: 0.021945366635918617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2681243419647217 0.052744388580322266

Final encoder loss: 0.022825799882411957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27959680557250977 0.05280351638793945

Final encoder loss: 0.022375673055648804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25553345680236816 0.05207180976867676

Final encoder loss: 0.022234410047531128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25968050956726074 0.05469989776611328

Final encoder loss: 0.022289320826530457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2686176300048828 0.05163121223449707

Final encoder loss: 0.022021135315299034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26865315437316895 0.05224776268005371

Final encoder loss: 0.021875670179724693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2700924873352051 0.053121328353881836

Final encoder loss: 0.022718489170074463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2573862075805664 0.05416679382324219

Final encoder loss: 0.022365083917975426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547934055328369 0.05109143257141113

Final encoder loss: 0.02208099327981472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25888991355895996 0.05223870277404785

Final encoder loss: 0.022077670320868492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2784264087677002 0.05308794975280762

Final encoder loss: 0.021805791184306145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25793910026550293 0.053951263427734375

Final encoder loss: 0.021618753671646118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.258420467376709 0.052483320236206055

Final encoder loss: 0.022479984909296036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26879143714904785 0.053133487701416016

Final encoder loss: 0.02201324887573719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25604748725891113 0.05221104621887207

Final encoder loss: 0.02195427194237709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576181888580322 0.052727699279785156

Final encoder loss: 0.02205418236553669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26860594749450684 0.05249643325805664

Final encoder loss: 0.02175854705274105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25979042053222656 0.05258059501647949

Final encoder loss: 0.021644635125994682
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2691335678100586 0.054160356521606445

Final encoder loss: 0.022504495456814766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2684025764465332 0.05288887023925781

Final encoder loss: 0.022066742181777954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25544190406799316 0.05201315879821777

Final encoder loss: 0.021815840154886246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25905513763427734 0.05263662338256836

Final encoder loss: 0.02182122878730297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25923728942871094 0.05476784706115723

Final encoder loss: 0.02155112288892269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2769649028778076 0.05227375030517578

Final encoder loss: 0.021357649937272072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2590188980102539 0.0521395206451416

Final encoder loss: 0.022250166162848473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2593040466308594 0.05126595497131348

Final encoder loss: 0.02178819663822651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25555872917175293 0.054174184799194336

Final encoder loss: 0.02174702286720276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2585752010345459 0.05262017250061035

Final encoder loss: 0.021870629861950874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588047981262207 0.05236172676086426

Final encoder loss: 0.02155495621263981
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584221363067627 0.054697275161743164

Final encoder loss: 0.02143224887549877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25756216049194336 0.0529017448425293

Final encoder loss: 0.02228548191487789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2587890625 0.05296683311462402

Final encoder loss: 0.0218835286796093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25655198097229004 0.05230545997619629

Final encoder loss: 0.021687692031264305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2577931880950928 0.05370974540710449

Final encoder loss: 0.021603940054774284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2602710723876953 0.05278325080871582

Final encoder loss: 0.02140854299068451
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2689664363861084 0.05258655548095703

Final encoder loss: 0.021144023165106773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2595491409301758 0.0517125129699707

Final encoder loss: 0.022018540650606155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27660083770751953 0.0554046630859375

Final encoder loss: 0.021562352776527405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25568175315856934 0.052225589752197266

Final encoder loss: 0.021564139053225517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25951504707336426 0.052550554275512695

Final encoder loss: 0.02170337550342083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2577998638153076 0.054692745208740234

Final encoder loss: 0.02136302925646305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.259291410446167 0.05232524871826172

Final encoder loss: 0.021250132471323013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25821828842163086 0.05226469039916992

Final encoder loss: 0.02208160236477852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2714858055114746 0.05242753028869629

Final encoder loss: 0.0217320304363966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541344165802002 0.05359697341918945

Final encoder loss: 0.021492647007107735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581615447998047 0.0527799129486084

Final encoder loss: 0.021498627960681915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2700834274291992 0.05378532409667969

Final encoder loss: 0.02128017507493496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25925779342651367 0.05124688148498535

Final encoder loss: 0.020968006923794746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26832056045532227 0.054540395736694336

Final encoder loss: 0.021829096600413322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579226493835449 0.05316305160522461

Final encoder loss: 0.021409204229712486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565639019012451 0.05332064628601074

Final encoder loss: 0.021430587396025658
Final encoder loss: 0.021092504262924194
Final encoder loss: 0.020252017304301262
Final encoder loss: 0.019489653408527374
Final encoder loss: 0.019536610692739487
Final encoder loss: 0.01840929128229618

Training emognition model
Final encoder loss: 0.02628400195947377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08250975608825684 0.23102712631225586

Final encoder loss: 0.02594396730497861
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08094573020935059 0.2307138442993164

Final encoder loss: 0.025129046790818864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0823211669921875 0.23075342178344727

Final encoder loss: 0.024733302349746116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08081865310668945 0.23084521293640137

Final encoder loss: 0.025369368042016706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08170485496520996 0.23060822486877441

Final encoder loss: 0.026215822873048508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08098340034484863 0.23094630241394043

Final encoder loss: 0.02485283545275676
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.0807337760925293 0.23200440406799316

Final encoder loss: 0.02518068892180792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08114480972290039 0.23110413551330566

Final encoder loss: 0.02744387543140164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08120107650756836 0.23087835311889648

Final encoder loss: 0.024606047370539914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08135390281677246 0.23111677169799805

Final encoder loss: 0.02494972935278426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08214306831359863 0.23076891899108887

Final encoder loss: 0.024198410703017037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08234715461730957 0.2310342788696289

Final encoder loss: 0.02426687470289846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08142971992492676 0.23087406158447266

Final encoder loss: 0.02357962712885506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.0820009708404541 0.2311992645263672

Final encoder loss: 0.024372120198822074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08136510848999023 0.2310190200805664

Final encoder loss: 0.02574342737380226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08135700225830078 0.22992968559265137


Training emognition model
Final encoder loss: 0.19357813894748688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2522127628326416 0.0488276481628418

Final encoder loss: 0.19497156143188477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24921083450317383 0.050252676010131836

Final encoder loss: 0.08948095142841339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24871420860290527 0.04915881156921387

Final encoder loss: 0.08896996080875397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2503843307495117 0.04892230033874512

Final encoder loss: 0.056561172008514404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24822497367858887 0.050980567932128906

Final encoder loss: 0.054968930780887604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24851274490356445 0.05044436454772949

Final encoder loss: 0.041806142777204514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2504241466522217 0.0487973690032959

Final encoder loss: 0.04063502326607704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24701189994812012 0.05126762390136719

Final encoder loss: 0.034266483038663864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24886083602905273 0.04852652549743652

Final encoder loss: 0.03351235017180443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24778509140014648 0.04964923858642578

Final encoder loss: 0.030061140656471252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24888920783996582 0.05077242851257324

Final encoder loss: 0.029546208679676056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2475576400756836 0.04912590980529785

Final encoder loss: 0.027660680934786797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502715587615967 0.04910898208618164

Final encoder loss: 0.027242999523878098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24863290786743164 0.04933500289916992

Final encoder loss: 0.02638387307524681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24908018112182617 0.04957699775695801

Final encoder loss: 0.026082778349518776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24915432929992676 0.04948782920837402

Final encoder loss: 0.025629643350839615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24899578094482422 0.04997611045837402

Final encoder loss: 0.025677723810076714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24868297576904297 0.049280643463134766

Final encoder loss: 0.02520005591213703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24686861038208008 0.048924922943115234

Final encoder loss: 0.025434790179133415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24579238891601562 0.04843759536743164

Final encoder loss: 0.024715280160307884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.246596097946167 0.04936575889587402

Final encoder loss: 0.025029020383954048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24585437774658203 0.04877018928527832

Final encoder loss: 0.024476755410432816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2468874454498291 0.04945826530456543

Final encoder loss: 0.024553192779421806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2457118034362793 0.04919862747192383

Final encoder loss: 0.02413192391395569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24704504013061523 0.04899168014526367

Final encoder loss: 0.02411411888897419
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2449941635131836 0.04959845542907715

Final encoder loss: 0.023975050076842308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.246626615524292 0.04924774169921875

Final encoder loss: 0.02403879724442959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24621963500976562 0.05002188682556152

Final encoder loss: 0.02385556511580944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2490675449371338 0.04920315742492676

Final encoder loss: 0.024023668840527534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468099594116211 0.04948925971984863

Final encoder loss: 0.02380228601396084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24850797653198242 0.049593210220336914

Final encoder loss: 0.024012312293052673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2479875087738037 0.04873085021972656

Final encoder loss: 0.02353951707482338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24885797500610352 0.05004549026489258

Final encoder loss: 0.02379530295729637
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2467820644378662 0.04928278923034668

Final encoder loss: 0.02345982939004898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24864578247070312 0.0496366024017334

Final encoder loss: 0.023662514984607697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2487201690673828 0.049462080001831055

Final encoder loss: 0.0232392605394125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24740886688232422 0.04988551139831543

Final encoder loss: 0.023445533588528633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24840712547302246 0.0493314266204834

Final encoder loss: 0.023172926157712936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24848198890686035 0.04958653450012207

Final encoder loss: 0.023386500775814056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2488265037536621 0.0490107536315918

Final encoder loss: 0.023009883239865303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2487480640411377 0.050359487533569336

Final encoder loss: 0.023342570289969444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24730253219604492 0.04980635643005371

Final encoder loss: 0.023011786863207817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24921607971191406 0.04905509948730469

Final encoder loss: 0.02321590855717659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.248274564743042 0.049486637115478516

Final encoder loss: 0.023029424250125885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2476029396057129 0.04937148094177246

Final encoder loss: 0.023200536146759987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2488250732421875 0.05076909065246582

Final encoder loss: 0.023036975413560867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24985575675964355 0.04965543746948242

Final encoder loss: 0.023098673671483994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24760222434997559 0.05151677131652832

Final encoder loss: 0.02287278324365616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2974119186401367 0.04945540428161621

Final encoder loss: 0.023118993267416954
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.34383249282836914 0.049696922302246094

Final encoder loss: 0.022816289216279984
Final encoder loss: 0.022205084562301636

Training empatch model
Final encoder loss: 0.03263984939801313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07119917869567871 0.17391061782836914

Final encoder loss: 0.03234829560569881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07083511352539062 0.1743764877319336

Final encoder loss: 0.031357929883957726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07155537605285645 0.17374825477600098

Final encoder loss: 0.03293891333478656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07246279716491699 0.17499852180480957

Final encoder loss: 0.03163976361805679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07165741920471191 0.1736140251159668

Final encoder loss: 0.03178838044675209
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0709226131439209 0.1738135814666748

Final encoder loss: 0.03053878647088109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07215499877929688 0.17402935028076172

Final encoder loss: 0.03340809573904941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07127571105957031 0.17351269721984863

Final encoder loss: 0.022923892549795145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07215142250061035 0.17383766174316406

Final encoder loss: 0.02393371039016518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07138609886169434 0.17369604110717773

Final encoder loss: 0.0239902950648287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07092761993408203 0.1739497184753418

Final encoder loss: 0.02212286610882815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07107067108154297 0.17447614669799805

Final encoder loss: 0.023060122265829843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0709681510925293 0.17391729354858398

Final encoder loss: 0.020583462355444424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07077884674072266 0.17405176162719727

Final encoder loss: 0.02066920953082726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07091212272644043 0.1739215850830078

Final encoder loss: 0.023595212052407156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07171344757080078 0.173384428024292


Training empatch model
Final encoder loss: 0.17117175459861755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17697525024414062 0.04501485824584961

Final encoder loss: 0.08055324852466583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17520570755004883 0.044573307037353516

Final encoder loss: 0.054837025701999664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17575860023498535 0.04434919357299805

Final encoder loss: 0.04228830337524414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1766202449798584 0.043306827545166016

Final encoder loss: 0.03510647639632225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17445039749145508 0.044384002685546875

Final encoder loss: 0.030605973675847054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17479538917541504 0.0434877872467041

Final encoder loss: 0.027644066140055656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17485594749450684 0.04369783401489258

Final encoder loss: 0.025614377111196518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17441868782043457 0.04370689392089844

Final encoder loss: 0.024236684665083885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17573761940002441 0.043796539306640625

Final encoder loss: 0.023356128484010696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748650074005127 0.0436553955078125

Final encoder loss: 0.022680625319480896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17626404762268066 0.04385256767272949

Final encoder loss: 0.022266242653131485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17599844932556152 0.04385685920715332

Final encoder loss: 0.02185964211821556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17554926872253418 0.04413318634033203

Final encoder loss: 0.021621081978082657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17435669898986816 0.043146371841430664

Final encoder loss: 0.02131291665136814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17637944221496582 0.042975664138793945

Final encoder loss: 0.021137362346053123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17455148696899414 0.043931007385253906

Final encoder loss: 0.02085905335843563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17634820938110352 0.04314303398132324

Final encoder loss: 0.020830277353525162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17576980590820312 0.04335379600524902

Final encoder loss: 0.02067403867840767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17669677734375 0.043338775634765625

Final encoder loss: 0.02067011408507824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17480254173278809 0.045874834060668945

Final encoder loss: 0.020495325326919556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17688202857971191 0.043442487716674805

Final encoder loss: 0.020370643585920334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21946120262145996 0.043378353118896484

Final encoder loss: 0.020276859402656555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21747303009033203 0.0425417423248291

Final encoder loss: 0.020218778401613235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21817493438720703 0.04523634910583496

Final encoder loss: 0.0202050618827343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.24407291412353516 0.04439878463745117

Final encoder loss: 0.02014162763953209

Training wesad model
Final encoder loss: 0.035159643896729854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07140183448791504 0.17397689819335938

Final encoder loss: 0.03266070288105766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07096219062805176 0.1735398769378662

Final encoder loss: 0.0317179336589142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07100534439086914 0.17361855506896973

Final encoder loss: 0.03542781333259976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07095837593078613 0.17426729202270508

Final encoder loss: 0.021639963188898458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07184219360351562 0.1734156608581543

Final encoder loss: 0.021294451403494136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07113933563232422 0.1737227439880371

Final encoder loss: 0.02206717142389938
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07201552391052246 0.17456340789794922

Final encoder loss: 0.024147255328365102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07123208045959473 0.1739182472229004

Final encoder loss: 0.016066140010895454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07149648666381836 0.17434000968933105

Final encoder loss: 0.018096835919167737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07101225852966309 0.1738748550415039

Final encoder loss: 0.01644051818739403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07114458084106445 0.17396116256713867

Final encoder loss: 0.018194617388488005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07115840911865234 0.17377829551696777

Final encoder loss: 0.01399680124932483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07090306282043457 0.17393732070922852

Final encoder loss: 0.014112638070059006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0718839168548584 0.1738598346710205

Final encoder loss: 0.01408131774395646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07121896743774414 0.17419910430908203

Final encoder loss: 0.014324748833637607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0710153579711914 0.17251133918762207


Training wesad model
Final encoder loss: 0.21560141444206238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10559391975402832 0.03272652626037598

Final encoder loss: 0.09835048019886017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10356926918029785 0.033217668533325195

Final encoder loss: 0.06324253231287003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10369133949279785 0.032703399658203125

Final encoder loss: 0.04584304243326187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10337996482849121 0.033156633377075195

Final encoder loss: 0.03579232469201088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10332107543945312 0.03247690200805664

Final encoder loss: 0.02959495596587658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10316991806030273 0.03337860107421875

Final encoder loss: 0.02556709758937359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10344147682189941 0.032666683197021484

Final encoder loss: 0.022856146097183228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10355806350708008 0.033277034759521484

Final encoder loss: 0.020923450589179993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10370397567749023 0.03336763381958008

Final encoder loss: 0.01958986185491085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10331082344055176 0.03343915939331055

Final encoder loss: 0.018682444468140602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10367083549499512 0.033370256423950195

Final encoder loss: 0.018121734261512756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442066192626953 0.03250598907470703

Final encoder loss: 0.017807921394705772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10375261306762695 0.033453941345214844

Final encoder loss: 0.01763983629643917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038362979888916 0.033377647399902344

Final encoder loss: 0.017608996480703354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10395956039428711 0.03261876106262207

Final encoder loss: 0.017690367996692657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10358500480651855 0.033440351486206055

Final encoder loss: 0.017504790797829628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10332059860229492 0.033007144927978516

Final encoder loss: 0.017338445410132408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10326409339904785 0.032677412033081055

Final encoder loss: 0.017230981960892677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10377120971679688 0.03329610824584961

Final encoder loss: 0.017289595678448677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10327816009521484 0.032619476318359375

Final encoder loss: 0.017239216715097427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10317850112915039 0.033289432525634766

Final encoder loss: 0.01721491478383541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10352516174316406 0.0330348014831543

Final encoder loss: 0.016976719722151756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10493826866149902 0.03327488899230957

Final encoder loss: 0.016956577077507973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10408997535705566 0.033492326736450195

Final encoder loss: 0.01697121001780033
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10442137718200684 0.03381466865539551

Final encoder loss: 0.017238592728972435

Calculating loss for amigos model
	Full Pass 0.7084865570068359
numFreeParamsPath 18
Reconstruction loss values: 0.024605263024568558 0.03356105461716652

Calculating loss for dapper model
	Full Pass 0.15658879280090332
numFreeParamsPath 18
Reconstruction loss values: 0.01954459212720394 0.022816190496087074

Calculating loss for case model
	Full Pass 0.9128692150115967
numFreeParamsPath 18
Reconstruction loss values: 0.03071851097047329 0.033612627536058426

Calculating loss for emognition model
	Full Pass 0.27974367141723633
numFreeParamsPath 18
Reconstruction loss values: 0.03214235231280327 0.039717547595500946

Calculating loss for empatch model
	Full Pass 0.10512709617614746
numFreeParamsPath 18
Reconstruction loss values: 0.032161809504032135 0.039624251425266266

Calculating loss for wesad model
	Full Pass 0.07789492607116699
numFreeParamsPath 18
Reconstruction loss values: 0.0334436260163784 0.05016987770795822
Total loss calculation time: 5.039490222930908

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
1 1
Total plotting time: 449.78604769706726
Total epoch time: 675.8976790904999

Epoch: 51

Training dapper model
Final encoder loss: 0.018307668616495538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06525182723999023 0.1556239128112793

Final encoder loss: 0.017656905167493624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06226658821105957 0.1510615348815918

Final encoder loss: 0.021126683476388623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062164306640625 0.15089106559753418

Final encoder loss: 0.01835652165465851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06342768669128418 0.15032649040222168

Final encoder loss: 0.017960593215038408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.0623927116394043 0.1500716209411621

Final encoder loss: 0.01742879418794312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06197214126586914 0.1524195671081543

Final encoder loss: 0.018568552993949678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062017202377319336 0.15069055557250977

Final encoder loss: 0.016899060237262873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06209444999694824 0.15102338790893555

Final encoder loss: 0.01744271973536271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06339263916015625 0.15168213844299316

Final encoder loss: 0.02018639231231644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06239581108093262 0.1503157615661621

Final encoder loss: 0.017334748266501866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06212425231933594 0.15186595916748047

Final encoder loss: 0.020136806706697315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06363058090209961 0.15126729011535645

Final encoder loss: 0.017018020807650597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06215953826904297 0.1509389877319336

Final encoder loss: 0.018828348337742332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06168508529663086 0.15219473838806152

Final encoder loss: 0.018692243323121756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.062151193618774414 0.1509110927581787

Final encoder loss: 0.015279283791178872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06183481216430664 0.14985966682434082


Training case model
Final encoder loss: 0.03100371399778752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09249496459960938 0.26670169830322266

Final encoder loss: 0.02836965230100548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09128618240356445 0.26499128341674805

Final encoder loss: 0.027678693924000835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09106087684631348 0.2660403251647949

Final encoder loss: 0.027342233747271124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09194278717041016 0.26691389083862305

Final encoder loss: 0.026066858115929032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09152436256408691 0.2656106948852539

Final encoder loss: 0.024845692671944117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09181594848632812 0.2690761089324951

Final encoder loss: 0.025513924757473272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09156179428100586 0.2662932872772217

Final encoder loss: 0.02510698166058814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0918588638305664 0.26639270782470703

Final encoder loss: 0.024515207253308472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09307456016540527 0.26520776748657227

Final encoder loss: 0.024389499762312694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09134078025817871 0.26530957221984863

Final encoder loss: 0.024296918108816522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09165191650390625 0.2661604881286621

Final encoder loss: 0.024335928526841823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09272551536560059 0.26642870903015137

Final encoder loss: 0.02392106820196297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09134387969970703 0.26628875732421875

Final encoder loss: 0.02415321360472304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09151339530944824 0.26531100273132324

Final encoder loss: 0.02417023889595647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09178996086120605 0.26599740982055664

Final encoder loss: 0.02359895883884988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.0883328914642334 0.2619626522064209


Training amigos model
Final encoder loss: 0.02350611188084311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10914158821105957 0.3894975185394287

Final encoder loss: 0.0255468084026876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10994124412536621 0.38971686363220215

Final encoder loss: 0.02330174157954437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10906410217285156 0.39047765731811523

Final encoder loss: 0.023346218008386925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10809993743896484 0.38954925537109375

Final encoder loss: 0.02295965127558049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10852909088134766 0.3890862464904785

Final encoder loss: 0.024649223716570814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10827946662902832 0.3894810676574707

Final encoder loss: 0.024913005673650433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1082611083984375 0.3895537853240967

Final encoder loss: 0.02547546025676349
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10846638679504395 0.3905608654022217

Final encoder loss: 0.02529500134319962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10883879661560059 0.389723539352417

Final encoder loss: 0.025125355914411666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10991024971008301 0.39017820358276367

Final encoder loss: 0.02458331260302145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10849738121032715 0.3906726837158203

Final encoder loss: 0.02423590618545445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10803627967834473 0.38890695571899414

Final encoder loss: 0.023772589871227434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10806608200073242 0.3891580104827881

Final encoder loss: 0.022798410521145417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10814785957336426 0.3892860412597656

Final encoder loss: 0.024011842960344552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10812950134277344 0.39049768447875977

Final encoder loss: 0.025447898244593356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10399270057678223 0.38417887687683105


Training emognition model
Final encoder loss: 0.0320151412626598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08499026298522949 0.27611565589904785

Final encoder loss: 0.03157081220419431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08344221115112305 0.27562856674194336

Final encoder loss: 0.030711118371124398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08385324478149414 0.2758796215057373

Final encoder loss: 0.03159481196265921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08350110054016113 0.27696895599365234

Final encoder loss: 0.031320245126962784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08391404151916504 0.2757604122161865

Final encoder loss: 0.030022624942958594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08519554138183594 0.2754685878753662

Final encoder loss: 0.03097243021048978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08326148986816406 0.2752985954284668

Final encoder loss: 0.03055186352087756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08355498313903809 0.2755246162414551

Final encoder loss: 0.03066530812542247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.084259033203125 0.2767758369445801

Final encoder loss: 0.030355893984646956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08351445198059082 0.2756829261779785

Final encoder loss: 0.03154401926199145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08469176292419434 0.2750511169433594

Final encoder loss: 0.03017957310327055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08353972434997559 0.27637529373168945

Final encoder loss: 0.030768291950046888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08397746086120605 0.2756531238555908

Final encoder loss: 0.028896797956319088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08499789237976074 0.2756025791168213

Final encoder loss: 0.03145803362987551
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08344030380249023 0.2753148078918457

Final encoder loss: 0.0334311677130405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0827035903930664 0.27491164207458496


Training amigos model
Final encoder loss: 0.01955341454734577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10721921920776367 0.34159064292907715

Final encoder loss: 0.01842389541731919
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10636448860168457 0.3414180278778076

Final encoder loss: 0.017952281593920157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10667157173156738 0.34161972999572754

Final encoder loss: 0.018359447789573605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10592222213745117 0.3417508602142334

Final encoder loss: 0.020048149927285777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10780048370361328 0.3418281078338623

Final encoder loss: 0.016945335616989333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1063392162322998 0.3419647216796875

Final encoder loss: 0.018538272194451075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10593581199645996 0.3417360782623291

Final encoder loss: 0.018408996331405136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10633563995361328 0.3416163921356201

Final encoder loss: 0.019660800647009265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10711956024169922 0.3416860103607178

Final encoder loss: 0.016683181124320632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.12509918212890625 0.3419618606567383

Final encoder loss: 0.017186538925259596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10612249374389648 0.34171438217163086

Final encoder loss: 0.01837283227036185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10631656646728516 0.3417222499847412

Final encoder loss: 0.019604496542620795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10622692108154297 0.34171009063720703

Final encoder loss: 0.01753674945602748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10699152946472168 0.3416328430175781

Final encoder loss: 0.018460275879723466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.1063385009765625 0.34195375442504883

Final encoder loss: 0.019181986078169296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10092639923095703 0.3386962413787842


Training amigos model
Final encoder loss: 0.1807817965745926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47681689262390137 0.0800473690032959

Final encoder loss: 0.1878378987312317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4656374454498291 0.07967066764831543

Final encoder loss: 0.18362107872962952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4676392078399658 0.08035874366760254

Final encoder loss: 0.07745425403118134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4705226421356201 0.07705926895141602

Final encoder loss: 0.07885336875915527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46573710441589355 0.08252120018005371

Final encoder loss: 0.07356639951467514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4695119857788086 0.08110237121582031

Final encoder loss: 0.044805873185396194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46765947341918945 0.07607221603393555

Final encoder loss: 0.045169081538915634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45525169372558594 0.07597780227661133

Final encoder loss: 0.04272935912013054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45635056495666504 0.07428860664367676

Final encoder loss: 0.031916696578264236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45485901832580566 0.07685399055480957

Final encoder loss: 0.03242480382323265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4552493095397949 0.07698774337768555

Final encoder loss: 0.031085912138223648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.457719087600708 0.07459330558776855

Final encoder loss: 0.02609420008957386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4704782962799072 0.0785360336303711

Final encoder loss: 0.026428138837218285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4630401134490967 0.07854461669921875

Final encoder loss: 0.025657394900918007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4678456783294678 0.08113431930541992

Final encoder loss: 0.023381222039461136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46933484077453613 0.07790327072143555

Final encoder loss: 0.023577416315674782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4623863697052002 0.07729411125183105

Final encoder loss: 0.022979462519288063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4680609703063965 0.07840704917907715

Final encoder loss: 0.022198008373379707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46964335441589355 0.0779876708984375

Final encoder loss: 0.02241610176861286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46633076667785645 0.07630205154418945

Final encoder loss: 0.02194642834365368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46559906005859375 0.07683134078979492

Final encoder loss: 0.021800434216856956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47025132179260254 0.07743501663208008

Final encoder loss: 0.02200835943222046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680767059326172 0.07807731628417969

Final encoder loss: 0.021905016154050827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4671902656555176 0.07420969009399414

Final encoder loss: 0.021313853561878204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4681107997894287 0.07796907424926758

Final encoder loss: 0.021252546459436417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46503639221191406 0.0803062915802002

Final encoder loss: 0.02124921604990959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46674299240112305 0.07596445083618164

Final encoder loss: 0.020536677911877632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46829652786254883 0.07667231559753418

Final encoder loss: 0.020374884828925133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46617627143859863 0.08133721351623535

Final encoder loss: 0.02050507441163063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46933889389038086 0.07619524002075195

Final encoder loss: 0.01983962208032608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46394968032836914 0.07759571075439453

Final encoder loss: 0.019679009914398193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46610140800476074 0.07999300956726074

Final encoder loss: 0.019537635147571564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47060632705688477 0.07509970664978027

Final encoder loss: 0.019472520798444748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46404051780700684 0.07807350158691406

Final encoder loss: 0.019319510087370872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680001735687256 0.08168578147888184

Final encoder loss: 0.01936827041208744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46859192848205566 0.07496976852416992

Final encoder loss: 0.01921640895307064
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46494388580322266 0.07864522933959961

Final encoder loss: 0.01921340823173523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4655723571777344 0.07938313484191895

Final encoder loss: 0.019137587398290634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46923112869262695 0.07627677917480469

Final encoder loss: 0.01910477690398693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46544957160949707 0.07736897468566895

Final encoder loss: 0.018804941326379776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4655632972717285 0.08167243003845215

Final encoder loss: 0.019046735018491745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4709780216217041 0.07806897163391113

Final encoder loss: 0.0189435426145792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4633622169494629 0.07695317268371582

Final encoder loss: 0.01845821551978588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4666461944580078 0.08045697212219238

Final encoder loss: 0.01859431155025959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4706733226776123 0.07726073265075684

Final encoder loss: 0.018769480288028717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46180176734924316 0.07823467254638672

Final encoder loss: 0.018225956708192825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4663074016571045 0.0790865421295166

Final encoder loss: 0.018577048555016518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46113061904907227 0.07446670532226562

Final encoder loss: 0.018335629254579544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45580029487609863 0.07622742652893066

Final encoder loss: 0.01817721128463745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4540712833404541 0.07447195053100586

Final encoder loss: 0.018255818635225296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45728182792663574 0.0739288330078125

Final encoder loss: 0.01833394356071949
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4551708698272705 0.07554125785827637

Final encoder loss: 0.017919469624757767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45783352851867676 0.07834434509277344

Final encoder loss: 0.01821073144674301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46952390670776367 0.07676935195922852

Final encoder loss: 0.018079767003655434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46685194969177246 0.07674765586853027

Final encoder loss: 0.017957502976059914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46647071838378906 0.07554388046264648

Final encoder loss: 0.01788482815027237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4669992923736572 0.07790040969848633

Final encoder loss: 0.01813032664358616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4649808406829834 0.08146476745605469

Final encoder loss: 0.017652103677392006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4653799533843994 0.08037734031677246

Final encoder loss: 0.017993446439504623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4693186283111572 0.07534909248352051

Final encoder loss: 0.017803065478801727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4672853946685791 0.07733821868896484

Final encoder loss: 0.017769625410437584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4679999351501465 0.07676935195922852

Final encoder loss: 0.017818409949541092
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4681699275970459 0.07594537734985352

Final encoder loss: 0.017900843173265457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.465726375579834 0.07710719108581543

Final encoder loss: 0.01755225472152233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46608448028564453 0.07796883583068848

Final encoder loss: 0.01796700991690159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46411752700805664 0.07444119453430176

Final encoder loss: 0.017828650772571564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4549424648284912 0.07616972923278809

Final encoder loss: 0.017422739416360855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45467591285705566 0.07755327224731445

Final encoder loss: 0.01767098344862461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4564371109008789 0.07492327690124512

Final encoder loss: 0.01777532510459423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45506787300109863 0.07520437240600586

Final encoder loss: 0.017353560775518417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45487070083618164 0.07561731338500977

Final encoder loss: 0.017613209784030914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4670431613922119 0.07577252388000488

Final encoder loss: 0.017659086734056473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46291279792785645 0.0814363956451416

Final encoder loss: 0.017236405983567238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4673123359680176 0.0773158073425293

Final encoder loss: 0.017417041584849358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4668869972229004 0.07683849334716797

Final encoder loss: 0.017658798024058342
Final encoder loss: 0.016298292204737663
Final encoder loss: 0.01593598537147045

Training dapper model
Final encoder loss: 0.016313768508805487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059891462326049805 0.10740518569946289

Final encoder loss: 0.014744161006154806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.059667348861694336 0.1072237491607666

Final encoder loss: 0.01746170044055619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05986976623535156 0.10733985900878906

Final encoder loss: 0.01569403472939499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059647321701049805 0.10715365409851074

Final encoder loss: 0.013939341516836753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.059917449951171875 0.10757040977478027

Final encoder loss: 0.014381233919280208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.060004472732543945 0.10718131065368652

Final encoder loss: 0.014737642916383964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.06005692481994629 0.10796213150024414

Final encoder loss: 0.014058312755248159
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060158729553222656 0.10691690444946289

Final encoder loss: 0.014770481269051094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06031942367553711 0.10804104804992676

Final encoder loss: 0.01389378578951393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06011033058166504 0.1066288948059082

Final encoder loss: 0.014867002149374309
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059015512466430664 0.10756874084472656

Final encoder loss: 0.015410703267660529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05913186073303223 0.10625147819519043

Final encoder loss: 0.016894040093629912
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.059255123138427734 0.10745763778686523

Final encoder loss: 0.013112439977797557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05971503257751465 0.10773611068725586

Final encoder loss: 0.013993117408266429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.0613255500793457 0.10846948623657227

Final encoder loss: 0.013739105659162434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.06072211265563965 0.10773086547851562


Training dapper model
Final encoder loss: 0.20245769619941711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11850476264953613 0.034200429916381836

Final encoder loss: 0.20820178091526031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11507511138916016 0.034739017486572266

Final encoder loss: 0.08601187914609909
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11289072036743164 0.03429579734802246

Final encoder loss: 0.08794131129980087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11727428436279297 0.03348255157470703

Final encoder loss: 0.05049637332558632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11213421821594238 0.03438711166381836

Final encoder loss: 0.050308700650930405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11441540718078613 0.033785104751586914

Final encoder loss: 0.034090105444192886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11203646659851074 0.03360748291015625

Final encoder loss: 0.03400629386305809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11467337608337402 0.034036874771118164

Final encoder loss: 0.025695957243442535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11252188682556152 0.0344696044921875

Final encoder loss: 0.025774603709578514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11461043357849121 0.03382611274719238

Final encoder loss: 0.02105201967060566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11214494705200195 0.03561687469482422

Final encoder loss: 0.02115763910114765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157081127166748 0.03419780731201172

Final encoder loss: 0.01832994818687439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11466670036315918 0.03494834899902344

Final encoder loss: 0.018411142751574516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11673879623413086 0.035268306732177734

Final encoder loss: 0.016706641763448715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11298799514770508 0.03426384925842285

Final encoder loss: 0.01669219322502613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11419343948364258 0.03410959243774414

Final encoder loss: 0.01572634093463421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11177277565002441 0.03536653518676758

Final encoder loss: 0.01568731851875782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1173555850982666 0.03450345993041992

Final encoder loss: 0.015087636187672615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11202597618103027 0.03444504737854004

Final encoder loss: 0.015024429187178612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11520576477050781 0.03373432159423828

Final encoder loss: 0.014687474817037582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11220955848693848 0.03561878204345703

Final encoder loss: 0.014768246561288834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1148989200592041 0.033722877502441406

Final encoder loss: 0.014630477875471115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11278128623962402 0.03444838523864746

Final encoder loss: 0.014586112461984158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11474275588989258 0.03402996063232422

Final encoder loss: 0.014627817086875439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1119070053100586 0.03387117385864258

Final encoder loss: 0.014501238241791725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11597371101379395 0.033968210220336914

Final encoder loss: 0.014523791149258614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11213350296020508 0.034028053283691406

Final encoder loss: 0.014203803613781929
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11409687995910645 0.034215450286865234

Final encoder loss: 0.014289210550487041
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11200547218322754 0.034081220626831055

Final encoder loss: 0.01370969321578741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11438775062561035 0.03410482406616211

Final encoder loss: 0.013721219263970852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11238503456115723 0.034256935119628906

Final encoder loss: 0.013695130124688148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11449885368347168 0.03308558464050293

Final encoder loss: 0.013261635787785053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11191868782043457 0.03404402732849121

Final encoder loss: 0.013396546244621277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11436295509338379 0.03389859199523926

Final encoder loss: 0.012944789603352547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11230015754699707 0.03376293182373047

Final encoder loss: 0.013184096664190292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11468386650085449 0.03348350524902344

Final encoder loss: 0.012811882421374321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11168265342712402 0.0336909294128418

Final encoder loss: 0.013078765943646431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11530923843383789 0.03426718711853027

Final encoder loss: 0.01297415979206562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11209511756896973 0.033858537673950195

Final encoder loss: 0.012891740538179874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11449456214904785 0.03460407257080078

Final encoder loss: 0.013070615008473396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1126561164855957 0.033722639083862305

Final encoder loss: 0.01269777026027441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11494612693786621 0.033921003341674805

Final encoder loss: 0.012994413264095783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11211943626403809 0.03355598449707031

Final encoder loss: 0.012544880621135235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1142568588256836 0.03398776054382324

Final encoder loss: 0.012699011713266373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11269760131835938 0.034247398376464844

Final encoder loss: 0.012472761794924736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11439633369445801 0.03418898582458496

Final encoder loss: 0.012412679381668568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11197638511657715 0.03389143943786621

Final encoder loss: 0.012623741291463375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11440205574035645 0.0340423583984375

Final encoder loss: 0.012243720702826977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11202883720397949 0.03483271598815918

Final encoder loss: 0.01260133646428585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11564373970031738 0.03519153594970703

Final encoder loss: 0.012141927145421505
Final encoder loss: 0.011563250795006752

Training case model
Final encoder loss: 0.022691136502972283
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08905243873596191 0.21889042854309082

Final encoder loss: 0.022065155565567103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08933854103088379 0.21834611892700195

Final encoder loss: 0.021402355042675854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08881020545959473 0.21838092803955078

Final encoder loss: 0.021346376441331475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08968424797058105 0.21844959259033203

Final encoder loss: 0.021485069725783512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0886533260345459 0.21834969520568848

Final encoder loss: 0.021699487761997645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0892021656036377 0.21836280822753906

Final encoder loss: 0.021503460036925867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08921098709106445 0.21851325035095215

Final encoder loss: 0.02127892105636678
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08893942832946777 0.21828413009643555

Final encoder loss: 0.021587194532069465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08912444114685059 0.21830034255981445

Final encoder loss: 0.021222764795228218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.0889132022857666 0.21850299835205078

Final encoder loss: 0.022138434997749867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08961868286132812 0.2185497283935547

Final encoder loss: 0.021015984357490573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08924531936645508 0.21859145164489746

Final encoder loss: 0.021235355854365763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08916568756103516 0.2185354232788086

Final encoder loss: 0.02131792474621422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08912825584411621 0.21949338912963867

Final encoder loss: 0.020985269175243522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08887577056884766 0.21840572357177734

Final encoder loss: 0.02176853657589362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.0854043960571289 0.21494531631469727


Training case model
Final encoder loss: 0.2029712200164795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26148533821105957 0.05161452293395996

Final encoder loss: 0.18891854584217072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25718069076538086 0.051770687103271484

Final encoder loss: 0.19015002250671387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257584810256958 0.05258584022521973

Final encoder loss: 0.19218096137046814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2564737796783447 0.05214810371398926

Final encoder loss: 0.18081197142601013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2679166793823242 0.051790475845336914

Final encoder loss: 0.191915825009346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25487232208251953 0.05127215385437012

Final encoder loss: 0.10606089234352112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25707578659057617 0.052019357681274414

Final encoder loss: 0.09559644758701324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572932243347168 0.05220437049865723

Final encoder loss: 0.09154324978590012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2583808898925781 0.05320620536804199

Final encoder loss: 0.09008876234292984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570657730102539 0.05308890342712402

Final encoder loss: 0.08174216002225876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25766706466674805 0.052471160888671875

Final encoder loss: 0.08555933833122253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551558017730713 0.0520939826965332

Final encoder loss: 0.06200951337814331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576892375946045 0.05309605598449707

Final encoder loss: 0.05604339763522148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25667786598205566 0.05167579650878906

Final encoder loss: 0.0537535585463047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575058937072754 0.05191230773925781

Final encoder loss: 0.0537022165954113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26799893379211426 0.053176164627075195

Final encoder loss: 0.05003637820482254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574775218963623 0.05225849151611328

Final encoder loss: 0.05240362137556076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2535521984100342 0.0517425537109375

Final encoder loss: 0.043319761753082275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573668956756592 0.0524294376373291

Final encoder loss: 0.03984304517507553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568669319152832 0.05106544494628906

Final encoder loss: 0.038287557661533356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566399574279785 0.052922964096069336

Final encoder loss: 0.03864233195781708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567756175994873 0.05204367637634277

Final encoder loss: 0.037227828055620193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585153579711914 0.052184104919433594

Final encoder loss: 0.038367897272109985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539675235748291 0.05177474021911621

Final encoder loss: 0.034695692360401154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25770044326782227 0.05182242393493652

Final encoder loss: 0.03289618715643883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25774073600769043 0.0532228946685791

Final encoder loss: 0.03157753497362137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566187381744385 0.05237841606140137

Final encoder loss: 0.03232994303107262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577207088470459 0.05274605751037598

Final encoder loss: 0.032124415040016174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25653696060180664 0.05287027359008789

Final encoder loss: 0.032324109226465225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549276351928711 0.051589012145996094

Final encoder loss: 0.030768636614084244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571592330932617 0.051639556884765625

Final encoder loss: 0.029801562428474426
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573244571685791 0.05267953872680664

Final encoder loss: 0.029012490063905716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25780510902404785 0.05314207077026367

Final encoder loss: 0.029541874304413795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574186325073242 0.05230212211608887

Final encoder loss: 0.030167164281010628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25716447830200195 0.05194973945617676

Final encoder loss: 0.02986394613981247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545046806335449 0.05100893974304199

Final encoder loss: 0.028323298320174217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570362091064453 0.05321073532104492

Final encoder loss: 0.027600791305303574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26831555366516113 0.05246233940124512

Final encoder loss: 0.027071308344602585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2569143772125244 0.05144476890563965

Final encoder loss: 0.02727939747273922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2573740482330322 0.050843238830566406

Final encoder loss: 0.027956774458289146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25670838356018066 0.05330371856689453

Final encoder loss: 0.027657851576805115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540912628173828 0.05185294151306152

Final encoder loss: 0.026566974818706512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26001739501953125 0.05105447769165039

Final encoder loss: 0.02593516930937767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25733327865600586 0.05317258834838867

Final encoder loss: 0.02535972185432911
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25691747665405273 0.05285048484802246

Final encoder loss: 0.02554834634065628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25818896293640137 0.05214691162109375

Final encoder loss: 0.02643299289047718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576563358306885 0.052704572677612305

Final encoder loss: 0.026140468195080757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.254580020904541 0.05136275291442871

Final encoder loss: 0.025297487154603004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2564668655395508 0.052962541580200195

Final encoder loss: 0.024994133040308952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571694850921631 0.0540616512298584

Final encoder loss: 0.02446608804166317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25675487518310547 0.05187201499938965

Final encoder loss: 0.024629371240735054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25708818435668945 0.05172562599182129

Final encoder loss: 0.025540603324770927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2594490051269531 0.05384063720703125

Final encoder loss: 0.02502918802201748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25441837310791016 0.051595211029052734

Final encoder loss: 0.02454998530447483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25711655616760254 0.053351640701293945

Final encoder loss: 0.0243288092315197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25841426849365234 0.052094459533691406

Final encoder loss: 0.023977162316441536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25700855255126953 0.053064823150634766

Final encoder loss: 0.023964252322912216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25701355934143066 0.05404925346374512

Final encoder loss: 0.024923834949731827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2571699619293213 0.05315756797790527

Final encoder loss: 0.0244780033826828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2560100555419922 0.05144643783569336

Final encoder loss: 0.023978926241397858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25736308097839355 0.052823781967163086

Final encoder loss: 0.023766789585351944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2595865726470947 0.05294394493103027

Final encoder loss: 0.023422878235578537
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25696849822998047 0.05206561088562012

Final encoder loss: 0.023461194708943367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2614579200744629 0.053479671478271484

Final encoder loss: 0.024359185248613358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579925060272217 0.051758527755737305

Final encoder loss: 0.023816455155611038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2593052387237549 0.0516815185546875

Final encoder loss: 0.023508748039603233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25815629959106445 0.05248379707336426

Final encoder loss: 0.023342173546552658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25994348526000977 0.05266237258911133

Final encoder loss: 0.02304919995367527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26058316230773926 0.05176949501037598

Final encoder loss: 0.022899674251675606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26034998893737793 0.05209708213806152

Final encoder loss: 0.02385784313082695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25817441940307617 0.05199909210205078

Final encoder loss: 0.023458803072571754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25637102127075195 0.05512523651123047

Final encoder loss: 0.023078691214323044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25888609886169434 0.05367636680603027

Final encoder loss: 0.023059338331222534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25941967964172363 0.054450273513793945

Final encoder loss: 0.022609613835811615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2589132785797119 0.05298256874084473

Final encoder loss: 0.02254154346883297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2601180076599121 0.05472826957702637

Final encoder loss: 0.023526888340711594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25970959663391113 0.05225205421447754

Final encoder loss: 0.023054011166095734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2567744255065918 0.05357980728149414

Final encoder loss: 0.02286803163588047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25890016555786133 0.05383181571960449

Final encoder loss: 0.02275640144944191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25776147842407227 0.05419778823852539

Final encoder loss: 0.022449681535363197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2733325958251953 0.05492448806762695

Final encoder loss: 0.02239655703306198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25887417793273926 0.05453944206237793

Final encoder loss: 0.02336021512746811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2586088180541992 0.05245542526245117

Final encoder loss: 0.022784287109971046
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540872097015381 0.05464506149291992

Final encoder loss: 0.022663557901978493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2590208053588867 0.05226540565490723

Final encoder loss: 0.022547895088791847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25739264488220215 0.05587124824523926

Final encoder loss: 0.02213311567902565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25939464569091797 0.05499148368835449

Final encoder loss: 0.022092491388320923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581291198730469 0.053148746490478516

Final encoder loss: 0.02295924723148346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25940942764282227 0.0535883903503418

Final encoder loss: 0.02250530570745468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25400280952453613 0.05186033248901367

Final encoder loss: 0.022455383092164993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25850486755371094 0.051871538162231445

Final encoder loss: 0.022247683256864548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25837254524230957 0.053168535232543945

Final encoder loss: 0.021982714533805847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2596304416656494 0.05238914489746094

Final encoder loss: 0.02182341180741787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572329044342041 0.05367779731750488

Final encoder loss: 0.022775083780288696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26051878929138184 0.052167654037475586

Final encoder loss: 0.022421276196837425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25597238540649414 0.05227231979370117

Final encoder loss: 0.022201213985681534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2616405487060547 0.05304312705993652

Final encoder loss: 0.02219858020544052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588050365447998 0.05305671691894531

Final encoder loss: 0.02186809480190277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2604548931121826 0.05089116096496582

Final encoder loss: 0.02171000838279724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.258575439453125 0.05357193946838379

Final encoder loss: 0.022506823763251305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26152467727661133 0.0525054931640625

Final encoder loss: 0.02207265794277191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25396275520324707 0.05289125442504883

Final encoder loss: 0.02210817113518715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26071691513061523 0.05776071548461914

Final encoder loss: 0.02203734591603279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25801992416381836 0.0539402961730957

Final encoder loss: 0.021695777773857117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2604105472564697 0.05506324768066406

Final encoder loss: 0.0215518306940794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25702881813049316 0.05386662483215332

Final encoder loss: 0.022475633770227432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590787410736084 0.053868770599365234

Final encoder loss: 0.02202974073588848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561955451965332 0.05119132995605469

Final encoder loss: 0.02197965979576111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.259737491607666 0.054038286209106445

Final encoder loss: 0.02181268483400345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25815629959106445 0.05235719680786133

Final encoder loss: 0.02155393548309803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2586188316345215 0.05264115333557129

Final encoder loss: 0.021360618993639946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25787353515625 0.05220508575439453

Final encoder loss: 0.02222667634487152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2598137855529785 0.05366253852844238

Final encoder loss: 0.021770654246211052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549905776977539 0.051500797271728516

Final encoder loss: 0.02183087170124054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2603018283843994 0.054000139236450195

Final encoder loss: 0.021751033142209053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25835204124450684 0.05324673652648926

Final encoder loss: 0.021404936909675598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25718164443969727 0.05446314811706543

Final encoder loss: 0.02118101716041565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2597320079803467 0.054381370544433594

Final encoder loss: 0.02216615155339241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25768280029296875 0.054314613342285156

Final encoder loss: 0.02174670435488224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561466693878174 0.0520024299621582

Final encoder loss: 0.02171528898179531
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2597079277038574 0.05309581756591797

Final encoder loss: 0.021690040826797485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25994181632995605 0.0524141788482666

Final encoder loss: 0.021281970664858818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2585172653198242 0.05266618728637695

Final encoder loss: 0.02116505615413189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26038289070129395 0.052381277084350586

Final encoder loss: 0.021987732499837875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2563598155975342 0.05314993858337402

Final encoder loss: 0.021531198173761368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2587916851043701 0.051642417907714844

Final encoder loss: 0.02166673168540001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2585868835449219 0.05309939384460449

Final encoder loss: 0.0215587355196476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2729473114013672 0.05219268798828125

Final encoder loss: 0.02124025858938694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2597026824951172 0.05205988883972168

Final encoder loss: 0.020952727645635605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25999879837036133 0.05116558074951172

Final encoder loss: 0.021959148347377777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25893592834472656 0.05248737335205078

Final encoder loss: 0.021517187356948853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25750088691711426 0.05129098892211914

Final encoder loss: 0.021538659930229187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25784945487976074 0.05342578887939453

Final encoder loss: 0.021462775766849518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2616696357727051 0.056589365005493164

Final encoder loss: 0.021169403567910194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25785112380981445 0.05189824104309082

Final encoder loss: 0.02096562273800373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2610161304473877 0.05622673034667969

Final encoder loss: 0.02180439606308937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25742602348327637 0.05285334587097168

Final encoder loss: 0.021354954689741135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25563812255859375 0.054158926010131836

Final encoder loss: 0.021452290937304497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2612757682800293 0.052304983139038086

Final encoder loss: 0.021323245018720627
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2612614631652832 0.054700374603271484

Final encoder loss: 0.021028859540820122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26942968368530273 0.051787614822387695

Final encoder loss: 0.02081478387117386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26003026962280273 0.055022478103637695

Final encoder loss: 0.02173919789493084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591722011566162 0.052829742431640625

Final encoder loss: 0.021356498822569847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25659799575805664 0.053221702575683594

Final encoder loss: 0.021380150690674782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2593698501586914 0.05239081382751465

Final encoder loss: 0.021334486082196236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25866055488586426 0.055609941482543945

Final encoder loss: 0.020958062261343002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25868988037109375 0.05362391471862793

Final encoder loss: 0.020732030272483826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2591116428375244 0.05389738082885742

Final encoder loss: 0.021598542109131813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25888609886169434 0.05225181579589844

Final encoder loss: 0.021147454157471657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25462961196899414 0.05551791191101074

Final encoder loss: 0.021364884451031685
Final encoder loss: 0.020804746076464653
Final encoder loss: 0.019980603829026222
Final encoder loss: 0.01909116841852665
Final encoder loss: 0.019209424033761024
Final encoder loss: 0.018187683075666428

Training emognition model
Final encoder loss: 0.02514827024615863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08199167251586914 0.23075532913208008

Final encoder loss: 0.022638061372854054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08131742477416992 0.2312934398651123

Final encoder loss: 0.023756914623688508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08241987228393555 0.23088502883911133

Final encoder loss: 0.023144736642223416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08099627494812012 0.23030948638916016

Final encoder loss: 0.02609044620968331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.0820169448852539 0.2312154769897461

Final encoder loss: 0.023052046253004402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08075428009033203 0.23109698295593262

Final encoder loss: 0.023659565185779604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08115720748901367 0.23085451126098633

Final encoder loss: 0.02367058363615773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08279633522033691 0.23108530044555664

Final encoder loss: 0.022945326589962407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08128190040588379 0.2311091423034668

Final encoder loss: 0.024324576883572636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08116388320922852 0.23088455200195312

Final encoder loss: 0.02313984351252775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08226156234741211 0.2314608097076416

Final encoder loss: 0.023018087779372576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0812826156616211 0.23099088668823242

Final encoder loss: 0.023883754771399413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0810248851776123 0.23127532005310059

Final encoder loss: 0.02416411242686971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08266496658325195 0.23087573051452637

Final encoder loss: 0.024024117664186002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08116412162780762 0.23087406158447266

Final encoder loss: 0.02544484453381467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08018970489501953 0.23089313507080078


Training emognition model
Final encoder loss: 0.19355341792106628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25156331062316895 0.049059152603149414

Final encoder loss: 0.19495636224746704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25250720977783203 0.05067753791809082

Final encoder loss: 0.08903654664754868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.248091459274292 0.04948067665100098

Final encoder loss: 0.08855895698070526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24777007102966309 0.04999375343322754

Final encoder loss: 0.0563911609351635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24843311309814453 0.048338890075683594

Final encoder loss: 0.054635439068078995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24801921844482422 0.04997968673706055

Final encoder loss: 0.04174721986055374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25023365020751953 0.04965710639953613

Final encoder loss: 0.04027356952428818
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24828481674194336 0.051235198974609375

Final encoder loss: 0.034204233437776566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25080251693725586 0.04930686950683594

Final encoder loss: 0.03310912847518921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24805736541748047 0.049123525619506836

Final encoder loss: 0.029985232278704643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2503929138183594 0.048612117767333984

Final encoder loss: 0.029112745076417923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24720191955566406 0.04933786392211914

Final encoder loss: 0.027554210275411606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24972295761108398 0.05043935775756836

Final encoder loss: 0.02683383971452713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24825644493103027 0.049787044525146484

Final encoder loss: 0.026222076267004013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24940013885498047 0.04953813552856445

Final encoder loss: 0.025647388771176338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24835681915283203 0.04905986785888672

Final encoder loss: 0.02543329820036888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483203411102295 0.05040383338928223

Final encoder loss: 0.025139926001429558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24790143966674805 0.04969191551208496

Final encoder loss: 0.025009077042341232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2493126392364502 0.050065040588378906

Final encoder loss: 0.024896370247006416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2479109764099121 0.048990726470947266

Final encoder loss: 0.024627620354294777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24791717529296875 0.04853105545043945

Final encoder loss: 0.02449442259967327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.25038933753967285 0.04969382286071777

Final encoder loss: 0.024362605065107346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24777770042419434 0.04938530921936035

Final encoder loss: 0.02414761856198311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24863958358764648 0.051508426666259766

Final encoder loss: 0.024037910625338554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24991703033447266 0.04937291145324707

Final encoder loss: 0.02379552461206913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2479410171508789 0.05163121223449707

Final encoder loss: 0.023953527212142944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.247694730758667 0.049356937408447266

Final encoder loss: 0.023811038583517075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24760723114013672 0.0510859489440918

Final encoder loss: 0.02375076338648796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2478468418121338 0.0484766960144043

Final encoder loss: 0.02375958301126957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24646759033203125 0.051297664642333984

Final encoder loss: 0.023581551387906075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2487318515777588 0.049560546875

Final encoder loss: 0.023526230826973915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24777436256408691 0.04919290542602539

Final encoder loss: 0.023318856954574585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2504737377166748 0.04885125160217285

Final encoder loss: 0.023258350789546967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24774909019470215 0.0493316650390625

Final encoder loss: 0.023256316781044006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2491142749786377 0.04944634437561035

Final encoder loss: 0.0230910312384367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2462160587310791 0.04994702339172363

Final encoder loss: 0.023063577711582184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24955368041992188 0.05005216598510742

Final encoder loss: 0.022982299327850342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24711847305297852 0.04911470413208008

Final encoder loss: 0.0229745302349329
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25044775009155273 0.04947924613952637

Final encoder loss: 0.022946150973439217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2470860481262207 0.05002593994140625

Final encoder loss: 0.022817954421043396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24977731704711914 0.04928469657897949

Final encoder loss: 0.022875705733895302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24736905097961426 0.04986143112182617

Final encoder loss: 0.022826751694083214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482893466949463 0.04977297782897949

Final encoder loss: 0.02282087691128254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24881196022033691 0.04955244064331055

Final encoder loss: 0.02287505753338337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2511179447174072 0.05143284797668457

Final encoder loss: 0.022779304534196854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24588274955749512 0.05009961128234863

Final encoder loss: 0.022783799096941948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24911165237426758 0.04873514175415039

Final encoder loss: 0.022796839475631714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24512505531311035 0.05000662803649902

Final encoder loss: 0.02261427231132984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24612975120544434 0.049018144607543945

Final encoder loss: 0.02275168150663376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2450571060180664 0.04903364181518555

Final encoder loss: 0.02258993312716484
Final encoder loss: 0.021815497428178787

Training empatch model
Final encoder loss: 0.03213264541696786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07069826126098633 0.17270183563232422

Final encoder loss: 0.030509177195870874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07053732872009277 0.17296075820922852

Final encoder loss: 0.03215435631260489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07032012939453125 0.17235636711120605

Final encoder loss: 0.030692947748169885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.06994438171386719 0.1725754737854004

Final encoder loss: 0.029144949875762383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07007074356079102 0.1726512908935547

Final encoder loss: 0.027749683962566052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07051968574523926 0.1728684902191162

Final encoder loss: 0.030642596031598166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07022547721862793 0.17372894287109375

Final encoder loss: 0.029449569638366785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07076692581176758 0.17484068870544434

Final encoder loss: 0.02359061384791058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07135295867919922 0.17367100715637207

Final encoder loss: 0.021541727795374364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07068562507629395 0.17360997200012207

Final encoder loss: 0.02224105326406139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07075738906860352 0.17364239692687988

Final encoder loss: 0.022756692829672853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07076454162597656 0.17369937896728516

Final encoder loss: 0.022238472637922748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07093358039855957 0.17379283905029297

Final encoder loss: 0.023065355879866085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07128572463989258 0.17390012741088867

Final encoder loss: 0.026057037901879056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07079648971557617 0.17417097091674805

Final encoder loss: 0.02316162892784586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07016229629516602 0.17313194274902344


Training empatch model
Final encoder loss: 0.17115317285060883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17650222778320312 0.0436398983001709

Final encoder loss: 0.0811179056763649
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17589449882507324 0.04320049285888672

Final encoder loss: 0.055020224303007126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1757678985595703 0.04344367980957031

Final encoder loss: 0.04236394166946411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17570757865905762 0.04368185997009277

Final encoder loss: 0.03509949520230293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752932071685791 0.04366350173950195

Final encoder loss: 0.030552899464964867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1752028465270996 0.043958187103271484

Final encoder loss: 0.02755931206047535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17606472969055176 0.04375267028808594

Final encoder loss: 0.025529593229293823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17617559432983398 0.04282855987548828

Final encoder loss: 0.024109160527586937
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17517924308776855 0.04449629783630371

Final encoder loss: 0.023203592747449875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17499065399169922 0.044069528579711914

Final encoder loss: 0.022557387128472328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17612433433532715 0.0435786247253418

Final encoder loss: 0.022126687690615654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17532873153686523 0.0443112850189209

Final encoder loss: 0.021729890257120132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17559194564819336 0.04420733451843262

Final encoder loss: 0.02139859087765217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17550086975097656 0.04443621635437012

Final encoder loss: 0.0211613979190588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1768026351928711 0.043207406997680664

Final encoder loss: 0.020939383655786514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17566680908203125 0.04351663589477539

Final encoder loss: 0.020722102373838425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17665719985961914 0.044739723205566406

Final encoder loss: 0.020511576905846596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17519879341125488 0.043897390365600586

Final encoder loss: 0.02036276087164879
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1751093864440918 0.04337143898010254

Final encoder loss: 0.02030281163752079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17489862442016602 0.043422698974609375

Final encoder loss: 0.0202003363519907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17544007301330566 0.044283390045166016

Final encoder loss: 0.0201064832508564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17492389678955078 0.04383420944213867

Final encoder loss: 0.02008216269314289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17630386352539062 0.04357290267944336

Final encoder loss: 0.020110709592700005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17542529106140137 0.04366159439086914

Final encoder loss: 0.020149925723671913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17591166496276855 0.04332399368286133

Final encoder loss: 0.020065978169441223

Training wesad model
Final encoder loss: 0.03453866868623474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07096099853515625 0.17326998710632324

Final encoder loss: 0.03368821590825791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07159256935119629 0.17380762100219727

Final encoder loss: 0.03282133123940934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07149767875671387 0.17375397682189941

Final encoder loss: 0.02964945982994168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07085180282592773 0.17374515533447266

Final encoder loss: 0.020554151181741198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07109355926513672 0.17371559143066406

Final encoder loss: 0.021951214887537145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07089948654174805 0.17390203475952148

Final encoder loss: 0.02085978226963836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07122969627380371 0.1735236644744873

Final encoder loss: 0.02251929809694348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07168221473693848 0.17324066162109375

Final encoder loss: 0.017081187201555385
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07152938842773438 0.17368268966674805

Final encoder loss: 0.01569896118095838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0834665298461914 0.17385554313659668

Final encoder loss: 0.016164565098345282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07132816314697266 0.17405915260314941

Final encoder loss: 0.017012922077756794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07102108001708984 0.1737656593322754

Final encoder loss: 0.013341249106524927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07076048851013184 0.17391705513000488

Final encoder loss: 0.014239033711595225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0709376335144043 0.173720121383667

Final encoder loss: 0.013131738370368597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07157516479492188 0.173292875289917

Final encoder loss: 0.013795856184376025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07088494300842285 0.1739051342010498


Training wesad model
Final encoder loss: 0.21559420228004456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10544061660766602 0.033799171447753906

Final encoder loss: 0.09954775869846344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10405540466308594 0.033414602279663086

Final encoder loss: 0.06400498747825623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10338020324707031 0.0343780517578125

Final encoder loss: 0.04626205191016197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10457348823547363 0.033243417739868164

Final encoder loss: 0.035918235778808594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1038367748260498 0.03414607048034668

Final encoder loss: 0.029467610642313957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10445976257324219 0.03358650207519531

Final encoder loss: 0.025295155122876167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1044471263885498 0.03323984146118164

Final encoder loss: 0.022493312135338783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10341024398803711 0.03263139724731445

Final encoder loss: 0.02058575488626957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10369277000427246 0.0332942008972168

Final encoder loss: 0.019301222637295723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10340619087219238 0.03357362747192383

Final encoder loss: 0.018356559798121452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10361766815185547 0.03305220603942871

Final encoder loss: 0.01772904209792614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10323286056518555 0.03356313705444336

Final encoder loss: 0.017327189445495605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1037595272064209 0.032744646072387695

Final encoder loss: 0.017117662355303764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10378456115722656 0.03325390815734863

Final encoder loss: 0.017094416543841362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1033174991607666 0.03403878211975098

Final encoder loss: 0.017092347145080566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10422134399414062 0.03339195251464844

Final encoder loss: 0.017123466357588768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10417985916137695 0.03311753273010254

Final encoder loss: 0.01709553971886635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10325193405151367 0.032759666442871094

Final encoder loss: 0.01695917174220085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1035928726196289 0.033126115798950195

Final encoder loss: 0.016751784831285477
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10365509986877441 0.0325314998626709

Final encoder loss: 0.016686150804162025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.103485107421875 0.03331780433654785

Final encoder loss: 0.016656305640935898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10398507118225098 0.03340268135070801

Final encoder loss: 0.016681937500834465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1032876968383789 0.033289432525634766

Final encoder loss: 0.01657087728381157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10347843170166016 0.03301286697387695

Final encoder loss: 0.016518158838152885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10341525077819824 0.03328847885131836

Final encoder loss: 0.01660808175802231

Calculating loss for amigos model
	Full Pass 0.6726682186126709
numFreeParamsPath 18
Reconstruction loss values: 0.024158144369721413 0.033319100737571716

Calculating loss for dapper model
	Full Pass 0.15410780906677246
numFreeParamsPath 18
Reconstruction loss values: 0.020238779485225677 0.02323526330292225

Calculating loss for case model
	Full Pass 0.8621764183044434
numFreeParamsPath 18
Reconstruction loss values: 0.030145103111863136 0.033139340579509735

Calculating loss for emognition model
	Full Pass 0.2805006504058838
numFreeParamsPath 18
Reconstruction loss values: 0.030821265652775764 0.038922931998968124

Calculating loss for empatch model
	Full Pass 0.10524725914001465
numFreeParamsPath 18
Reconstruction loss values: 0.03173113986849785 0.03926112875342369

Calculating loss for wesad model
	Full Pass 0.07727813720703125
numFreeParamsPath 18
Reconstruction loss values: 0.032156266272068024 0.04958328977227211
Total loss calculation time: 4.1621153354644775

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.189997673034668
Total epoch time: 229.14468455314636

Epoch: 52

Training case model
Final encoder loss: 0.03030860719589003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.10006546974182129 0.27313828468322754

Final encoder loss: 0.02720317848369056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09169936180114746 0.2670552730560303

Final encoder loss: 0.0269010925515656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0914154052734375 0.2656087875366211

Final encoder loss: 0.025506604062265258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09264516830444336 0.2654547691345215

Final encoder loss: 0.026234568756185916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09174346923828125 0.2659902572631836

Final encoder loss: 0.02538443659469053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09157085418701172 0.26616764068603516

Final encoder loss: 0.02479883046015422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09252643585205078 0.26608753204345703

Final encoder loss: 0.025101998486244396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09096860885620117 0.2659111022949219

Final encoder loss: 0.02481680861533156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.0912775993347168 0.2673177719116211

Final encoder loss: 0.024556997187294483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09213972091674805 0.2667264938354492

Final encoder loss: 0.023967794474250518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09124422073364258 0.2660255432128906

Final encoder loss: 0.02357077708948577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09154486656188965 0.26712656021118164

Final encoder loss: 0.023228576225956258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09210395812988281 0.2663228511810303

Final encoder loss: 0.023713769952985016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09110522270202637 0.26539182662963867

Final encoder loss: 0.02373819842497214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09178662300109863 0.2652263641357422

Final encoder loss: 0.023203912634763173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08825516700744629 0.2630951404571533


Training emognition model
Final encoder loss: 0.03098381086126457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08368587493896484 0.2755453586578369

Final encoder loss: 0.029705016021361687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08459639549255371 0.2756643295288086

Final encoder loss: 0.03129043535303646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.0832517147064209 0.27550363540649414

Final encoder loss: 0.031304822892135556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08346939086914062 0.27599191665649414

Final encoder loss: 0.030435868090784152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.0834054946899414 0.27716851234436035

Final encoder loss: 0.031167687835053883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08367323875427246 0.2760038375854492

Final encoder loss: 0.030392459023882684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08513975143432617 0.27497315406799316

Final encoder loss: 0.029923936528400013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.0831151008605957 0.2754063606262207

Final encoder loss: 0.030065766809002685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08372902870178223 0.27532386779785156

Final encoder loss: 0.029670218498936248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08466720581054688 0.2766287326812744

Final encoder loss: 0.030432534159805395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08354496955871582 0.2765176296234131

Final encoder loss: 0.0304656282456153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08312606811523438 0.2748856544494629

Final encoder loss: 0.030107219555305004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08338737487792969 0.27667832374572754

Final encoder loss: 0.030089708219120048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08382630348205566 0.2757563591003418

Final encoder loss: 0.028736448440139242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08482742309570312 0.27608728408813477

Final encoder loss: 0.029446177434871025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08269405364990234 0.27460527420043945


Training amigos model
Final encoder loss: 0.026074842714288143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.1084897518157959 0.38952040672302246

Final encoder loss: 0.02407652641785288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10863947868347168 0.38925933837890625

Final encoder loss: 0.026030903078984733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10947060585021973 0.38907814025878906

Final encoder loss: 0.023589915778890774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10907840728759766 0.39071035385131836

Final encoder loss: 0.02412786793693153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10818886756896973 0.3913304805755615

Final encoder loss: 0.02557619305166514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1083991527557373 0.38896799087524414

Final encoder loss: 0.02398631334648763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10854864120483398 0.3897514343261719

Final encoder loss: 0.0249018156650334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10884976387023926 0.3897109031677246

Final encoder loss: 0.023052476247439346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10829734802246094 0.3901379108428955

Final encoder loss: 0.02501916319013621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10880327224731445 0.38926267623901367

Final encoder loss: 0.024662792277061325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10934591293334961 0.39084434509277344

Final encoder loss: 0.025971181659972736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.1092233657836914 0.39052605628967285

Final encoder loss: 0.024246595746636554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10854983329772949 0.3898015022277832

Final encoder loss: 0.024562229675970923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10851478576660156 0.38970351219177246

Final encoder loss: 0.023614806138948015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10858917236328125 0.3896455764770508

Final encoder loss: 0.026192504269935104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10330843925476074 0.3850898742675781


Training dapper model
Final encoder loss: 0.02165353838141459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06329727172851562 0.15024137496948242

Final encoder loss: 0.02094190398533746
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06254220008850098 0.15136146545410156

Final encoder loss: 0.018395919976684653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.062349557876586914 0.1527097225189209

Final encoder loss: 0.02076102208857987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06203031539916992 0.1514286994934082

Final encoder loss: 0.017518946598584408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06264114379882812 0.15172052383422852

Final encoder loss: 0.019575792509658328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06350278854370117 0.15167808532714844

Final encoder loss: 0.01768338568118599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.062489986419677734 0.15102100372314453

Final encoder loss: 0.02093329011345306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.0623018741607666 0.15163350105285645

Final encoder loss: 0.019261297316744492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06365299224853516 0.15150904655456543

Final encoder loss: 0.019203282375870337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.0623018741607666 0.15128684043884277

Final encoder loss: 0.0189922359797837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.062101125717163086 0.15218210220336914

Final encoder loss: 0.018372987681697248
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06251931190490723 0.15077781677246094

Final encoder loss: 0.018706580759396827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06240034103393555 0.15098881721496582

Final encoder loss: 0.018006897235983026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06293344497680664 0.1521298885345459

Final encoder loss: 0.018133045607613897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.061876535415649414 0.15047836303710938

Final encoder loss: 0.014956079008483654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.061740875244140625 0.15039944648742676


Training amigos model
Final encoder loss: 0.01897666476475079
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10683083534240723 0.341503381729126

Final encoder loss: 0.018931047475863414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1071927547454834 0.34192347526550293

Final encoder loss: 0.019362131777579082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10605263710021973 0.34160804748535156

Final encoder loss: 0.018429929972131377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10676097869873047 0.341977596282959

Final encoder loss: 0.018310810614078923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10608148574829102 0.3417365550994873

Final encoder loss: 0.019497500524722204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10718297958374023 0.34194183349609375

Final encoder loss: 0.018290341835179292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1059563159942627 0.34185361862182617

Final encoder loss: 0.01872061869035941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10654783248901367 0.3425312042236328

Final encoder loss: 0.01836813073087501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10615754127502441 0.341888427734375

Final encoder loss: 0.017353872759563082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10669517517089844 0.34149909019470215

Final encoder loss: 0.01778762935313541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10683059692382812 0.3426544666290283

Final encoder loss: 0.017248323544491198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10633111000061035 0.3415679931640625

Final encoder loss: 0.01780133749438663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1065664291381836 0.34183597564697266

Final encoder loss: 0.020949269499128374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10621142387390137 0.3419485092163086

Final encoder loss: 0.01891152978747001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10747170448303223 0.3420529365539551

Final encoder loss: 0.01920031033071151
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10076546669006348 0.3388216495513916


Training amigos model
Final encoder loss: 0.18079307675361633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4520680904388428 0.0783073902130127

Final encoder loss: 0.1878277212381363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44197821617126465 0.07558822631835938

Final encoder loss: 0.183624729514122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4230926036834717 0.07962703704833984

Final encoder loss: 0.0771595686674118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4594118595123291 0.07535004615783691

Final encoder loss: 0.07853563874959946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4728562831878662 0.07790827751159668

Final encoder loss: 0.07366205006837845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43357419967651367 0.07538866996765137

Final encoder loss: 0.04441611096262932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4554464817047119 0.08141875267028809

Final encoder loss: 0.04478832706809044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4728429317474365 0.07624220848083496

Final encoder loss: 0.042704127728939056
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42585206031799316 0.07818794250488281

Final encoder loss: 0.031663503497838974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45792531967163086 0.07726430892944336

Final encoder loss: 0.03193243220448494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47086167335510254 0.08090448379516602

Final encoder loss: 0.031074611470103264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4235374927520752 0.07485771179199219

Final encoder loss: 0.025736188516020775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4291706085205078 0.08095788955688477

Final encoder loss: 0.026019984856247902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45502424240112305 0.07855939865112305

Final encoder loss: 0.025741200894117355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42238736152648926 0.07600665092468262

Final encoder loss: 0.023011932149529457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4539051055908203 0.08286833763122559

Final encoder loss: 0.023262154310941696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4758429527282715 0.07903528213500977

Final encoder loss: 0.023133428767323494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44271230697631836 0.07589292526245117

Final encoder loss: 0.021809939295053482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45612382888793945 0.07657980918884277

Final encoder loss: 0.021931065246462822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46958470344543457 0.08135008811950684

Final encoder loss: 0.02214193344116211
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44318056106567383 0.07744169235229492

Final encoder loss: 0.02157372236251831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4415006637573242 0.0767369270324707

Final encoder loss: 0.021395135670900345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4560058116912842 0.07686829566955566

Final encoder loss: 0.021799884736537933
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42441606521606445 0.07997417449951172

Final encoder loss: 0.02117271162569523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4418923854827881 0.07473134994506836

Final encoder loss: 0.020742524415254593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4599323272705078 0.07633709907531738

Final encoder loss: 0.02122824639081955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42064976692199707 0.07560944557189941

Final encoder loss: 0.02030007727444172
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4578278064727783 0.0812993049621582

Final encoder loss: 0.02012093923985958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4122927188873291 0.08159017562866211

Final encoder loss: 0.020473862066864967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42482423782348633 0.0771331787109375

Final encoder loss: 0.019461819902062416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.41378164291381836 0.0781857967376709

Final encoder loss: 0.019566833972930908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4541325569152832 0.07722139358520508

Final encoder loss: 0.019592512398958206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42386960983276367 0.07816791534423828

Final encoder loss: 0.01932208426296711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4385645389556885 0.07824468612670898

Final encoder loss: 0.01908036507666111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.41498899459838867 0.0723731517791748

Final encoder loss: 0.01936197280883789
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4241011142730713 0.0743405818939209

Final encoder loss: 0.018920686095952988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4120488166809082 0.07603597640991211

Final encoder loss: 0.018645239993929863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45662379264831543 0.08338141441345215

Final encoder loss: 0.01923498511314392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4214959144592285 0.07802891731262207

Final encoder loss: 0.018879739567637444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4454491138458252 0.0798177719116211

Final encoder loss: 0.018420133739709854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4380314350128174 0.07685256004333496

Final encoder loss: 0.019103504717350006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42110276222229004 0.07552313804626465

Final encoder loss: 0.01844332553446293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45646190643310547 0.08130359649658203

Final encoder loss: 0.018081894144415855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4753305912017822 0.0757744312286377

Final encoder loss: 0.018702324479818344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43802309036254883 0.07712292671203613

Final encoder loss: 0.018282311037182808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4501802921295166 0.07731890678405762

Final encoder loss: 0.017865704372525215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47159814834594727 0.0817563533782959

Final encoder loss: 0.018458010628819466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43981504440307617 0.07300305366516113

Final encoder loss: 0.018271151930093765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45975422859191895 0.07733511924743652

Final encoder loss: 0.017647789791226387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4680802822113037 0.07655191421508789

Final encoder loss: 0.0182977095246315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4243621826171875 0.07957315444946289

Final encoder loss: 0.018141768872737885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45575475692749023 0.07465863227844238

Final encoder loss: 0.017574718222022057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47400641441345215 0.0776057243347168

Final encoder loss: 0.018202226608991623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4381272792816162 0.07741570472717285

Final encoder loss: 0.01790485344827175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4125044345855713 0.07747697830200195

Final encoder loss: 0.017427777871489525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45464515686035156 0.08155298233032227

Final encoder loss: 0.018025781959295273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4492299556732178 0.07697558403015137

Final encoder loss: 0.01769975572824478
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44279980659484863 0.07949471473693848

Final encoder loss: 0.0174567773938179
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.455333948135376 0.07723712921142578

Final encoder loss: 0.018012356013059616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42255306243896484 0.08028078079223633

Final encoder loss: 0.01778554916381836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4131596088409424 0.07593703269958496

Final encoder loss: 0.017277562990784645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44390296936035156 0.07754874229431152

Final encoder loss: 0.01786666177213192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43608522415161133 0.07738757133483887

Final encoder loss: 0.017707103863358498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4280736446380615 0.07744407653808594

Final encoder loss: 0.01720147207379341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4608626365661621 0.07658886909484863

Final encoder loss: 0.01779831573367119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4384572505950928 0.07924127578735352

Final encoder loss: 0.017467431724071503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4140317440032959 0.07808184623718262

Final encoder loss: 0.017079241573810577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4354560375213623 0.0755615234375

Final encoder loss: 0.017569968476891518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.41733384132385254 0.07539176940917969

Final encoder loss: 0.01724296808242798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4473378658294678 0.0732259750366211

Final encoder loss: 0.016897140070796013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4459810256958008 0.07701325416564941

Final encoder loss: 0.017660098150372505
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4192507266998291 0.0720977783203125

Final encoder loss: 0.017333678901195526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.42128586769104004 0.07775640487670898

Final encoder loss: 0.01669391058385372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45442986488342285 0.07843971252441406

Final encoder loss: 0.017545945942401886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43712687492370605 0.07872200012207031

Final encoder loss: 0.017352044582366943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45810580253601074 0.07841944694519043

Final encoder loss: 0.01693573221564293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43961310386657715 0.07806205749511719

Final encoder loss: 0.017560835927724838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.43916940689086914 0.08052897453308105

Final encoder loss: 0.01739676669239998
Final encoder loss: 0.01601599156856537
Final encoder loss: 0.01590339094400406

Training dapper model
Final encoder loss: 0.015626296377408935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06027650833129883 0.10761857032775879

Final encoder loss: 0.014514740807873345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06011366844177246 0.10756993293762207

Final encoder loss: 0.015721592935475297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06100583076477051 0.10781335830688477

Final encoder loss: 0.014708056550660854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.06026411056518555 0.10789990425109863

Final encoder loss: 0.014352034573136354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.060158491134643555 0.1076512336730957

Final encoder loss: 0.0161125530085103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.06006884574890137 0.10833883285522461

Final encoder loss: 0.015694498943594423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0599818229675293 0.1073293685913086

Final encoder loss: 0.013229129539159976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.059809207916259766 0.10730195045471191

Final encoder loss: 0.013838711805246946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.06025099754333496 0.10846114158630371

Final encoder loss: 0.01475474831133352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06057143211364746 0.1076817512512207

Final encoder loss: 0.01632250050236791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.06010937690734863 0.10731649398803711

Final encoder loss: 0.012614309829739401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05995059013366699 0.10826921463012695

Final encoder loss: 0.013261418653434491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.061003684997558594 0.10717153549194336

Final encoder loss: 0.012793173144954994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06024646759033203 0.1075890064239502

Final encoder loss: 0.013661292753322464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05996441841125488 0.10788583755493164

Final encoder loss: 0.013144307322157128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.060126543045043945 0.10759615898132324


Training dapper model
Final encoder loss: 0.20242522656917572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11472845077514648 0.0344538688659668

Final encoder loss: 0.2081844061613083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11571717262268066 0.03465127944946289

Final encoder loss: 0.08576982468366623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11529064178466797 0.03504538536071777

Final encoder loss: 0.08794678747653961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11644411087036133 0.03465986251831055

Final encoder loss: 0.05060700699687004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.114227294921875 0.034044742584228516

Final encoder loss: 0.0504646971821785
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11839795112609863 0.03360915184020996

Final encoder loss: 0.03420942276716232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11279082298278809 0.034604787826538086

Final encoder loss: 0.034106213599443436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11576581001281738 0.03424239158630371

Final encoder loss: 0.025621118023991585
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11441421508789062 0.0353548526763916

Final encoder loss: 0.025740740820765495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11573004722595215 0.03469133377075195

Final encoder loss: 0.02083118073642254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11352300643920898 0.034368038177490234

Final encoder loss: 0.021050631999969482
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11753082275390625 0.03533792495727539

Final encoder loss: 0.01803329586982727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11315464973449707 0.0344700813293457

Final encoder loss: 0.018249668180942535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11614131927490234 0.033853769302368164

Final encoder loss: 0.01647479273378849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11368083953857422 0.034937381744384766

Final encoder loss: 0.016554556787014008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11682248115539551 0.034296274185180664

Final encoder loss: 0.015533342026174068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11300921440124512 0.034119606018066406

Final encoder loss: 0.015639962628483772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11678099632263184 0.03512287139892578

Final encoder loss: 0.01476491428911686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11315703392028809 0.03349781036376953

Final encoder loss: 0.014954881742596626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11669230461120605 0.0348200798034668

Final encoder loss: 0.014275595545768738
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1135866641998291 0.03548431396484375

Final encoder loss: 0.01464945636689663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11691808700561523 0.03440546989440918

Final encoder loss: 0.014047541655600071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11324930191040039 0.03489565849304199

Final encoder loss: 0.01433232519775629
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11583280563354492 0.03487396240234375

Final encoder loss: 0.014381518587470055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11406421661376953 0.03397250175476074

Final encoder loss: 0.014268939383327961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11668133735656738 0.034494876861572266

Final encoder loss: 0.014185679145157337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1119842529296875 0.03475761413574219

Final encoder loss: 0.014038982801139355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11844134330749512 0.03413653373718262

Final encoder loss: 0.013747000135481358
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11288166046142578 0.03517889976501465

Final encoder loss: 0.013697713613510132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11554884910583496 0.03442502021789551

Final encoder loss: 0.013324286788702011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11452794075012207 0.03446006774902344

Final encoder loss: 0.013418103568255901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11540389060974121 0.034623146057128906

Final encoder loss: 0.012975428253412247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11480307579040527 0.034999847412109375

Final encoder loss: 0.013027560897171497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11699461936950684 0.035063743591308594

Final encoder loss: 0.01265611033886671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11272811889648438 0.03446221351623535

Final encoder loss: 0.01274632103741169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11550760269165039 0.0344541072845459

Final encoder loss: 0.012688005343079567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11308979988098145 0.03475236892700195

Final encoder loss: 0.012743217870593071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11553430557250977 0.034297943115234375

Final encoder loss: 0.012582841329276562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11299967765808105 0.034459829330444336

Final encoder loss: 0.01262297946959734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11623644828796387 0.03542494773864746

Final encoder loss: 0.01266548503190279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11280369758605957 0.034630775451660156

Final encoder loss: 0.012584032490849495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1158912181854248 0.033888816833496094

Final encoder loss: 0.01250388938933611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11182332038879395 0.03476905822753906

Final encoder loss: 0.012417764402925968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11708950996398926 0.03451824188232422

Final encoder loss: 0.012236898764967918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11322355270385742 0.03410959243774414

Final encoder loss: 0.01239133533090353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11542272567749023 0.03530144691467285

Final encoder loss: 0.011951331980526447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11424827575683594 0.03420615196228027

Final encoder loss: 0.012192274443805218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11676883697509766 0.03426480293273926

Final encoder loss: 0.011969701386988163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11225080490112305 0.03481650352478027

Final encoder loss: 0.012063170783221722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1178884506225586 0.03419613838195801

Final encoder loss: 0.01209604274481535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11296558380126953 0.03445720672607422

Final encoder loss: 0.01191534660756588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11552858352661133 0.03401517868041992

Final encoder loss: 0.011978008784353733
Final encoder loss: 0.011166243813931942

Training case model
Final encoder loss: 0.022578530893453564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08916521072387695 0.21924567222595215

Final encoder loss: 0.02205105864917303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09086084365844727 0.21915864944458008

Final encoder loss: 0.022009105514219557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08915328979492188 0.21955561637878418

Final encoder loss: 0.021477113747729697
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08968544006347656 0.21899914741516113

Final encoder loss: 0.021885410996077114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.0898745059967041 0.2189030647277832

Final encoder loss: 0.02118145625520182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0895395278930664 0.21900510787963867

Final encoder loss: 0.021383673954218533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08998274803161621 0.21932458877563477

Final encoder loss: 0.02175806966018655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08909058570861816 0.21906495094299316

Final encoder loss: 0.02119325747680019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.09040641784667969 0.2190091609954834

Final encoder loss: 0.02121265266968175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08922576904296875 0.21968412399291992

Final encoder loss: 0.021615559609063268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08937501907348633 0.21904253959655762

Final encoder loss: 0.021517054531229412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08977746963500977 0.21882128715515137

Final encoder loss: 0.02134120043455463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08915948867797852 0.21909570693969727

Final encoder loss: 0.021157336047593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.09047317504882812 0.21917414665222168

Final encoder loss: 0.0206991304002091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08916854858398438 0.21935653686523438

Final encoder loss: 0.021939613030544872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08785820007324219 0.2158050537109375


Training case model
Final encoder loss: 0.2029791623353958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2652013301849365 0.052011728286743164

Final encoder loss: 0.18890853226184845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2593653202056885 0.05349373817443848

Final encoder loss: 0.19015038013458252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2588160037994385 0.05278348922729492

Final encoder loss: 0.19217973947525024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2588667869567871 0.053275346755981445

Final encoder loss: 0.18081198632717133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.259751558303833 0.0533900260925293

Final encoder loss: 0.19191111624240875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25759196281433105 0.05211186408996582

Final encoder loss: 0.1061997339129448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25904011726379395 0.05322575569152832

Final encoder loss: 0.09582401812076569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25943708419799805 0.05390286445617676

Final encoder loss: 0.09177057445049286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25876474380493164 0.05321907997131348

Final encoder loss: 0.09057287126779556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26014161109924316 0.052720069885253906

Final encoder loss: 0.08184977620840073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26047515869140625 0.053452491760253906

Final encoder loss: 0.08572188764810562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25856518745422363 0.0516512393951416

Final encoder loss: 0.062023162841796875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581779956817627 0.051242828369140625

Final encoder loss: 0.05611789599061012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25690126419067383 0.052484989166259766

Final encoder loss: 0.05395947024226189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563331127166748 0.05194735527038574

Final encoder loss: 0.054020676761865616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25659775733947754 0.05215811729431152

Final encoder loss: 0.050158221274614334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2568542957305908 0.05207252502441406

Final encoder loss: 0.05244201421737671
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542293071746826 0.05220818519592285

Final encoder loss: 0.043194450438022614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25633692741394043 0.05145430564880371

Final encoder loss: 0.03971794992685318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25791335105895996 0.053551435470581055

Final encoder loss: 0.03838857263326645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25870656967163086 0.05158686637878418

Final encoder loss: 0.038877151906490326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25893592834472656 0.05249905586242676

Final encoder loss: 0.0373116172850132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25847744941711426 0.05335402488708496

Final encoder loss: 0.038325317203998566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25771427154541016 0.051980018615722656

Final encoder loss: 0.034445635974407196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25913143157958984 0.05219101905822754

Final encoder loss: 0.03270479291677475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2585175037384033 0.0529332160949707

Final encoder loss: 0.03173644840717316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25939178466796875 0.05297970771789551

Final encoder loss: 0.0323566272854805
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26767826080322266 0.05413413047790527

Final encoder loss: 0.03237545117735863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25804615020751953 0.055071115493774414

Final encoder loss: 0.0322178453207016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25493884086608887 0.05284857749938965

Final encoder loss: 0.030582036823034286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25817346572875977 0.05248403549194336

Final encoder loss: 0.02951519750058651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2592768669128418 0.053464412689208984

Final encoder loss: 0.028948796913027763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.258807897567749 0.052137136459350586

Final encoder loss: 0.029386844485998154
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26025938987731934 0.05433177947998047

Final encoder loss: 0.029924754053354263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25885677337646484 0.0513761043548584

Final encoder loss: 0.02961835451424122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25546979904174805 0.05056595802307129

Final encoder loss: 0.028260204941034317
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25920653343200684 0.052210092544555664

Final encoder loss: 0.02745863050222397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2582414150238037 0.05195784568786621

Final encoder loss: 0.026883043348789215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25826215744018555 0.050858497619628906

Final encoder loss: 0.027051275596022606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25820493698120117 0.05397915840148926

Final encoder loss: 0.027744513005018234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25899767875671387 0.052980661392211914

Final encoder loss: 0.027408132329583168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25578856468200684 0.052748680114746094

Final encoder loss: 0.026290664449334145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25884103775024414 0.052791595458984375

Final encoder loss: 0.02575894631445408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25643372535705566 0.05243277549743652

Final encoder loss: 0.025332773104310036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2558321952819824 0.052420616149902344

Final encoder loss: 0.025389226153492928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571370601654053 0.053143978118896484

Final encoder loss: 0.026353826746344566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25673484802246094 0.05153012275695801

Final encoder loss: 0.025752296671271324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548394203186035 0.051645755767822266

Final encoder loss: 0.02503330074250698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25673794746398926 0.05183267593383789

Final encoder loss: 0.02468118630349636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25655150413513184 0.05259346961975098

Final encoder loss: 0.02439274825155735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25676703453063965 0.052359819412231445

Final encoder loss: 0.0245099775493145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2554142475128174 0.05293750762939453

Final encoder loss: 0.02538708597421646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2596473693847656 0.0534968376159668

Final encoder loss: 0.024880558252334595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555663585662842 0.0515744686126709

Final encoder loss: 0.024355173110961914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25818896293640137 0.052484989166259766

Final encoder loss: 0.024109994992613792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2676734924316406 0.053812265396118164

Final encoder loss: 0.02381300926208496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2675137519836426 0.051897287368774414

Final encoder loss: 0.02374497428536415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2577197551727295 0.05174064636230469

Final encoder loss: 0.024991964921355247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2590935230255127 0.052995920181274414

Final encoder loss: 0.02414354495704174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25536108016967773 0.052671194076538086

Final encoder loss: 0.023803433403372765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2576751708984375 0.05211520195007324

Final encoder loss: 0.02365148812532425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588357925415039 0.053243160247802734

Final encoder loss: 0.023251470178365707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575650215148926 0.05163836479187012

Final encoder loss: 0.02318800427019596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2578268051147461 0.053690195083618164

Final encoder loss: 0.024280358105897903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25795555114746094 0.05199432373046875

Final encoder loss: 0.023602552711963654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547729015350342 0.05201911926269531

Final encoder loss: 0.023231448605656624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25740647315979004 0.05149102210998535

Final encoder loss: 0.02315211482346058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258624792098999 0.054514169692993164

Final encoder loss: 0.022969605401158333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25652003288269043 0.05158376693725586

Final encoder loss: 0.022748153656721115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25736236572265625 0.05268239974975586

Final encoder loss: 0.02394450642168522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2613344192504883 0.05368304252624512

Final encoder loss: 0.023050500079989433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25465893745422363 0.052243947982788086

Final encoder loss: 0.02297103777527809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581601142883301 0.0532994270324707

Final encoder loss: 0.02280895970761776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25719428062438965 0.054071664810180664

Final encoder loss: 0.02258189767599106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26624608039855957 0.05218100547790527

Final encoder loss: 0.02241930365562439
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25837039947509766 0.05543637275695801

Final encoder loss: 0.023459814488887787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2596776485443115 0.05156111717224121

Final encoder loss: 0.02283826656639576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25337696075439453 0.053666114807128906

Final encoder loss: 0.02261229045689106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582967281341553 0.05243659019470215

Final encoder loss: 0.022526640444993973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25961852073669434 0.0525057315826416

Final encoder loss: 0.02233787253499031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570173740386963 0.05253887176513672

Final encoder loss: 0.022063415497541428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25783872604370117 0.05367136001586914

Final encoder loss: 0.023180151358246803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25867271423339844 0.05161738395690918

Final encoder loss: 0.022512784227728844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547721862792969 0.05217742919921875

Final encoder loss: 0.022410711273550987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25650954246520996 0.052506446838378906

Final encoder loss: 0.022334448993206024
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572810649871826 0.05133962631225586

Final encoder loss: 0.022126683965325356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25686216354370117 0.05321240425109863

Final encoder loss: 0.021871786564588547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25725507736206055 0.05165362358093262

Final encoder loss: 0.022825412452220917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2604818344116211 0.05313467979431152

Final encoder loss: 0.022279271855950356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255587100982666 0.05097651481628418

Final encoder loss: 0.022154372185468674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25681614875793457 0.053304195404052734

Final encoder loss: 0.022127963602542877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2586326599121094 0.05320620536804199

Final encoder loss: 0.021904420107603073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25712156295776367 0.0528407096862793

Final encoder loss: 0.021643420681357384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2666049003601074 0.0536198616027832

Final encoder loss: 0.022792749106884003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2573857307434082 0.05224728584289551

Final encoder loss: 0.02206801064312458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25384998321533203 0.05155372619628906

Final encoder loss: 0.022034676745533943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573230266571045 0.05406928062438965

Final encoder loss: 0.021956775337457657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2569241523742676 0.05200695991516113

Final encoder loss: 0.021762549877166748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25675320625305176 0.052323341369628906

Final encoder loss: 0.021471498534083366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25693488121032715 0.05276322364807129

Final encoder loss: 0.022569628432393074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.265531063079834 0.05186867713928223

Final encoder loss: 0.02184201218187809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25453686714172363 0.05187344551086426

Final encoder loss: 0.021843411028385162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25647878646850586 0.05227780342102051

Final encoder loss: 0.021772010251879692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571725845336914 0.05255913734436035

Final encoder loss: 0.021629920229315758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2574446201324463 0.05263257026672363

Final encoder loss: 0.02133086323738098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25699472427368164 0.0519561767578125

Final encoder loss: 0.022325804457068443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256274938583374 0.053254127502441406

Final encoder loss: 0.021737374365329742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25443458557128906 0.05043625831604004

Final encoder loss: 0.021784545853734016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26005125045776367 0.05511784553527832

Final encoder loss: 0.021717969328165054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2581486701965332 0.05272412300109863

Final encoder loss: 0.021464582532644272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25641775131225586 0.051445722579956055

Final encoder loss: 0.021103400737047195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2568356990814209 0.052461862564086914

Final encoder loss: 0.02211645059287548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25661277770996094 0.051801443099975586

Final encoder loss: 0.02158435247838497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539820671081543 0.05130791664123535

Final encoder loss: 0.02158982865512371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25685620307922363 0.05218029022216797

Final encoder loss: 0.021517185494303703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25792717933654785 0.05323219299316406

Final encoder loss: 0.021335851401090622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25768351554870605 0.0533750057220459

Final encoder loss: 0.021049268543720245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2596464157104492 0.05275917053222656

Final encoder loss: 0.02222283184528351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.256666898727417 0.051987409591674805

Final encoder loss: 0.021451788023114204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25421810150146484 0.051932334899902344

Final encoder loss: 0.0215466246008873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25722813606262207 0.05257368087768555

Final encoder loss: 0.021467896178364754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2564213275909424 0.051853179931640625

Final encoder loss: 0.021220490336418152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582693099975586 0.05238151550292969

Final encoder loss: 0.020891018211841583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25778722763061523 0.052459716796875

Final encoder loss: 0.021966267377138138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2580704689025879 0.05455780029296875

Final encoder loss: 0.021375572308897972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25451183319091797 0.05167698860168457

Final encoder loss: 0.02136426791548729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.255786657333374 0.05218839645385742

Final encoder loss: 0.021291278302669525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25698089599609375 0.052439212799072266

Final encoder loss: 0.02116446942090988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563164234161377 0.05215930938720703

Final encoder loss: 0.02082832157611847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2578709125518799 0.053289175033569336

Final encoder loss: 0.021800901740789413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2569742202758789 0.05227947235107422

Final encoder loss: 0.02118024416267872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2543501853942871 0.05233502388000488

Final encoder loss: 0.021377338096499443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25626230239868164 0.051305294036865234

Final encoder loss: 0.0213168878108263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2574622631072998 0.05173516273498535

Final encoder loss: 0.021082410588860512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26639795303344727 0.051946401596069336

Final encoder loss: 0.02073953114449978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25673365592956543 0.05210113525390625

Final encoder loss: 0.021716011688113213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25732874870300293 0.05187249183654785

Final encoder loss: 0.021150829270482063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548985481262207 0.05132412910461426

Final encoder loss: 0.021208040416240692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25733089447021484 0.05267786979675293

Final encoder loss: 0.021089861169457436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2562141418457031 0.054070234298706055

Final encoder loss: 0.020967824384570122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25731396675109863 0.051708221435546875

Final encoder loss: 0.02067694440484047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.256561279296875 0.05240011215209961

Final encoder loss: 0.02165285125374794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25690579414367676 0.05350375175476074

Final encoder loss: 0.02102142944931984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.254197359085083 0.0518949031829834

Final encoder loss: 0.021190309897065163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2563133239746094 0.05137300491333008

Final encoder loss: 0.02113533392548561
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572627067565918 0.05304908752441406

Final encoder loss: 0.020863333716988564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25697946548461914 0.05278182029724121

Final encoder loss: 0.020530223846435547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25638294219970703 0.052468061447143555

Final encoder loss: 0.021549446508288383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26649045944213867 0.052484750747680664

Final encoder loss: 0.021011412143707275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537658214569092 0.052225589752197266

Final encoder loss: 0.021115336567163467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25660109519958496 0.05161619186401367

Final encoder loss: 0.020972885191440582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25627923011779785 0.05379080772399902

Final encoder loss: 0.020858734846115112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25722455978393555 0.051598310470581055

Final encoder loss: 0.020505301654338837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25579166412353516 0.05366230010986328

Final encoder loss: 0.02151176705956459
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25637030601501465 0.05209684371948242

Final encoder loss: 0.020865172147750854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2530810832977295 0.05131244659423828

Final encoder loss: 0.021069025620818138
Final encoder loss: 0.020587384700775146
Final encoder loss: 0.019840465858578682
Final encoder loss: 0.01888790726661682
Final encoder loss: 0.01908104307949543
Final encoder loss: 0.017895689234137535

Training emognition model
Final encoder loss: 0.02511748906062503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08130264282226562 0.2308976650238037

Final encoder loss: 0.026067933826507433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08075428009033203 0.2304997444152832

Final encoder loss: 0.023617090174443935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08272647857666016 0.23077630996704102

Final encoder loss: 0.024094310463316605
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0810847282409668 0.2313072681427002

Final encoder loss: 0.023852400967862366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08363890647888184 0.23177480697631836

Final encoder loss: 0.023812150024418503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.0815122127532959 0.230576753616333

Final encoder loss: 0.02330364044361592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08108687400817871 0.23142409324645996

Final encoder loss: 0.02437586217705178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08239507675170898 0.23069167137145996

Final encoder loss: 0.02434856164414955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08138751983642578 0.2312469482421875

Final encoder loss: 0.02386710199495728
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.0817105770111084 0.23096132278442383

Final encoder loss: 0.024115199292550977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08085966110229492 0.23061060905456543

Final encoder loss: 0.024839503322276538
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08112335205078125 0.23113107681274414

Final encoder loss: 0.02393138694263923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08224058151245117 0.23233866691589355

Final encoder loss: 0.024615271337306094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08166360855102539 0.23086953163146973

Final encoder loss: 0.02331379274335687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08075737953186035 0.23074913024902344

Final encoder loss: 0.024826527572362193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08148407936096191 0.23067784309387207


Training emognition model
Final encoder loss: 0.19356150925159454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2535972595214844 0.049457550048828125

Final encoder loss: 0.19497424364089966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2502753734588623 0.049072265625

Final encoder loss: 0.08909286558628082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24879765510559082 0.048935651779174805

Final encoder loss: 0.08855117857456207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2493133544921875 0.0515897274017334

Final encoder loss: 0.0563705749809742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24996328353881836 0.04934954643249512

Final encoder loss: 0.054742004722356796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2489922046661377 0.05075573921203613

Final encoder loss: 0.04172265902161598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24876070022583008 0.04937148094177246

Final encoder loss: 0.0404461994767189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24807047843933105 0.049758195877075195

Final encoder loss: 0.034150488674640656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24853968620300293 0.04946565628051758

Final encoder loss: 0.033273398876190186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24720549583435059 0.050075531005859375

Final encoder loss: 0.029827551916241646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24895095825195312 0.04904627799987793

Final encoder loss: 0.029231911525130272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468717098236084 0.049558401107788086

Final encoder loss: 0.027338672429323196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25122785568237305 0.05022263526916504

Final encoder loss: 0.026906685903668404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24894952774047852 0.04972028732299805

Final encoder loss: 0.026039212942123413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2506093978881836 0.048766374588012695

Final encoder loss: 0.025759601965546608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2476975917816162 0.05064058303833008

Final encoder loss: 0.02533312328159809
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2502443790435791 0.05069088935852051

Final encoder loss: 0.025251690298318863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2478954792022705 0.05011129379272461

Final encoder loss: 0.024861063808202744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24929594993591309 0.04930901527404785

Final encoder loss: 0.025064656510949135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471320629119873 0.05025148391723633

Final encoder loss: 0.024427834898233414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25063204765319824 0.04996991157531738

Final encoder loss: 0.024629756808280945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24796819686889648 0.04935932159423828

Final encoder loss: 0.02419079653918743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24935221672058105 0.04874253273010254

Final encoder loss: 0.024253521114587784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24759197235107422 0.04828977584838867

Final encoder loss: 0.023961562663316727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2505810260772705 0.0489809513092041

Final encoder loss: 0.024021049961447716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24983954429626465 0.04844069480895996

Final encoder loss: 0.02375216968357563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2509725093841553 0.04930305480957031

Final encoder loss: 0.023834146559238434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24872636795043945 0.05214548110961914

Final encoder loss: 0.02354031801223755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2509589195251465 0.04997563362121582

Final encoder loss: 0.02368927374482155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24747991561889648 0.050687313079833984

Final encoder loss: 0.023440707474946976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2498769760131836 0.04939007759094238

Final encoder loss: 0.023507069796323776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24828648567199707 0.0511479377746582

Final encoder loss: 0.02311098761856556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24895548820495605 0.049794912338256836

Final encoder loss: 0.02339714579284191
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24793291091918945 0.050560951232910156

Final encoder loss: 0.022975033149123192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2496802806854248 0.0500330924987793

Final encoder loss: 0.023199351504445076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24724340438842773 0.04870915412902832

Final encoder loss: 0.02283215895295143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2514204978942871 0.048177480697631836

Final encoder loss: 0.023054970428347588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2487025260925293 0.04991579055786133

Final encoder loss: 0.022758016362786293
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25176310539245605 0.049192190170288086

Final encoder loss: 0.02286410704255104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2476191520690918 0.05016922950744629

Final encoder loss: 0.02280755154788494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2509021759033203 0.050421714782714844

Final encoder loss: 0.022826703265309334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471938133239746 0.04934811592102051

Final encoder loss: 0.022667625918984413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25049519538879395 0.05047297477722168

Final encoder loss: 0.022827237844467163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24742698669433594 0.05097627639770508

Final encoder loss: 0.022478012368083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25050783157348633 0.05094575881958008

Final encoder loss: 0.022738397121429443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24707484245300293 0.04982137680053711

Final encoder loss: 0.02258148044347763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2503819465637207 0.049921274185180664

Final encoder loss: 0.022783547639846802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24783658981323242 0.04998040199279785

Final encoder loss: 0.022487403824925423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24915075302124023 0.05017232894897461

Final encoder loss: 0.02258099801838398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2506527900695801 0.049463510513305664

Final encoder loss: 0.02250910922884941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.26183390617370605 0.049926042556762695

Final encoder loss: 0.022623755037784576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2639448642730713 0.04889512062072754

Final encoder loss: 0.022433552891016006
Final encoder loss: 0.021662637591362

Training empatch model
Final encoder loss: 0.03295598873549221
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07230257987976074 0.1744375228881836

Final encoder loss: 0.03272303089002468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07189321517944336 0.17403435707092285

Final encoder loss: 0.03257692816791708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0713803768157959 0.17364239692687988

Final encoder loss: 0.03236619543694017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07229948043823242 0.17489910125732422

Final encoder loss: 0.032357006516269174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0714418888092041 0.17380237579345703

Final encoder loss: 0.02892500495629834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.071807861328125 0.17400217056274414

Final encoder loss: 0.02923038684184237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.08931803703308105 0.17403435707092285

Final encoder loss: 0.02666167216568051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07244133949279785 0.17473721504211426

Final encoder loss: 0.023222385273213998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07138609886169434 0.1742877960205078

Final encoder loss: 0.024970183111177192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07102704048156738 0.17397284507751465

Final encoder loss: 0.021431683288151423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0722815990447998 0.17452120780944824

Final encoder loss: 0.021441278794023622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07132506370544434 0.17397689819335938

Final encoder loss: 0.023387389078006998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07190227508544922 0.1740865707397461

Final encoder loss: 0.021317682282550054
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07116818428039551 0.1742405891418457

Final encoder loss: 0.022533723141625467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07248187065124512 0.17455625534057617

Final encoder loss: 0.02351492250927681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07147097587585449 0.17360591888427734


Training empatch model
Final encoder loss: 0.17115212976932526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1774756908416748 0.044902801513671875

Final encoder loss: 0.08139662444591522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17778801918029785 0.04393410682678223

Final encoder loss: 0.05521097406744957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1749410629272461 0.04441213607788086

Final encoder loss: 0.04247114807367325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17528629302978516 0.044267892837524414

Final encoder loss: 0.03510194271802902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17752432823181152 0.04357576370239258

Final encoder loss: 0.030519219115376472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17505455017089844 0.04464316368103027

Final encoder loss: 0.027498146519064903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17622661590576172 0.04383277893066406

Final encoder loss: 0.02542768232524395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17725658416748047 0.04471898078918457

Final encoder loss: 0.023983728140592575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17493653297424316 0.04472088813781738

Final encoder loss: 0.022976534441113472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1743783950805664 0.04499650001525879

Final encoder loss: 0.02234136499464512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17725634574890137 0.04594087600708008

Final encoder loss: 0.021955756470561028
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1735982894897461 0.04419898986816406

Final encoder loss: 0.02166399173438549
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17577195167541504 0.04429912567138672

Final encoder loss: 0.02132144756615162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17677998542785645 0.045331716537475586

Final encoder loss: 0.021038109436631203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1750171184539795 0.04388833045959473

Final encoder loss: 0.02071148157119751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1756877899169922 0.04384469985961914

Final encoder loss: 0.020555494353175163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1769404411315918 0.044832468032836914

Final encoder loss: 0.020352324470877647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1749436855316162 0.04409909248352051

Final encoder loss: 0.020397434011101723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.175706148147583 0.042992353439331055

Final encoder loss: 0.020284602418541908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17691469192504883 0.044420480728149414

Final encoder loss: 0.020200859755277634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1761341094970703 0.04429817199707031

Final encoder loss: 0.019989782944321632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17549467086791992 0.04485201835632324

Final encoder loss: 0.019965339452028275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18624091148376465 0.04480314254760742

Final encoder loss: 0.0198662132024765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18551278114318848 0.04405856132507324

Final encoder loss: 0.019828151911497116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18532013893127441 0.04435324668884277

Final encoder loss: 0.01979544758796692
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1859126091003418 0.045548439025878906

Final encoder loss: 0.019795555621385574

Training wesad model
Final encoder loss: 0.033885719168875096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0716853141784668 0.17432355880737305

Final encoder loss: 0.033738274323067495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07095789909362793 0.17387652397155762

Final encoder loss: 0.03330615428118664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0722203254699707 0.17449116706848145

Final encoder loss: 0.029909470154125286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07072615623474121 0.17384886741638184

Final encoder loss: 0.02118504197860769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07088470458984375 0.17364048957824707

Final encoder loss: 0.02174887284467524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0708458423614502 0.1738135814666748

Final encoder loss: 0.021529100774811186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07342720031738281 0.17491555213928223

Final encoder loss: 0.021535216035359794
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0713355541229248 0.17405033111572266

Final encoder loss: 0.01649126271838679
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07100415229797363 0.1739640235900879

Final encoder loss: 0.0157604559318833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07226014137268066 0.17426586151123047

Final encoder loss: 0.01691462325553152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0706782341003418 0.17359590530395508

Final encoder loss: 0.01601919326655453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07096505165100098 0.173445463180542

Final encoder loss: 0.013135726154069802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07104086875915527 0.174546480178833

Final encoder loss: 0.013531191510348217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07266426086425781 0.1743612289428711

Final encoder loss: 0.014396843574632305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07122468948364258 0.17392468452453613

Final encoder loss: 0.012966089388147473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07121515274047852 0.17390656471252441


Training wesad model
Final encoder loss: 0.21562258899211884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1078042984008789 0.03392338752746582

Final encoder loss: 0.09979461133480072
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10671830177307129 0.033109426498413086

Final encoder loss: 0.06408610194921494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10361838340759277 0.03382563591003418

Final encoder loss: 0.04625672101974487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10403084754943848 0.032797813415527344

Final encoder loss: 0.035860825330019
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10376501083374023 0.03348183631896973

Final encoder loss: 0.02940388023853302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10474586486816406 0.03431439399719238

Final encoder loss: 0.025193331763148308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10513925552368164 0.03452110290527344

Final encoder loss: 0.02236456237733364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10489583015441895 0.03311324119567871

Final encoder loss: 0.020426014438271523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10483598709106445 0.033072710037231445

Final encoder loss: 0.019100962206721306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10418558120727539 0.033484697341918945

Final encoder loss: 0.01823459193110466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10403299331665039 0.033414363861083984

Final encoder loss: 0.017721185460686684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10463142395019531 0.03413081169128418

Final encoder loss: 0.01742202416062355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10561633110046387 0.034645795822143555

Final encoder loss: 0.017178164795041084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10330986976623535 0.03325819969177246

Final encoder loss: 0.017005005851387978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10451841354370117 0.033682823181152344

Final encoder loss: 0.016972597688436508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10425019264221191 0.03354239463806152

Final encoder loss: 0.016852760687470436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10500383377075195 0.03310728073120117

Final encoder loss: 0.01676257885992527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10522699356079102 0.03371739387512207

Final encoder loss: 0.016615115106105804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1068427562713623 0.032610177993774414

Final encoder loss: 0.016633180901408195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10314393043518066 0.03310227394104004

Final encoder loss: 0.016720570623874664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10394120216369629 0.03336453437805176

Final encoder loss: 0.01674477569758892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10436749458312988 0.03297257423400879

Final encoder loss: 0.016587132588028908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10452437400817871 0.03315091133117676

Final encoder loss: 0.0164746455848217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10585451126098633 0.034180641174316406

Final encoder loss: 0.016431959345936775
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10593152046203613 0.0334780216217041

Final encoder loss: 0.016442764550447464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10425806045532227 0.033341169357299805

Final encoder loss: 0.016530293971300125

Calculating loss for amigos model
	Full Pass 0.688159704208374
numFreeParamsPath 18
Reconstruction loss values: 0.02386292815208435 0.03305503726005554

Calculating loss for dapper model
	Full Pass 0.1521458625793457
numFreeParamsPath 18
Reconstruction loss values: 0.01867895945906639 0.022019673138856888

Calculating loss for case model
	Full Pass 0.8666162490844727
numFreeParamsPath 18
Reconstruction loss values: 0.03013199381530285 0.033164020627737045

Calculating loss for emognition model
	Full Pass 0.2920093536376953
numFreeParamsPath 18
Reconstruction loss values: 0.031249424442648888 0.038999415934085846

Calculating loss for empatch model
	Full Pass 0.10576677322387695
numFreeParamsPath 18
Reconstruction loss values: 0.03207192197442055 0.0381607748568058

Calculating loss for wesad model
	Full Pass 0.07729196548461914
numFreeParamsPath 18
Reconstruction loss values: 0.031710993498563766 0.04825493320822716
Total loss calculation time: 4.364045858383179

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.467189311981201
Total epoch time: 234.4357030391693

Epoch: 53

Training amigos model
Final encoder loss: 0.02281777594012975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.11704492568969727 0.39193177223205566

Final encoder loss: 0.02164722002733324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10849833488464355 0.3897731304168701

Final encoder loss: 0.023403336783786973
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10930728912353516 0.39117980003356934

Final encoder loss: 0.024193650526730098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10877752304077148 0.3900146484375

Final encoder loss: 0.023351898403899528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10961580276489258 0.3915712833404541

Final encoder loss: 0.021257750397883614
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10986971855163574 0.39012932777404785

Final encoder loss: 0.023402687730956558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10944151878356934 0.38887572288513184

Final encoder loss: 0.02286936498199827
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10899496078491211 0.38967227935791016

Final encoder loss: 0.02125058437517842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10866498947143555 0.3901793956756592

Final encoder loss: 0.024127681462953143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10975241661071777 0.3915741443634033

Final encoder loss: 0.024176888882017713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10930156707763672 0.39014291763305664

Final encoder loss: 0.022604634284107736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.11009573936462402 0.3916282653808594

Final encoder loss: 0.022482485534480234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10877609252929688 0.390606164932251

Final encoder loss: 0.02541314116435444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10922455787658691 0.38915157318115234

Final encoder loss: 0.02318090632198067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.108612060546875 0.3902895450592041

Final encoder loss: 0.02594354401463884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10326313972473145 0.3844106197357178


Training dapper model
Final encoder loss: 0.021458681767099093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06223297119140625 0.15213418006896973

Final encoder loss: 0.01840702887694995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06343698501586914 0.15509438514709473

Final encoder loss: 0.016691985270062083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06304121017456055 0.15016698837280273

Final encoder loss: 0.018474974224488745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06331562995910645 0.15254449844360352

Final encoder loss: 0.0189095063459009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06299185752868652 0.1516585350036621

Final encoder loss: 0.01965853777045045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.061948299407958984 0.15124058723449707

Final encoder loss: 0.018824747099005168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06418037414550781 0.1520860195159912

Final encoder loss: 0.017983969314695116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06161642074584961 0.14881062507629395

Final encoder loss: 0.01964810473554886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06122899055480957 0.1490325927734375

Final encoder loss: 0.018741542017900025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06097054481506348 0.14899778366088867

Final encoder loss: 0.019161268525024516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06121635437011719 0.14887332916259766

Final encoder loss: 0.016426133020488575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06125020980834961 0.1484684944152832

Final encoder loss: 0.017244527220173535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06116938591003418 0.14938807487487793

Final encoder loss: 0.01836858657100567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06138443946838379 0.1488633155822754

Final encoder loss: 0.018704628557328958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06116962432861328 0.14932036399841309

Final encoder loss: 0.020242675252045643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.06067252159118652 0.1481015682220459


Training case model
Final encoder loss: 0.03094829197058606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09302234649658203 0.263887882232666

Final encoder loss: 0.02891415468857173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09077215194702148 0.2640197277069092

Final encoder loss: 0.027171140494410873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09096693992614746 0.26441097259521484

Final encoder loss: 0.025743698647206097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09083747863769531 0.26361680030822754

Final encoder loss: 0.026109569198321105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09125089645385742 0.2683835029602051

Final encoder loss: 0.024370353553722975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09176468849182129 0.2646903991699219

Final encoder loss: 0.024772127133467783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09129500389099121 0.2652242183685303

Final encoder loss: 0.0248543400824669
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09112977981567383 0.2657003402709961

Final encoder loss: 0.02478088547197691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09137082099914551 0.265012264251709

Final encoder loss: 0.024122126121721266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09137821197509766 0.2647514343261719

Final encoder loss: 0.024166986482970648
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09140133857727051 0.2646925449371338

Final encoder loss: 0.023830555657468812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09126091003417969 0.2655913829803467

Final encoder loss: 0.023215398212398264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09203004837036133 0.26528453826904297

Final encoder loss: 0.023548887898929526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09139823913574219 0.2646524906158447

Final encoder loss: 0.023302585678352902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09140253067016602 0.26552700996398926

Final encoder loss: 0.022564540506332855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08815646171569824 0.262371301651001


Training emognition model
Final encoder loss: 0.032493309192413895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08377218246459961 0.27478742599487305

Final encoder loss: 0.03144503268316757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08363914489746094 0.27501440048217773

Final encoder loss: 0.03032978757535499
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08363175392150879 0.27492642402648926

Final encoder loss: 0.02976015735517359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08776164054870605 0.27555108070373535

Final encoder loss: 0.028831464611059825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08311891555786133 0.2754836082458496

Final encoder loss: 0.030199455994605964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08380770683288574 0.27550458908081055

Final encoder loss: 0.028913973767773105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08346939086914062 0.27635741233825684

Final encoder loss: 0.030061206202531693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08288407325744629 0.27492284774780273

Final encoder loss: 0.029275002869093455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08330416679382324 0.2760281562805176

Final encoder loss: 0.03032951254446677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08367586135864258 0.2754085063934326

Final encoder loss: 0.030103661951268137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.0833590030670166 0.2756626605987549

Final encoder loss: 0.029830274207348968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08364748954772949 0.2751889228820801

Final encoder loss: 0.02787561517281714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08306622505187988 0.27538561820983887

Final encoder loss: 0.02858015946511934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08350658416748047 0.2757992744445801

Final encoder loss: 0.030914973667340525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0834801197052002 0.2751944065093994

Final encoder loss: 0.029173799661434626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08286142349243164 0.27466654777526855


Training amigos model
Final encoder loss: 0.01603403472556859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10624933242797852 0.34134674072265625

Final encoder loss: 0.02070466414101804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10639381408691406 0.34168457984924316

Final encoder loss: 0.01727394450542979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10612058639526367 0.3416268825531006

Final encoder loss: 0.01738731676938562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10658431053161621 0.3415248394012451

Final encoder loss: 0.01795298277873333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10574173927307129 0.34146642684936523

Final encoder loss: 0.01737218940815792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1062612533569336 0.341705322265625

Final encoder loss: 0.01793487048583625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10603904724121094 0.3416285514831543

Final encoder loss: 0.01756731517089958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.1062767505645752 0.3417634963989258

Final encoder loss: 0.016557238912197396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10644054412841797 0.34186244010925293

Final encoder loss: 0.01955007546711689
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10655951499938965 0.3416781425476074

Final encoder loss: 0.01700770483804572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10644268989562988 0.34151625633239746

Final encoder loss: 0.018877063624287257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10596728324890137 0.3414273262023926

Final encoder loss: 0.01971575869329066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1057584285736084 0.34122514724731445

Final encoder loss: 0.017768121299793993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.1062772274017334 0.3410375118255615

Final encoder loss: 0.018804836052817315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10605573654174805 0.34123682975769043

Final encoder loss: 0.019858438395912507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10077285766601562 0.3373749256134033


Training amigos model
Final encoder loss: 0.18076778948307037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4642918109893799 0.07674837112426758

Final encoder loss: 0.18780741095542908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45805788040161133 0.07589888572692871

Final encoder loss: 0.1836281418800354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4702877998352051 0.0760035514831543

Final encoder loss: 0.07728533446788788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46649169921875 0.07425308227539062

Final encoder loss: 0.07837777584791183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46636009216308594 0.08089590072631836

Final encoder loss: 0.07348009943962097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47019481658935547 0.07350325584411621

Final encoder loss: 0.0444694422185421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4687073230743408 0.07479381561279297

Final encoder loss: 0.044749315828084946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4658811092376709 0.07661104202270508

Final encoder loss: 0.042369578033685684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47622084617614746 0.07992815971374512

Final encoder loss: 0.03156829625368118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47068023681640625 0.07244277000427246

Final encoder loss: 0.03195341303944588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46404123306274414 0.07752728462219238

Final encoder loss: 0.030841482803225517
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46628522872924805 0.07468008995056152

Final encoder loss: 0.025643419474363327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46562719345092773 0.08298158645629883

Final encoder loss: 0.026050111278891563
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.471118688583374 0.07832002639770508

Final encoder loss: 0.025479881092905998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46534299850463867 0.07603263854980469

Final encoder loss: 0.022841935977339745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46703648567199707 0.08156895637512207

Final encoder loss: 0.02322072722017765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46933746337890625 0.07335138320922852

Final encoder loss: 0.022985663264989853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4720747470855713 0.07512092590332031

Final encoder loss: 0.021983902901411057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46398258209228516 0.07699847221374512

Final encoder loss: 0.021990574896335602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46743297576904297 0.08039712905883789

Final encoder loss: 0.021961182355880737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46810483932495117 0.07403445243835449

Final encoder loss: 0.021642614156007767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4688875675201416 0.0763387680053711

Final encoder loss: 0.02162831276655197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47022438049316406 0.07810115814208984

Final encoder loss: 0.021504966542124748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.467850923538208 0.08150553703308105

Final encoder loss: 0.020983457565307617
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47254419326782227 0.07560133934020996

Final encoder loss: 0.021145885810256004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4667072296142578 0.07787036895751953

Final encoder loss: 0.020796766504645348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46827220916748047 0.07929730415344238

Final encoder loss: 0.020097117871046066
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4646751880645752 0.07383131980895996

Final encoder loss: 0.020464671775698662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4729785919189453 0.07639884948730469

Final encoder loss: 0.020186280831694603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47779083251953125 0.07583808898925781

Final encoder loss: 0.019564002752304077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46624255180358887 0.08141589164733887

Final encoder loss: 0.01945815049111843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46797680854797363 0.0751340389251709

Final encoder loss: 0.01958492584526539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4690072536468506 0.07598423957824707

Final encoder loss: 0.019375519827008247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4655759334564209 0.07506966590881348

Final encoder loss: 0.01922191120684147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4690868854522705 0.08129668235778809

Final encoder loss: 0.019331136718392372
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47165751457214355 0.07471370697021484

Final encoder loss: 0.019011173397302628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46381306648254395 0.07592082023620605

Final encoder loss: 0.01873202994465828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4648141860961914 0.07494568824768066

Final encoder loss: 0.01904393546283245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4663846492767334 0.08104395866394043

Final encoder loss: 0.018611030653119087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4712984561920166 0.07633161544799805

Final encoder loss: 0.01849508285522461
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4647395610809326 0.07505536079406738

Final encoder loss: 0.018775058910250664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4677913188934326 0.08180427551269531

Final encoder loss: 0.018378369510173798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4673008918762207 0.07514429092407227

Final encoder loss: 0.018152477219700813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4710574150085449 0.07610464096069336

Final encoder loss: 0.018369248136878014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46271419525146484 0.07635307312011719

Final encoder loss: 0.018241025507450104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46585822105407715 0.07980227470397949

Final encoder loss: 0.017958588898181915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4683380126953125 0.07653284072875977

Final encoder loss: 0.01826123334467411
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46912717819213867 0.07571220397949219

Final encoder loss: 0.0181012824177742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4683859348297119 0.0759730339050293

Final encoder loss: 0.01773398369550705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46469974517822266 0.08026814460754395

Final encoder loss: 0.018014399334788322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47231054306030273 0.07543396949768066

Final encoder loss: 0.01795983873307705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46442174911499023 0.07610440254211426

Final encoder loss: 0.01791686750948429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4657459259033203 0.07635831832885742

Final encoder loss: 0.018084736540913582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4672565460205078 0.0768892765045166

Final encoder loss: 0.01767043210566044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4697415828704834 0.07767653465270996

Final encoder loss: 0.017485160380601883
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4651181697845459 0.07595419883728027

Final encoder loss: 0.01791619509458542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4661712646484375 0.08112978935241699

Final encoder loss: 0.017600523307919502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4667980670928955 0.07393145561218262

Final encoder loss: 0.017411066219210625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47101688385009766 0.07645893096923828

Final encoder loss: 0.017808349803090096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46245861053466797 0.07620835304260254

Final encoder loss: 0.017510974779725075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46656227111816406 0.08084654808044434

Final encoder loss: 0.017204787582159042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46973538398742676 0.0753183364868164

Final encoder loss: 0.017505500465631485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.47091221809387207 0.07664251327514648

Final encoder loss: 0.017523428425192833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4677438735961914 0.0759885311126709

Final encoder loss: 0.017347555607557297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46728992462158203 0.08184981346130371

Final encoder loss: 0.01772790588438511
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4722926616668701 0.0773463249206543

Final encoder loss: 0.017596464604139328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4671964645385742 0.07494926452636719

Final encoder loss: 0.017256060615181923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4676055908203125 0.08057999610900879

Final encoder loss: 0.017498880624771118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4670851230621338 0.07370710372924805

Final encoder loss: 0.017487291246652603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47001051902770996 0.07597994804382324

Final encoder loss: 0.017079610377550125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4669973850250244 0.07720470428466797

Final encoder loss: 0.01750515028834343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4688756465911865 0.07963395118713379

Final encoder loss: 0.01726045459508896
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46849608421325684 0.0759284496307373

Final encoder loss: 0.01679244078695774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4689443111419678 0.07544755935668945

Final encoder loss: 0.017285700887441635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46715426445007324 0.07518815994262695

Final encoder loss: 0.017203381285071373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4670896530151367 0.08069181442260742

Final encoder loss: 0.016801899299025536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46967220306396484 0.0745229721069336

Final encoder loss: 0.017308706417679787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46666979789733887 0.07629704475402832

Final encoder loss: 0.017128562554717064
Final encoder loss: 0.016010671854019165
Final encoder loss: 0.015624587424099445

Training dapper model
Final encoder loss: 0.015284654783480326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059929609298706055 0.1084907054901123

Final encoder loss: 0.015259711154238107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.060579538345336914 0.10748791694641113

Final encoder loss: 0.014645432900440335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.060263633728027344 0.10734820365905762

Final encoder loss: 0.014324981259589392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059618473052978516 0.10783982276916504

Final encoder loss: 0.016035942691321494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06148338317871094 0.10810661315917969

Final encoder loss: 0.01559662375927684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.059992313385009766 0.10754227638244629

Final encoder loss: 0.011945172084059133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.059981584548950195 0.1077280044555664

Final encoder loss: 0.013902376029479665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.06040358543395996 0.10830283164978027

Final encoder loss: 0.015053534678231558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05964303016662598 0.10767722129821777

Final encoder loss: 0.014649793663696663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.06003904342651367 0.10782003402709961

Final encoder loss: 0.013333293316661592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059845924377441406 0.10795831680297852

Final encoder loss: 0.014115130742796873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.0611116886138916 0.10760259628295898

Final encoder loss: 0.012640413235251158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.060025691986083984 0.10788631439208984

Final encoder loss: 0.01557882615792042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06032705307006836 0.1076517105102539

Final encoder loss: 0.013892653776428412
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06104397773742676 0.10808396339416504

Final encoder loss: 0.014005690783519043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.059513092041015625 0.10740780830383301


Training dapper model
Final encoder loss: 0.20245768129825592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11498665809631348 0.03480267524719238

Final encoder loss: 0.20821243524551392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11695408821105957 0.035535335540771484

Final encoder loss: 0.08665376156568527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11351203918457031 0.0345606803894043

Final encoder loss: 0.08800407499074936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1155087947845459 0.034501075744628906

Final encoder loss: 0.05111866816878319
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11413359642028809 0.034888505935668945

Final encoder loss: 0.05056590959429741
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11724686622619629 0.03435230255126953

Final encoder loss: 0.03464967757463455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.112640380859375 0.034515380859375

Final encoder loss: 0.03421018272638321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11518239974975586 0.034552812576293945

Final encoder loss: 0.026007212698459625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11495041847229004 0.03438568115234375

Final encoder loss: 0.02584046497941017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11528658866882324 0.03354310989379883

Final encoder loss: 0.021143998950719833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11219310760498047 0.03448915481567383

Final encoder loss: 0.02113184705376625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11692285537719727 0.03494596481323242

Final encoder loss: 0.01831723004579544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11261892318725586 0.03480792045593262

Final encoder loss: 0.018341679126024246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11514711380004883 0.03391861915588379

Final encoder loss: 0.016640789806842804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11374807357788086 0.034706830978393555

Final encoder loss: 0.01663941517472267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1162869930267334 0.03455972671508789

Final encoder loss: 0.015485122799873352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11325860023498535 0.03462791442871094

Final encoder loss: 0.015640370547771454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1157219409942627 0.03500652313232422

Final encoder loss: 0.014786634594202042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11481833457946777 0.034151315689086914

Final encoder loss: 0.015139260329306126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11468648910522461 0.034512996673583984

Final encoder loss: 0.014373541809618473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11311888694763184 0.034481048583984375

Final encoder loss: 0.0146033251658082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11664795875549316 0.03528571128845215

Final encoder loss: 0.01416211761534214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11263108253479004 0.035135746002197266

Final encoder loss: 0.01420908235013485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11523723602294922 0.03465890884399414

Final encoder loss: 0.014157403260469437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11295604705810547 0.035315513610839844

Final encoder loss: 0.01397453248500824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11631298065185547 0.03448605537414551

Final encoder loss: 0.014267816208302975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11406445503234863 0.03490424156188965

Final encoder loss: 0.013850277289748192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11531901359558105 0.034746646881103516

Final encoder loss: 0.013927235268056393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11432433128356934 0.03394293785095215

Final encoder loss: 0.013730132952332497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152031421661377 0.03420758247375488

Final encoder loss: 0.013549046590924263
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11300015449523926 0.03359723091125488

Final encoder loss: 0.013505798764526844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11668229103088379 0.03496742248535156

Final encoder loss: 0.013143227435648441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11285042762756348 0.0342869758605957

Final encoder loss: 0.013288380578160286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1151435375213623 0.03427910804748535

Final encoder loss: 0.012844274751842022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11336803436279297 0.035324811935424805

Final encoder loss: 0.012929745949804783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11634993553161621 0.035022735595703125

Final encoder loss: 0.0125832324847579
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11191296577453613 0.034467220306396484

Final encoder loss: 0.01262595970183611
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11466622352600098 0.03490328788757324

Final encoder loss: 0.012556467205286026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11513853073120117 0.034093379974365234

Final encoder loss: 0.012482577934861183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11487793922424316 0.03469991683959961

Final encoder loss: 0.012559328228235245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1132349967956543 0.03365206718444824

Final encoder loss: 0.01232762448489666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11717796325683594 0.03536677360534668

Final encoder loss: 0.012552095577120781
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11256527900695801 0.03530693054199219

Final encoder loss: 0.012310526333749294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11513781547546387 0.0345001220703125

Final encoder loss: 0.01232637744396925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11382246017456055 0.03506183624267578

Final encoder loss: 0.012225736863911152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11649584770202637 0.034555912017822266

Final encoder loss: 0.01216899137943983
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1126699447631836 0.034477949142456055

Final encoder loss: 0.012156994082033634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11538124084472656 0.034719228744506836

Final encoder loss: 0.01219868753105402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1147146224975586 0.03409695625305176

Final encoder loss: 0.012174117378890514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11472463607788086 0.035018205642700195

Final encoder loss: 0.012028315104544163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11322689056396484 0.03400087356567383

Final encoder loss: 0.012017384171485901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11681723594665527 0.03508496284484863

Final encoder loss: 0.012087813578546047
Final encoder loss: 0.011063220910727978

Training case model
Final encoder loss: 0.02112169021427913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08920145034790039 0.21905755996704102

Final encoder loss: 0.021159995127181253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08922219276428223 0.2191462516784668

Final encoder loss: 0.020386251764887748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08918452262878418 0.21930956840515137

Final encoder loss: 0.02065396052794972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08994770050048828 0.21938300132751465

Final encoder loss: 0.020492012225150773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08982300758361816 0.2194228172302246

Final encoder loss: 0.02087314198973797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.09028291702270508 0.21918869018554688

Final encoder loss: 0.021190065090877622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.0907595157623291 0.21924495697021484

Final encoder loss: 0.02008347340540003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08949780464172363 0.21912717819213867

Final encoder loss: 0.020728019897499755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.0904698371887207 0.21950435638427734

Final encoder loss: 0.020713653746463626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08916974067687988 0.21928811073303223

Final encoder loss: 0.020068800870658984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.09036397933959961 0.2192518711090088

Final encoder loss: 0.02093310557781927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08985447883605957 0.21943163871765137

Final encoder loss: 0.020667619005520242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08925652503967285 0.219069242477417

Final encoder loss: 0.020668775098704236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08908987045288086 0.21933197975158691

Final encoder loss: 0.021024190616253203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08929014205932617 0.2194981575012207

Final encoder loss: 0.020960711991303088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08635973930358887 0.21599483489990234


Training case model
Final encoder loss: 0.20296551287174225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27176928520202637 0.05329394340515137

Final encoder loss: 0.18889960646629333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25936198234558105 0.05303835868835449

Final encoder loss: 0.19013844430446625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25847530364990234 0.052945852279663086

Final encoder loss: 0.19218555092811584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2608218193054199 0.052254676818847656

Final encoder loss: 0.18081016838550568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25782132148742676 0.05483436584472656

Final encoder loss: 0.19191288948059082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562134265899658 0.0523533821105957

Final encoder loss: 0.10621511191129684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25895023345947266 0.053458213806152344

Final encoder loss: 0.09575114399194717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2602396011352539 0.05340862274169922

Final encoder loss: 0.09168704599142075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25807905197143555 0.05380129814147949

Final encoder loss: 0.09032537043094635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25904107093811035 0.05298423767089844

Final encoder loss: 0.08175734430551529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.260434627532959 0.052217960357666016

Final encoder loss: 0.08561357855796814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539329528808594 0.05268287658691406

Final encoder loss: 0.0619569830596447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2584104537963867 0.05265378952026367

Final encoder loss: 0.05604460835456848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579934597015381 0.053320884704589844

Final encoder loss: 0.05363951250910759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.259840726852417 0.05232834815979004

Final encoder loss: 0.05377275124192238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25760412216186523 0.05257058143615723

Final encoder loss: 0.049932412803173065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26105618476867676 0.052430152893066406

Final encoder loss: 0.052291810512542725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25708866119384766 0.0522463321685791

Final encoder loss: 0.043127648532390594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25758957862854004 0.05321860313415527

Final encoder loss: 0.039659250527620316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2585582733154297 0.05272626876831055

Final encoder loss: 0.038101740181446075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25913143157958984 0.05253124237060547

Final encoder loss: 0.0386144295334816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2589716911315918 0.05289959907531738

Final encoder loss: 0.037061940878629684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25836944580078125 0.05324268341064453

Final encoder loss: 0.03824242576956749
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555418014526367 0.05187058448791504

Final encoder loss: 0.0343819186091423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25966858863830566 0.05138230323791504

Final encoder loss: 0.03258093446493149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2593421936035156 0.05199766159057617

Final encoder loss: 0.031532466411590576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25899577140808105 0.05135750770568848

Final encoder loss: 0.032001130282878876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25604724884033203 0.05234575271606445

Final encoder loss: 0.031810253858566284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25660133361816406 0.05177569389343262

Final encoder loss: 0.032058801501989365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25310420989990234 0.0526585578918457

Final encoder loss: 0.030366042628884315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25652527809143066 0.05207467079162598

Final encoder loss: 0.02961226925253868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2562260627746582 0.05163240432739258

Final encoder loss: 0.028794897720217705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563939094543457 0.05158567428588867

Final encoder loss: 0.02920413762331009
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25611257553100586 0.0527033805847168

Final encoder loss: 0.02982778288424015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.255434513092041 0.05189204216003418

Final encoder loss: 0.029460614547133446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25310611724853516 0.0513463020324707

Final encoder loss: 0.027917133644223213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582414150238037 0.05225348472595215

Final encoder loss: 0.02715650573372841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2596416473388672 0.052522897720336914

Final encoder loss: 0.026588115841150284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25863027572631836 0.0524907112121582

Final encoder loss: 0.02675500325858593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2584195137023926 0.05380654335021973

Final encoder loss: 0.027618389576673508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25803279876708984 0.05294299125671387

Final encoder loss: 0.027208367362618446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546844482421875 0.05153059959411621

Final encoder loss: 0.026075683534145355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25878286361694336 0.05373549461364746

Final encoder loss: 0.025635620579123497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26020312309265137 0.052755117416381836

Final encoder loss: 0.02508946694433689
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2582101821899414 0.05420804023742676

Final encoder loss: 0.02521376870572567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25826025009155273 0.05202293395996094

Final encoder loss: 0.02612355165183544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2582268714904785 0.05296897888183594

Final encoder loss: 0.025665270164608955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559170722961426 0.05124926567077637

Final encoder loss: 0.024806389585137367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25815773010253906 0.052919626235961914

Final encoder loss: 0.024529604241251945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25858592987060547 0.05291628837585449

Final encoder loss: 0.02412589080631733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26015472412109375 0.052401065826416016

Final encoder loss: 0.024051442742347717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2599067687988281 0.0531768798828125

Final encoder loss: 0.025119945406913757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2588016986846924 0.05341792106628418

Final encoder loss: 0.024536319077014923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547016143798828 0.051439523696899414

Final encoder loss: 0.024099688977003098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25888895988464355 0.0533900260925293

Final encoder loss: 0.023904062807559967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25916361808776855 0.05342364311218262

Final encoder loss: 0.02358825132250786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587771415710449 0.05506396293640137

Final encoder loss: 0.02357589267194271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2576169967651367 0.05355048179626465

Final encoder loss: 0.02471095509827137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2595233917236328 0.05312347412109375

Final encoder loss: 0.02403889037668705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.26374149322509766 0.05135965347290039

Final encoder loss: 0.023467104882001877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25919175148010254 0.052608489990234375

Final encoder loss: 0.023276004940271378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25815629959106445 0.05503487586975098

Final encoder loss: 0.022966813296079636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25734806060791016 0.05331110954284668

Final encoder loss: 0.02283404767513275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2582254409790039 0.05301094055175781

Final encoder loss: 0.023891082033514977
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25858044624328613 0.052216529846191406

Final encoder loss: 0.02334132231771946
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546226978302002 0.0531008243560791

Final encoder loss: 0.0230536051094532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25740575790405273 0.05267620086669922

Final encoder loss: 0.02294686622917652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25900697708129883 0.0513150691986084

Final encoder loss: 0.02251490205526352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25972819328308105 0.05365633964538574

Final encoder loss: 0.022502591833472252
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25722336769104004 0.054651737213134766

Final encoder loss: 0.02363699860870838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2575416564941406 0.052739620208740234

Final encoder loss: 0.02285921573638916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541494369506836 0.0511326789855957

Final encoder loss: 0.02260493114590645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25983715057373047 0.0527036190032959

Final encoder loss: 0.022473789751529694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2581338882446289 0.05556535720825195

Final encoder loss: 0.022282777354121208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2576277256011963 0.053092241287231445

Final encoder loss: 0.02199077233672142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25839948654174805 0.05238962173461914

Final encoder loss: 0.023128803819417953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25850772857666016 0.051853179931640625

Final encoder loss: 0.02246248535811901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558324337005615 0.05089092254638672

Final encoder loss: 0.02248290553689003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582554817199707 0.054718017578125

Final encoder loss: 0.02229096181690693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584519386291504 0.052059173583984375

Final encoder loss: 0.021978558972477913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25856566429138184 0.05299210548400879

Final encoder loss: 0.021849166601896286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25870704650878906 0.05261087417602539

Final encoder loss: 0.022971762344241142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257457971572876 0.05311226844787598

Final encoder loss: 0.02239503152668476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25366878509521484 0.0519258975982666

Final encoder loss: 0.02207179367542267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25981855392456055 0.05201458930969238

Final encoder loss: 0.02200985886156559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2599360942840576 0.05258321762084961

Final encoder loss: 0.021833589300513268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25779271125793457 0.05465841293334961

Final encoder loss: 0.021484127268195152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25722813606262207 0.0543370246887207

Final encoder loss: 0.022514071315526962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581441402435303 0.053313493728637695

Final encoder loss: 0.021960681304335594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25545430183410645 0.05197286605834961

Final encoder loss: 0.02197674661874771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2575716972351074 0.05421304702758789

Final encoder loss: 0.021841585636138916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2579526901245117 0.05382347106933594

Final encoder loss: 0.021500593051314354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25884318351745605 0.05229377746582031

Final encoder loss: 0.021355006843805313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25806760787963867 0.05332636833190918

Final encoder loss: 0.022480381652712822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25899791717529297 0.05259537696838379

Final encoder loss: 0.021833650767803192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542264461517334 0.05428957939147949

Final encoder loss: 0.021687036380171776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257857084274292 0.05233311653137207

Final encoder loss: 0.021639462560415268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583625316619873 0.05354046821594238

Final encoder loss: 0.021399004384875298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587413787841797 0.05168437957763672

Final encoder loss: 0.02108093537390232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25765323638916016 0.054621219635009766

Final encoder loss: 0.022035101428627968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25766706466674805 0.053424835205078125

Final encoder loss: 0.021612120792269707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25275182723999023 0.05158638954162598

Final encoder loss: 0.021615175530314445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25592899322509766 0.05267810821533203

Final encoder loss: 0.021533550694584846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25702857971191406 0.05122804641723633

Final encoder loss: 0.021251965314149857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25647950172424316 0.05221843719482422

Final encoder loss: 0.021003300324082375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635838508605957 0.0522313117980957

Final encoder loss: 0.02222490683197975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2566969394683838 0.052610158920288086

Final encoder loss: 0.021518347784876823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2526969909667969 0.05206561088562012

Final encoder loss: 0.021428968757390976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570047378540039 0.052149057388305664

Final encoder loss: 0.021351438015699387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2560265064239502 0.05256295204162598

Final encoder loss: 0.021168312057852745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2580099105834961 0.05241537094116211

Final encoder loss: 0.02074163593351841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583274841308594 0.05204606056213379

Final encoder loss: 0.02171163819730282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25920629501342773 0.053267717361450195

Final encoder loss: 0.02127464860677719
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25461816787719727 0.05205702781677246

Final encoder loss: 0.02133340574800968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25887584686279297 0.05408191680908203

Final encoder loss: 0.02122909389436245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257793664932251 0.05309271812438965

Final encoder loss: 0.021016495302319527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2574944496154785 0.05278301239013672

Final encoder loss: 0.020738966763019562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2579059600830078 0.05197262763977051

Final encoder loss: 0.021820593625307083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2594454288482666 0.05257415771484375

Final encoder loss: 0.021250741556286812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2557685375213623 0.05124020576477051

Final encoder loss: 0.021199043840169907
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25876331329345703 0.05212116241455078

Final encoder loss: 0.021103231236338615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2574431896209717 0.05262017250061035

Final encoder loss: 0.020870767533779144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2566711902618408 0.05524468421936035

Final encoder loss: 0.020553477108478546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2578558921813965 0.05342841148376465

Final encoder loss: 0.021493462845683098
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2568087577819824 0.05363774299621582

Final encoder loss: 0.020954973995685577
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25386595726013184 0.054228782653808594

Final encoder loss: 0.021131472662091255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2579803466796875 0.051839590072631836

Final encoder loss: 0.02104603499174118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2596018314361572 0.05194497108459473

Final encoder loss: 0.02076980471611023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25819849967956543 0.053292274475097656

Final encoder loss: 0.020465195178985596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2599210739135742 0.05404806137084961

Final encoder loss: 0.021613512188196182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579329013824463 0.05259370803833008

Final encoder loss: 0.021087350323796272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25482940673828125 0.05253410339355469

Final encoder loss: 0.02100674994289875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25830936431884766 0.052837371826171875

Final encoder loss: 0.020920099690556526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257936954498291 0.05145072937011719

Final encoder loss: 0.020708803087472916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563803195953369 0.05225706100463867

Final encoder loss: 0.020327908918261528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25632143020629883 0.05159401893615723

Final encoder loss: 0.021389814093708992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2561800479888916 0.05201363563537598

Final encoder loss: 0.02081209234893322
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2525193691253662 0.051419973373413086

Final encoder loss: 0.020933615043759346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25681352615356445 0.05222678184509277

Final encoder loss: 0.020859887823462486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2555661201477051 0.05184054374694824

Final encoder loss: 0.020656578242778778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25829100608825684 0.05197477340698242

Final encoder loss: 0.020269883796572685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2561368942260742 0.05185437202453613

Final encoder loss: 0.021410048007965088
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25768446922302246 0.0531306266784668

Final encoder loss: 0.02088126726448536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2531130313873291 0.05116081237792969

Final encoder loss: 0.020865660160779953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570960521697998 0.051880836486816406

Final encoder loss: 0.020795615389943123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25830841064453125 0.053127288818359375

Final encoder loss: 0.02054465003311634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2571446895599365 0.05255866050720215

Final encoder loss: 0.020195772871375084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25760602951049805 0.05172109603881836

Final encoder loss: 0.021070361137390137
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25795817375183105 0.05277252197265625

Final encoder loss: 0.02061956189572811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2537109851837158 0.05236077308654785

Final encoder loss: 0.020791446790099144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2578597068786621 0.052709341049194336

Final encoder loss: 0.020712880417704582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571990489959717 0.05267691612243652

Final encoder loss: 0.0204317606985569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25763702392578125 0.05279088020324707

Final encoder loss: 0.020151901990175247
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25701260566711426 0.05269312858581543

Final encoder loss: 0.021260270848870277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25800275802612305 0.05369257926940918

Final encoder loss: 0.020742803812026978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2540407180786133 0.051455020904541016

Final encoder loss: 0.02070978656411171
Final encoder loss: 0.02016514725983143
Final encoder loss: 0.019458472728729248
Final encoder loss: 0.01845688559114933
Final encoder loss: 0.018692465499043465
Final encoder loss: 0.017561664804816246

Training emognition model
Final encoder loss: 0.02489665994248368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.0827481746673584 0.23189234733581543

Final encoder loss: 0.02344294535716431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08104443550109863 0.23018813133239746

Final encoder loss: 0.023353720167130203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08114957809448242 0.23067927360534668

Final encoder loss: 0.023476376630971105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.0807948112487793 0.23040413856506348

Final encoder loss: 0.022552029353730556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08123517036437988 0.23064661026000977

Final encoder loss: 0.023226323871010483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08087944984436035 0.23104596138000488

Final encoder loss: 0.021876654239054917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08116531372070312 0.23097968101501465

Final encoder loss: 0.02272655941158792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08259367942810059 0.23132634162902832

Final encoder loss: 0.02271927648208806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08224773406982422 0.2310810089111328

Final encoder loss: 0.023717308397579566
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08094978332519531 0.23053479194641113

Final encoder loss: 0.023217110612777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08256125450134277 0.23102593421936035

Final encoder loss: 0.022916981442351003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08134889602661133 0.23086237907409668

Final encoder loss: 0.022998976671056444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08141255378723145 0.2314748764038086

Final encoder loss: 0.02363076793922384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08059430122375488 0.2298595905303955

Final encoder loss: 0.023226629484761594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08084726333618164 0.23097562789916992

Final encoder loss: 0.02411534874188934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08040857315063477 0.22948551177978516


Training emognition model
Final encoder loss: 0.1935674399137497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3397810459136963 0.0496363639831543

Final encoder loss: 0.1949566900730133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24618864059448242 0.04835009574890137

Final encoder loss: 0.08919969201087952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24742484092712402 0.04960513114929199

Final encoder loss: 0.08860961347818375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24666762351989746 0.04990386962890625

Final encoder loss: 0.05642176419496536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24802827835083008 0.049041032791137695

Final encoder loss: 0.054743871092796326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24598121643066406 0.04954171180725098

Final encoder loss: 0.04165364429354668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2481691837310791 0.04860329627990723

Final encoder loss: 0.04038079082965851
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2458498477935791 0.04887866973876953

Final encoder loss: 0.03401608020067215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2483961582183838 0.04954719543457031

Final encoder loss: 0.03316301107406616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24599504470825195 0.04987502098083496

Final encoder loss: 0.029760662466287613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24793434143066406 0.04966616630554199

Final encoder loss: 0.029115401208400726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2457265853881836 0.049535274505615234

Final encoder loss: 0.027293149381875992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2486720085144043 0.049819231033325195

Final encoder loss: 0.026804545894265175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24636363983154297 0.04859805107116699

Final encoder loss: 0.02593073435127735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24724149703979492 0.048676252365112305

Final encoder loss: 0.025630122050642967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24594974517822266 0.04887080192565918

Final encoder loss: 0.025216830894351006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24700140953063965 0.048972368240356445

Final encoder loss: 0.02510077692568302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24531984329223633 0.048081398010253906

Final encoder loss: 0.024765314534306526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24750566482543945 0.04869222640991211

Final encoder loss: 0.024800747632980347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24564766883850098 0.0479583740234375

Final encoder loss: 0.02430770732462406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24790549278259277 0.04886960983276367

Final encoder loss: 0.02432607300579548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24572253227233887 0.04871344566345215

Final encoder loss: 0.023983286693692207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2472221851348877 0.05035066604614258

Final encoder loss: 0.023936575278639793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24676799774169922 0.04882192611694336

Final encoder loss: 0.023694317787885666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24550175666809082 0.04914045333862305

Final encoder loss: 0.023638103157281876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24608683586120605 0.04823470115661621

Final encoder loss: 0.023350460454821587
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24640631675720215 0.0496978759765625

Final encoder loss: 0.023473266512155533
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24488329887390137 0.04805946350097656

Final encoder loss: 0.02320248633623123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2459111213684082 0.0492091178894043

Final encoder loss: 0.023253237828612328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24508285522460938 0.049385786056518555

Final encoder loss: 0.023081455379724503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.246138334274292 0.048406124114990234

Final encoder loss: 0.02317272312939167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24606847763061523 0.047960519790649414

Final encoder loss: 0.022889984771609306
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2466418743133545 0.04858589172363281

Final encoder loss: 0.022987250238656998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24688458442687988 0.04925036430358887

Final encoder loss: 0.022738786414265633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24651336669921875 0.049233436584472656

Final encoder loss: 0.022812800481915474
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24564266204833984 0.04910993576049805

Final encoder loss: 0.02268269658088684
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25595641136169434 0.049834489822387695

Final encoder loss: 0.022623121738433838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24538660049438477 0.048969268798828125

Final encoder loss: 0.022549297660589218
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24703478813171387 0.049326419830322266

Final encoder loss: 0.02250613272190094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24558377265930176 0.04828929901123047

Final encoder loss: 0.022456495091319084
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2473280429840088 0.049637556076049805

Final encoder loss: 0.022520676255226135
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468397617340088 0.04787945747375488

Final encoder loss: 0.022271502763032913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24770760536193848 0.04873156547546387

Final encoder loss: 0.022518647834658623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24528741836547852 0.04875016212463379

Final encoder loss: 0.022332889959216118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24824261665344238 0.048439741134643555

Final encoder loss: 0.0224921815097332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469801902770996 0.04954838752746582

Final encoder loss: 0.022408634424209595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24700284004211426 0.04835796356201172

Final encoder loss: 0.0224138256162405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24527907371520996 0.05001354217529297

Final encoder loss: 0.022194968536496162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24607610702514648 0.04901552200317383

Final encoder loss: 0.022284457460045815
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24560260772705078 0.04852104187011719

Final encoder loss: 0.022115498781204224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24713134765625 0.04896259307861328

Final encoder loss: 0.02214774861931801
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2458338737487793 0.04846954345703125

Final encoder loss: 0.022084107622504234
Final encoder loss: 0.02135293558239937

Training empatch model
Final encoder loss: 0.029556914114772476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07159781455993652 0.17330431938171387

Final encoder loss: 0.03237789089108253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07066512107849121 0.17335891723632812

Final encoder loss: 0.028704493565999776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0705866813659668 0.172562837600708

Final encoder loss: 0.02971786861997184
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07028079032897949 0.17316555976867676

Final encoder loss: 0.034348397116902435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07100367546081543 0.17271947860717773

Final encoder loss: 0.031428541433304584
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07071828842163086 0.17261481285095215

Final encoder loss: 0.028509465518831593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07030177116394043 0.17241811752319336

Final encoder loss: 0.027355081970978978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.0701453685760498 0.17255711555480957

Final encoder loss: 0.021359892083850213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0702369213104248 0.17297768592834473

Final encoder loss: 0.02457393816284302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.0707087516784668 0.17292237281799316

Final encoder loss: 0.021805660771422295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.0705556869506836 0.17281246185302734

Final encoder loss: 0.02157206370491441
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07072877883911133 0.17316150665283203

Final encoder loss: 0.0204321905418773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07094645500183105 0.17277216911315918

Final encoder loss: 0.022034653999420086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07040715217590332 0.1722872257232666

Final encoder loss: 0.021683616244593067
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07055330276489258 0.17302918434143066

Final encoder loss: 0.02354635698806693
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07065105438232422 0.1724696159362793


Training empatch model
Final encoder loss: 0.17115887999534607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17676544189453125 0.04408669471740723

Final encoder loss: 0.08108093589544296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17540931701660156 0.044528961181640625

Final encoder loss: 0.054846905171871185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17521238327026367 0.043236494064331055

Final encoder loss: 0.042010996490716934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17565703392028809 0.04479575157165527

Final encoder loss: 0.03466164693236351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17502045631408691 0.044512033462524414

Final encoder loss: 0.030088918283581734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17429733276367188 0.04345965385437012

Final encoder loss: 0.027086345478892326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17529749870300293 0.043268442153930664

Final encoder loss: 0.025006314739584923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1743779182434082 0.04440951347351074

Final encoder loss: 0.02353609912097454
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17462944984436035 0.04440808296203613

Final encoder loss: 0.022546689957380295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17449474334716797 0.04400300979614258

Final encoder loss: 0.02185075543820858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17473268508911133 0.04384303092956543

Final encoder loss: 0.021427998319268227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748058795928955 0.043286800384521484

Final encoder loss: 0.02113453671336174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1747267246246338 0.04479169845581055

Final encoder loss: 0.020881453529000282
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17560386657714844 0.04363417625427246

Final encoder loss: 0.02053864300251007
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17466092109680176 0.04427361488342285

Final encoder loss: 0.0202817153185606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17493915557861328 0.042966365814208984

Final encoder loss: 0.020082322880625725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17473316192626953 0.04387831687927246

Final encoder loss: 0.019928811118006706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1748354434967041 0.04451489448547363

Final encoder loss: 0.019833002239465714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17449355125427246 0.043790340423583984

Final encoder loss: 0.019721264019608498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17460036277770996 0.04421257972717285

Final encoder loss: 0.019664214923977852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1744706630706787 0.04395771026611328

Final encoder loss: 0.01956246793270111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17415833473205566 0.04392123222351074

Final encoder loss: 0.01952211745083332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1745619773864746 0.04442286491394043

Final encoder loss: 0.019475426524877548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1742570400238037 0.04403281211853027

Final encoder loss: 0.01937118172645569
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17430567741394043 0.04347944259643555

Final encoder loss: 0.01932908035814762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17478394508361816 0.04369330406188965

Final encoder loss: 0.019308893010020256

Training wesad model
Final encoder loss: 0.03046938423727567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07070231437683105 0.17278409004211426

Final encoder loss: 0.027890042552242623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07062435150146484 0.1727433204650879

Final encoder loss: 0.031031033051714806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07045173645019531 0.1733992099761963

Final encoder loss: 0.03051235618194287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07167553901672363 0.17409968376159668

Final encoder loss: 0.01957372878377722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07120347023010254 0.17439031600952148

Final encoder loss: 0.019712968685728863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07240128517150879 0.17389845848083496

Final encoder loss: 0.019822363127093953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07123351097106934 0.17387604713439941

Final encoder loss: 0.020158939062726595
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0710747241973877 0.17402982711791992

Final encoder loss: 0.014501767382469373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07194089889526367 0.1750326156616211

Final encoder loss: 0.015513414141501926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0722804069519043 0.1739652156829834

Final encoder loss: 0.015610142757833468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07152628898620605 0.17397379875183105

Final encoder loss: 0.016013207230338413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07148098945617676 0.1745283603668213

Final encoder loss: 0.01189329769991975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07263779640197754 0.1743319034576416

Final encoder loss: 0.012781711868617615
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07117676734924316 0.17386484146118164

Final encoder loss: 0.012737812419662495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07100963592529297 0.17391753196716309

Final encoder loss: 0.013214963449770677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07214140892028809 0.17479681968688965


Training wesad model
Final encoder loss: 0.21560661494731903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10585379600524902 0.032738685607910156

Final encoder loss: 0.09871338307857513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10452437400817871 0.03389286994934082

Final encoder loss: 0.06339557468891144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10439586639404297 0.032801151275634766

Final encoder loss: 0.04571838304400444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10491037368774414 0.033632755279541016

Final encoder loss: 0.035387322306632996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10502219200134277 0.033876895904541016

Final encoder loss: 0.02898530103266239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10552740097045898 0.034607648849487305

Final encoder loss: 0.024836594238877296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10427427291870117 0.033728599548339844

Final encoder loss: 0.022039666771888733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10497260093688965 0.032881736755371094

Final encoder loss: 0.020085956901311874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10402226448059082 0.032730817794799805

Final encoder loss: 0.018739420920610428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10371708869934082 0.03364276885986328

Final encoder loss: 0.017794614657759666
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10593605041503906 0.03371763229370117

Final encoder loss: 0.017225291579961777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10620570182800293 0.03340506553649902

Final encoder loss: 0.016894713044166565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10323977470397949 0.03330850601196289

Final encoder loss: 0.0167603250592947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1051182746887207 0.03367900848388672

Final encoder loss: 0.016744142398238182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10388898849487305 0.03362631797790527

Final encoder loss: 0.01668638549745083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10341000556945801 0.034346580505371094

Final encoder loss: 0.016606036573648453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10548806190490723 0.034159183502197266

Final encoder loss: 0.01643279567360878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10537290573120117 0.03310751914978027

Final encoder loss: 0.01624218188226223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10361742973327637 0.03374147415161133

Final encoder loss: 0.016081510111689568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10434913635253906 0.03354215621948242

Final encoder loss: 0.016094481572508812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10398054122924805 0.03307986259460449

Final encoder loss: 0.016207503154873848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1046607494354248 0.034093618392944336

Final encoder loss: 0.016151264309883118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1054677963256836 0.034395456314086914

Final encoder loss: 0.016086643561720848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10385394096374512 0.033501386642456055

Final encoder loss: 0.0159484650939703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1042623519897461 0.03364753723144531

Final encoder loss: 0.01603301800787449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.10389566421508789 0.03371906280517578

Final encoder loss: 0.01616743579506874

Calculating loss for amigos model
	Full Pass 0.7564949989318848
numFreeParamsPath 18
Reconstruction loss values: 0.024063237011432648 0.032504089176654816

Calculating loss for dapper model
	Full Pass 0.1526174545288086
numFreeParamsPath 18
Reconstruction loss values: 0.01933685876429081 0.022928182035684586

Calculating loss for case model
	Full Pass 0.8624265193939209
numFreeParamsPath 18
Reconstruction loss values: 0.029330270364880562 0.03248021751642227

Calculating loss for emognition model
	Full Pass 0.2834343910217285
numFreeParamsPath 18
Reconstruction loss values: 0.029552627354860306 0.03790508210659027

Calculating loss for empatch model
	Full Pass 0.1064612865447998
numFreeParamsPath 18
Reconstruction loss values: 0.030926203355193138 0.03779313713312149

Calculating loss for wesad model
	Full Pass 0.0772705078125
numFreeParamsPath 18
Reconstruction loss values: 0.031347278505563736 0.048223815858364105
Total loss calculation time: 4.106980323791504

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.239542245864868
Total epoch time: 235.48008227348328

Epoch: 54

Training amigos model
Final encoder loss: 0.02423908542083578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.119140625 0.3974282741546631

Final encoder loss: 0.02247489932967759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10959315299987793 0.39080238342285156

Final encoder loss: 0.023461029893039463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10882425308227539 0.39032959938049316

Final encoder loss: 0.023440517473390144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10874152183532715 0.3888206481933594

Final encoder loss: 0.02201519037622869
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10863065719604492 0.38995361328125

Final encoder loss: 0.02343421410804989
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.1079869270324707 0.38948607444763184

Final encoder loss: 0.024414207271158193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10919427871704102 0.3907344341278076

Final encoder loss: 0.02350856682389597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10897707939147949 0.3898489475250244

Final encoder loss: 0.02359589721038853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.1095590591430664 0.3912332057952881

Final encoder loss: 0.023784870410573273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10873579978942871 0.39026689529418945

Final encoder loss: 0.02273558978063081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10868430137634277 0.389676570892334

Final encoder loss: 0.022244272344027753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10859131813049316 0.3902606964111328

Final encoder loss: 0.02396851050883496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10877776145935059 0.3900904655456543

Final encoder loss: 0.023289877101705145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10952043533325195 0.39079880714416504

Final encoder loss: 0.025055930762549947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10839366912841797 0.39045071601867676

Final encoder loss: 0.021882661244666514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10535478591918945 0.38498353958129883


Training dapper model
Final encoder loss: 0.020649826988783267
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06219482421875 0.14992976188659668

Final encoder loss: 0.01828700201122038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06230282783508301 0.1530170440673828

Final encoder loss: 0.021244951369591494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.06445860862731934 0.15166664123535156

Final encoder loss: 0.01840109775039784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06210136413574219 0.15178251266479492

Final encoder loss: 0.01744935257711231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.062010765075683594 0.1510605812072754

Final encoder loss: 0.01731221845664783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06266379356384277 0.15213465690612793

Final encoder loss: 0.01733300933550536
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06422305107116699 0.15129613876342773

Final encoder loss: 0.017393871399689523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06215262413024902 0.15187454223632812

Final encoder loss: 0.017540821968248334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06237626075744629 0.15128445625305176

Final encoder loss: 0.020589290558733465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06256842613220215 0.1525280475616455

Final encoder loss: 0.017218111211791644
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06432318687438965 0.1517314910888672

Final encoder loss: 0.018120783148712374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06212663650512695 0.1522536277770996

Final encoder loss: 0.018322962226371045
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06245732307434082 0.1509699821472168

Final encoder loss: 0.017173028461889336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06313943862915039 0.15340352058410645

Final encoder loss: 0.019344901687935494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06439495086669922 0.15129947662353516

Final encoder loss: 0.018764596692203893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.062389373779296875 0.15110230445861816


Training emognition model
Final encoder loss: 0.03081603563292572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08369088172912598 0.27573347091674805

Final encoder loss: 0.030396875325894104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08496832847595215 0.2769038677215576

Final encoder loss: 0.03155408171984107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08400774002075195 0.27538061141967773

Final encoder loss: 0.029903768329540645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08384132385253906 0.27707695960998535

Final encoder loss: 0.028331959513671442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08344793319702148 0.2752957344055176

Final encoder loss: 0.02926884244433971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08388161659240723 0.27634239196777344

Final encoder loss: 0.030296122119868155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08723735809326172 0.2756671905517578

Final encoder loss: 0.030235068459284704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08390307426452637 0.2755413055419922

Final encoder loss: 0.029366333255892038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08520889282226562 0.2770392894744873

Final encoder loss: 0.02963184833184523
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.0841526985168457 0.27603626251220703

Final encoder loss: 0.029236033801709027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08345341682434082 0.27631139755249023

Final encoder loss: 0.028910237213426344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08431744575500488 0.275681734085083

Final encoder loss: 0.02873907110896348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.0838160514831543 0.276444673538208

Final encoder loss: 0.030707138435879813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08546233177185059 0.27653002738952637

Final encoder loss: 0.030295547451407764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08393669128417969 0.27529168128967285

Final encoder loss: 0.028684919782087367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08265852928161621 0.2757077217102051


Training case model
Final encoder loss: 0.029554797882905756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09156274795532227 0.26499390602111816

Final encoder loss: 0.028195640176558148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09163403511047363 0.2664611339569092

Final encoder loss: 0.026398726288475113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09406852722167969 0.2649500370025635

Final encoder loss: 0.02596755811584594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09151744842529297 0.26632094383239746

Final encoder loss: 0.025451343720192848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09306073188781738 0.2669365406036377

Final encoder loss: 0.025395073011858012
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09161734580993652 0.26642417907714844

Final encoder loss: 0.025265758076707542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09150457382202148 0.2670412063598633

Final encoder loss: 0.024581593259417396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.09135699272155762 0.26539087295532227

Final encoder loss: 0.024611224725276076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09138083457946777 0.26615190505981445

Final encoder loss: 0.023879753919107828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09341740608215332 0.26562976837158203

Final encoder loss: 0.02386649819029031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09169435501098633 0.2650630474090576

Final encoder loss: 0.02341629487993249
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09353184700012207 0.2668297290802002

Final encoder loss: 0.023173158403673892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09178614616394043 0.2660543918609619

Final encoder loss: 0.02353800981761109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0914463996887207 0.26698851585388184

Final encoder loss: 0.023445539736324357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09102964401245117 0.2653076648712158

Final encoder loss: 0.02372708299294594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08861446380615234 0.26288533210754395


Training amigos model
Final encoder loss: 0.018439861080588083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10790514945983887 0.34156250953674316

Final encoder loss: 0.01732980217021445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1062018871307373 0.34172606468200684

Final encoder loss: 0.01783587640895443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10695481300354004 0.34134793281555176

Final encoder loss: 0.017658713986117406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10628962516784668 0.342602014541626

Final encoder loss: 0.01775431887657703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10698986053466797 0.34154176712036133

Final encoder loss: 0.01680901269084859
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.1061849594116211 0.3417367935180664

Final encoder loss: 0.01736673652523049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10602498054504395 0.34280896186828613

Final encoder loss: 0.017903747754891418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10655474662780762 0.34166669845581055

Final encoder loss: 0.017910236459018104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10590410232543945 0.3416328430175781

Final encoder loss: 0.017397661752762725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10634183883666992 0.34191274642944336

Final encoder loss: 0.017778228485118446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10604667663574219 0.3416557312011719

Final encoder loss: 0.018130581014728784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10630369186401367 0.34165263175964355

Final encoder loss: 0.018162368947544943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10630464553833008 0.34261536598205566

Final encoder loss: 0.01734914879632425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10724496841430664 0.3419175148010254

Final encoder loss: 0.01828224573709841
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10657691955566406 0.3415515422821045

Final encoder loss: 0.016355955516696948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.1024169921875 0.3386802673339844


Training amigos model
Final encoder loss: 0.18077367544174194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47360801696777344 0.08216691017150879

Final encoder loss: 0.18784227967262268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4396064281463623 0.0830540657043457

Final encoder loss: 0.18363840878009796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4371302127838135 0.0781087875366211

Final encoder loss: 0.07777317613363266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4467132091522217 0.07826781272888184

Final encoder loss: 0.07929027825593948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.5219495296478271 0.07838726043701172

Final encoder loss: 0.07397150248289108
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.459453821182251 0.07561731338500977

Final encoder loss: 0.04478641226887703
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4400932788848877 0.08659005165100098

Final encoder loss: 0.04518270120024681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.43637824058532715 0.0819234848022461

Final encoder loss: 0.042726390063762665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4721062183380127 0.07386136054992676

Final encoder loss: 0.03159501776099205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4501762390136719 0.07529997825622559

Final encoder loss: 0.03204959258437157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46544361114501953 0.0764768123626709

Final encoder loss: 0.030991829931735992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.442887544631958 0.07370948791503906

Final encoder loss: 0.02552114985883236
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4533865451812744 0.08189249038696289

Final encoder loss: 0.02595796249806881
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.459043025970459 0.08098912239074707

Final encoder loss: 0.025416770949959755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46210646629333496 0.07783627510070801

Final encoder loss: 0.022648928686976433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4509861469268799 0.0779573917388916

Final encoder loss: 0.023049375042319298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46636080741882324 0.07913875579833984

Final encoder loss: 0.02274886518716812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45915699005126953 0.07489800453186035

Final encoder loss: 0.021482249721884727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4706001281738281 0.08504056930541992

Final encoder loss: 0.02170097455382347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45334410667419434 0.08019542694091797

Final encoder loss: 0.02183551713824272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4550821781158447 0.07516360282897949

Final encoder loss: 0.021094325929880142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44652295112609863 0.0762336254119873

Final encoder loss: 0.021368542686104774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4649786949157715 0.07791328430175781

Final encoder loss: 0.02166234515607357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45981693267822266 0.07463574409484863

Final encoder loss: 0.020608266815543175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47054529190063477 0.08344221115112305

Final encoder loss: 0.020537998527288437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45116710662841797 0.08293461799621582

Final encoder loss: 0.020770763978362083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.42449426651000977 0.0817101001739502

Final encoder loss: 0.019760441035032272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4574124813079834 0.07497262954711914

Final encoder loss: 0.019807303324341774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4550333023071289 0.07442021369934082

Final encoder loss: 0.02008114941418171
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4432103633880615 0.07548737525939941

Final encoder loss: 0.01910223439335823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4559762477874756 0.07471370697021484

Final encoder loss: 0.019309746101498604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.44292759895324707 0.07295989990234375

Final encoder loss: 0.019291959702968597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4452238082885742 0.0765988826751709

Final encoder loss: 0.01874658651649952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45110440254211426 0.07628846168518066

Final encoder loss: 0.018820272758603096
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4632420539855957 0.07453083992004395

Final encoder loss: 0.01911398582160473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4526028633117676 0.07502126693725586

Final encoder loss: 0.01853945292532444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46263837814331055 0.07570338249206543

Final encoder loss: 0.01829877495765686
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4519824981689453 0.07762718200683594

Final encoder loss: 0.018799740821123123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45052027702331543 0.07722783088684082

Final encoder loss: 0.018042372539639473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45057082176208496 0.0783076286315918

Final encoder loss: 0.01800980232656002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4661850929260254 0.07555389404296875

Final encoder loss: 0.01869061216711998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45125699043273926 0.07821154594421387

Final encoder loss: 0.017900845035910606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4650688171386719 0.07532119750976562

Final encoder loss: 0.017827143892645836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4490625858306885 0.07294201850891113

Final encoder loss: 0.018191300332546234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4381527900695801 0.07662582397460938

Final encoder loss: 0.01779332384467125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4487578868865967 0.07823586463928223

Final encoder loss: 0.01780753582715988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4633340835571289 0.07652688026428223

Final encoder loss: 0.017996614798903465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4528837203979492 0.07787966728210449

Final encoder loss: 0.017626535147428513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4638235569000244 0.07760739326477051

Final encoder loss: 0.01754751242697239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4512355327606201 0.07796335220336914

Final encoder loss: 0.01773642562329769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.421588659286499 0.07166028022766113

Final encoder loss: 0.01734892651438713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.450054407119751 0.07673525810241699

Final encoder loss: 0.01729256846010685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4641714096069336 0.08253979682922363

Final encoder loss: 0.017805583775043488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46244096755981445 0.07403707504272461

Final encoder loss: 0.017274338752031326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45677638053894043 0.0756690502166748

Final encoder loss: 0.017139457166194916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4421560764312744 0.0771632194519043

Final encoder loss: 0.017607957124710083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44655275344848633 0.07572245597839355

Final encoder loss: 0.01735284924507141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.44333529472351074 0.07277774810791016

Final encoder loss: 0.016951996833086014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4576592445373535 0.07431936264038086

Final encoder loss: 0.017559468746185303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4434189796447754 0.0772712230682373

Final encoder loss: 0.017125606536865234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46194028854370117 0.07678961753845215

Final encoder loss: 0.01699066162109375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4513590335845947 0.08026409149169922

Final encoder loss: 0.017395202070474625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45459413528442383 0.07391977310180664

Final encoder loss: 0.017051490023732185
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45665526390075684 0.0762169361114502

Final encoder loss: 0.016842393204569817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4629018306732178 0.07729291915893555

Final encoder loss: 0.01745574362576008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45215535163879395 0.0811002254486084

Final encoder loss: 0.016859175637364388
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46724653244018555 0.07683086395263672

Final encoder loss: 0.016793876886367798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45662665367126465 0.07864737510681152

Final encoder loss: 0.017444169148802757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45091795921325684 0.07773852348327637

Final encoder loss: 0.016893737018108368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45214414596557617 0.07882857322692871

Final encoder loss: 0.016767362132668495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.468062162399292 0.07378911972045898

Final encoder loss: 0.01731213554739952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4262988567352295 0.07734298706054688

Final encoder loss: 0.016770629212260246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.450608491897583 0.07491374015808105

Final encoder loss: 0.01659741997718811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45037198066711426 0.08205962181091309

Final encoder loss: 0.017131851986050606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4221470355987549 0.08138275146484375

Final encoder loss: 0.016822071745991707
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45673108100891113 0.07822179794311523

Final encoder loss: 0.0164303258061409
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4649190902709961 0.07751607894897461

Final encoder loss: 0.017021136358380318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.44997549057006836 0.07583069801330566

Final encoder loss: 0.016720781102776527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4644467830657959 0.08209419250488281

Final encoder loss: 0.01648687571287155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45664095878601074 0.07910394668579102

Final encoder loss: 0.01693318970501423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45091843605041504 0.07509541511535645

Final encoder loss: 0.01674516685307026
Final encoder loss: 0.01563454419374466
Final encoder loss: 0.015479886904358864

Training dapper model
Final encoder loss: 0.015227231658255696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.06089067459106445 0.10877299308776855

Final encoder loss: 0.013274366117672928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.06083822250366211 0.10718846321105957

Final encoder loss: 0.017082832725984824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.06001543998718262 0.10772085189819336

Final encoder loss: 0.01455028578326934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.07894778251647949 0.10820960998535156

Final encoder loss: 0.01574141829986196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.06120896339416504 0.10815954208374023

Final encoder loss: 0.01349469414679945
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.0601344108581543 0.10791730880737305

Final encoder loss: 0.014746505361835055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.0597681999206543 0.10745882987976074

Final encoder loss: 0.014979264054122966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.060472965240478516 0.10847306251525879

Final encoder loss: 0.014018875012176791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.059856414794921875 0.10722541809082031

Final encoder loss: 0.015000467927349947
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.0598912239074707 0.10710787773132324

Final encoder loss: 0.013553334282320527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.059923410415649414 0.10804915428161621

Final encoder loss: 0.01620220449289203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.061220407485961914 0.10688328742980957

Final encoder loss: 0.013740468423510118
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05950117111206055 0.10765385627746582

Final encoder loss: 0.014298207544864104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.06028485298156738 0.10798025131225586

Final encoder loss: 0.013540987797852268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.06094980239868164 0.10783076286315918

Final encoder loss: 0.012503277697439691
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05936884880065918 0.1071772575378418


Training dapper model
Final encoder loss: 0.20244143903255463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11787891387939453 0.034430503845214844

Final encoder loss: 0.2081928551197052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11730766296386719 0.03526878356933594

Final encoder loss: 0.08685388416051865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11653351783752441 0.03414726257324219

Final encoder loss: 0.08850736171007156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11565494537353516 0.033902883529663086

Final encoder loss: 0.05095979943871498
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11707282066345215 0.03484177589416504

Final encoder loss: 0.050696078687906265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11837363243103027 0.034375905990600586

Final encoder loss: 0.03442571684718132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11690878868103027 0.0345921516418457

Final encoder loss: 0.034241173416376114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11519145965576172 0.03515052795410156

Final encoder loss: 0.025835327804088593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11848902702331543 0.034178733825683594

Final encoder loss: 0.02586117759346962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1156151294708252 0.034224510192871094

Final encoder loss: 0.021037684753537178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11551690101623535 0.03394961357116699

Final encoder loss: 0.02108684554696083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11812949180603027 0.03478240966796875

Final encoder loss: 0.018208174034953117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11563658714294434 0.03441905975341797

Final encoder loss: 0.0182553268969059
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11500000953674316 0.034288644790649414

Final encoder loss: 0.016451550647616386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11677932739257812 0.03536653518676758

Final encoder loss: 0.016532104462385178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11650657653808594 0.033861398696899414

Final encoder loss: 0.015425784513354301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11569738388061523 0.034155845642089844

Final encoder loss: 0.015440170653164387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11712503433227539 0.03476858139038086

Final encoder loss: 0.014661755412817001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11798787117004395 0.03409218788146973

Final encoder loss: 0.014730571769177914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11576390266418457 0.034561872482299805

Final encoder loss: 0.014274587854743004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11616182327270508 0.0341801643371582

Final encoder loss: 0.014253821223974228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11790251731872559 0.03419685363769531

Final encoder loss: 0.014050912111997604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11664056777954102 0.034509897232055664

Final encoder loss: 0.01396114006638527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11583447456359863 0.03447461128234863

Final encoder loss: 0.014104903675615788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11737513542175293 0.03533291816711426

Final encoder loss: 0.013724282383918762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11551189422607422 0.03508186340332031

Final encoder loss: 0.014060617424547672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11633658409118652 0.03448939323425293

Final encoder loss: 0.013624613173305988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1168363094329834 0.03464102745056152

Final encoder loss: 0.013798066414892673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11774349212646484 0.034178972244262695

Final encoder loss: 0.013399194926023483
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1156768798828125 0.033921241760253906

Final encoder loss: 0.013092989102005959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11584258079528809 0.03489995002746582

Final encoder loss: 0.013213042169809341
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11775517463684082 0.03397083282470703

Final encoder loss: 0.012877751141786575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11604571342468262 0.03438615798950195

Final encoder loss: 0.012786148115992546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11541318893432617 0.03481125831604004

Final encoder loss: 0.012665514834225178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11754226684570312 0.03556203842163086

Final encoder loss: 0.012597331777215004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11523127555847168 0.03393864631652832

Final encoder loss: 0.01243065856397152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11835980415344238 0.03421664237976074

Final encoder loss: 0.012367252260446548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11609053611755371 0.034836769104003906

Final encoder loss: 0.012365156784653664
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11664104461669922 0.034352779388427734

Final encoder loss: 0.012344414368271828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11620259284973145 0.034787893295288086

Final encoder loss: 0.012335739098489285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1163787841796875 0.03505754470825195

Final encoder loss: 0.01231448259204626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11794137954711914 0.03367114067077637

Final encoder loss: 0.012217598967254162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11690592765808105 0.034523725509643555

Final encoder loss: 0.012116532772779465
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11534667015075684 0.03461813926696777

Final encoder loss: 0.012197431176900864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11848616600036621 0.033770084381103516

Final encoder loss: 0.011946706101298332
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11613798141479492 0.034644365310668945

Final encoder loss: 0.012097298167645931
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11542296409606934 0.03519320487976074

Final encoder loss: 0.011877666227519512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11769461631774902 0.03548479080200195

Final encoder loss: 0.011928411200642586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11635828018188477 0.03492093086242676

Final encoder loss: 0.011868113651871681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1154935359954834 0.03429079055786133

Final encoder loss: 0.011769724078476429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11668848991394043 0.035323143005371094

Final encoder loss: 0.011744838207960129
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11703324317932129 0.03503894805908203

Final encoder loss: 0.011735497042536736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11605072021484375 0.03472542762756348

Final encoder loss: 0.011669901199638844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11548662185668945 0.034201622009277344

Final encoder loss: 0.011752098798751831
Final encoder loss: 0.010865890420973301

Training case model
Final encoder loss: 0.02057690051838902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08930444717407227 0.2193770408630371

Final encoder loss: 0.02028043402137486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.09068441390991211 0.21909332275390625

Final encoder loss: 0.019779428427468798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0893239974975586 0.21939516067504883

Final encoder loss: 0.02019533685573017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.09065747261047363 0.2191905975341797

Final encoder loss: 0.02012911458989591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08971476554870605 0.21940398216247559

Final encoder loss: 0.020658193069150198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08965516090393066 0.21938157081604004

Final encoder loss: 0.020312337658239532
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08979654312133789 0.21910643577575684

Final encoder loss: 0.02013620093876539
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08922362327575684 0.2189035415649414

Final encoder loss: 0.01966251382118821
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08984804153442383 0.21918320655822754

Final encoder loss: 0.020563989708743534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08959484100341797 0.2192831039428711

Final encoder loss: 0.019677080683007164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08975028991699219 0.21943283081054688

Final encoder loss: 0.02021221626037173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08956766128540039 0.21921992301940918

Final encoder loss: 0.019803269303925473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.09025359153747559 0.21935415267944336

Final encoder loss: 0.02048855700958231
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08967137336730957 0.21925759315490723

Final encoder loss: 0.019791587449500065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.09031105041503906 0.21911025047302246

Final encoder loss: 0.020199366009623753
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08611607551574707 0.21570968627929688


Training case model
Final encoder loss: 0.20296183228492737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27258825302124023 0.05223655700683594

Final encoder loss: 0.1889057755470276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26109814643859863 0.05380511283874512

Final encoder loss: 0.19015462696552277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2606024742126465 0.052962303161621094

Final encoder loss: 0.1921868920326233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2747008800506592 0.05403423309326172

Final encoder loss: 0.18081052601337433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2587299346923828 0.0534970760345459

Final encoder loss: 0.1919250190258026
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25609540939331055 0.0522158145904541

Final encoder loss: 0.1060759425163269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26074934005737305 0.0526578426361084

Final encoder loss: 0.09529208391904831
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2683136463165283 0.0544126033782959

Final encoder loss: 0.0915151834487915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2684619426727295 0.051557302474975586

Final encoder loss: 0.08999323844909668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26798200607299805 0.05175185203552246

Final encoder loss: 0.08159229159355164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2688901424407959 0.05360746383666992

Final encoder loss: 0.0853283628821373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2565157413482666 0.05198264122009277

Final encoder loss: 0.061722613871097565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25894737243652344 0.05233001708984375

Final encoder loss: 0.05557737872004509
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2617313861846924 0.052626609802246094

Final encoder loss: 0.05346645042300224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25818514823913574 0.053513526916503906

Final encoder loss: 0.0535292774438858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26828932762145996 0.05351662635803223

Final encoder loss: 0.04972412809729576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27019214630126953 0.05252981185913086

Final encoder loss: 0.052020687609910965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25689005851745605 0.05080866813659668

Final encoder loss: 0.04279793053865433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2580432891845703 0.053832054138183594

Final encoder loss: 0.03930146247148514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.258939266204834 0.05417275428771973

Final encoder loss: 0.03784601390361786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25873756408691406 0.05281519889831543

Final encoder loss: 0.03844306245446205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2695810794830322 0.05298733711242676

Final encoder loss: 0.03680513799190521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585446834564209 0.052614688873291016

Final encoder loss: 0.0379004143178463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558784484863281 0.053002357482910156

Final encoder loss: 0.033961910754442215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588052749633789 0.05184769630432129

Final encoder loss: 0.03231552243232727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2694516181945801 0.05591750144958496

Final encoder loss: 0.031083498150110245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25765538215637207 0.05257010459899902

Final encoder loss: 0.03192584216594696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2592940330505371 0.05304431915283203

Final encoder loss: 0.03184304013848305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2698194980621338 0.053444862365722656

Final encoder loss: 0.031736064702272415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25621986389160156 0.05286717414855957

Final encoder loss: 0.030205583199858665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2581627368927002 0.05187058448791504

Final encoder loss: 0.029250016435980797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2680778503417969 0.053589820861816406

Final encoder loss: 0.028426704928278923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25951647758483887 0.05206775665283203

Final encoder loss: 0.029125506058335304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.267911434173584 0.054166555404663086

Final encoder loss: 0.029987389221787453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576329708099365 0.05282473564147949

Final encoder loss: 0.029439913108944893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548868656158447 0.052747488021850586

Final encoder loss: 0.027689382433891296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2595055103302002 0.053658246994018555

Final encoder loss: 0.026845496147871017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583119869232178 0.05251741409301758

Final encoder loss: 0.02647288143634796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593057155609131 0.051908016204833984

Final encoder loss: 0.026684174314141273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26970887184143066 0.052670955657958984

Final encoder loss: 0.027394073083996773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25798916816711426 0.05378985404968262

Final encoder loss: 0.026844199746847153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255859375 0.05115675926208496

Final encoder loss: 0.025759050622582436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2593204975128174 0.05245471000671387

Final encoder loss: 0.02530766651034355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2690281867980957 0.05168509483337402

Final encoder loss: 0.024782545864582062
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2689826488494873 0.05477428436279297

Final encoder loss: 0.025020649656653404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28438544273376465 0.05204200744628906

Final encoder loss: 0.026015914976596832
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583763599395752 0.05383801460266113

Final encoder loss: 0.02558327279984951
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2573556900024414 0.051732778549194336

Final encoder loss: 0.024567564949393272
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2585005760192871 0.05316519737243652

Final encoder loss: 0.02422420121729374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26791906356811523 0.05312323570251465

Final encoder loss: 0.02382706291973591
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25893688201904297 0.0520172119140625

Final encoder loss: 0.02391490340232849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.27541232109069824 0.05430769920349121

Final encoder loss: 0.024986451491713524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2573719024658203 0.05266904830932617

Final encoder loss: 0.024292731657624245
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25858330726623535 0.0525052547454834

Final encoder loss: 0.023879101499915123
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2592952251434326 0.05253314971923828

Final encoder loss: 0.023693935945630074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25859642028808594 0.054016828536987305

Final encoder loss: 0.023362785577774048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26860642433166504 0.052032470703125

Final encoder loss: 0.023313364014029503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25858283042907715 0.05368185043334961

Final encoder loss: 0.024500612169504166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2581977844238281 0.053223609924316406

Final encoder loss: 0.023747490718960762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25611329078674316 0.05445146560668945

Final encoder loss: 0.023173654451966286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2590453624725342 0.051583051681518555

Final encoder loss: 0.02296948805451393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2580845355987549 0.05343294143676758

Final encoder loss: 0.02274913340806961
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27689504623413086 0.05605340003967285

Final encoder loss: 0.022640779614448547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25769782066345215 0.05194687843322754

Final encoder loss: 0.023806918412446976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2675285339355469 0.05357551574707031

Final encoder loss: 0.023008672520518303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25737476348876953 0.05219125747680664

Final encoder loss: 0.02279248647391796
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25726747512817383 0.05383181571960449

Final encoder loss: 0.022590838372707367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2829463481903076 0.05296921730041504

Final encoder loss: 0.02228398434817791
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25994205474853516 0.05294466018676758

Final encoder loss: 0.022341737523674965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25991153717041016 0.05220913887023926

Final encoder loss: 0.02342214621603489
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25762009620666504 0.05356454849243164

Final encoder loss: 0.022795217111706734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553985118865967 0.051599979400634766

Final encoder loss: 0.02241308055818081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2601461410522461 0.051433563232421875

Final encoder loss: 0.022274717688560486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2599942684173584 0.053255558013916016

Final encoder loss: 0.021943964064121246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26964354515075684 0.056440114974975586

Final encoder loss: 0.021764300763607025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26755285263061523 0.05242562294006348

Final encoder loss: 0.022843599319458008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25910425186157227 0.053292036056518555

Final encoder loss: 0.02218247391283512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2552366256713867 0.05327439308166504

Final encoder loss: 0.02216806635260582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25824522972106934 0.0525972843170166

Final encoder loss: 0.02198309265077114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.269787073135376 0.05262279510498047

Final encoder loss: 0.021784966811537743
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25977563858032227 0.05235886573791504

Final encoder loss: 0.0216483436524868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2778141498565674 0.05494070053100586

Final encoder loss: 0.022763025015592575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579348087310791 0.05340003967285156

Final encoder loss: 0.02213372103869915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25592803955078125 0.05201387405395508

Final encoder loss: 0.02176019921898842
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25922656059265137 0.05258965492248535

Final encoder loss: 0.021636588498950005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26876211166381836 0.052961111068725586

Final encoder loss: 0.021400662139058113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26757121086120605 0.0529022216796875

Final encoder loss: 0.021241672337055206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2736997604370117 0.05212879180908203

Final encoder loss: 0.022351950407028198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.268338680267334 0.052272796630859375

Final encoder loss: 0.02173704467713833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2548058032989502 0.05278801918029785

Final encoder loss: 0.021762529388070107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2589409351348877 0.05339550971984863

Final encoder loss: 0.021564630791544914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2775449752807617 0.05390119552612305

Final encoder loss: 0.021314803510904312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2580728530883789 0.05419659614562988

Final encoder loss: 0.021164488047361374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2581164836883545 0.052838802337646484

Final encoder loss: 0.022201435640454292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583286762237549 0.05220913887023926

Final encoder loss: 0.02162608876824379
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25742435455322266 0.0516810417175293

Final encoder loss: 0.021386194974184036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571530342102051 0.054158926010131836

Final encoder loss: 0.021295227110385895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2689394950866699 0.05268692970275879

Final encoder loss: 0.02112826332449913
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.27442431449890137 0.052520751953125

Final encoder loss: 0.020840391516685486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26853036880493164 0.052171945571899414

Final encoder loss: 0.0218206737190485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257244348526001 0.05497431755065918

Final encoder loss: 0.021265024319291115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2553548812866211 0.05167889595031738

Final encoder loss: 0.021334022283554077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2589256763458252 0.054277658462524414

Final encoder loss: 0.021194929257035255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25865793228149414 0.05480813980102539

Final encoder loss: 0.020981015637516975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2587471008300781 0.052399635314941406

Final encoder loss: 0.020768214017152786
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2669789791107178 0.053179264068603516

Final encoder loss: 0.021901454776525497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2695944309234619 0.05191397666931152

Final encoder loss: 0.02132556401193142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2549464702606201 0.05383491516113281

Final encoder loss: 0.021087050437927246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25781941413879395 0.05305314064025879

Final encoder loss: 0.020980585366487503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2806832790374756 0.05130147933959961

Final encoder loss: 0.02075965702533722
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2597496509552002 0.05131673812866211

Final encoder loss: 0.020569385960698128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2675766944885254 0.0529634952545166

Final encoder loss: 0.021561911329627037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2584819793701172 0.05179929733276367

Final encoder loss: 0.020943593233823776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2554607391357422 0.051110029220581055

Final encoder loss: 0.021072357892990112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2599966526031494 0.05247092247009277

Final encoder loss: 0.02096538618206978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.28226208686828613 0.05309724807739258

Final encoder loss: 0.020733049139380455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25796008110046387 0.05195498466491699

Final encoder loss: 0.020484546199440956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2582976818084717 0.05123305320739746

Final encoder loss: 0.021528663113713264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2584238052368164 0.05418658256530762

Final encoder loss: 0.020967820659279823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25504398345947266 0.052480459213256836

Final encoder loss: 0.020838558673858643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586956024169922 0.05292940139770508

Final encoder loss: 0.020755460485816002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2593843936920166 0.05217385292053223

Final encoder loss: 0.020579272881150246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25736451148986816 0.05424046516418457

Final encoder loss: 0.02031475491821766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25862956047058105 0.05385589599609375

Final encoder loss: 0.021245203912258148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2692253589630127 0.052678823471069336

Final encoder loss: 0.0207678135484457
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2574329376220703 0.052215576171875

Final encoder loss: 0.020885784178972244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25823450088500977 0.05133795738220215

Final encoder loss: 0.020712807774543762
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25846362113952637 0.05379223823547363

Final encoder loss: 0.02050016075372696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25935840606689453 0.05273151397705078

Final encoder loss: 0.02032809890806675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26819682121276855 0.051133155822753906

Final encoder loss: 0.021383151412010193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2711358070373535 0.05271172523498535

Final encoder loss: 0.02076791413128376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25364136695861816 0.05214953422546387

Final encoder loss: 0.02065422013401985
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25705409049987793 0.051482200622558594

Final encoder loss: 0.020547883585095406
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26563358306884766 0.05188775062561035

Final encoder loss: 0.020371954888105392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25639915466308594 0.0510404109954834

Final encoder loss: 0.0200711227953434
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2655036449432373 0.051522016525268555

Final encoder loss: 0.02102888934314251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27857398986816406 0.053368330001831055

Final encoder loss: 0.02053273655474186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2547793388366699 0.05228137969970703

Final encoder loss: 0.02069208025932312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25701332092285156 0.05402112007141113

Final encoder loss: 0.020535269752144814
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26923370361328125 0.05264759063720703

Final encoder loss: 0.020388266071677208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.273592472076416 0.05307722091674805

Final encoder loss: 0.020085975527763367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574312686920166 0.05263948440551758

Final encoder loss: 0.021092304959893227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26822662353515625 0.052901268005371094

Final encoder loss: 0.020549705252051353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2571139335632324 0.05282425880432129

Final encoder loss: 0.020505551248788834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582409381866455 0.05463910102844238

Final encoder loss: 0.02044914849102497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2674391269683838 0.053058624267578125

Final encoder loss: 0.020228121429681778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25805091857910156 0.051671504974365234

Final encoder loss: 0.019903160631656647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.28591442108154297 0.0534970760345459

Final encoder loss: 0.020924728363752365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591867446899414 0.052651166915893555

Final encoder loss: 0.020358633249998093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550218105316162 0.05467677116394043

Final encoder loss: 0.020518407225608826
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2588160037994385 0.05234169960021973

Final encoder loss: 0.02034892328083515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2589704990386963 0.05388379096984863

Final encoder loss: 0.020172934979200363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25947046279907227 0.05087471008300781

Final encoder loss: 0.019993184134364128
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2680172920227051 0.05458784103393555

Final encoder loss: 0.02100830152630806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579789161682129 0.05187225341796875

Final encoder loss: 0.020409774035215378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25506138801574707 0.053115129470825195

Final encoder loss: 0.0203885305672884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26013851165771484 0.052709341049194336

Final encoder loss: 0.020340988412499428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26824069023132324 0.05475330352783203

Final encoder loss: 0.020076924934983253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2674906253814697 0.05500054359436035

Final encoder loss: 0.01979382149875164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25910449028015137 0.05304741859436035

Final encoder loss: 0.020652396604418755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25780248641967773 0.0537111759185791

Final encoder loss: 0.0202234648168087
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2561831474304199 0.05234789848327637

Final encoder loss: 0.0204204972833395
Final encoder loss: 0.019821474328637123
Final encoder loss: 0.019080111756920815
Final encoder loss: 0.018195899203419685
Final encoder loss: 0.018424218520522118
Final encoder loss: 0.017285162582993507

Training emognition model
Final encoder loss: 0.022930286822372022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08195066452026367 0.23073053359985352

Final encoder loss: 0.022282458607633567
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08202481269836426 0.23054766654968262

Final encoder loss: 0.02456479729134402
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.0812370777130127 0.23105072975158691

Final encoder loss: 0.022859227433774408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08152222633361816 0.23037147521972656

Final encoder loss: 0.022319061213258422
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08075547218322754 0.2305591106414795

Final encoder loss: 0.023417208695248142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08057498931884766 0.2302863597869873

Final encoder loss: 0.02283528659030813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.0809793472290039 0.23111486434936523

Final encoder loss: 0.023508374640534856
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08078956604003906 0.23070120811462402

Final encoder loss: 0.0229398244375162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08209228515625 0.23092079162597656

Final encoder loss: 0.02375710107148799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08190155029296875 0.23068761825561523

Final encoder loss: 0.023816485973870344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08145833015441895 0.23057150840759277

Final encoder loss: 0.023714242018223173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.08098602294921875 0.2307605743408203

Final encoder loss: 0.023270142559624698
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08053159713745117 0.23031973838806152

Final encoder loss: 0.02250079139281797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08081674575805664 0.23068809509277344

Final encoder loss: 0.023615456327867518
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08092069625854492 0.2306680679321289

Final encoder loss: 0.02308553116225213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.08164024353027344 0.23073649406433105


Training emognition model
Final encoder loss: 0.193551704287529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3000004291534424 0.04795384407043457

Final encoder loss: 0.1949693113565445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2493298053741455 0.050371646881103516

Final encoder loss: 0.09055646508932114
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25014376640319824 0.04984760284423828

Final encoder loss: 0.08991744369268417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24779987335205078 0.05017423629760742

Final encoder loss: 0.05706842243671417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25083088874816895 0.049221038818359375

Final encoder loss: 0.05534638464450836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24659442901611328 0.048906564712524414

Final encoder loss: 0.041727516800165176
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24975347518920898 0.04892849922180176

Final encoder loss: 0.04052366316318512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2486581802368164 0.04817843437194824

Final encoder loss: 0.03386734798550606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24823737144470215 0.050095558166503906

Final encoder loss: 0.033101171255111694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2577016353607178 0.04868197441101074

Final encoder loss: 0.029465027153491974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24904656410217285 0.04929828643798828

Final encoder loss: 0.028952889144420624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24868440628051758 0.048859596252441406

Final encoder loss: 0.026891041547060013
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24927878379821777 0.049628257751464844

Final encoder loss: 0.02657061442732811
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24665188789367676 0.04984736442565918

Final encoder loss: 0.025527875870466232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24927783012390137 0.04959917068481445

Final encoder loss: 0.025247689336538315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24821972846984863 0.04984426498413086

Final encoder loss: 0.024690475314855576
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2494063377380371 0.0511937141418457

Final encoder loss: 0.02459423430263996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24769306182861328 0.048918962478637695

Final encoder loss: 0.024249600246548653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24924063682556152 0.05063509941101074

Final encoder loss: 0.024220585823059082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24834871292114258 0.04993104934692383

Final encoder loss: 0.023834386840462685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2475886344909668 0.04958057403564453

Final encoder loss: 0.023968996480107307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24719858169555664 0.04824542999267578

Final encoder loss: 0.023509087041020393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2600135803222656 0.05028891563415527

Final encoder loss: 0.0236138254404068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24616432189941406 0.04985547065734863

Final encoder loss: 0.02312723733484745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24916934967041016 0.04946756362915039

Final encoder loss: 0.02327420935034752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2472834587097168 0.04940676689147949

Final encoder loss: 0.022879736497998238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24933934211730957 0.04953169822692871

Final encoder loss: 0.022997060790657997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24865961074829102 0.04844379425048828

Final encoder loss: 0.022710012272000313
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2485342025756836 0.048941612243652344

Final encoder loss: 0.022828055545687675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24667716026306152 0.04908895492553711

Final encoder loss: 0.022677380591630936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24851274490356445 0.04842996597290039

Final encoder loss: 0.022621873766183853
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24640226364135742 0.04900026321411133

Final encoder loss: 0.022347478196024895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24813604354858398 0.04990053176879883

Final encoder loss: 0.022513536736369133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24552607536315918 0.04970097541809082

Final encoder loss: 0.02224079892039299
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2482442855834961 0.049532175064086914

Final encoder loss: 0.022461995482444763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24550080299377441 0.04884076118469238

Final encoder loss: 0.02201949618756771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24778032302856445 0.04937887191772461

Final encoder loss: 0.022435860708355904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24693036079406738 0.050214290618896484

Final encoder loss: 0.02200194261968136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24860239028930664 0.051616668701171875

Final encoder loss: 0.02223130129277706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24802303314208984 0.04944586753845215

Final encoder loss: 0.021894728764891624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24935269355773926 0.05134773254394531

Final encoder loss: 0.02218211069703102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2464144229888916 0.05069684982299805

Final encoder loss: 0.021983375772833824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24829769134521484 0.05169057846069336

Final encoder loss: 0.02206902951002121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24650335311889648 0.04993033409118652

Final encoder loss: 0.02197076380252838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24867701530456543 0.049744606018066406

Final encoder loss: 0.02201724611222744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2471466064453125 0.05080389976501465

Final encoder loss: 0.02195374108850956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24856925010681152 0.05029153823852539

Final encoder loss: 0.02190101332962513
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24631547927856445 0.05077552795410156

Final encoder loss: 0.02173427678644657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24815082550048828 0.05078244209289551

Final encoder loss: 0.021967992186546326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.31078338623046875 0.04856586456298828

Final encoder loss: 0.021647876128554344
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3911783695220947 0.05073404312133789

Final encoder loss: 0.02187470719218254
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.43402695655822754 0.049468994140625

Final encoder loss: 0.02158157154917717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3700242042541504 0.04841899871826172

Final encoder loss: 0.02187732234597206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3585689067840576 0.04925370216369629

Final encoder loss: 0.021587418392300606
Final encoder loss: 0.021041404455900192

Training empatch model
Final encoder loss: 0.03312646525107408
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.0722811222076416 0.17401504516601562

Final encoder loss: 0.028309037344706793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07139205932617188 0.17354869842529297

Final encoder loss: 0.029991612407291778
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07139849662780762 0.17408156394958496

Final encoder loss: 0.027397184096167506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07196187973022461 0.17423629760742188

Final encoder loss: 0.029129249276240887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07165169715881348 0.17368483543395996

Final encoder loss: 0.029455533210596685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0716240406036377 0.1738743782043457

Final encoder loss: 0.027893749518347735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.0711512565612793 0.17522811889648438

Final encoder loss: 0.028012625821391524
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07163047790527344 0.17360758781433105

Final encoder loss: 0.020517753767471596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07178521156311035 0.17333102226257324

Final encoder loss: 0.020928078501108758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07058882713317871 0.17296600341796875

Final encoder loss: 0.022591599105670147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07067990303039551 0.17264580726623535

Final encoder loss: 0.021222304730748823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07046151161193848 0.17328786849975586

Final encoder loss: 0.02038913399269204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07053661346435547 0.17255711555480957

Final encoder loss: 0.021342993516084714
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.0703728199005127 0.17223334312438965

Final encoder loss: 0.020850310351054193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07023453712463379 0.17207551002502441

Final encoder loss: 0.02314967169692202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07037734985351562 0.17249846458435059


Training empatch model
Final encoder loss: 0.1711600124835968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17568612098693848 0.04383683204650879

Final encoder loss: 0.08138532936573029
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1769733428955078 0.04388093948364258

Final encoder loss: 0.05498892068862915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17397785186767578 0.04316544532775879

Final encoder loss: 0.04206875339150429
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17399191856384277 0.04356193542480469

Final encoder loss: 0.034614674746990204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17450904846191406 0.04333972930908203

Final encoder loss: 0.029967980459332466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1741793155670166 0.0435795783996582

Final encoder loss: 0.026899758726358414
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17412018775939941 0.04407835006713867

Final encoder loss: 0.02479374408721924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17514610290527344 0.04428243637084961

Final encoder loss: 0.023359375074505806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17522430419921875 0.044109344482421875

Final encoder loss: 0.022359415888786316
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17449545860290527 0.04410219192504883

Final encoder loss: 0.021665778011083603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17454075813293457 0.04418206214904785

Final encoder loss: 0.021203355863690376
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2184140682220459 0.04318356513977051

Final encoder loss: 0.0208571907132864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.23429250717163086 0.04405641555786133

Final encoder loss: 0.02057179994881153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2618739604949951 0.042855262756347656

Final encoder loss: 0.020298156887292862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2785613536834717 0.04374885559082031

Final encoder loss: 0.020111428573727608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.25536584854125977 0.04291057586669922

Final encoder loss: 0.019869251176714897
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2296919822692871 0.0445096492767334

Final encoder loss: 0.019695138558745384
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18619227409362793 0.04356193542480469

Final encoder loss: 0.019538966938853264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18459010124206543 0.04338812828063965

Final encoder loss: 0.01943671703338623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18462157249450684 0.0428922176361084

Final encoder loss: 0.019285304471850395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.18433141708374023 0.04498434066772461

Final encoder loss: 0.019254079088568687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.19196009635925293 0.04370594024658203

Final encoder loss: 0.019133536145091057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.28144145011901855 0.04409456253051758

Final encoder loss: 0.019197894260287285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.24048519134521484 0.0439608097076416

Final encoder loss: 0.019132450222969055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.26235365867614746 0.04426884651184082

Final encoder loss: 0.019131740555167198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.23099613189697266 0.04400157928466797

Final encoder loss: 0.01901959255337715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2121295928955078 0.04491162300109863

Final encoder loss: 0.01901174522936344

Training wesad model
Final encoder loss: 0.03162785518456243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07100820541381836 0.17302870750427246

Final encoder loss: 0.029907238322716124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07079005241394043 0.17337346076965332

Final encoder loss: 0.028810446170440908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07053399085998535 0.17352819442749023

Final encoder loss: 0.029202730011182124
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07118535041809082 0.17452192306518555

Final encoder loss: 0.02072934610386942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0728156566619873 0.17465686798095703

Final encoder loss: 0.018937576905592342
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0722956657409668 0.17363595962524414

Final encoder loss: 0.019398683890836207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07123041152954102 0.17378687858581543

Final encoder loss: 0.019560494762460495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07241320610046387 0.1740126609802246

Final encoder loss: 0.014218433510355158
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07110238075256348 0.17339658737182617

Final encoder loss: 0.015112779181294559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07115530967712402 0.17371082305908203

Final encoder loss: 0.01632071614948995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07082104682922363 0.17335295677185059

Final encoder loss: 0.01584143539913972
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0708150863647461 0.17333173751831055

Final encoder loss: 0.012306467022627984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07095456123352051 0.1743934154510498

Final encoder loss: 0.011460142105366725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07175517082214355 0.1748058795928955

Final encoder loss: 0.012830589342358752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0715489387512207 0.1745903491973877

Final encoder loss: 0.013302099052355148
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.0712592601776123 0.17373442649841309


Training wesad model
Final encoder loss: 0.21558867394924164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.18299579620361328 0.03411245346069336

Final encoder loss: 0.09989036619663239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12391090393066406 0.033255577087402344

Final encoder loss: 0.06395862996578217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11926889419555664 0.0336461067199707

Final encoder loss: 0.04587576910853386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1374497413635254 0.03289079666137695

Final encoder loss: 0.035394828766584396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.15196609497070312 0.03323984146118164

Final encoder loss: 0.02893383428454399
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12766051292419434 0.03315997123718262

Final encoder loss: 0.024732673540711403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12226414680480957 0.03328132629394531

Final encoder loss: 0.021885765716433525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1246953010559082 0.03327059745788574

Final encoder loss: 0.019901016727089882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1330866813659668 0.03313899040222168

Final encoder loss: 0.018537206575274467
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.16518020629882812 0.03401470184326172

Final encoder loss: 0.017600994557142258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13552546501159668 0.034089088439941406

Final encoder loss: 0.017011111602187157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12642979621887207 0.03329777717590332

Final encoder loss: 0.016714945435523987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12495064735412598 0.03329300880432129

Final encoder loss: 0.016535911709070206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12218642234802246 0.033028602600097656

Final encoder loss: 0.016433047130703926
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1146848201751709 0.03311634063720703

Final encoder loss: 0.016378074884414673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.14912748336791992 0.0330047607421875

Final encoder loss: 0.016252942383289337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13951635360717773 0.0333712100982666

Final encoder loss: 0.01611870713531971
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.14735913276672363 0.03294944763183594

Final encoder loss: 0.01599160023033619
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12436485290527344 0.03346610069274902

Final encoder loss: 0.015890086069703102
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13184213638305664 0.033386945724487305

Final encoder loss: 0.015881795436143875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11740303039550781 0.033312320709228516

Final encoder loss: 0.01581328734755516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1245877742767334 0.03265070915222168

Final encoder loss: 0.015846263617277145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11846351623535156 0.033075809478759766

Final encoder loss: 0.01579464040696621
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11516451835632324 0.03314614295959473

Final encoder loss: 0.015857387334108353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11804008483886719 0.03274679183959961

Final encoder loss: 0.015759648755192757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12624526023864746 0.03316497802734375

Final encoder loss: 0.01583188585937023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11634206771850586 0.0334162712097168

Final encoder loss: 0.01579563319683075

Calculating loss for amigos model
	Full Pass 0.6976892948150635
numFreeParamsPath 18
Reconstruction loss values: 0.023574531078338623 0.03227352350950241

Calculating loss for dapper model
	Full Pass 0.1513981819152832
numFreeParamsPath 18
Reconstruction loss values: 0.019067935645580292 0.021406883373856544

Calculating loss for case model
	Full Pass 0.8592958450317383
numFreeParamsPath 18
Reconstruction loss values: 0.02800176478922367 0.031020130962133408

Calculating loss for emognition model
	Full Pass 0.2803642749786377
numFreeParamsPath 18
Reconstruction loss values: 0.029766660183668137 0.03745131194591522

Calculating loss for empatch model
	Full Pass 0.10448193550109863
numFreeParamsPath 18
Reconstruction loss values: 0.02955612912774086 0.0367073118686676

Calculating loss for wesad model
	Full Pass 0.07685256004333496
numFreeParamsPath 18
Reconstruction loss values: 0.030517036095261574 0.0472518615424633
Total loss calculation time: 4.743715763092041

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.128540515899658
Total epoch time: 245.48449563980103

Epoch: 55

Training case model
Final encoder loss: 0.027868260079979515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09963321685791016 0.266735315322876

Final encoder loss: 0.02527916162199852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09091043472290039 0.26389145851135254

Final encoder loss: 0.02427662332239612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.09050917625427246 0.2649350166320801

Final encoder loss: 0.02420044512812928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09123826026916504 0.2640187740325928

Final encoder loss: 0.024018574512191273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09040403366088867 0.2631864547729492

Final encoder loss: 0.024340147179000862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09061670303344727 0.26337480545043945

Final encoder loss: 0.023417336744277015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09025192260742188 0.2641913890838623

Final encoder loss: 0.023038766734429193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0907440185546875 0.26407933235168457

Final encoder loss: 0.023459968449183632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09046268463134766 0.2642967700958252

Final encoder loss: 0.023662569431239
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09125113487243652 0.2639908790588379

Final encoder loss: 0.023046965540411347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09075641632080078 0.2639923095703125

Final encoder loss: 0.02220122458331276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09055161476135254 0.26381707191467285

Final encoder loss: 0.023169152743845726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09047341346740723 0.26430702209472656

Final encoder loss: 0.02243035331320011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.09108424186706543 0.26449155807495117

Final encoder loss: 0.02293831196621501
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09086060523986816 0.26476144790649414

Final encoder loss: 0.023299394432659763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08855223655700684 0.261167049407959


Training dapper model
Final encoder loss: 0.017841537650136668
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06149601936340332 0.14882326126098633

Final encoder loss: 0.01842222287649816
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.061473846435546875 0.14825224876403809

Final encoder loss: 0.01722269294329964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.061263322830200195 0.14862680435180664

Final encoder loss: 0.017354360213635216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06122088432312012 0.1484699249267578

Final encoder loss: 0.018623190146687302
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06119251251220703 0.15017032623291016

Final encoder loss: 0.017780202053164367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06308794021606445 0.15259718894958496

Final encoder loss: 0.01754847395511486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.06265783309936523 0.15157270431518555

Final encoder loss: 0.017733131261897055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.06180143356323242 0.15035319328308105

Final encoder loss: 0.01702902589799107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.06173515319824219 0.14890503883361816

Final encoder loss: 0.017877944020216783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.06075620651245117 0.14950203895568848

Final encoder loss: 0.019464144368421366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.061583518981933594 0.14833450317382812

Final encoder loss: 0.01670424564956915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.061011314392089844 0.14925122261047363

Final encoder loss: 0.01691361951668494
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06119108200073242 0.1491718292236328

Final encoder loss: 0.017205096598814442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.06139206886291504 0.14922022819519043

Final encoder loss: 0.016801923095349152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06129288673400879 0.14940714836120605

Final encoder loss: 0.01565514334821866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.060924530029296875 0.14833283424377441


Training amigos model
Final encoder loss: 0.024432584394878675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10830879211425781 0.38788342475891113

Final encoder loss: 0.023916994496902884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10770034790039062 0.38799571990966797

Final encoder loss: 0.024333865882579887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10765409469604492 0.3885500431060791

Final encoder loss: 0.02465887289202803
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10944414138793945 0.39075541496276855

Final encoder loss: 0.021978435018807002
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10801434516906738 0.3886895179748535

Final encoder loss: 0.02416138559009055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10791659355163574 0.3882737159729004

Final encoder loss: 0.024773606866949436
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.1089932918548584 0.3882486820220947

Final encoder loss: 0.02461009963041452
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10784292221069336 0.389007568359375

Final encoder loss: 0.02477143929410729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10839438438415527 0.3890845775604248

Final encoder loss: 0.02473325920733688
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10778141021728516 0.38832569122314453

Final encoder loss: 0.02310826853614097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10807490348815918 0.38837528228759766

Final encoder loss: 0.022273527875694997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10776329040527344 0.3884146213531494

Final encoder loss: 0.02407961704932515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10808181762695312 0.3879098892211914

Final encoder loss: 0.023831063503795127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10788583755493164 0.38939499855041504

Final encoder loss: 0.02504762836587866
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10796332359313965 0.38837361335754395

Final encoder loss: 0.02206845838760863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10222554206848145 0.3840475082397461


Training emognition model
Final encoder loss: 0.03022387487141506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08292341232299805 0.27302122116088867

Final encoder loss: 0.03063169433482227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08267092704772949 0.27364134788513184

Final encoder loss: 0.0287391158455273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08307576179504395 0.27387213706970215

Final encoder loss: 0.03019383228124795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08579492568969727 0.27259302139282227

Final encoder loss: 0.03058721487559935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08264493942260742 0.27402710914611816

Final encoder loss: 0.029119863533339736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.0824117660522461 0.2733423709869385

Final encoder loss: 0.02967353806396125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.0827794075012207 0.2738368511199951

Final encoder loss: 0.02954398741972338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08266639709472656 0.27451419830322266

Final encoder loss: 0.02991596449752552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.08291053771972656 0.2737243175506592

Final encoder loss: 0.02833206337841787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08258938789367676 0.27356863021850586

Final encoder loss: 0.02901339665700995
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08271956443786621 0.2730371952056885

Final encoder loss: 0.029921560175349488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08292555809020996 0.27382969856262207

Final encoder loss: 0.028328004126341677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08251023292541504 0.2736635208129883

Final encoder loss: 0.030531386571730747
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08254623413085938 0.2739529609680176

Final encoder loss: 0.028747775736804277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.0824437141418457 0.2734687328338623

Final encoder loss: 0.02910310976724646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.0815126895904541 0.2723507881164551


Training amigos model
Final encoder loss: 0.017051329115370797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.10617494583129883 0.3409547805786133

Final encoder loss: 0.019016493002049602
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.10593509674072266 0.34060192108154297

Final encoder loss: 0.017667276605568386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10542154312133789 0.3407309055328369

Final encoder loss: 0.01651409641631834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10540056228637695 0.34142088890075684

Final encoder loss: 0.01680577850097206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10634970664978027 0.3417966365814209

Final encoder loss: 0.018734186272560974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10666346549987793 0.34204721450805664

Final encoder loss: 0.017805153273662486
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.1068720817565918 0.34173583984375

Final encoder loss: 0.01830501330583586
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10614919662475586 0.3416280746459961

Final encoder loss: 0.017720704530465748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10593152046203613 0.3407285213470459

Final encoder loss: 0.018045077692090763
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10590600967407227 0.3415796756744385

Final encoder loss: 0.01710355923856702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10595154762268066 0.34076642990112305

Final encoder loss: 0.0183985773993932
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10575675964355469 0.34247756004333496

Final encoder loss: 0.017678272063633788
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.10577130317687988 0.3408639430999756

Final encoder loss: 0.017176730182800328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10586667060852051 0.34096264839172363

Final encoder loss: 0.01776848004727311
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10521411895751953 0.3406102657318115

Final encoder loss: 0.018450271933803268
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10039162635803223 0.3376772403717041


Training amigos model
Final encoder loss: 0.18077312409877777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46408510208129883 0.07769584655761719

Final encoder loss: 0.18782691657543182
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45989537239074707 0.0768742561340332

Final encoder loss: 0.18364132940769196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46143054962158203 0.07470250129699707

Final encoder loss: 0.0779900997877121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45307183265686035 0.07410335540771484

Final encoder loss: 0.0792626291513443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45419979095458984 0.07659339904785156

Final encoder loss: 0.07404976338148117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4569404125213623 0.07535767555236816

Final encoder loss: 0.045038606971502304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4531877040863037 0.07567405700683594

Final encoder loss: 0.04531312361359596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.456040620803833 0.07596206665039062

Final encoder loss: 0.042836856096982956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4555528163909912 0.07503151893615723

Final encoder loss: 0.03184350207448006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45455002784729004 0.07548761367797852

Final encoder loss: 0.03225714713335037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45653676986694336 0.0759282112121582

Final encoder loss: 0.031108448281884193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4547402858734131 0.07331418991088867

Final encoder loss: 0.025690240785479546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4555821418762207 0.07555556297302246

Final encoder loss: 0.026184380054473877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45461130142211914 0.07523870468139648

Final encoder loss: 0.02565598115324974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45461058616638184 0.07305264472961426

Final encoder loss: 0.022692985832691193
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4555013179779053 0.07572150230407715

Final encoder loss: 0.023187747225165367
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4533224105834961 0.07425379753112793

Final encoder loss: 0.023013880476355553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4541754722595215 0.07567739486694336

Final encoder loss: 0.021403608843684196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45413637161254883 0.07602214813232422

Final encoder loss: 0.021909279748797417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4523615837097168 0.07449769973754883

Final encoder loss: 0.02189645729959011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45661354064941406 0.0746922492980957

Final encoder loss: 0.021221062168478966
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4544026851654053 0.0765082836151123

Final encoder loss: 0.021543873474001884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4616048336029053 0.07735419273376465

Final encoder loss: 0.02158321999013424
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4637417793273926 0.07410264015197754

Final encoder loss: 0.020724495872855186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4552342891693115 0.07485485076904297

Final encoder loss: 0.02082982286810875
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45506930351257324 0.0755915641784668

Final encoder loss: 0.020907461643218994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45581626892089844 0.07260370254516602

Final encoder loss: 0.01988530345261097
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.455202579498291 0.07596254348754883

Final encoder loss: 0.01975286193192005
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4535231590270996 0.07726907730102539

Final encoder loss: 0.02003910019993782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4549601078033447 0.07535266876220703

Final encoder loss: 0.01907571218907833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.455472469329834 0.07643675804138184

Final encoder loss: 0.0190363097935915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4547154903411865 0.07636809349060059

Final encoder loss: 0.019321715459227562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4548909664154053 0.0764310359954834

Final encoder loss: 0.018806092441082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4556558132171631 0.07576394081115723

Final encoder loss: 0.018776793032884598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46884965896606445 0.07753944396972656

Final encoder loss: 0.0191335491836071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45842790603637695 0.07586312294006348

Final encoder loss: 0.01832760125398636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45696210861206055 0.07405662536621094

Final encoder loss: 0.018674535676836967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45449376106262207 0.07631540298461914

Final encoder loss: 0.018894199281930923
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4560725688934326 0.07523036003112793

Final encoder loss: 0.0182757880538702
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45457029342651367 0.0753169059753418

Final encoder loss: 0.018352877348661423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4541609287261963 0.07640933990478516

Final encoder loss: 0.018822945654392242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4669606685638428 0.07712674140930176

Final encoder loss: 0.017977429553866386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4684321880340576 0.07909893989562988

Final encoder loss: 0.017712516710162163
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.467282772064209 0.07743000984191895

Final encoder loss: 0.018237100914120674
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4685032367706299 0.07938838005065918

Final encoder loss: 0.01804558001458645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.464949369430542 0.07879233360290527

Final encoder loss: 0.01752224937081337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46778154373168945 0.07792854309082031

Final encoder loss: 0.017991896718740463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4696016311645508 0.0749671459197998

Final encoder loss: 0.017584610730409622
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4672379493713379 0.07883429527282715

Final encoder loss: 0.01745772548019886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46686244010925293 0.08103775978088379

Final encoder loss: 0.01779360882937908
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4650909900665283 0.07650351524353027

Final encoder loss: 0.01740017719566822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4657933712005615 0.07905387878417969

Final encoder loss: 0.0173922311514616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46625518798828125 0.07246828079223633

Final encoder loss: 0.017734671011567116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4679875373840332 0.07441997528076172

Final encoder loss: 0.01739455573260784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45793676376342773 0.07461905479431152

Final encoder loss: 0.01722550019621849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4658675193786621 0.07614564895629883

Final encoder loss: 0.017648305743932724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45975494384765625 0.07509660720825195

Final encoder loss: 0.017221327871084213
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45876598358154297 0.07599401473999023

Final encoder loss: 0.017001323401927948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45459890365600586 0.07642436027526855

Final encoder loss: 0.01749086193740368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45607972145080566 0.0724954605102539

Final encoder loss: 0.017161868512630463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4546382427215576 0.07574748992919922

Final encoder loss: 0.01690143719315529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45371341705322266 0.0750730037689209

Final encoder loss: 0.017467493191361427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45537900924682617 0.07370734214782715

Final encoder loss: 0.01702152006328106
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4550597667694092 0.07633709907531738

Final encoder loss: 0.016842307522892952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4513218402862549 0.07638740539550781

Final encoder loss: 0.017358629032969475
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45431995391845703 0.07366442680358887

Final encoder loss: 0.016938060522079468
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45317554473876953 0.0769190788269043

Final encoder loss: 0.016889559105038643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4519953727722168 0.07713890075683594

Final encoder loss: 0.017318854108452797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45628881454467773 0.07405853271484375

Final encoder loss: 0.01680244691669941
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4537534713745117 0.07496523857116699

Final encoder loss: 0.01677899807691574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4539937973022461 0.07467794418334961

Final encoder loss: 0.01723066344857216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4546167850494385 0.07395124435424805

Final encoder loss: 0.01672348566353321
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4532742500305176 0.07593202590942383

Final encoder loss: 0.016544660553336143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4557771682739258 0.07389283180236816

Final encoder loss: 0.017103584483265877
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45442914962768555 0.07582521438598633

Final encoder loss: 0.016688620671629906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4531893730163574 0.07427716255187988

Final encoder loss: 0.016452547162771225
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4531238079071045 0.07518219947814941

Final encoder loss: 0.017027080059051514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45450282096862793 0.07420516014099121

Final encoder loss: 0.01676478236913681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45319342613220215 0.07589507102966309

Final encoder loss: 0.01629367657005787
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46471381187438965 0.0774688720703125

Final encoder loss: 0.017096251249313354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45664548873901367 0.07281875610351562

Final encoder loss: 0.016744142398238182
Final encoder loss: 0.015602284111082554
Final encoder loss: 0.015485863201320171

Training dapper model
Final encoder loss: 0.013796926477739111
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.060860633850097656 0.1087186336517334

Final encoder loss: 0.015450978797888872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.060738563537597656 0.1077113151550293

Final encoder loss: 0.01699366848317766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.060369014739990234 0.1068108081817627

Final encoder loss: 0.01408429646005147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.059378862380981445 0.10638260841369629

Final encoder loss: 0.016964388685668994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05888772010803223 0.10627913475036621

Final encoder loss: 0.012891614372451957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.05918526649475098 0.10637760162353516

Final encoder loss: 0.014238692276848023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.05871915817260742 0.10626697540283203

Final encoder loss: 0.013239280710770261
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05898857116699219 0.10613775253295898

Final encoder loss: 0.014153108315565878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.05891156196594238 0.10644721984863281

Final encoder loss: 0.01280402266252659
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05904746055603027 0.1057882308959961

Final encoder loss: 0.013515079803763338
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05911898612976074 0.1060481071472168

Final encoder loss: 0.013184799392750378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05912327766418457 0.10664129257202148

Final encoder loss: 0.013626286261807808
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.059325456619262695 0.10662412643432617

Final encoder loss: 0.013733770965852534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05910491943359375 0.10654377937316895

Final encoder loss: 0.013872150747209792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05924582481384277 0.10602331161499023

Final encoder loss: 0.012895653263032143
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.058715105056762695 0.10581493377685547


Training dapper model
Final encoder loss: 0.20244744420051575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11768150329589844 0.03542280197143555

Final encoder loss: 0.20819637179374695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11690473556518555 0.03437662124633789

Final encoder loss: 0.08643487840890884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559534072875977 0.034300804138183594

Final encoder loss: 0.08795860409736633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559462547302246 0.03420114517211914

Final encoder loss: 0.051122210919857025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1156926155090332 0.03405594825744629

Final encoder loss: 0.05075722187757492
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11519503593444824 0.03351020812988281

Final encoder loss: 0.03468456491827965
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11498594284057617 0.03397178649902344

Final encoder loss: 0.0343661829829216
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11593317985534668 0.03422069549560547

Final encoder loss: 0.026061618700623512
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11635589599609375 0.03506827354431152

Final encoder loss: 0.025888336822390556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11749124526977539 0.03481912612915039

Final encoder loss: 0.021156907081604004
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11562180519104004 0.034186601638793945

Final encoder loss: 0.02108471281826496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527752876281738 0.03430461883544922

Final encoder loss: 0.01825789175927639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11586904525756836 0.03366899490356445

Final encoder loss: 0.018198184669017792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11551237106323242 0.03399395942687988

Final encoder loss: 0.0165200587362051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11451387405395508 0.033980369567871094

Final encoder loss: 0.016464803367853165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1156613826751709 0.034421443939208984

Final encoder loss: 0.015408179722726345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11559247970581055 0.033888816833496094

Final encoder loss: 0.015414925292134285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1153867244720459 0.03374004364013672

Final encoder loss: 0.014676294289529324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11570596694946289 0.03369283676147461

Final encoder loss: 0.014756397344172001
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152336597442627 0.03424882888793945

Final encoder loss: 0.01442177314311266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11474442481994629 0.033826351165771484

Final encoder loss: 0.0142534663900733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11614203453063965 0.034285783767700195

Final encoder loss: 0.014391276054084301
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11462855339050293 0.03392457962036133

Final encoder loss: 0.014010128565132618
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11536049842834473 0.03420686721801758

Final encoder loss: 0.014119964092969894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11626839637756348 0.03394937515258789

Final encoder loss: 0.013880747370421886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159219741821289 0.03413224220275879

Final encoder loss: 0.014030097052454948
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11509156227111816 0.03467082977294922

Final encoder loss: 0.013826890848577023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11592912673950195 0.03393912315368652

Final encoder loss: 0.013735424727201462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11467432975769043 0.033383846282958984

Final encoder loss: 0.013463192619383335
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11522197723388672 0.034337759017944336

Final encoder loss: 0.013264180161058903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11536693572998047 0.03387784957885742

Final encoder loss: 0.013325025327503681
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11522173881530762 0.03378009796142578

Final encoder loss: 0.012891384772956371
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11490201950073242 0.03427481651306152

Final encoder loss: 0.012978383339941502
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11611580848693848 0.03443193435668945

Final encoder loss: 0.012703624553978443
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11439371109008789 0.033692121505737305

Final encoder loss: 0.012557607144117355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11579012870788574 0.033617496490478516

Final encoder loss: 0.012666678056120872
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11586570739746094 0.03376269340515137

Final encoder loss: 0.012305106036365032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11565804481506348 0.03426718711853027

Final encoder loss: 0.012505263090133667
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580991744995117 0.03412485122680664

Final encoder loss: 0.012248861603438854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1159677505493164 0.03404831886291504

Final encoder loss: 0.012509437277913094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11495471000671387 0.03415703773498535

Final encoder loss: 0.012330224737524986
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11561775207519531 0.034468650817871094

Final encoder loss: 0.012372787110507488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11586761474609375 0.03374767303466797

Final encoder loss: 0.01211525872349739
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11533021926879883 0.03421354293823242

Final encoder loss: 0.012067458592355251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11460471153259277 0.03392148017883301

Final encoder loss: 0.01207813248038292
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11534833908081055 0.03347611427307129

Final encoder loss: 0.011895799078047276
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11517500877380371 0.03408241271972656

Final encoder loss: 0.011799197643995285
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11542868614196777 0.034374237060546875

Final encoder loss: 0.01197376474738121
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11520695686340332 0.03395247459411621

Final encoder loss: 0.011824467219412327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11584806442260742 0.034275054931640625

Final encoder loss: 0.011940385214984417
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11470198631286621 0.03424835205078125

Final encoder loss: 0.011772810481488705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11559391021728516 0.03442263603210449

Final encoder loss: 0.011999974027276039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1153573989868164 0.034087419509887695

Final encoder loss: 0.011722265742719173
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11537861824035645 0.03435087203979492

Final encoder loss: 0.011882997117936611
Final encoder loss: 0.01079277042299509

Training case model
Final encoder loss: 0.020994773634866223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08912324905395508 0.21860337257385254

Final encoder loss: 0.021707426470626075
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08989882469177246 0.2180335521697998

Final encoder loss: 0.02090281438239431
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.0885164737701416 0.218414306640625

Final encoder loss: 0.020573826206992527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08887195587158203 0.21828484535217285

Final encoder loss: 0.02092902521449253
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08861732482910156 0.2181384563446045

Final encoder loss: 0.01992215224571448
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.0886378288269043 0.21819067001342773

Final encoder loss: 0.020677055448299227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08886289596557617 0.21840667724609375

Final encoder loss: 0.020261163296987
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08861279487609863 0.21850919723510742

Final encoder loss: 0.0200861217411383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08881878852844238 0.21865057945251465

Final encoder loss: 0.020351533711199438
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08885455131530762 0.21848773956298828

Final encoder loss: 0.020806033743395274
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08888602256774902 0.21834254264831543

Final encoder loss: 0.019732446434396318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08885550498962402 0.2184288501739502

Final encoder loss: 0.020451885837088575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08861827850341797 0.21830391883850098

Final encoder loss: 0.020272735431458733
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08869242668151855 0.21805691719055176

Final encoder loss: 0.02041016072412817
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08901810646057129 0.21889948844909668

Final encoder loss: 0.020395956841055347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.0858774185180664 0.2154097557067871


Training case model
Final encoder loss: 0.20296387374401093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27170753479003906 0.0527646541595459

Final encoder loss: 0.18890579044818878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25978994369506836 0.05236315727233887

Final encoder loss: 0.19015666842460632
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2584092617034912 0.054215192794799805

Final encoder loss: 0.19218352437019348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.267902135848999 0.052902936935424805

Final encoder loss: 0.1808137744665146
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25751614570617676 0.05290412902832031

Final encoder loss: 0.19192758202552795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25587916374206543 0.052004098892211914

Final encoder loss: 0.10630422830581665
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25697803497314453 0.05138421058654785

Final encoder loss: 0.09555359184741974
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2583913803100586 0.05294919013977051

Final encoder loss: 0.09194004535675049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25653743743896484 0.05234956741333008

Final encoder loss: 0.09010478109121323
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25793004035949707 0.051557302474975586

Final encoder loss: 0.0815330371260643
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2580862045288086 0.05357170104980469

Final encoder loss: 0.08548253774642944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.255678653717041 0.051470279693603516

Final encoder loss: 0.06200353801250458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25698065757751465 0.053374290466308594

Final encoder loss: 0.05590660870075226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2681925296783447 0.05267500877380371

Final encoder loss: 0.05382940173149109
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572958469390869 0.05291891098022461

Final encoder loss: 0.05367716774344444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26688051223754883 0.053484201431274414

Final encoder loss: 0.04987164959311485
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25817060470581055 0.05235910415649414

Final encoder loss: 0.05218507722020149
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.254194974899292 0.05192995071411133

Final encoder loss: 0.04306197538971901
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.257190465927124 0.052327632904052734

Final encoder loss: 0.03937983512878418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2569739818572998 0.053476572036743164

Final encoder loss: 0.03813382610678673
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570042610168457 0.05209016799926758

Final encoder loss: 0.03839804604649544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25620460510253906 0.05182814598083496

Final encoder loss: 0.036882802844047546
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2578887939453125 0.051106929779052734

Final encoder loss: 0.03799687325954437
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25367116928100586 0.05065655708312988

Final encoder loss: 0.03424971550703049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25692129135131836 0.052004098892211914

Final encoder loss: 0.03217528760433197
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2567408084869385 0.05252671241760254

Final encoder loss: 0.031365033239126205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2664375305175781 0.05212140083312988

Final encoder loss: 0.0317855067551136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2585768699645996 0.05304074287414551

Final encoder loss: 0.03167448937892914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2666130065917969 0.05176544189453125

Final encoder loss: 0.03172016143798828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534186840057373 0.05217385292053223

Final encoder loss: 0.030190447345376015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2589435577392578 0.052808284759521484

Final encoder loss: 0.029174968600273132
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566852569580078 0.05219602584838867

Final encoder loss: 0.02870827354490757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25798606872558594 0.052393436431884766

Final encoder loss: 0.028910646215081215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25614070892333984 0.051694631576538086

Final encoder loss: 0.02988923341035843
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.257540225982666 0.054152727127075195

Final encoder loss: 0.029333345592021942
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2550654411315918 0.052466630935668945

Final encoder loss: 0.02779814787209034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566676139831543 0.05324292182922363

Final encoder loss: 0.026783397421240807
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25729799270629883 0.05200934410095215

Final encoder loss: 0.026500573381781578
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572824954986572 0.052850961685180664

Final encoder loss: 0.026666760444641113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25682711601257324 0.05293416976928711

Final encoder loss: 0.027531245723366737
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25736260414123535 0.053000450134277344

Final encoder loss: 0.026916520670056343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544121742248535 0.05269885063171387

Final encoder loss: 0.025708826258778572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2564671039581299 0.05269885063171387

Final encoder loss: 0.025030547752976418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2714095115661621 0.05277419090270996

Final encoder loss: 0.024783052504062653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.256605863571167 0.052644968032836914

Final encoder loss: 0.024918343871831894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2561194896697998 0.053192138671875

Final encoder loss: 0.025892505422234535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2561781406402588 0.05205726623535156

Final encoder loss: 0.02533653751015663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2545506954193115 0.05160260200500488

Final encoder loss: 0.02464504912495613
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25744056701660156 0.052215576171875

Final encoder loss: 0.024209091439843178
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566220760345459 0.053215980529785156

Final encoder loss: 0.0238876361399889
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25768280029296875 0.052306175231933594

Final encoder loss: 0.02393900789320469
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.256711483001709 0.05308127403259277

Final encoder loss: 0.02508608065545559
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25673961639404297 0.052247047424316406

Final encoder loss: 0.024296527728438377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25537729263305664 0.05185198783874512

Final encoder loss: 0.023913782089948654
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25847721099853516 0.052427053451538086

Final encoder loss: 0.023443588986992836
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25787973403930664 0.052215576171875

Final encoder loss: 0.023369895294308662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2575395107269287 0.0520627498626709

Final encoder loss: 0.023334918543696404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635433197021484 0.05202841758728027

Final encoder loss: 0.024621346965432167
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.258408784866333 0.052255868911743164

Final encoder loss: 0.0236891470849514
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25512051582336426 0.05096578598022461

Final encoder loss: 0.023211048915982246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2583332061767578 0.05202841758728027

Final encoder loss: 0.02287265658378601
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.256572961807251 0.05129098892211914

Final encoder loss: 0.022787636145949364
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25867509841918945 0.05231618881225586

Final encoder loss: 0.022699253633618355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25586843490600586 0.05193805694580078

Final encoder loss: 0.023754296824336052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2591099739074707 0.052904605865478516

Final encoder loss: 0.022961029782891273
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2579164505004883 0.051244258880615234

Final encoder loss: 0.022704847157001495
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25755739212036133 0.052420616149902344

Final encoder loss: 0.022483069449663162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2594418525695801 0.05243968963623047

Final encoder loss: 0.02236795797944069
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2580866813659668 0.052454233169555664

Final encoder loss: 0.022234205156564713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26257920265197754 0.05274653434753418

Final encoder loss: 0.02328088879585266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2655069828033447 0.05186724662780762

Final encoder loss: 0.02269216626882553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25525784492492676 0.052306413650512695

Final encoder loss: 0.022385355085134506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582359313964844 0.05252885818481445

Final encoder loss: 0.022143518552184105
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2575037479400635 0.052022695541381836

Final encoder loss: 0.022081270813941956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25684237480163574 0.052709102630615234

Final encoder loss: 0.02192552760243416
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572157382965088 0.052613258361816406

Final encoder loss: 0.022944970056414604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25693845748901367 0.05248689651489258

Final encoder loss: 0.022243637591600418
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25397682189941406 0.052124977111816406

Final encoder loss: 0.022081345319747925
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2565124034881592 0.05172848701477051

Final encoder loss: 0.021837498992681503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25870370864868164 0.05346989631652832

Final encoder loss: 0.02176600880920887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25714755058288574 0.05134177207946777

Final encoder loss: 0.021591894328594208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2564256191253662 0.05336737632751465

Final encoder loss: 0.02273387648165226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2562992572784424 0.054695844650268555

Final encoder loss: 0.021922260522842407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558097839355469 0.05300784111022949

Final encoder loss: 0.02178858034312725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2596745491027832 0.0538637638092041

Final encoder loss: 0.02163315936923027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2584993839263916 0.05276751518249512

Final encoder loss: 0.021472202613949776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2596862316131592 0.0524449348449707

Final encoder loss: 0.021280091255903244
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25777769088745117 0.05384254455566406

Final encoder loss: 0.02226470597088337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25978899002075195 0.0522618293762207

Final encoder loss: 0.02169778384268284
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2562260627746582 0.054662466049194336

Final encoder loss: 0.021571114659309387
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2591557502746582 0.05168485641479492

Final encoder loss: 0.021424224600195885
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2592005729675293 0.05394291877746582

Final encoder loss: 0.021376920863986015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25876283645629883 0.05279350280761719

Final encoder loss: 0.021091338247060776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2693338394165039 0.05324840545654297

Final encoder loss: 0.022149356082081795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.27452611923217773 0.05529046058654785

Final encoder loss: 0.02151273377239704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556467056274414 0.053522586822509766

Final encoder loss: 0.02145286835730076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2591378688812256 0.05277514457702637

Final encoder loss: 0.021274099126458168
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25865745544433594 0.05303764343261719

Final encoder loss: 0.02114632911980152
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2585103511810303 0.05266833305358887

Final encoder loss: 0.020988797768950462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25774383544921875 0.05245327949523926

Final encoder loss: 0.02197791449725628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2585916519165039 0.05147838592529297

Final encoder loss: 0.02129332162439823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2555820941925049 0.051873207092285156

Final encoder loss: 0.02124910056591034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25850462913513184 0.05238676071166992

Final encoder loss: 0.021063916385173798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568686008453369 0.05324244499206543

Final encoder loss: 0.021051833406090736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2583334445953369 0.05248546600341797

Final encoder loss: 0.020690059289336205
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570059299468994 0.05287623405456543

Final encoder loss: 0.021744046360254288
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26019740104675293 0.05336403846740723

Final encoder loss: 0.0211178008466959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551913261413574 0.05146145820617676

Final encoder loss: 0.021153487265110016
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25631189346313477 0.05224204063415527

Final encoder loss: 0.02091379091143608
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25821971893310547 0.05231785774230957

Final encoder loss: 0.020906466990709305
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25811171531677246 0.05325722694396973

Final encoder loss: 0.020589660853147507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2571423053741455 0.052274227142333984

Final encoder loss: 0.021576276049017906
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25705480575561523 0.053322792053222656

Final encoder loss: 0.021003482863307
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2544095516204834 0.05149531364440918

Final encoder loss: 0.02097567357122898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25768208503723145 0.05272340774536133

Final encoder loss: 0.020885588601231575
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25610995292663574 0.05284881591796875

Final encoder loss: 0.020787090063095093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2568850517272949 0.05131387710571289

Final encoder loss: 0.02046421356499195
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2561953067779541 0.05291032791137695

Final encoder loss: 0.02153000421822071
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25782060623168945 0.05202794075012207

Final encoder loss: 0.020932478830218315
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541220188140869 0.051609039306640625

Final encoder loss: 0.020875589922070503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2572641372680664 0.05149364471435547

Final encoder loss: 0.02074269764125347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26712584495544434 0.05291247367858887

Final encoder loss: 0.020636044442653656
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2716851234436035 0.05229592323303223

Final encoder loss: 0.020390843972563744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2662353515625 0.05271720886230469

Final encoder loss: 0.021331964060664177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2574925422668457 0.05282282829284668

Final encoder loss: 0.020763278007507324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25379085540771484 0.05125737190246582

Final encoder loss: 0.02078314498066902
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25794315338134766 0.051740169525146484

Final encoder loss: 0.020611394196748734
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2561676502227783 0.05305290222167969

Final encoder loss: 0.02056824415922165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25872182846069336 0.05191636085510254

Final encoder loss: 0.020212773233652115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25725650787353516 0.05222368240356445

Final encoder loss: 0.021216198801994324
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25774240493774414 0.05381917953491211

Final encoder loss: 0.020608926191926003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2539389133453369 0.051406145095825195

Final encoder loss: 0.020697010681033134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25624847412109375 0.05294013023376465

Final encoder loss: 0.020473673939704895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25704336166381836 0.0515444278717041

Final encoder loss: 0.020458003506064415
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25635838508605957 0.052664756774902344

Final encoder loss: 0.02010166645050049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2682044506072998 0.05270743370056152

Final encoder loss: 0.021076399832963943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2657127380371094 0.05187511444091797

Final encoder loss: 0.020588507875800133
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25437092781066895 0.05185341835021973

Final encoder loss: 0.020625820383429527
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2557239532470703 0.05244326591491699

Final encoder loss: 0.020496265962719917
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2569582462310791 0.05301237106323242

Final encoder loss: 0.02039501629769802
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25815343856811523 0.05299186706542969

Final encoder loss: 0.02006346359848976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25719428062438965 0.05242204666137695

Final encoder loss: 0.020989635959267616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2579958438873291 0.05211901664733887

Final encoder loss: 0.020488718524575233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25320935249328613 0.05167245864868164

Final encoder loss: 0.020489294081926346
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571072578430176 0.0520939826965332

Final encoder loss: 0.020389027893543243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2573552131652832 0.05319976806640625

Final encoder loss: 0.020367806777358055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25650858879089355 0.053002357482910156

Final encoder loss: 0.019986676052212715
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.26519155502319336 0.05183839797973633

Final encoder loss: 0.020919017493724823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2580106258392334 0.053499698638916016

Final encoder loss: 0.02034720778465271
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25623655319213867 0.0515599250793457

Final encoder loss: 0.02052145265042782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2582979202270508 0.052907705307006836

Final encoder loss: 0.020298488438129425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2565732002258301 0.052759647369384766

Final encoder loss: 0.020244162529706955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570028305053711 0.0512847900390625

Final encoder loss: 0.0198855921626091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2561452388763428 0.05268049240112305

Final encoder loss: 0.020957212895154953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25788044929504395 0.052263498306274414

Final encoder loss: 0.020302601158618927
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.252744197845459 0.05209970474243164

Final encoder loss: 0.020377248525619507
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.267728328704834 0.05452990531921387

Final encoder loss: 0.020228147506713867
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257188081741333 0.05355572700500488

Final encoder loss: 0.02017820067703724
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2572619915008545 0.052614688873291016

Final encoder loss: 0.01979827880859375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.256655216217041 0.05232810974121094

Final encoder loss: 0.02075834386050701
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25572872161865234 0.052306413650512695

Final encoder loss: 0.02025267481803894
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2683525085449219 0.05164670944213867

Final encoder loss: 0.020333299413323402
Final encoder loss: 0.019793493673205376
Final encoder loss: 0.019181618466973305
Final encoder loss: 0.018242089077830315
Final encoder loss: 0.018415991216897964
Final encoder loss: 0.01725153438746929

Training emognition model
Final encoder loss: 0.022582904425612844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08146405220031738 0.22966766357421875

Final encoder loss: 0.02323981624956806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08087420463562012 0.22971630096435547

Final encoder loss: 0.023116452939625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08070087432861328 0.22964763641357422

Final encoder loss: 0.022582024720153224
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08041691780090332 0.2294754981994629

Final encoder loss: 0.02286298732767389
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08059096336364746 0.22915029525756836

Final encoder loss: 0.022631677256540354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08042359352111816 0.2296144962310791

Final encoder loss: 0.02190787830699748
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08051443099975586 0.2294635772705078

Final encoder loss: 0.022546978477414444
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08057236671447754 0.2295079231262207

Final encoder loss: 0.0222716372276812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08046269416809082 0.22931909561157227

Final encoder loss: 0.023540788205834103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08035993576049805 0.22983765602111816

Final encoder loss: 0.023944587233629334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08073282241821289 0.22920989990234375

Final encoder loss: 0.02420176026059189
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.0805361270904541 0.22958087921142578

Final encoder loss: 0.022294743728802777
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.0804891586303711 0.2294313907623291

Final encoder loss: 0.0226264645318893
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08070993423461914 0.22970819473266602

Final encoder loss: 0.022560991567543234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08052539825439453 0.22970938682556152

Final encoder loss: 0.023063765354435255
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07968688011169434 0.22908687591552734


Training emognition model
Final encoder loss: 0.19356143474578857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25963544845581055 0.04894757270812988

Final encoder loss: 0.1949581354856491
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24732112884521484 0.04852557182312012

Final encoder loss: 0.090879425406456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2486252784729004 0.049468040466308594

Final encoder loss: 0.0900941789150238
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468733787536621 0.04915046691894531

Final encoder loss: 0.05731375515460968
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24925780296325684 0.04837369918823242

Final encoder loss: 0.055619172751903534
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24741530418395996 0.0494074821472168

Final encoder loss: 0.041960809379816055
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24779391288757324 0.048879146575927734

Final encoder loss: 0.040711063891649246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24656391143798828 0.048773765563964844

Final encoder loss: 0.033959295600652695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24835681915283203 0.04894566535949707

Final encoder loss: 0.03317274898290634
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24612164497375488 0.04925346374511719

Final encoder loss: 0.02948623150587082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2484292984008789 0.05024266242980957

Final encoder loss: 0.028974270448088646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24641752243041992 0.0489649772644043

Final encoder loss: 0.026904629543423653
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.25212764739990234 0.0499720573425293

Final encoder loss: 0.026497015729546547
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24648809432983398 0.04868626594543457

Final encoder loss: 0.02544158138334751
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24753832817077637 0.0490267276763916

Final encoder loss: 0.025141779333353043
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.246138334274292 0.04832339286804199

Final encoder loss: 0.024751540273427963
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24810791015625 0.04949545860290527

Final encoder loss: 0.02444443851709366
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24708271026611328 0.04832601547241211

Final encoder loss: 0.02424749918282032
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24797987937927246 0.050415992736816406

Final encoder loss: 0.02421376295387745
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24509096145629883 0.04886984825134277

Final encoder loss: 0.023725200444459915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24832653999328613 0.0486750602722168

Final encoder loss: 0.023858830332756042
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24640607833862305 0.049916744232177734

Final encoder loss: 0.023350344970822334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2488880157470703 0.04920077323913574

Final encoder loss: 0.023456169292330742
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468721866607666 0.04830050468444824

Final encoder loss: 0.02304457314312458
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24856877326965332 0.04984641075134277

Final encoder loss: 0.02306102216243744
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24599027633666992 0.04918646812438965

Final encoder loss: 0.022972233593463898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24710464477539062 0.04935312271118164

Final encoder loss: 0.022825313732028008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24686956405639648 0.04928302764892578

Final encoder loss: 0.0227139163762331
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24692082405090332 0.04835367202758789

Final encoder loss: 0.022658204659819603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.245314359664917 0.04957270622253418

Final encoder loss: 0.022567536681890488
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24805831909179688 0.05034661293029785

Final encoder loss: 0.022527216002345085
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2463371753692627 0.05009126663208008

Final encoder loss: 0.02206549048423767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24780631065368652 0.0493624210357666

Final encoder loss: 0.02235705405473709
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24749112129211426 0.04912114143371582

Final encoder loss: 0.02206544764339924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24842619895935059 0.050211429595947266

Final encoder loss: 0.0222926028072834
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24668169021606445 0.04781389236450195

Final encoder loss: 0.021952010691165924
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24877524375915527 0.04970526695251465

Final encoder loss: 0.022214051336050034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24654483795166016 0.04916262626647949

Final encoder loss: 0.021994316950440407
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24797892570495605 0.04940605163574219

Final encoder loss: 0.022074993699789047
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24614715576171875 0.04969930648803711

Final encoder loss: 0.021767329424619675
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24970531463623047 0.04968905448913574

Final encoder loss: 0.021941563114523888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24546122550964355 0.04907345771789551

Final encoder loss: 0.021752135828137398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24841570854187012 0.049306392669677734

Final encoder loss: 0.021866030991077423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24697589874267578 0.04833531379699707

Final encoder loss: 0.02154535800218582
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24880433082580566 0.049685001373291016

Final encoder loss: 0.02183043770492077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2469799518585205 0.04871654510498047

Final encoder loss: 0.02160700038075447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24813294410705566 0.049333810806274414

Final encoder loss: 0.02180887572467327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2781386375427246 0.049455881118774414

Final encoder loss: 0.021513959392905235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.39789342880249023 0.049226999282836914

Final encoder loss: 0.021743297576904297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3935999870300293 0.0488591194152832

Final encoder loss: 0.021593794226646423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3468623161315918 0.04935622215270996

Final encoder loss: 0.02167217805981636
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.30745720863342285 0.04894518852233887

Final encoder loss: 0.021535275503993034
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.30776286125183105 0.048714637756347656

Final encoder loss: 0.02161010541021824
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3762483596801758 0.04821634292602539

Final encoder loss: 0.02148333378136158
Final encoder loss: 0.020809974521398544

Training empatch model
Final encoder loss: 0.0297306931687704
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07106971740722656 0.17293858528137207

Final encoder loss: 0.029354591363025755
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07028555870056152 0.17258691787719727

Final encoder loss: 0.03007755153148169
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07022261619567871 0.17234468460083008

Final encoder loss: 0.028505427286998484
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07045340538024902 0.17223048210144043

Final encoder loss: 0.02914704888869126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.0708763599395752 0.17240214347839355

Final encoder loss: 0.028303078211759217
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07015728950500488 0.17248010635375977

Final encoder loss: 0.027488883946897393
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07008481025695801 0.17249155044555664

Final encoder loss: 0.03005128130681289
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07112836837768555 0.17221593856811523

Final encoder loss: 0.02020960504545073
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 22: 0.07007527351379395 0.17278480529785156

Final encoder loss: 0.02042396172717308
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 44: 0.07071089744567871 0.1728196144104004

Final encoder loss: 0.021686476517334845
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 66: 0.07037925720214844 0.1726369857788086

Final encoder loss: 0.02125351209645655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 88: 0.07010889053344727 0.17236733436584473

Final encoder loss: 0.019860995831990854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 110: 0.07040524482727051 0.17253518104553223

Final encoder loss: 0.021319031996949126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 132: 0.07002568244934082 0.17242002487182617

Final encoder loss: 0.020355308406544828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 154: 0.07071638107299805 0.17246150970458984

Final encoder loss: 0.023688718444675162
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.07003235816955566 0.17199134826660156


Training empatch model
Final encoder loss: 0.17116817831993103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1765427589416504 0.044261932373046875

Final encoder loss: 0.08164212852716446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1747729778289795 0.043894052505493164

Final encoder loss: 0.05505448579788208
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17443633079528809 0.044300079345703125

Final encoder loss: 0.04214118793606758
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746523380279541 0.04351973533630371

Final encoder loss: 0.034691400825977325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17535018920898438 0.04294586181640625

Final encoder loss: 0.0300739798694849
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746511459350586 0.04315996170043945

Final encoder loss: 0.0270097553730011
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17479467391967773 0.043622493743896484

Final encoder loss: 0.02493305876851082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17506980895996094 0.04401254653930664

Final encoder loss: 0.023480679839849472
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.1746366024017334 0.04403948783874512

Final encoder loss: 0.022467240691184998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.17897963523864746 0.04372358322143555

Final encoder loss: 0.021712426096200943
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.22014403343200684 0.04345703125

Final encoder loss: 0.021158959716558456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.212982177734375 0.04425764083862305

Final encoder loss: 0.02074524760246277
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2107410430908203 0.04377937316894531

Final encoder loss: 0.020547598600387573
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.20617985725402832 0.04402470588684082

Final encoder loss: 0.020274419337511063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2137000560760498 0.043592214584350586

Final encoder loss: 0.0200693029910326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.20533514022827148 0.043912649154663086

Final encoder loss: 0.019787462428212166
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.20761919021606445 0.04436445236206055

Final encoder loss: 0.019608961418271065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21187925338745117 0.04410839080810547

Final encoder loss: 0.01949680782854557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.20975375175476074 0.042845726013183594

Final encoder loss: 0.019377287477254868
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.21839046478271484 0.043221235275268555

Final encoder loss: 0.01927398145198822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2872781753540039 0.04307103157043457

Final encoder loss: 0.019204458221793175
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.2699577808380127 0.04381752014160156

Final encoder loss: 0.019141772761940956
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.23534917831420898 0.04243206977844238

Final encoder loss: 0.019069259986281395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.23376989364624023 0.04391598701477051

Final encoder loss: 0.019047200679779053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.23521971702575684 0.044175148010253906

Final encoder loss: 0.019020920619368553
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.25612831115722656 0.04402303695678711

Final encoder loss: 0.019009094685316086
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training empatch 165: 0.27775144577026367 0.04413151741027832

Final encoder loss: 0.0188768208026886

Training wesad model
Final encoder loss: 0.02923194554130848
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.0708913803100586 0.17299389839172363

Final encoder loss: 0.03148926706770493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07043170928955078 0.17287826538085938

Final encoder loss: 0.03269161962558396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07050442695617676 0.1724085807800293

Final encoder loss: 0.029493343593206232
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07042670249938965 0.1721043586730957

Final encoder loss: 0.020360868082503515
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07041263580322266 0.17301702499389648

Final encoder loss: 0.020206378836242874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07066035270690918 0.17300915718078613

Final encoder loss: 0.020837105912416325
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07080197334289551 0.17341017723083496

Final encoder loss: 0.020779577753259854
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07103586196899414 0.17281365394592285

Final encoder loss: 0.014701109238803287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07113981246948242 0.17299580574035645

Final encoder loss: 0.01644553608832031
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.07088065147399902 0.17277312278747559

Final encoder loss: 0.01598792276590048
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.0704963207244873 0.1726841926574707

Final encoder loss: 0.01671973649682479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07038474082946777 0.17405319213867188

Final encoder loss: 0.013032134545376828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 15: 0.07163786888122559 0.1743149757385254

Final encoder loss: 0.012683371687204122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 30: 0.0709526538848877 0.1730484962463379

Final encoder loss: 0.0125802055888998
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 45: 0.07010483741760254 0.17314767837524414

Final encoder loss: 0.013893269955197556
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.07118701934814453 0.17288994789123535


Training wesad model
Final encoder loss: 0.21558800339698792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12055516242980957 0.03288722038269043

Final encoder loss: 0.09982862323522568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12782001495361328 0.03330636024475098

Final encoder loss: 0.0641729086637497
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12263321876525879 0.03290534019470215

Final encoder loss: 0.046321969479322433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12276005744934082 0.03258395195007324

Final encoder loss: 0.03590703755617142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1139218807220459 0.03332853317260742

Final encoder loss: 0.02940564788877964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.15186381340026855 0.03331899642944336

Final encoder loss: 0.025156112387776375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13700413703918457 0.033211708068847656

Final encoder loss: 0.022276105359196663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13194942474365234 0.03257131576538086

Final encoder loss: 0.02023785375058651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12611150741577148 0.03268170356750488

Final encoder loss: 0.018808186054229736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12450742721557617 0.03318381309509277

Final encoder loss: 0.017827743664383888
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12972378730773926 0.03339409828186035

Final encoder loss: 0.01726391166448593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11954760551452637 0.033269643783569336

Final encoder loss: 0.016886839643120766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12038874626159668 0.03300285339355469

Final encoder loss: 0.016606606543064117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12006735801696777 0.03321647644042969

Final encoder loss: 0.016413863748311996
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12019777297973633 0.03372812271118164

Final encoder loss: 0.016392644494771957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11910653114318848 0.0331416130065918

Final encoder loss: 0.016370678320527077
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12943553924560547 0.03302597999572754

Final encoder loss: 0.016428912058472633
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.11710524559020996 0.032989501953125

Final encoder loss: 0.01636938937008381
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1419086456298828 0.03363847732543945

Final encoder loss: 0.016255181282758713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13538479804992676 0.033675193786621094

Final encoder loss: 0.01614031009376049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.1197664737701416 0.03337550163269043

Final encoder loss: 0.01607663556933403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12481951713562012 0.03331875801086426

Final encoder loss: 0.016077829524874687
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.14503049850463867 0.03386497497558594

Final encoder loss: 0.015912974253296852
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13935542106628418 0.033451080322265625

Final encoder loss: 0.01592024229466915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12499380111694336 0.03366971015930176

Final encoder loss: 0.015826301649212837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.12054800987243652 0.03404045104980469

Final encoder loss: 0.015801016241312027
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training wesad 60: 0.13887953758239746 0.03375506401062012

Final encoder loss: 0.01578492484986782

Calculating loss for amigos model
	Full Pass 0.7502336502075195
numFreeParamsPath 18
Reconstruction loss values: 0.0230113472789526 0.031657081097364426

Calculating loss for dapper model
	Full Pass 0.15837931632995605
numFreeParamsPath 18
Reconstruction loss values: 0.01914086565375328 0.022173339501023293

Calculating loss for case model
	Full Pass 0.9149832725524902
numFreeParamsPath 18
Reconstruction loss values: 0.02880925126373768 0.031808990985155106

Calculating loss for emognition model
	Full Pass 0.29254817962646484
numFreeParamsPath 18
Reconstruction loss values: 0.02864047884941101 0.03700399398803711

Calculating loss for empatch model
	Full Pass 0.10454034805297852
numFreeParamsPath 18
Reconstruction loss values: 0.03003527596592903 0.036974068731069565

Calculating loss for wesad model
	Full Pass 0.07717013359069824
numFreeParamsPath 18
Reconstruction loss values: 0.030074244365096092 0.04611065611243248
Total loss calculation time: 5.096018552780151

Plotting results for the amigos model

Plotting results for the dapper model

Plotting results for the case model

Plotting results for the emognition model

Plotting results for the empatch model

Plotting results for the wesad model

Calculating loss for model comparison
Total plotting time: 5.238151550292969
Total epoch time: 245.0182056427002

Epoch: 56

Training dapper model
Final encoder loss: 0.017820168907286993
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 24: 0.06739139556884766 0.15755486488342285

Final encoder loss: 0.01877947416083392
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 48: 0.06125783920288086 0.14967799186706543

Final encoder loss: 0.017952615508097354
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 72: 0.0610966682434082 0.14836621284484863

Final encoder loss: 0.017835405146683542
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 96: 0.06101536750793457 0.1489272117614746

Final encoder loss: 0.018024635423297568
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 120: 0.06104254722595215 0.14925146102905273

Final encoder loss: 0.01738225568074078
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 144: 0.06139683723449707 0.1490633487701416

Final encoder loss: 0.018306068704014328
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 168: 0.061258792877197266 0.14911460876464844

Final encoder loss: 0.01740250266341607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 192: 0.061473846435546875 0.14920902252197266

Final encoder loss: 0.019126470573290887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 216: 0.061173200607299805 0.14845490455627441

Final encoder loss: 0.017824249574789588
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 240: 0.0610501766204834 0.14912915229797363

Final encoder loss: 0.01798162911538964
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 264: 0.06144523620605469 0.14889240264892578

Final encoder loss: 0.018414958684369598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 288: 0.06092190742492676 0.14864397048950195

Final encoder loss: 0.01687674677057598
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 312: 0.06094980239868164 0.14850354194641113

Final encoder loss: 0.01530038134529037
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 336: 0.060921430587768555 0.14945292472839355

Final encoder loss: 0.01714501283548562
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 360: 0.06123161315917969 0.14879512786865234

Final encoder loss: 0.020537518892835394
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training dapper 364: 0.0606839656829834 0.14830780029296875


Training emognition model
Final encoder loss: 0.029193013963910934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 26: 0.08290219306945801 0.2736964225769043

Final encoder loss: 0.030420907304825207
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 52: 0.08257055282592773 0.27284955978393555

Final encoder loss: 0.028649349490390528
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 78: 0.08223962783813477 0.2744178771972656

Final encoder loss: 0.02799430289629765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 104: 0.08303284645080566 0.27348875999450684

Final encoder loss: 0.028506844487347773
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 130: 0.08240771293640137 0.2736332416534424

Final encoder loss: 0.027890732868979348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 156: 0.08319401741027832 0.27318429946899414

Final encoder loss: 0.02800288987047487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 182: 0.08290243148803711 0.2735331058502197

Final encoder loss: 0.02799921409048194
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 208: 0.08284807205200195 0.27294325828552246

Final encoder loss: 0.029112024724857265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 234: 0.10071849822998047 0.2731447219848633

Final encoder loss: 0.027899193781146428
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 260: 0.08206772804260254 0.27282261848449707

Final encoder loss: 0.02815467853747262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 286: 0.08241033554077148 0.27318453788757324

Final encoder loss: 0.02846954239882541
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 312: 0.08217334747314453 0.2732236385345459

Final encoder loss: 0.027239033346615378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 338: 0.08249330520629883 0.27336883544921875

Final encoder loss: 0.02937358124549661
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 364: 0.08206486701965332 0.2740013599395752

Final encoder loss: 0.028959541181267156
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 390: 0.08256697654724121 0.2735769748687744

Final encoder loss: 0.028692567944616804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training emognition 407: 0.08151102066040039 0.27248501777648926


Training amigos model
Final encoder loss: 0.026394809510581884
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 43: 0.10798287391662598 0.38772034645080566

Final encoder loss: 0.02288655081975196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 86: 0.10766863822937012 0.3878364562988281

Final encoder loss: 0.023947764797558304
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 129: 0.10785079002380371 0.38752102851867676

Final encoder loss: 0.02122852021987596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 172: 0.10769486427307129 0.3876974582672119

Final encoder loss: 0.022615111876246756
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 215: 0.10784435272216797 0.3877687454223633

Final encoder loss: 0.022240798225947347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 258: 0.10802769660949707 0.3879225254058838

Final encoder loss: 0.02408964594934333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 301: 0.10822916030883789 0.38800787925720215

Final encoder loss: 0.024045028349955623
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 344: 0.10785603523254395 0.38838815689086914

Final encoder loss: 0.023097374242784727
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 387: 0.10750174522399902 0.38857126235961914

Final encoder loss: 0.02248391230293269
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 430: 0.10811805725097656 0.38828396797180176

Final encoder loss: 0.022690883392964423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 473: 0.10762643814086914 0.38840746879577637

Final encoder loss: 0.024274885354735157
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 516: 0.10813593864440918 0.38797950744628906

Final encoder loss: 0.025491028366721718
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 559: 0.10854101181030273 0.3892548084259033

Final encoder loss: 0.024286341718998256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 602: 0.10869169235229492 0.389481782913208

Final encoder loss: 0.023307372397963145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 645: 0.10808920860290527 0.3895134925842285

Final encoder loss: 0.024321275886166265
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training amigos 673: 0.10405373573303223 0.38416314125061035


Training case model
Final encoder loss: 0.031083786335564113
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 91: 0.09145522117614746 0.26406240463256836

Final encoder loss: 0.027199736575548944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 182: 0.09091520309448242 0.2637794017791748

Final encoder loss: 0.0263114956467203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 273: 0.0911862850189209 0.26468586921691895

Final encoder loss: 0.02561963813660592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 364: 0.09110736846923828 0.26417970657348633

Final encoder loss: 0.02428868634398038
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 455: 0.09162735939025879 0.26451992988586426

Final encoder loss: 0.02463600550852844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 546: 0.09123849868774414 0.2657008171081543

Final encoder loss: 0.024187301866565574
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 637: 0.09185338020324707 0.2644624710083008

Final encoder loss: 0.02475716735306126
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 728: 0.0907599925994873 0.26476573944091797

Final encoder loss: 0.023628928207186334
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 819: 0.09272384643554688 0.2659933567047119

Final encoder loss: 0.023829905145928882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 910: 0.09145307540893555 0.2650458812713623

Final encoder loss: 0.02373524488150493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1001: 0.09113478660583496 0.26392626762390137

Final encoder loss: 0.02393138608108962
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1092: 0.09131169319152832 0.26451921463012695

Final encoder loss: 0.023508550879624873
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1183: 0.09108519554138184 0.2643616199493408

Final encoder loss: 0.02267068704162204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1274: 0.0909116268157959 0.2645883560180664

Final encoder loss: 0.022343986215439583
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1365: 0.09087252616882324 0.2648599147796631

Final encoder loss: 0.0227037625180051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
Shared layer training case 1442: 0.08756113052368164 0.26050829887390137


Training amigos model
Final encoder loss: 0.016262103124571766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 43: 0.1058039665222168 0.3407750129699707

Final encoder loss: 0.016994663015265296
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 86: 0.1058816909790039 0.3419666290283203

Final encoder loss: 0.017428384317143594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 129: 0.10598492622375488 0.34085750579833984

Final encoder loss: 0.0180883728113023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 172: 0.10595536231994629 0.3406682014465332

Final encoder loss: 0.017509061815617663
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 215: 0.10566043853759766 0.34076619148254395

Final encoder loss: 0.015503138966573604
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 258: 0.10582304000854492 0.34232521057128906

Final encoder loss: 0.016556832980727337
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 301: 0.10643768310546875 0.34186553955078125

Final encoder loss: 0.01681506580806638
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 344: 0.10660243034362793 0.3413412570953369

Final encoder loss: 0.01683085242087662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 387: 0.10581564903259277 0.34174418449401855

Final encoder loss: 0.017530322096288187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 430: 0.10635089874267578 0.34090089797973633

Final encoder loss: 0.01787107830676599
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 473: 0.10556173324584961 0.3408854007720947

Final encoder loss: 0.017746695255416025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 516: 0.10559654235839844 0.34058451652526855

Final encoder loss: 0.0176217316277365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 559: 0.1054224967956543 0.34089016914367676

Final encoder loss: 0.018093293913411822
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 602: 0.10557889938354492 0.34093666076660156

Final encoder loss: 0.016902849509949944
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 645: 0.10518383979797363 0.3408348560333252

Final encoder loss: 0.017488153009333953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.10036563873291016 0.3369629383087158


Training amigos model
Final encoder loss: 0.1807824671268463
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.46165919303894043 0.07494068145751953

Final encoder loss: 0.18781857192516327
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4584531784057617 0.07541489601135254

Final encoder loss: 0.1836434006690979
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4550743103027344 0.07374191284179688

Final encoder loss: 0.0785142257809639
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4583592414855957 0.07609248161315918

Final encoder loss: 0.07949414104223251
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4591495990753174 0.07541894912719727

Final encoder loss: 0.07458842545747757
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4543168544769287 0.07442092895507812

Final encoder loss: 0.045215848833322525
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45981860160827637 0.07437419891357422

Final encoder loss: 0.04530053213238716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46196579933166504 0.07560610771179199

Final encoder loss: 0.043015219271183014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45699310302734375 0.07407045364379883

Final encoder loss: 0.031908489763736725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45791077613830566 0.0762631893157959

Final encoder loss: 0.03219839930534363
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46057558059692383 0.07602190971374512

Final encoder loss: 0.031087232753634453
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4562568664550781 0.07479476928710938

Final encoder loss: 0.025584429502487183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4576401710510254 0.07422065734863281

Final encoder loss: 0.026029430329799652
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45993828773498535 0.07594823837280273

Final encoder loss: 0.025377409532666206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4577944278717041 0.07483839988708496

Final encoder loss: 0.022614918649196625
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47067689895629883 0.07805728912353516

Final encoder loss: 0.023115428164601326
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4702303409576416 0.0761561393737793

Final encoder loss: 0.02259901724755764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45334672927856445 0.07226681709289551

Final encoder loss: 0.02131393738090992
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4577467441558838 0.07672524452209473

Final encoder loss: 0.02184462547302246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4573209285736084 0.07666826248168945

Final encoder loss: 0.02164023369550705
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4561347961425781 0.07518196105957031

Final encoder loss: 0.020782165229320526
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45797133445739746 0.07610940933227539

Final encoder loss: 0.02131161279976368
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45830869674682617 0.07662653923034668

Final encoder loss: 0.02126041054725647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4540259838104248 0.07418251037597656

Final encoder loss: 0.02025059424340725
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45804429054260254 0.0759439468383789

Final encoder loss: 0.020455341786146164
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4584226608276367 0.07598996162414551

Final encoder loss: 0.020549872890114784
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45714426040649414 0.07513856887817383

Final encoder loss: 0.019693072885274887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4568476676940918 0.0762014389038086

Final encoder loss: 0.019713137298822403
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4609346389770508 0.07536554336547852

Final encoder loss: 0.019717711955308914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45291900634765625 0.07443571090698242

Final encoder loss: 0.018906107172369957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4594597816467285 0.07403063774108887

Final encoder loss: 0.019010214135050774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45978426933288574 0.07550525665283203

Final encoder loss: 0.01907818391919136
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4537925720214844 0.07289576530456543

Final encoder loss: 0.018564289435744286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45854854583740234 0.07493257522583008

Final encoder loss: 0.01865471713244915
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4582052230834961 0.07262325286865234

Final encoder loss: 0.018805930390954018
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45354533195495605 0.0748450756072998

Final encoder loss: 0.018369130790233612
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45995521545410156 0.07609105110168457

Final encoder loss: 0.018282026052474976
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46109962463378906 0.07875728607177734

Final encoder loss: 0.018561458215117455
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4599335193634033 0.0763547420501709

Final encoder loss: 0.018044233322143555
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4598202705383301 0.07454371452331543

Final encoder loss: 0.018046176061034203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4595143795013428 0.0769662857055664

Final encoder loss: 0.01835322380065918
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4541287422180176 0.07252383232116699

Final encoder loss: 0.01762508600950241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4586904048919678 0.07725954055786133

Final encoder loss: 0.01773104816675186
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4591379165649414 0.07650065422058105

Final encoder loss: 0.018030434846878052
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4557185173034668 0.07534360885620117

Final encoder loss: 0.017351755872368813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4583263397216797 0.0767507553100586

Final encoder loss: 0.017496341839432716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4596376419067383 0.07497501373291016

Final encoder loss: 0.017865721136331558
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45444703102111816 0.07489252090454102

Final encoder loss: 0.017341753467917442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4660012722015381 0.07735323905944824

Final encoder loss: 0.01726267673075199
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4616987705230713 0.07532024383544922

Final encoder loss: 0.01768830604851246
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4557335376739502 0.07564353942871094

Final encoder loss: 0.017327824607491493
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4581890106201172 0.0754084587097168

Final encoder loss: 0.017277993261814117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4606287479400635 0.07492542266845703

Final encoder loss: 0.017495276406407356
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4535233974456787 0.07509255409240723

Final encoder loss: 0.01709928549826145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45984435081481934 0.07589221000671387

Final encoder loss: 0.016910413280129433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4669764041900635 0.07821393013000488

Final encoder loss: 0.017237849533557892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4709017276763916 0.07780003547668457

Final encoder loss: 0.016811566427350044
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4729032516479492 0.07925605773925781

Final encoder loss: 0.016789337620139122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4725770950317383 0.07941246032714844

Final encoder loss: 0.017316550016403198
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4682598114013672 0.0780179500579834

Final encoder loss: 0.016807399690151215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47030162811279297 0.07939815521240234

Final encoder loss: 0.016694456338882446
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47130918502807617 0.07697701454162598

Final encoder loss: 0.017199549823999405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46984267234802246 0.07570624351501465

Final encoder loss: 0.01677938923239708
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.47281312942504883 0.07738041877746582

Final encoder loss: 0.016756469383835793
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.47037696838378906 0.07745218276977539

Final encoder loss: 0.01717676781117916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.46769213676452637 0.07668256759643555

Final encoder loss: 0.016823433339595795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4692857265472412 0.0752108097076416

Final encoder loss: 0.016551490873098373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.470203161239624 0.07595443725585938

Final encoder loss: 0.017135484144091606
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4604330062866211 0.0751190185546875

Final encoder loss: 0.01665681041777134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4714539051055908 0.0756380558013916

Final encoder loss: 0.01652812398970127
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.46087193489074707 0.0751795768737793

Final encoder loss: 0.016970118507742882
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.45816779136657715 0.07576513290405273

Final encoder loss: 0.01663396693766117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4614434242248535 0.07520627975463867

Final encoder loss: 0.01639922708272934
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.4591023921966553 0.07639598846435547

Final encoder loss: 0.01686365157365799
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4553689956665039 0.07532715797424316

Final encoder loss: 0.016519146040081978
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.4578127861022949 0.07604479789733887

Final encoder loss: 0.01628691516816616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.459475040435791 0.07635164260864258

Final encoder loss: 0.016906732693314552
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4540829658508301 0.0707249641418457

Final encoder loss: 0.016501540318131447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45761942863464355 0.0764455795288086

Final encoder loss: 0.01626826636493206
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45689940452575684 0.0773308277130127

Final encoder loss: 0.016809504479169846
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.4530959129333496 0.07340383529663086

Final encoder loss: 0.0163249634206295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 225: 0.45600295066833496 0.0754704475402832

Final encoder loss: 0.016258077695965767
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 450: 0.45909810066223145 0.07617950439453125

Final encoder loss: 0.016823967918753624
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training amigos 673: 0.5373916625976562 0.07304954528808594

Final encoder loss: 0.016345370560884476
Final encoder loss: 0.015431159175932407
Final encoder loss: 0.015195909887552261

Training dapper model
Final encoder loss: 0.014082833978524797
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 24: 0.059926748275756836 0.10679268836975098

Final encoder loss: 0.015095071878426116
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 48: 0.058895111083984375 0.10575032234191895

Final encoder loss: 0.013948002030168685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 72: 0.05902385711669922 0.10607361793518066

Final encoder loss: 0.0154442815294713
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 96: 0.05910849571228027 0.10587859153747559

Final encoder loss: 0.014230764264639779
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 120: 0.05906939506530762 0.10616517066955566

Final encoder loss: 0.01473432663268921
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 144: 0.058847665786743164 0.10639786720275879

Final encoder loss: 0.01464337838639771
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 168: 0.058798789978027344 0.1063985824584961

Final encoder loss: 0.013037898175682057
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 192: 0.05884408950805664 0.10625123977661133

Final encoder loss: 0.013120832142045754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 216: 0.058988094329833984 0.10618019104003906

Final encoder loss: 0.01384044455806375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 240: 0.05877852439880371 0.1063685417175293

Final encoder loss: 0.014759239238150241
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 264: 0.05931496620178223 0.10619544982910156

Final encoder loss: 0.01192714136927262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 288: 0.05883288383483887 0.10622644424438477

Final encoder loss: 0.013696657818206984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 312: 0.05916404724121094 0.10652971267700195

Final encoder loss: 0.012022183091243557
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 336: 0.05910658836364746 0.10582304000854492

Final encoder loss: 0.014203408211782117
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 360: 0.05901765823364258 0.10582351684570312

Final encoder loss: 0.011515735614994391
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.05881857872009277 0.10570359230041504


Training dapper model
Final encoder loss: 0.202433243393898
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11812567710876465 0.03456425666809082

Final encoder loss: 0.20821325480937958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11603522300720215 0.03403592109680176

Final encoder loss: 0.0861562043428421
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1165916919708252 0.034102439880371094

Final encoder loss: 0.08788378536701202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1154184341430664 0.03391599655151367

Final encoder loss: 0.05075102671980858
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11605381965637207 0.03403449058532715

Final encoder loss: 0.05049771070480347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527562141418457 0.03408646583557129

Final encoder loss: 0.03429822996258736
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11529850959777832 0.03392362594604492

Final encoder loss: 0.03414962440729141
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11558389663696289 0.033931732177734375

Final encoder loss: 0.02566302753984928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11569976806640625 0.03432178497314453

Final encoder loss: 0.025731835514307022
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11514949798583984 0.03388500213623047

Final encoder loss: 0.02083272859454155
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11591601371765137 0.03412127494812012

Final encoder loss: 0.020918916910886765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11579227447509766 0.034893035888671875

Final encoder loss: 0.01796528324484825
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11640000343322754 0.03529834747314453

Final encoder loss: 0.018002603203058243
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11680340766906738 0.035134077072143555

Final encoder loss: 0.016217969357967377
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11642622947692871 0.03441333770751953

Final encoder loss: 0.016225965693593025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11510348320007324 0.03403639793395996

Final encoder loss: 0.015115759335458279
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11497044563293457 0.034320831298828125

Final encoder loss: 0.015177019871771336
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11537766456604004 0.03378868103027344

Final encoder loss: 0.01440407894551754
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1163489818572998 0.033971548080444336

Final encoder loss: 0.01452904473990202
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1152341365814209 0.03381991386413574

Final encoder loss: 0.014030479826033115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11630916595458984 0.03411078453063965

Final encoder loss: 0.014236840419471264
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1148831844329834 0.03441286087036133

Final encoder loss: 0.013887855224311352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11511826515197754 0.03443622589111328

Final encoder loss: 0.013704820536077023
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11568307876586914 0.03421187400817871

Final encoder loss: 0.01387978158891201
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11632418632507324 0.03399991989135742

Final encoder loss: 0.013599940575659275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11466002464294434 0.033583641052246094

Final encoder loss: 0.013827825896441936
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11565542221069336 0.03421664237976074

Final encoder loss: 0.013360912911593914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11509346961975098 0.03433942794799805

Final encoder loss: 0.013590361922979355
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11740446090698242 0.03437376022338867

Final encoder loss: 0.013314991258084774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11691761016845703 0.03422355651855469

Final encoder loss: 0.013256403617560863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11580252647399902 0.034050703048706055

Final encoder loss: 0.013115125708281994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11539673805236816 0.034323930740356445

Final encoder loss: 0.012814421206712723
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11780405044555664 0.034475088119506836

Final encoder loss: 0.012815105728805065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1148538589477539 0.03401017189025879

Final encoder loss: 0.012409822084009647
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11585307121276855 0.03412342071533203

Final encoder loss: 0.012414821423590183
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11558890342712402 0.033814430236816406

Final encoder loss: 0.012077659368515015
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11497306823730469 0.03517913818359375

Final encoder loss: 0.012085627764463425
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11527848243713379 0.034429311752319336

Final encoder loss: 0.012180645018815994
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.1184539794921875 0.03413963317871094

Final encoder loss: 0.011964584700763226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.11477255821228027 0.0340571403503418

Final encoder loss: 0.012355261482298374
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.11568713188171387 0.03362727165222168

Final encoder loss: 0.011957447044551373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.13129377365112305 0.034773826599121094

Final encoder loss: 0.012121749110519886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.14437365531921387 0.034323930740356445

Final encoder loss: 0.012097198516130447
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.16655182838439941 0.03407001495361328

Final encoder loss: 0.011763960123062134
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.20919299125671387 0.03413701057434082

Final encoder loss: 0.01186464074999094
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.1726846694946289 0.03401780128479004

Final encoder loss: 0.011668172664940357
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.17730045318603516 0.03407144546508789

Final encoder loss: 0.011654033325612545
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.16485095024108887 0.03425168991088867

Final encoder loss: 0.011664750054478645
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.14958930015563965 0.035254716873168945

Final encoder loss: 0.011421109549701214
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.14535880088806152 0.035334110260009766

Final encoder loss: 0.011776244267821312
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.16533493995666504 0.03487706184387207

Final encoder loss: 0.011419113725423813
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.16578102111816406 0.0340878963470459

Final encoder loss: 0.011725401505827904
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.15696239471435547 0.03412175178527832

Final encoder loss: 0.011427020654082298
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.17165231704711914 0.03351879119873047

Final encoder loss: 0.011688010767102242
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 182: 0.22536921501159668 0.0342867374420166

Final encoder loss: 0.01149735040962696
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training dapper 364: 0.14885377883911133 0.03381800651550293

Final encoder loss: 0.011422031559050083
Final encoder loss: 0.010648342780768871

Training case model
Final encoder loss: 0.020116916577106857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 91: 0.08933043479919434 0.2183067798614502

Final encoder loss: 0.01956828331011535
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 182: 0.08904600143432617 0.21864652633666992

Final encoder loss: 0.019714136743526435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 273: 0.08858180046081543 0.21886301040649414

Final encoder loss: 0.019555159793504112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 364: 0.08972477912902832 0.21864795684814453

Final encoder loss: 0.02015013139665487
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 455: 0.08956193923950195 0.2183089256286621

Final encoder loss: 0.019846365355966122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 546: 0.08897948265075684 0.21932387351989746

Final encoder loss: 0.019544259694355397
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 637: 0.08874988555908203 0.2181849479675293

Final encoder loss: 0.01953155728931303
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 728: 0.08884239196777344 0.21850180625915527

Final encoder loss: 0.019898136518779752
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 819: 0.08892440795898438 0.21861052513122559

Final encoder loss: 0.019581079791201658
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 910: 0.08875346183776855 0.21852970123291016

Final encoder loss: 0.019788657155059935
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1001: 0.08908390998840332 0.21844696998596191

Final encoder loss: 0.019198445545548375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1092: 0.08889913558959961 0.21838164329528809

Final encoder loss: 0.01996503658213286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1183: 0.08884000778198242 0.21849966049194336

Final encoder loss: 0.019936574119971955
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1274: 0.08874177932739258 0.2184281349182129

Final encoder loss: 0.019737258227779435
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1365: 0.08860135078430176 0.2183842658996582

Final encoder loss: 0.01988152730176844
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.08553910255432129 0.2148139476776123


Training case model
Final encoder loss: 0.20296825468540192
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27110958099365234 0.0524744987487793

Final encoder loss: 0.1889071762561798
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26689815521240234 0.05379629135131836

Final encoder loss: 0.19014914333820343
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.265655517578125 0.05243539810180664

Final encoder loss: 0.19218172132968903
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2567892074584961 0.0545048713684082

Final encoder loss: 0.18080992996692657
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25635290145874023 0.05266308784484863

Final encoder loss: 0.19193421304225922
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542088031768799 0.05193758010864258

Final encoder loss: 0.10657265782356262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2565922737121582 0.05320382118225098

Final encoder loss: 0.09548559039831161
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571828365325928 0.05317974090576172

Final encoder loss: 0.09201313555240631
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25797438621520996 0.05262303352355957

Final encoder loss: 0.0904776081442833
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2574746608734131 0.05379128456115723

Final encoder loss: 0.08173881471157074
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25740861892700195 0.05209660530090332

Final encoder loss: 0.08539523929357529
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2542550563812256 0.0520014762878418

Final encoder loss: 0.06218246743083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2571449279785156 0.053777456283569336

Final encoder loss: 0.05561905354261398
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25762057304382324 0.052559852600097656

Final encoder loss: 0.053685665130615234
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2574610710144043 0.05369758605957031

Final encoder loss: 0.0537322498857975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25722575187683105 0.052410125732421875

Final encoder loss: 0.049802687019109726
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2571418285369873 0.052381038665771484

Final encoder loss: 0.05205922573804855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25557613372802734 0.05182600021362305

Final encoder loss: 0.04301360249519348
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25861096382141113 0.05293464660644531

Final encoder loss: 0.03900914639234543
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2571585178375244 0.05304241180419922

Final encoder loss: 0.037881866097450256
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25741028785705566 0.051824092864990234

Final encoder loss: 0.038433920592069626
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25708580017089844 0.053304195404052734

Final encoder loss: 0.036715876311063766
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25652313232421875 0.053292274475097656

Final encoder loss: 0.037876877933740616
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25359582901000977 0.05132007598876953

Final encoder loss: 0.03404330834746361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2716951370239258 0.051924943923950195

Final encoder loss: 0.0316542349755764
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.257732629776001 0.05256843566894531

Final encoder loss: 0.031081445515155792
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.257157564163208 0.0525670051574707

Final encoder loss: 0.0318116769194603
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2556014060974121 0.05299258232116699

Final encoder loss: 0.03138798102736473
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2576603889465332 0.05362200736999512

Final encoder loss: 0.0316656120121479
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2546384334564209 0.05127120018005371

Final encoder loss: 0.02997848577797413
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2573378086090088 0.05245161056518555

Final encoder loss: 0.028469253331422806
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25878238677978516 0.05378246307373047

Final encoder loss: 0.028358429670333862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25612306594848633 0.05265378952026367

Final encoder loss: 0.028781257569789886
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25595903396606445 0.05128765106201172

Final encoder loss: 0.02946879155933857
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25835657119750977 0.0527491569519043

Final encoder loss: 0.029106348752975464
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.253093957901001 0.05278277397155762

Final encoder loss: 0.027370113879442215
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25733518600463867 0.05374026298522949

Final encoder loss: 0.026326626539230347
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2561023235321045 0.052117109298706055

Final encoder loss: 0.026054855436086655
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25617361068725586 0.05303788185119629

Final encoder loss: 0.026446660980582237
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635862350463867 0.05278491973876953

Final encoder loss: 0.02707994356751442
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25676894187927246 0.05383133888244629

Final encoder loss: 0.026708383113145828
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2538731098175049 0.051462411880493164

Final encoder loss: 0.025464629754424095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.255748987197876 0.05136847496032715

Final encoder loss: 0.024685034528374672
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2566056251525879 0.05245494842529297

Final encoder loss: 0.02438577637076378
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2563321590423584 0.05361747741699219

Final encoder loss: 0.02463749796152115
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25652360916137695 0.0519404411315918

Final encoder loss: 0.02559780143201351
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25699543952941895 0.052385807037353516

Final encoder loss: 0.025169439613819122
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25360703468322754 0.05234050750732422

Final encoder loss: 0.024186808615922928
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2565648555755615 0.05218648910522461

Final encoder loss: 0.02368495985865593
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25855469703674316 0.05414748191833496

Final encoder loss: 0.023432575166225433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25600171089172363 0.052820444107055664

Final encoder loss: 0.0236553605645895
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25669145584106445 0.05366683006286621

Final encoder loss: 0.02481848932802677
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25795936584472656 0.052954673767089844

Final encoder loss: 0.024198811501264572
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2559640407562256 0.05141592025756836

Final encoder loss: 0.023489147424697876
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2658240795135498 0.05304980278015137

Final encoder loss: 0.02297130599617958
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2572903633117676 0.05250263214111328

Final encoder loss: 0.02299007773399353
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25740957260131836 0.05318713188171387

Final encoder loss: 0.02304779551923275
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25617074966430664 0.05380058288574219

Final encoder loss: 0.024086011573672295
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2669336795806885 0.05172228813171387

Final encoder loss: 0.02347899228334427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2589285373687744 0.051711320877075195

Final encoder loss: 0.02291865088045597
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.27235841751098633 0.05424237251281738

Final encoder loss: 0.022502202540636063
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2591552734375 0.05211496353149414

Final encoder loss: 0.022304026409983635
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2620694637298584 0.05250740051269531

Final encoder loss: 0.022373996675014496
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570610046386719 0.05248451232910156

Final encoder loss: 0.02331172674894333
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2563209533691406 0.051932573318481445

Final encoder loss: 0.022783808410167694
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2541358470916748 0.05187630653381348

Final encoder loss: 0.02231447957456112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2570841312408447 0.05241107940673828

Final encoder loss: 0.022051101550459862
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26652956008911133 0.05286884307861328

Final encoder loss: 0.021863644942641258
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2558481693267822 0.05376410484313965

Final encoder loss: 0.02189764380455017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2570023536682129 0.05265522003173828

Final encoder loss: 0.022845324128866196
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2660098075866699 0.052945852279663086

Final encoder loss: 0.022463785484433174
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2556788921356201 0.05207228660583496

Final encoder loss: 0.021990666165947914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.265728235244751 0.05280900001525879

Final encoder loss: 0.021702632308006287
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2669386863708496 0.05229377746582031

Final encoder loss: 0.02166529931128025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.26575517654418945 0.053633928298950195

Final encoder loss: 0.02150299958884716
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25674986839294434 0.052660226821899414

Final encoder loss: 0.022487258538603783
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2660863399505615 0.053488969802856445

Final encoder loss: 0.022070322185754776
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25311923027038574 0.051526546478271484

Final encoder loss: 0.02173810638487339
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25677943229675293 0.05350947380065918

Final encoder loss: 0.021467916667461395
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25679469108581543 0.05336737632751465

Final encoder loss: 0.02135101892054081
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25722742080688477 0.0529019832611084

Final encoder loss: 0.021221449598670006
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2559959888458252 0.05209040641784668

Final encoder loss: 0.022266125306487083
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2570993900299072 0.05407404899597168

Final encoder loss: 0.021730583161115646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25331783294677734 0.05163097381591797

Final encoder loss: 0.021458664909005165
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2569413185119629 0.053529977798461914

Final encoder loss: 0.021183308213949203
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25612545013427734 0.05231046676635742

Final encoder loss: 0.021030612289905548
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2570304870605469 0.05181121826171875

Final encoder loss: 0.020982809364795685
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2665114402770996 0.05345630645751953

Final encoder loss: 0.021908467635512352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25826597213745117 0.05472826957702637

Final encoder loss: 0.0214840117841959
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25366640090942383 0.05183219909667969

Final encoder loss: 0.02123372256755829
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2671058177947998 0.052648067474365234

Final encoder loss: 0.02106848917901516
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2660999298095703 0.05255246162414551

Final encoder loss: 0.020843246951699257
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.256439208984375 0.05298471450805664

Final encoder loss: 0.020741183310747147
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25635266304016113 0.05286574363708496

Final encoder loss: 0.021757470443844795
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2560558319091797 0.051082611083984375

Final encoder loss: 0.02126672863960266
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25372886657714844 0.05119919776916504

Final encoder loss: 0.021045831963419914
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25609278678894043 0.052649497985839844

Final encoder loss: 0.02083914913237095
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2568166255950928 0.05259084701538086

Final encoder loss: 0.020732456818223
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25607872009277344 0.05279207229614258

Final encoder loss: 0.02054988220334053
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25895071029663086 0.054357051849365234

Final encoder loss: 0.0214605163782835
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2583451271057129 0.05410313606262207

Final encoder loss: 0.02106878161430359
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25592756271362305 0.0513911247253418

Final encoder loss: 0.02091366983950138
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26865410804748535 0.054137229919433594

Final encoder loss: 0.020691227167844772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.26865649223327637 0.054918527603149414

Final encoder loss: 0.02058841660618782
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2593655586242676 0.05426383018493652

Final encoder loss: 0.020371900871396065
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2594938278198242 0.05421161651611328

Final encoder loss: 0.02136158011853695
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25788164138793945 0.05475211143493652

Final encoder loss: 0.02085043489933014
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2558479309082031 0.051972389221191406

Final encoder loss: 0.02077358029782772
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2586369514465332 0.05389142036437988

Final encoder loss: 0.020534062758088112
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2580125331878662 0.05259346961975098

Final encoder loss: 0.020462848246097565
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.258253812789917 0.0538332462310791

Final encoder loss: 0.020261293277144432
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2583632469177246 0.05408644676208496

Final encoder loss: 0.02111246809363365
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.26810479164123535 0.0525662899017334

Final encoder loss: 0.02073337882757187
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2551758289337158 0.05266904830932617

Final encoder loss: 0.020633772015571594
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.26787662506103516 0.0528411865234375

Final encoder loss: 0.020438719540834427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25826525688171387 0.05242443084716797

Final encoder loss: 0.020297685638070107
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2676568031311035 0.054428815841674805

Final encoder loss: 0.020075395703315735
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2580564022064209 0.05291247367858887

Final encoder loss: 0.02112049050629139
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25819993019104004 0.05328559875488281

Final encoder loss: 0.02066488191485405
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25367045402526855 0.05229592323303223

Final encoder loss: 0.020506419241428375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566699981689453 0.052474260330200195

Final encoder loss: 0.02030322141945362
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25675177574157715 0.053438663482666016

Final encoder loss: 0.02020382694900036
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2598443031311035 0.05253767967224121

Final encoder loss: 0.01996818743646145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25699591636657715 0.05218315124511719

Final encoder loss: 0.020904647186398506
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25755977630615234 0.052403926849365234

Final encoder loss: 0.02045402303338051
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25485777854919434 0.052617549896240234

Final encoder loss: 0.020448248833417892
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25776171684265137 0.05283522605895996

Final encoder loss: 0.020265812054276466
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2588307857513428 0.05208277702331543

Final encoder loss: 0.02015601098537445
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.256197452545166 0.05436873435974121

Final encoder loss: 0.019865205511450768
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.2572150230407715 0.05183863639831543

Final encoder loss: 0.020766397938132286
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.255795955657959 0.05379652976989746

Final encoder loss: 0.02039285935461521
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2536313533782959 0.051917076110839844

Final encoder loss: 0.02035553939640522
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2566862106323242 0.0520021915435791

Final encoder loss: 0.020116513594985008
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.2574284076690674 0.052228450775146484

Final encoder loss: 0.02000897005200386
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2573211193084717 0.05296659469604492

Final encoder loss: 0.019746940582990646
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25730323791503906 0.053989410400390625

Final encoder loss: 0.020746316760778427
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25594258308410645 0.053255558013916016

Final encoder loss: 0.02026650868356228
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2535586357116699 0.05187535285949707

Final encoder loss: 0.020260825753211975
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25537681579589844 0.05255246162414551

Final encoder loss: 0.020043136551976204
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25669026374816895 0.052080631256103516

Final encoder loss: 0.019957847893238068
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25609779357910156 0.05337333679199219

Final encoder loss: 0.01972816325724125
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25788092613220215 0.05112433433532715

Final encoder loss: 0.02058291807770729
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2565321922302246 0.05287671089172363

Final encoder loss: 0.020223429426550865
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2529125213623047 0.051070451736450195

Final encoder loss: 0.020139457657933235
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.2563283443450928 0.05199289321899414

Final encoder loss: 0.019979946315288544
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.25596070289611816 0.05335497856140137

Final encoder loss: 0.01990208402276039
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.2569403648376465 0.05302166938781738

Final encoder loss: 0.0195716992020607
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25611114501953125 0.05231666564941406

Final encoder loss: 0.02049916423857212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.2568531036376953 0.052725791931152344

Final encoder loss: 0.0200712401419878
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.2534191608428955 0.05160832405090332

Final encoder loss: 0.0201573483645916
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.25792860984802246 0.052934885025024414

Final encoder loss: 0.01992877572774887
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.256561279296875 0.052085161209106445

Final encoder loss: 0.0197913721203804
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.25724101066589355 0.051097869873046875

Final encoder loss: 0.01955828070640564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.25542235374450684 0.053340911865234375

Final encoder loss: 0.02043924480676651
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.25559425354003906 0.05216789245605469

Final encoder loss: 0.020007891580462456
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.25528836250305176 0.05147981643676758

Final encoder loss: 0.020080816000699997
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.36442995071411133 0.0535733699798584

Final encoder loss: 0.019809316843748093
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.3853466510772705 0.053713083267211914

Final encoder loss: 0.01977415382862091
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.4029567241668701 0.05514121055603027

Final encoder loss: 0.01947399601340294
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.36435747146606445 0.05343890190124512

Final encoder loss: 0.020268267020583153
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.3883392810821533 0.05364346504211426

Final encoder loss: 0.01993991620838642
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.3546931743621826 0.052181243896484375

Final encoder loss: 0.019969141110777855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 241: 0.37024831771850586 0.05086636543273926

Final encoder loss: 0.019780049100518227
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 482: 0.3521242141723633 0.05391812324523926

Final encoder loss: 0.01968824863433838
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 723: 0.3792273998260498 0.05298614501953125

Final encoder loss: 0.01941322162747383
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 964: 0.36900973320007324 0.05359601974487305

Final encoder loss: 0.020333418622612953
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1205: 0.41834235191345215 0.05316424369812012

Final encoder loss: 0.01991601102054119
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training case 1442: 0.4592163562774658 0.05153059959411621

Final encoder loss: 0.01992879807949066
Final encoder loss: 0.019279353320598602
Final encoder loss: 0.018699126318097115
Final encoder loss: 0.017810698598623276
Final encoder loss: 0.01794782653450966
Final encoder loss: 0.01689254306256771

Training emognition model
Final encoder loss: 0.022202047070498564
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 26: 0.08147335052490234 0.2296905517578125

Final encoder loss: 0.02476335359623759
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 52: 0.08059883117675781 0.22916388511657715

Final encoder loss: 0.02308974074133984
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 78: 0.08045125007629395 0.22948265075683594

Final encoder loss: 0.02305600147830361
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 104: 0.08039689064025879 0.22931528091430664

Final encoder loss: 0.023181255474966144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 130: 0.08048439025878906 0.22966361045837402

Final encoder loss: 0.023616688842601957
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 156: 0.08062410354614258 0.22929906845092773

Final encoder loss: 0.023127456937587103
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 182: 0.08051228523254395 0.22940492630004883

Final encoder loss: 0.02221347740776262
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 208: 0.08066415786743164 0.23026680946350098

Final encoder loss: 0.022910851446764025
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 234: 0.08098435401916504 0.22963356971740723

Final encoder loss: 0.022714785894687596
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 260: 0.08065128326416016 0.22964072227478027

Final encoder loss: 0.02206545235931401
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 286: 0.08066344261169434 0.2307121753692627

Final encoder loss: 0.022978947139218952
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 312: 0.082244873046875 0.23119568824768066

Final encoder loss: 0.022835740910956373
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 338: 0.08110976219177246 0.22990655899047852

Final encoder loss: 0.022745734902571035
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 364: 0.08027005195617676 0.22940969467163086

Final encoder loss: 0.022296539877671433
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 390: 0.08031797409057617 0.2296006679534912

Final encoder loss: 0.022377322198236717
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.07960033416748047 0.22861146926879883


Training emognition model
Final encoder loss: 0.19355186820030212
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2590484619140625 0.05054783821105957

Final encoder loss: 0.19498416781425476
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24770808219909668 0.04865837097167969

Final encoder loss: 0.09168039262294769
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2555234432220459 0.04888105392456055

Final encoder loss: 0.09086766093969345
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2468585968017578 0.04891514778137207

Final encoder loss: 0.05787251517176628
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467195987701416 0.049062490463256836

Final encoder loss: 0.05598023161292076
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2481389045715332 0.04984760284423828

Final encoder loss: 0.04220999404788017
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2467787265777588 0.049929141998291016

Final encoder loss: 0.04079869017004967
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24745988845825195 0.049025774002075195

Final encoder loss: 0.0340842641890049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2465653419494629 0.0488433837890625

Final encoder loss: 0.033099155873060226
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24768519401550293 0.048233985900878906

Final encoder loss: 0.029511531814932823
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24688124656677246 0.04896950721740723

Final encoder loss: 0.028758330270648003
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.247406005859375 0.04828333854675293

Final encoder loss: 0.026839789003133774
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24678993225097656 0.0485234260559082

Final encoder loss: 0.02623557485640049
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24788880348205566 0.048393964767456055

Final encoder loss: 0.025273924693465233
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2461099624633789 0.04871392250061035

Final encoder loss: 0.024836333468556404
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24640274047851562 0.048244476318359375

Final encoder loss: 0.024426380172371864
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24734759330749512 0.04894304275512695

Final encoder loss: 0.02418554201722145
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2476487159729004 0.048720359802246094

Final encoder loss: 0.023963656276464462
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24633049964904785 0.04889726638793945

Final encoder loss: 0.023882446810603142
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24735260009765625 0.049106597900390625

Final encoder loss: 0.02357427589595318
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24637174606323242 0.04794144630432129

Final encoder loss: 0.023527711629867554
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24772071838378906 0.04884481430053711

Final encoder loss: 0.02323966473340988
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24645733833312988 0.04871869087219238

Final encoder loss: 0.02321738749742508
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24797940254211426 0.0485992431640625

Final encoder loss: 0.02282894216477871
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.2473158836364746 0.04974532127380371

Final encoder loss: 0.022878549993038177
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24826669692993164 0.04910731315612793

Final encoder loss: 0.022707410156726837
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24671101570129395 0.04865002632141113

Final encoder loss: 0.0224975124001503
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24657154083251953 0.04883694648742676

Final encoder loss: 0.02244691364467144
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24714350700378418 0.05046534538269043

Final encoder loss: 0.022342121228575706
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.2480607032775879 0.04950380325317383

Final encoder loss: 0.02235555648803711
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24686431884765625 0.04906606674194336

Final encoder loss: 0.022238940000534058
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24768805503845215 0.04896664619445801

Final encoder loss: 0.02208142727613449
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24785447120666504 0.049703359603881836

Final encoder loss: 0.022145673632621765
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.24782180786132812 0.049674034118652344

Final encoder loss: 0.021980537101626396
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.24682402610778809 0.048941850662231445

Final encoder loss: 0.022047925740480423
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3311028480529785 0.050524234771728516

Final encoder loss: 0.0217592790722847
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3968687057495117 0.04993081092834473

Final encoder loss: 0.021896785125136375
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3484213352203369 0.0489962100982666

Final encoder loss: 0.021817270666360855
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.37444639205932617 0.04948019981384277

Final encoder loss: 0.021742086857557297
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3390312194824219 0.04813981056213379

Final encoder loss: 0.021560445427894592
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3181467056274414 0.0481414794921875

Final encoder loss: 0.021648680791258812
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.30858922004699707 0.04901123046875

Final encoder loss: 0.021539835259318352
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3169701099395752 0.04892754554748535

Final encoder loss: 0.021609516814351082
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3156723976135254 0.04945087432861328

Final encoder loss: 0.02147437259554863
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3128085136413574 0.04790520668029785

Final encoder loss: 0.02165714092552662
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.32245588302612305 0.04909658432006836

Final encoder loss: 0.02166168950498104
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 204: 0.3107762336730957 0.04996228218078613

Final encoder loss: 0.021577686071395874
Backprop with LR: [0.0003, 0.0001, 0.0003, 0.067]
	Specific layer training emognition 407: 0.3100552558898926 0.04909658432006836

Traceback (most recent call last):
  File "/home/ssolomon/python3.12/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/ssolomon/.local/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/ssolomon/.local/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1172, in launch_command
    simple_launcher(args)
  File "/home/ssolomon/.local/lib/python3.12/site-packages/accelerate/commands/launch.py", line 762, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/ssolomon/python3.12/bin/python3.12', './../../metaTrainingControl.py', '--numSharedEncoderLayers', '2', '--numSpecificEncoderLayers', '1', '--encodedDimension', '512', '--numProfileShots', '32', '--deviceListed', 'HPC-GPU', '--submodel', 'signalEncoderModel', '--waveletType', 'bior3.1', '--optimizerType', 'NAdam', '--profileLR', '0.067', '--reversibleLR', '3e-4', '--physGenLR', '1e-4', '--profileDimension', '128', '--profileWD', '0', '--reversibleWD', '0', '--physGenWD', '0', '--beta1', '0.7', '--beta2', '0.8', '--momentum_decay', '0.001', '--cullingEpoch', '1', '--angularThresholdMin', '1', '--angularThresholdMax', '45', '--percentParamsKeeping', '4']' died with <Signals.SIGKILL: 9>.
Final encoder loss: 0.02143869176506996slurmstepd: error: Detected 1 oom-kill event(s) in StepId=47233190.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: hpc-22-34: task 0: Out Of Memory
Runtime: 9373 seconds
